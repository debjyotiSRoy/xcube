{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69e757c-1d09-45b4-ba3f-8f6a360ab34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "! [ -e /content ] && pip install -Uqq xcube  # upgrade fastai on colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7ece6e-cd30-465c-8e45-8e445a929440",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da2b3c4-035c-4d68-b296-192587a74edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "from typing import Union\n",
    "from fastai.imports import *\n",
    "from fastai.torch_imports import *\n",
    "from fastai.torch_core import *\n",
    "from fastai.layers import *\n",
    "from fastai.text.models.awdlstm import EmbeddingDropout, RNNDropout\n",
    "\n",
    "from xcube.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c1482e-61fe-4214-81a6-f54f1bcc5339",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a6c5e1-e381-4042-8a13-cdf71aab31b1",
   "metadata": {},
   "source": [
    "# Layers\n",
    "\n",
    ">Some layers which tops up the ones in [fastai](https://docs.fast.ai/layers.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50b5292-83d9-4d18-a79b-55c849925b41",
   "metadata": {},
   "source": [
    "## Basic manipulations and resizing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182e3813-5eb7-4488-9560-73d7bb76ddfe",
   "metadata": {},
   "source": [
    "One can easily create a beautiful layer with minimum boilerplate using fastai utilities. We will show a few simple examples here. For details and extensive illustrations please refer to [decorated fastai layers](https://docs.fast.ai/layers.html#Basic-manipulations-and-resize)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688ca1d5-0653-4aec-a169-501211001bc5",
   "metadata": {},
   "source": [
    "An easy way to create a pytorch layer for a simple `func`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3ad71f-31a8-4d24-ad64-80e9d7b64f74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/fastai/fastai/blob/master/fastai/layers.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Lambda\n",
       "\n",
       ">      Lambda (func)\n",
       "\n",
       "An easy way to create a pytorch layer for a simple `func`"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/fastai/fastai/blob/master/fastai/layers.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Lambda\n",
       "\n",
       ">      Lambda (func)\n",
       "\n",
       "An easy way to create a pytorch layer for a simple `func`"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(Lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a74e72-46fe-42d7-a0b5-af26895c7d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _add2(x): return x+2\n",
    "tst = Lambda(_add2)\n",
    "x = torch.randn(10,20)\n",
    "test_eq(tst(x), x+2)\n",
    "tst2 = pickle.loads(pickle.dumps(tst))\n",
    "test_eq(tst2(x), x+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33ce3cc-388b-4bda-8271-df15e2215e23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/fastai/fastai/blob/master/fastai/layers.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### PartialLambda\n",
       "\n",
       ">      PartialLambda (func)\n",
       "\n",
       "Layer that applies `partial(func, **kwargs)`"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/fastai/fastai/blob/master/fastai/layers.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### PartialLambda\n",
       "\n",
       ">      PartialLambda (func)\n",
       "\n",
       "Layer that applies `partial(func, **kwargs)`"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(PartialLambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31b1570-1d92-44c7-ba01-d13a11cb4be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_func(a,b=2): return a+b\n",
    "tst = PartialLambda(test_func, b=5)\n",
    "test_eq(tst(x), x+5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c267bc-2980-4b84-aa38-d57247e38236",
   "metadata": {},
   "source": [
    "## Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854a8777-014f-4a2b-a34f-41f807c0b05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _create_bias(size, with_zeros=False):\n",
    "    if with_zeros: return nn.Parameter(torch.zeros(*size))\n",
    "    return nn.Parameter(torch.zeros(*size).uniform_(-0.1, 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ce6199-c520-4a5a-90e0-e984c1b6e324",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ElemWiseLin(Module):\n",
    "    initrange=0.1\n",
    "    def __init__(self, dim0, dim1, add_bias=False, **kwargs):\n",
    "        store_attr()\n",
    "        self.lin = nn.Linear(dim1, dim0, **kwargs)\n",
    "        # init_default(self.lin, func=partial(torch.nn.init.uniform_, a=-self.initrange, b=self.initrange))\n",
    "        init_default(self.lin)\n",
    "        if self.add_bias: self.bias = _create_bias((1, ))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        res = torch.addcmul(self.bias if self.add_bias else x.new_zeros(1), x, self.lin.weight)# * self.lin.weight\n",
    "        return res #+ self.bias if self.add_bias else res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56744c7e-8aa2-4f67-a15f-50e1031ce125",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs, dim0, dim1 = 10, 1271, 400\n",
    "tst = ElemWiseLin(dim0, dim1)\n",
    "test_eq(tst.lin.weight.shape, (dim0, dim1))\n",
    "x = torch.randn(bs, dim0, dim1)\n",
    "test_eq(x * tst.lin.weight, tst(x))\n",
    "test_eq(tst(x).shape, (bs, dim0, dim1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c725aa-cd29-4bce-94bd-65f0bed0e682",
   "metadata": {},
   "source": [
    "## BatchNorm Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b29a17-0e3f-4126-b8d7-984e638b5bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class LinBnFlatDrop(nn.Sequential):\n",
    "    \"Module grouping `BatchNorm1dFlat`, `Dropout` and `Linear` layers\"\n",
    "    def __init__(self, n_in, n_out, bn=True, p=0., act=None, lin_first=False):\n",
    "        layers = [BatchNorm1dFlat(n_out if lin_first else n_in)] if bn else []\n",
    "        if p != 0: layers.append(nn.Dropout(p))\n",
    "        lin = [nn.Linear(n_in, n_out, bias=not bn)]\n",
    "        if act is not None: lin.append(act)\n",
    "        layers = lin+layers if lin_first else layers+lin\n",
    "        super().__init__(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9ab037-d625-44e8-a825-8d8675137a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class LinBnDrop(nn.Sequential):\n",
    "    \"Module grouping `BatchNorm1d`, `Dropout` and `Linear` layers\"\n",
    "    def __init__(self, n_in, n_out=None, bn=True, ln=True, p=0., act=None, lin_first=False, ndim=1):\n",
    "        if not ln and lin_first: raise Exception(AssertionError)\n",
    "        layers = [BatchNorm(n_out if ln and lin_first else n_in, ndim=ndim)] if bn else []\n",
    "        if p != 0: layers.append(nn.Dropout(p))\n",
    "        lin = [nn.Linear(n_in, n_out, bias=not bn)] if ln else []\n",
    "        if ln and act is not None: lin.append(act)\n",
    "        layers = lin+layers if lin_first else layers+lin\n",
    "        super().__init__(*layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4d5799-65f4-43d8-bce0-285b8f9e4222",
   "metadata": {},
   "source": [
    "`LinBnDrop` is just like [fastai's LinBnDrop](https://github.com/fastai/fastai/blob/master/fastai/layers.py#L174) with an extra modality `ln` which provides the option of skipping the linear layer. That is, `BatchNorm` or the `Linear` layer is skipped if `bn=False` or `ln=False`, as is the dropout if `p=0`. Optionally, you can add an activation for after the linear layer with act."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9880666-3e6e-4700-8758-bca2b9a62b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "tst = LinBnDrop(10, 20)\n",
    "mods = list(tst.children())\n",
    "assert isinstance(mods[0], nn.BatchNorm1d)\n",
    "assert isinstance(mods[1], nn.Linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb3f9ff-e93b-4192-b90c-3bce1b259898",
   "metadata": {},
   "source": [
    "The `LinBnDrop` layer is not going to add an activation (even if provided) if `ln` is `False` but raise an error if `not ln and ln_first`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2eceb17-dec3-442e-9076-5d399bc9a4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tst = LinBnDrop(10, 20, ln=False, p=0.02, act=nn.ReLU(inplace=True))\n",
    "mods = list(tst.children())\n",
    "assert isinstance(mods[0], nn.BatchNorm1d)\n",
    "assert isinstance(mods[1], nn.Dropout)\n",
    "test_fail(lambda : LinBnDrop(10, 20, ln=False, lin_first=True), contains='AssertionError')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb5eb5c-be53-4529-b84e-f9798002de72",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3d817c-801e-4178-820b-080027f625e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Embedding(nn.Embedding):\n",
    "    \"Embedding layer with truncated normal initialization\"\n",
    "    def __init__(self, ni, nf, std=0.01, **kwargs):\n",
    "        super().__init__(ni, nf, **kwargs)\n",
    "        trunc_normal_(self.weight.data, std=std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae54db6f-39fa-4a74-bbe8-4cd1ee991dc8",
   "metadata": {},
   "source": [
    "## Attention Layers for Extreme Multi-Label Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9e4907-1f1e-4647-82fa-694428ba82fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _linear_attention(sentc:Tensor, # Sentence typically `(bs, bptt, nh)` output of `SentenceEncoder`\n",
    "                      based_on: nn.Embedding|Module # xcube's `Embedding(n_lbs, nh)` layer holding the label embeddings or a full fledged model\n",
    "                  ):\n",
    "    return sentc @ based_on.weight.transpose(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7dcfc9-9295-48d5-b12e-c74df1e8a0e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/debjyotiSRoy/xcube/blob/main/xcube/layers.py#L68){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### _linear_attention\n",
       "\n",
       ">      _linear_attention (sentc:torch.Tensor,\n",
       ">                         based_on:torch.nn.modules.sparse.Embedding|fastai.torc\n",
       ">                         h_core.Module)\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| sentc | Tensor | Sentence typically `(bs, bptt, nh)` output of `SentenceEncoder` |\n",
       "| based_on | nn.Embedding \\| Module | xcube's `Embedding(n_lbs, nh)` layer holding the label embeddings or a full fledged model |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/debjyotiSRoy/xcube/blob/main/xcube/layers.py#L68){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### _linear_attention\n",
       "\n",
       ">      _linear_attention (sentc:torch.Tensor,\n",
       ">                         based_on:torch.nn.modules.sparse.Embedding|fastai.torc\n",
       ">                         h_core.Module)\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| sentc | Tensor | Sentence typically `(bs, bptt, nh)` output of `SentenceEncoder` |\n",
       "| based_on | nn.Embedding \\| Module | xcube's `Embedding(n_lbs, nh)` layer holding the label embeddings or a full fledged model |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(_linear_attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f42a271-b9ed-4b34-9a4e-9f01d46289ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _planted_attention(sentc: Tensor, # Sentence typically `(bs, bptt)` containing the vocab idxs that goes inside the encoder\n",
    "                       brain: Tensor # label specific attn wgts for each token in vocab, typically of shape `(vocab_sz, n_lbs)`\n",
    "                     ):\n",
    "    return brain[sentc.long()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f962b52-8d93-4bec-945c-8db9c4473af1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/debjyotiSRoy/xcube/blob/main/xcube/layers.py#L74){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### _planted_attention\n",
       "\n",
       ">      _planted_attention (sentc:torch.Tensor, brain:torch.Tensor)\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| sentc | Tensor | Sentence typically `(bs, bptt)` containing the vocab idxs that goes inside the encoder |\n",
       "| brain | Tensor | label specific attn wgts for each token in vocab, typically of shape `(vocab_sz, n_lbs)` |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/debjyotiSRoy/xcube/blob/main/xcube/layers.py#L74){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### _planted_attention\n",
       "\n",
       ">      _planted_attention (sentc:torch.Tensor, brain:torch.Tensor)\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| sentc | Tensor | Sentence typically `(bs, bptt)` containing the vocab idxs that goes inside the encoder |\n",
       "| brain | Tensor | label specific attn wgts for each token in vocab, typically of shape `(vocab_sz, n_lbs)` |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(_planted_attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750f945f-9468-4617-b8ef-b3a1cc7e43e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _diffntble_planted_attention(sentc_dec: Tensor, # Sentence `(bs, bptt)` typically containing the vocab idxs obtained after decoding what comes out of the encoder\n",
    "                         l2r: nn.ModuleDict # containing `nn.Embedding` for `token_factors`, `token_bias`, `label_factors` and `label_bias` from pretrained L2R model\n",
    "                        ):\n",
    "    \n",
    "    return l2r['token_factors'](sentc_dec.long()) @ l2r['label_factors'].weight.T + l2r['token_bias'](sentc_dec.long()) + l2r['label_bias'].weight.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4049df32-831a-423a-8f04-660d5e038d59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/debjyotiSRoy/xcube/blob/main/xcube/layers.py#L80){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### _diffntble_planted_attention\n",
       "\n",
       ">      _diffntble_planted_attention (sentc_dec:torch.Tensor,\n",
       ">                                    l2r:torch.nn.modules.container.ModuleDict)\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| sentc_dec | Tensor | Sentence `(bs, bptt)` typically containing the vocab idxs obtained after decoding what comes out of the encoder |\n",
       "| l2r | nn.ModuleDict | containing `nn.Embedding` for `token_factors`, `token_bias`, `label_factors` and `label_bias` from pretrained L2R model |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/debjyotiSRoy/xcube/blob/main/xcube/layers.py#L80){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### _diffntble_planted_attention\n",
       "\n",
       ">      _diffntble_planted_attention (sentc_dec:torch.Tensor,\n",
       ">                                    l2r:torch.nn.modules.container.ModuleDict)\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| sentc_dec | Tensor | Sentence `(bs, bptt)` typically containing the vocab idxs obtained after decoding what comes out of the encoder |\n",
       "| l2r | nn.ModuleDict | containing `nn.Embedding` for `token_factors`, `token_bias`, `label_factors` and `label_bias` from pretrained L2R model |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(_diffntble_planted_attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ab3555-f42d-4049-9bf1-c9a16e7f7219",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class _Pay_Attention:\n",
    "    def __init__(self, f, based_on): store_attr('f,based_on')\n",
    "    def __call__(self, sentc): return self.f(sentc, self.based_on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89c0a3c-9394-4911-bc47-0d40d8f795d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def Linear_Attention(based_on: Module): return _Pay_Attention(_linear_attention, based_on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49d52b8-f67f-435e-8d0e-e50f19005739",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs, bptt, nh, n_lbs = 16, 72, 100, 10\n",
    "tst_lbs = Embedding(n_lbs, nh)\n",
    "tst_Lin_Attn = Linear_Attention(tst_lbs)\n",
    "attn_layer = Lambda(tst_Lin_Attn)\n",
    "sentc = torch.randn(bs, bptt, nh)\n",
    "test_eq(tst_Lin_Attn(sentc).shape , (bs, bptt, n_lbs))\n",
    "test_eqs(attn_layer(sentc), tst_Lin_Attn(sentc), sentc @ tst_lbs.weight.transpose(0,1))\n",
    "\n",
    "attn_layer2 = pickle.loads(pickle.dumps(attn_layer))\n",
    "test_eqs(attn_layer2(sentc), sentc @ tst_lbs.weight.transpose(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034ab335-124d-4b41-8981-8a788ad5cf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def Planted_Attention(brain: Tensor): return _Pay_Attention(_planted_attention, brain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e117b22b-5374-4698-a18d-98ff78748f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs, bptt, vocab_sz, n_lbs = 16, 72, 100, 10\n",
    "inp = torch.zeros((bs, bptt)).random_(vocab_sz)\n",
    "brain = torch.randn(vocab_sz, n_lbs)\n",
    "tst_planted_Attn = Planted_Attention(brain)\n",
    "attn_layer = Lambda(tst_planted_Attn)\n",
    "attn = brain[inp.long()]\n",
    "test_eq(attn.shape, (bs, bptt, n_lbs))\n",
    "test_eqs(attn, tst_planted_Attn(inp), attn_layer(inp))\n",
    "# test_eq(brain[sentc[8].long()][:, 4], attn[8, :, 4]) # looking at the attn wgts of the 8th sentence and 4th label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c1fda4-6008-4e31-bca4-63a257a1d200",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class PlantedLMDecoder(Module):\n",
    "    def __init__(self, \n",
    "        n_out:int, # vocab_sz \n",
    "        n_hid:int, # Number of features in encoder last layer output\n",
    "        output_p:float=0.1, # Input dropout probability\n",
    "        plant_wgts:dict=None, # If supplied loads `plant_wgts` into decoder\n",
    "        bias:bool=True # If `False` the layer will not learn additive bias\n",
    "    ):\n",
    "        self.decoder = nn.Linear(n_hid, n_out, bias=bias)\n",
    "        self.output_dp = RNNDropout(output_p)\n",
    "        if plant_wgts: self.load_state_dict(plant_wgts)\n",
    "\n",
    "    def forward(self, input):\n",
    "        dp_inp = self.output_dp(input)\n",
    "        return self.decoder(dp_inp).softmax(dim=-1).argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60aa15d2-4244-4615-a699-35f434bc8ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text.models.awdlstm import awd_lstm_lm_config\n",
    "from fastai.text.models.core import AWD_LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc01948e-daf3-4ea1-970b-c049bb7cdec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs, bptt, vocab_sz, n_lbs, n_fac = 16, 72, 100, 10, 200\n",
    "config = awd_lstm_lm_config.copy()\n",
    "emb_sz, output_p, out_bias = map(config.get, ['emb_sz', 'output_p', 'out_bias'])\n",
    "lm_decoder_pretrained_wgts = {'decoder.weight': torch.randn(vocab_sz, emb_sz), \n",
    "                'decoder.bias': torch.randn(vocab_sz, )}\n",
    "lm_decoder = PlantedLMDecoder(vocab_sz, emb_sz, output_p=output_p*0.3, plant_wgts=lm_decoder_pretrained_wgts, bias=out_bias)\n",
    "test_eq(lm_decoder.decoder.weight, lm_decoder_pretrained_wgts['decoder.weight'])\n",
    "test_eq(lm_decoder.decoder.bias, lm_decoder_pretrained_wgts['decoder.bias'])\n",
    "enc = AWD_LSTM(vocab_sz, emb_sz, 10, 3)\n",
    "inp = torch.randint(0, vocab_sz, (bs,bptt))\n",
    "# inp = torch.zeros((bs, bptt)).random_(vocab_sz)\n",
    "sentc = enc(inp)\n",
    "sentc_decoded = lm_decoder(sentc)\n",
    "test_eq(sentc_decoded.shape, inp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06149a85-2cbe-4ab0-8794-8b4494d8542b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def Diffntble_Planted_Attention(l2r: nn.ModuleDict): return _Pay_Attention(_diffntble_planted_attention, l2r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347f7415-b7a2-4454-b9a3-ca9c3fd03ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = torch.randn(vocab_sz, n_fac)\n",
    "tb = torch.randn(vocab_sz, 1)\n",
    "lf = torch.randn(n_lbs, n_fac)\n",
    "lb = torch.randn(n_lbs, 1)\n",
    "wgts = dict(token_factors=tf, token_bias=tb, label_factors=lf, label_bias=lb)\n",
    "l2r = nn.ModuleDict({k: nn.Embedding(*v.size()) for k,v in wgts.items()})\n",
    "assert isinstance(l2r, nn.Module)\n",
    "test_eq(l2r.keys(), ['token_factors', 'token_bias', 'label_factors', 'label_bias'])\n",
    "tst_diffntble_planted_Attn = Diffntble_Planted_Attention(l2r)\n",
    "attn_layer = Lambda(tst_diffntble_planted_Attn)\n",
    "# attn = attn_layer(lm_decoder(sentc))\n",
    "attn = attn_layer(sentc_decoded)\n",
    "test_eq(attn.shape, (bs, bptt, n_lbs))\n",
    "test_eqs(attn,\n",
    "         tst_diffntble_planted_Attn(sentc_decoded),\n",
    "         l2r['token_factors'](sentc_decoded.long()) @ l2r['label_factors'].weight.T + l2r['token_bias'](sentc_decoded.long()) + l2r['label_bias'].weight.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638e4e59-8c7a-4ca9-a23f-65658b917e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def lincomb(t, wgts=None):\n",
    "    \"returns the linear combination of the dim1 of a 3d tensor of `t` based on `wgts` (if `wgts` is `None` just adds the rows)\"\n",
    "    if wgts is None: wgts = t.new_ones(t.size(0), 1, t.size(1))\n",
    "    return torch.bmm(wgts, t) # wgts@t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99494e2a-1766-4068-ade7-a749d8bd0afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.randn(16, 72, 100)\n",
    "wgts = t.new_ones(t.size(0), 1, t.size(1))\n",
    "test_eq(torch.bmm(wgts, t), lincomb(t))\n",
    "rand_wgts = t.new_empty(t.size(0), 15, t.size(1)).random_(10)\n",
    "# test_eq(lincomb(t, wgts=rand_wgts), torch.bmm(rand_wgts, t))\n",
    "tst_LinComb = PartialLambda(lincomb, wgts=rand_wgts)\n",
    "test_eq(tst_LinComb(t), torch.bmm(rand_wgts, t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b836be12-e82b-4b27-83bc-efbfcd1e5267",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "@torch.no_grad()\n",
    "def topkmax(self:Tensor, k=None, dim=1):\n",
    "    \"\"\"\n",
    "    returns softmax of the 1th dim of 3d tensor x after zeroing out values in x smaller than `k`th largest.\n",
    "    If k is `None` behaves like `x.softmax(dim=dim). Intuitively, `topkmax` hedges more compared to `F.softmax``\n",
    "    \"\"\"\n",
    "    if dim!=1: raise NotImplementedError\n",
    "    k = min(k if k is not None else np.inf, self.size(dim)-1)\n",
    "    kth_largest = self.sort(dim=dim, descending=True).values[:,k,:][:,None,:].repeat(1, self.size(dim), 1)\n",
    "    self[self < kth_largest] = 0.\n",
    "    return self.softmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c87d8c-35ac-40d0-bc99-4497057241cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def split_sort(t, sp_dim, sort_dim, sp_sz=500, **kwargs):\n",
    "    if t.ndim==1: return t.sort(dim=sort_dim, **kwargs).values\n",
    "    return torch.cat([s.sort(dim=sort_dim, **kwargs).values for s in torch.split(t, split_size_or_sections=sp_sz, dim=sp_dim)], dim=sp_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc59b478-b9ab-4204-85e4-a78d35277648",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.randn(16, 106, 819)\n",
    "s_t = split_sort(t, sp_dim=1, sort_dim=-1, sp_sz=14)\n",
    "test_eq(t.sort(dim=-1).values, s_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf9871a-3ffa-4e75-8e60-c43799a9afb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "@torch.no_grad()\n",
    "def inattention(self:Tensor, k=None, sort_dim=0, sp_dim=0):\n",
    "    \"\"\"\n",
    "    returns `self` after zeroing out values smaller than `k`th largest in dimension `dim`.\n",
    "    If k is `None` behaves like returns self.\n",
    "    \"\"\"\n",
    "    k = min(k if k is not None else np.inf, self.size(sort_dim)-1)\n",
    "    k_slice= [slice(None)]*self.ndim\n",
    "    # rep = [1]*self.ndim\n",
    "    k_slice[sort_dim] = k\n",
    "    if len(k_slice) == 1: k_slice=k_slice[0]\n",
    "    # rep[sort_dim] = self.size(sort_dim)\n",
    "    kth_largest = split_sort(self, sp_dim=sp_dim, sort_dim=sort_dim, descending=True)[k_slice].unsqueeze(dim=sort_dim)#.repeat(*rep)\n",
    "    clone = self.detach().clone()\n",
    "    clone[clone < kth_largest] = 0.\n",
    "    return clone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4604c57b-27fd-48e9-8864-be94de63446a",
   "metadata": {},
   "source": [
    "TODO: DEB \n",
    "- ~~Make it work for other dims~~\n",
    "- Hyperparmam schedule the k in topkmax (start with high gradually decrease)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757dc667-1577-4ece-b573-7dc5461c9303",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn((2, 7, 3))\n",
    "test_eq(x.topkmax() , F.softmax(x, dim=1))\n",
    "# test_fail(topkmax, args=(x, ), kwargs=dict(dim=-1)) # NotImplemented\n",
    "test_fail(x.topkmax, kwargs=dict(dim=-1)) # NotImplemented\n",
    "test_eq(x.inattention(k=2, sort_dim=-1), \n",
    "        torch.where(x < x.sort(dim=-1, descending=True).values[:, :, 2].unsqueeze(dim=-1), 0, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cff298-7e71-41c9-973c-aa02272b5786",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn((8820,) )\n",
    "x_inattn = torch.where(x < x.sort(dim=0, descending=True).values[2].unsqueeze(dim=0), 0, x)\n",
    "x_inattn1 = x.inattention(k=2, sort_dim=0)\n",
    "test_eq(x_inattn, x_inattn1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c571df2e-1c4c-469b-b32e-3044224c2799",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from xcube.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b574784-04b0-4546-88cf-3441ea0a76f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class XMLAttention(Module):\n",
    "    \"Compute label specific attention weights for each token in a sequence\"\n",
    "    def __init__(self, n_lbs, emb_sz, embed_p=0.0, plant=0.5):\n",
    "        store_attr('n_lbs,emb_sz,embed_p,plant')\n",
    "        self.lbs = Embedding(n_lbs, emb_sz)\n",
    "        # self.lbs_weight_dp = EmbeddingDropout(self.lbs_weight, embed_p)\n",
    "        self.attn = Lambda(Linear_Attention(self.lbs))\n",
    "    \n",
    "    @property\n",
    "    def attn(self): return self._attn\n",
    "    @attn.setter\n",
    "    def attn(self, a): self._attn = a\n",
    "    \n",
    "    def forward(self, inp, sentc, mask):\n",
    "        # sent is the ouput of SentenceEncoder i.e., (bs, max_len tokens, nh)\n",
    "        test_eqs(inp.shape, sentc.shape[:-1], mask.shape)\n",
    "        if self.attn.func.f is _linear_attention:\n",
    "            top_tok_attn_wgts = F.softmax(self.attn(sentc), dim=1).masked_fill(mask[:,:,None], 0) # lbl specific wts for each token (bs, max_len, n_lbs)\n",
    "            lbs_cf = None\n",
    "        elif self.attn.func.f is _planted_attention:\n",
    "            attn_wgts = self.attn(inp).masked_fill(mask[:,:,None], 0)\n",
    "            top_tok_attn_wgts =attn_wgts.inattention(k=15, sort_dim=1)\n",
    "            top_lbs_attn_wgts = attn_wgts.clone().permute(0,2,1).inattention(k=5, sort_dim=1).permute(0,2,1).contiguous() # applying `inattention` across the lbs dim\n",
    "            lbs_cf = top_lbs_attn_wgts.sum(dim=1) #shape (bs, n_lbs)\n",
    "        elif self.attn.func.f is _diffntble_planted_attention: #raise NotImplementedError\n",
    "            # top_tok_attn_wgts = F.softmax(self.attn(self.lm_decoder(sentc)), dim=1).masked_fill(mask[:,:,None], 0).inattention(k=15, sort_dim=1) # lbl specific wts for each token (bs, max_len, n_lbs)\n",
    "            # top_tok_attn_wgts0 = self.plant_attn(inp).masked_fill(mask[:,:,None], 0).inattention(k=15, sort_dim=1)\n",
    "            top_tok_lin_attn_wgts = self.lin_attn(sentc).softmax(dim=1).masked_fill(mask[:,:,None], 0) # lbl specific wts for each token (bs, max_len, n_lbs)\n",
    "            \n",
    "            # change\n",
    "            top_tok_plant_attn_wgts = self.attn(self.lm_decoder(sentc)).masked_fill(mask[:,:,None], 0).inattention(k=30, sort_dim=1).softmax(dim=1) # lbl specific wts for each token (bs, max_len, n_lbs)\n",
    "            # top_tok_plant_attn_wgts = self.attn(inp).masked_fill(mask[:,:,None], 0).inattention(k=30, sort_dim=1).softmax(dim=1) # lbl specific wts for each token (bs, max_len, n_lbs)\n",
    "            # change\n",
    "\n",
    "            top_tok_attn_wgts = (1-self.plant)*top_tok_lin_attn_wgts + self.plant*top_tok_plant_attn_wgts\n",
    "            # top_tok_attn_wgts = F.softmax(self.attn(inp), dim=1).masked_fill(mask[:,:,None], 0).inattention(k=15, sort_dim=1) # lbl specific wts for each token (bs, max_len, n_lbs)\n",
    "            # attn_wgts = self.attn(inp).masked_fill(mask[:,:,None], 0)\n",
    "            # top_tok_attn_wgts = attn_wgts.inattention(k=15, sort_dim=1)\n",
    "            lbs_cf = None\n",
    "        return lincomb(sentc, wgts=top_tok_attn_wgts.transpose(1,2)), top_tok_attn_wgts, lbs_cf # for each lbl do a linear combi of all the tokens based on attn_wgts (bs, num_lbs, nh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667dc23c-c391-4e69-9cb0-72e4a49ddc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing linear attention\n",
    "inp = torch.zeros(bs, bptt).random_(100)\n",
    "sentc = torch.randn(bs, bptt, nh)\n",
    "mask = sentc.new_empty(sentc.size()[:-1]).random_(2).bool()\n",
    "test_eq(mask.unique(), tensor([0., 1.]))\n",
    "xml_attn = XMLAttention(n_lbs, nh)\n",
    "attn, tok_wgts, lbs_cf = xml_attn(inp, sentc, mask)\n",
    "test_eq(attn.shape, (bs, n_lbs, nh))\n",
    "tst_lbs = xml_attn.lbs\n",
    "tst_Lin_Attn = Linear_Attention(tst_lbs)\n",
    "lin_attn_layer = Lambda(tst_Lin_Attn)\n",
    "attn_wgts = F.softmax(lin_attn_layer(sentc), dim=1) # topkmax(attn_layer(sentc), dim=1)\n",
    "test_eq(attn, torch.bmm(attn_wgts.masked_fill(mask[:, :, None], 0).transpose(1,2), sentc))\n",
    "\n",
    "# testing planted attention followed by inattention\n",
    "assert xml_attn.attn.func.f is _linear_attention\n",
    "inp = torch.zeros((bs, bptt)).random_(vocab_sz)\n",
    "brain = torch.randn(vocab_sz, n_lbs)\n",
    "plant_attn_layer = Lambda(Planted_Attention(brain))\n",
    "# xml_attn.attn = plant_attn_layer\n",
    "setattr(xml_attn, 'attn', plant_attn_layer)\n",
    "assert xml_attn.attn.func.f is _planted_attention\n",
    "attn, tok_wgts, lbs_cf = xml_attn(inp, sentc, mask)\n",
    "test_eqs(tok_wgts, \n",
    "         plant_attn_layer(inp).masked_fill(mask[:,:,None], 0).inattention(k=15, sort_dim=1), \n",
    "         brain[inp.long()].masked_fill(mask[:,:,None], 0).inattention(k=15, sort_dim=1)\n",
    "        )\n",
    "test_eq(attn, \n",
    "        lincomb(sentc, \n",
    "                wgts=brain[inp.long()].masked_fill(mask[:,:,None], 0).inattention(k=15, sort_dim=1).transpose(1,2)\n",
    "               )\n",
    "       )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e082a2-f073-4496-bd27-744ca9b12ac6",
   "metadata": {},
   "source": [
    "Test masking works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c11b5b-29b1-49fe-ac88-3aa79c3037f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for attn_layer in (lin_attn_layer, plant_attn_layer):\n",
    "    setattr(xml_attn, 'attn', attn_layer)\n",
    "    inp = torch.zeros(bs, bptt).random_(100)\n",
    "    sentc = torch.randn(bs, bptt, nh)\n",
    "    sentc = sentc.masked_fill(mask[:, :, None], 0)\n",
    "    assert sentc[mask].sum().item() == 0\n",
    "    attn, tok_wgts, lbs_cf = xml_attn(inp, sentc, mask)\n",
    "    assert sentc[mask].sum().item() == 0\n",
    "    attn_wgts = F.softmax(attn_layer(sentc), dim=1) if attn_layer is lin_attn_layer else attn_layer(inp).masked_fill(mask[:,:,None], 0).inattention(k=15, sort_dim=1)# topkmax(attn_layer(sentc), dim=1)\n",
    "    test_eq(attn, torch.bmm(attn_wgts.transpose(1,2), sentc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16eac5df-a268-428e-9748-d335f38d8760",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68491dd-f9d6-4633-8ff1-9326a0b2ce5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
