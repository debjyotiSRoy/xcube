{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69e757c-1d09-45b4-ba3f-8f6a360ab34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "! [ -e /content ] && pip install -Uqq xcube  # upgrade fastai on colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7ece6e-cd30-465c-8e45-8e445a929440",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da2b3c4-035c-4d68-b296-192587a74edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from fastai.imports import *\n",
    "from fastai.torch_imports import *\n",
    "from fastai.torch_core import *\n",
    "from fastai.layers import *\n",
    "from fastai.text.models.awdlstm import EmbeddingDropout, RNNDropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c1482e-61fe-4214-81a6-f54f1bcc5339",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a6c5e1-e381-4042-8a13-cdf71aab31b1",
   "metadata": {},
   "source": [
    "# Layers\n",
    "\n",
    ">Some layers which tops up the ones in [fastai](https://docs.fast.ai/layers.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50b5292-83d9-4d18-a79b-55c849925b41",
   "metadata": {},
   "source": [
    "## Basic manipulations and resizing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182e3813-5eb7-4488-9560-73d7bb76ddfe",
   "metadata": {},
   "source": [
    "One can easily create a beautiful layer with minimum boilerplate using fastai utilities. We will show a simple example here. For details and extensive illustrations please refer to [decorated fastai layers](https://docs.fast.ai/layers.html#Basic-manipulations-and-resize)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688ca1d5-0653-4aec-a169-501211001bc5",
   "metadata": {},
   "source": [
    "An easy way to create a pytorch layer for a simple `func`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a74e72-46fe-42d7-a0b5-af26895c7d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _add2(x): return x+2\n",
    "tst = Lambda(_add2)\n",
    "x = torch.randn(10,20)\n",
    "test_eq(tst(x), x+2)\n",
    "tst2 = pickle.loads(pickle.dumps(tst))\n",
    "test_eq(tst2(x), x+2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c725aa-cd29-4bce-94bd-65f0bed0e682",
   "metadata": {},
   "source": [
    "## BatchNorm Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b29a17-0e3f-4126-b8d7-984e638b5bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class LinBnFlatDrop(nn.Sequential):\n",
    "    \"Module grouping `BatchNorm1dFlat`, `Dropout` and `Linear` layers\"\n",
    "    def __init__(self, n_in, n_out, bn=True, p=0., act=None, lin_first=False):\n",
    "        layers = [BatchNorm1dFlat(n_out if lin_first else n_in)] if bn else []\n",
    "        if p != 0: layers.append(nn.Dropout(p))\n",
    "        lin = [nn.Linear(n_in, n_out, bias=not bn)]\n",
    "        if act is not None: lin.append(act)\n",
    "        layers = lin+layers if lin_first else layers+lin\n",
    "        super().__init__(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f78860f-5a2d-4b14-811c-1f1d595674e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tst = LinBnFlatDrop(400, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f47fd04-8f27-4ac8-b4f8-9625622c1362",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinBnFlatDrop(\n",
       "  (0): BatchNorm1dFlat(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (1): Linear(in_features=400, out_features=1, bias=False)\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9ab037-d625-44e8-a825-8d8675137a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class LinBnDrop(nn.Sequential):\n",
    "    \"Module grouping `BatchNorm1d`, `Dropout` and `Linear` layers\"\n",
    "    def __init__(self, n_in, n_out=None, bn=True, ln=True, p=0., act=None, lin_first=False, ndim=1):\n",
    "        if not ln and lin_first: raise Exception(AssertionError)\n",
    "        layers = [BatchNorm(n_out if ln and lin_first else n_in, ndim=ndim)] if bn else []\n",
    "        if p != 0: layers.append(nn.Dropout(p))\n",
    "        lin = [nn.Linear(n_in, n_out, bias=not bn)] if ln else []\n",
    "        if ln and act is not None: lin.append(act)\n",
    "        layers = lin+layers if lin_first else layers+lin\n",
    "        super().__init__(*layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4d5799-65f4-43d8-bce0-285b8f9e4222",
   "metadata": {},
   "source": [
    "`LinBnDrop` is just like [fastai's LinBnDrop](https://github.com/fastai/fastai/blob/master/fastai/layers.py#L174) with an extra modality `ln` which provides the option of skipping the linear layer. That is, `BatchNorm` or the `Linear` layer is skipped if `bn=False` or `ln=False`, as is the dropout if `p=0`. Optionally, you can add an activation for after the linear layer with act."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9880666-3e6e-4700-8758-bca2b9a62b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "tst = LinBnDrop(10, 20)\n",
    "mods = list(tst.children())\n",
    "assert isinstance(mods[0], nn.BatchNorm1d)\n",
    "assert isinstance(mods[1], nn.Linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb3f9ff-e93b-4192-b90c-3bce1b259898",
   "metadata": {},
   "source": [
    "The `LinBnDrop` layer is not going to add an activation (even if provided) if `ln` is `False` but raise an error if `not ln and ln_first`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2eceb17-dec3-442e-9076-5d399bc9a4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tst = LinBnDrop(10, 20, ln=False, p=0.02, act=nn.ReLU(inplace=True))\n",
    "mods = list(tst.children())\n",
    "assert isinstance(mods[0], nn.BatchNorm1d)\n",
    "assert isinstance(mods[1], nn.Dropout)\n",
    "test_fail(lambda : LinBnDrop(10, 20, ln=False, lin_first=True), contains='AssertionError')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb5eb5c-be53-4529-b84e-f9798002de72",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3d817c-801e-4178-820b-080027f625e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Embedding(nn.Embedding):\n",
    "    \"Embedding layer with truncated normal initialization\"\n",
    "    def __init__(self, ni, nf, std=0.01, **kwargs):\n",
    "        super().__init__(ni, nf, **kwargs)\n",
    "        trunc_normal_(self.weight.data, std=std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae54db6f-39fa-4a74-bbe8-4cd1ee991dc8",
   "metadata": {},
   "source": [
    "## Attention Layers for Extreme Multi-Label Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e74bfc-0c7c-425e-bf3d-df9b7cfcea5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class XMLAttention(Module):\n",
    "    \"Compute label specific attention weights for each token in a sequence\"\n",
    "    def __init__(self, n_lbs, emb_sz, embed_p=0.0):\n",
    "         store_attr('n_lbs,emb_sz,embed_p')\n",
    "         self.lbs_weight = Embedding(n_lbs, emb_sz)\n",
    "         # self.lbs_weight_dp = EmbeddingDropout(self.lbs_weight, embed_p)\n",
    "         # self.lbs_weight.weight.data.normal_(0, 0.01)   \n",
    "         # self.input_dp = RNNDropout(0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is the ouput of SentenceEncoder i.e., (bs, max_len tokens, nh)\n",
    "        # lbs_emb = self.lbs_weight(torch.arange(self.n_lbs, device=x.device)) # pulling out the lbs embeddings\n",
    "        lbs_emb = self.lbs_weight.weight\n",
    "        # x_dp = self.input_dp(x)\n",
    "        attn_wgts = F.softmax(x @ lbs_emb.transpose(0,1), dim=1) # lbl specific wts for each token (bs, max_len, n_lbs)\n",
    "        return attn_wgts.transpose(1,2) @ x # for each lbl do a linear combi of all the tokens based on attn_wgts (bs, num_lbs, nh)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16eac5df-a268-428e-9748-d335f38d8760",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68491dd-f9d6-4633-8ff1-9326a0b2ce5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b41117d-6259-4371-bf42-d750e84559ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepK",
   "language": "python",
   "name": "deepk"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
