{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea3b7cc-8ead-4343-a437-bc0fbfc28ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "! [ -e /content ] && pip install -Uqq xcube  # upgrade fastai on colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c1d2dc-4959-4a79-b27a-77acbc54e5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "from fastai.data.all import *\n",
    "from fastai.text.models.core import *\n",
    "from fastai.text.models.awdlstm import *\n",
    "from xcube.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b4e9f7-bc4b-454a-abb3-d7af066b7321",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp text.models.core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ff76fb-51c8-48eb-9a56-0820d20b2312",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c319081-5bc4-4dca-a277-e725860f2258",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "_model_meta = {AWD_LSTM: {'hid_name':'emb_sz', 'url':URLs.WT103_FWD, 'url_bwd':URLs.WT103_BWD,\n",
    "                          'config_lm':awd_lstm_lm_config, 'split_lm': awd_lstm_lm_split,\n",
    "                          'config_clas':awd_lstm_clas_config, 'split_clas': awd_lstm_clas_split},}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f138b28-457d-4c8c-b2a5-5a620c60c992",
   "metadata": {},
   "source": [
    "# Core XML Text Modules\n",
    "> Contain the modules needed to build different XML architectures and the generic functions to get those models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9247354-c15b-453e-9a60-5f5ba0336391",
   "metadata": {},
   "source": [
    "The models provided here are variations of the ones provided by [fastai](https://docs.fast.ai/text.models.core.html) with modifications tailored for XML."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d811afdb-22d7-4e86-b15a-77b63e1a511e",
   "metadata": {},
   "source": [
    "## Basic Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1c5748-ef36-401c-bc6b-72f0fcb4dbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SequentialRNN(nn.Sequential):\n",
    "    \"A sequential pytorch module that passes the reset call to its children.\"\n",
    "    def reset(self):\n",
    "        for c in self.children(): getattr(c, 'reset', noop)()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8b84e3-067f-4110-9546-3500b69ceff2",
   "metadata": {},
   "source": [
    "## Classification Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e098324-afd1-41ed-9d3e-a4f040f7a239",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _pad_tensor(t, bs):\n",
    "    if t.size(0) < bs: return torch.cat([t, t.new_zeros(bs-t.size(0), *t.shape[1:])])\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fd05e3-f3ee-4080-a4a8-d22c93d2a83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class AttentiveSentenceEncoder(Module):\n",
    "    \"Create an encoder over `module` that can process a full sentence.\"\n",
    "    def __init__(self, bptt, module, pad_idx=1, max_len=None): store_attr('bptt,module,pad_idx,max_len')\n",
    "    def reset(self): getattr(self.module, 'reset', noop)()\n",
    "\n",
    "    def forward(self, input):\n",
    "        bs,sl = input.size()\n",
    "        self.reset()\n",
    "        mask = input == self.pad_idx\n",
    "        outs,masks = [],[]\n",
    "        for i in range(0, sl, self.bptt):\n",
    "            #Note: this expects that sequence really begins on a round multiple of bptt\n",
    "            real_bs = (input[:,i] != self.pad_idx).long().sum()\n",
    "            o = self.module(input[:real_bs,i: min(i+self.bptt, sl)])\n",
    "            if self.max_len is None or sl-i <= self.max_len:\n",
    "                outs.append(o)\n",
    "                masks.append(mask[:,i: min(i+self.bptt, sl)])\n",
    "        outs = torch.cat([_pad_tensor(o, bs) for o in outs], dim=1)\n",
    "        mask = torch.cat(masks, dim=1)\n",
    "        return outs,mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d355c2-825a-4bb9-aae9-da6b3d56bb78",
   "metadata": {},
   "source": [
    ":::{.callout-warning}\n",
    "\n",
    "This module expects the inputs padded with most of the padding first, with the sequence beginning at a round multiple of bptt (and the rest of the padding at the end). Use `pad_input_chunk` to get your data in a suitable format.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cbbcff-6828-4e9e-9fed-2fe7ec1ab0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def masked_concat_pool(output, mask, bptt):\n",
    "    \"Pool `MultiBatchEncoder` outputs into one vector [last_hidden, max_pool, avg_pool]\"\n",
    "    lens = output.shape[1] - mask.long().sum(dim=1)\n",
    "    last_lens = mask[:,-bptt:].long().sum(dim=1)\n",
    "    avg_pool = output.masked_fill(mask[:, :, None], 0).sum(dim=1)\n",
    "    avg_pool.div_(lens.type(avg_pool.dtype)[:,None])\n",
    "    max_pool = output.masked_fill(mask[:,:,None], -float('inf')).max(dim=1)[0]\n",
    "    x = torch.cat([output[torch.arange(0, output.size(0)),-last_lens-1], max_pool, avg_pool], 1) #Concat pooling.\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f56bd99-2f56-4053-9792-7555b7893064",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class OurPoolingLinearClassifier(Module):\n",
    "    def __init__(self, dims, ps, bptt, y_range=None):\n",
    "        self.layer = LinBnDrop(dims[0], dims[1], p=ps, act=None)\n",
    "        self.bptt = bptt\n",
    "\n",
    "    def forward(self, input):\n",
    "        out, mask = input\n",
    "        x = masked_concat_pool(out, mask, self.bptt)\n",
    "        x = self.layer(x)\n",
    "        return x, out, out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae922f0-c0cc-4842-a2f9-23670ace4ade",
   "metadata": {},
   "source": [
    "Note that `OurPoolingLinearClassifier` is exactly same as fastai's [`PoolingLinearClassifier`](https://docs.fast.ai/text.models.core.html#poolinglinearclassifier) except that we do not do the feature compression from 1200 to 50 linear features. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba9d961-79ea-4342-874e-f9e10fb60daf",
   "metadata": {},
   "source": [
    "Note: Also try `OurPoolingLinearClassifier` w/o dropouts and batch normalization (Verify this, but as far as what I found it does not work well as compared to /w batch normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d48768-d9b8-4330-9c15-c30047a0a56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class LabelAttentionClassifier(Module):\n",
    "    initrange=0.1\n",
    "    def __init__(self, n_hidden, n_lbs, y_range=None):\n",
    "        store_attr('n_hidden,n_lbs,y_range')\n",
    "        self.pay_attn = XMLAttention(self.n_lbs, self.n_hidden)\n",
    "        self.final_lin = nn.Linear(self.n_hidden, self.n_lbs) \n",
    "        init_default(self.final_lin, func=partial(torch.nn.init.uniform_, a=-self.initrange, b=self.initrange))\n",
    "    \n",
    "    def forward(self, input):\n",
    "        out, _ = input\n",
    "        ctx = self.pay_attn(out) #shape (bs, n_lbs, n_hidden)\n",
    "        x = (self.final_lin.weight * ctx).sum(dim=2) + self.final_lin.bias\n",
    "        \n",
    "        if self.y_range is not None: x = sigmoid_range(x, *self.y_range)\n",
    "        return x, out, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313090ee-6b96-4cb0-bd77-27df2c9941bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_clas = LabelAttentionClassifier(400, 1271)\n",
    "test_eq(getattrs(attn_clas, 'n_hidden', 'n_lbs'), (400, 1271))\n",
    "outs, mask = (torch.randn(16, 72*20, 400), torch.randint(2, size=(16, 72*20)))\n",
    "x, *_ = attn_clas((outs, mask))\n",
    "test_eq(x.shape, (16, 1271))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914f7b3e-f778-4758-bfb1-6eed725de8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_xmltext_classifier(arch, vocab_sz, n_class, seq_len=72, config=None, drop_mult=1., pad_idx=1, max_len=72*20, y_range=None):\n",
    "    \"Create a text classifier from `arch` and its `config`, maybe `pretrained`\"\n",
    "    meta = _model_meta[arch]\n",
    "    config = ifnone(config, meta['config_clas']).copy()\n",
    "    for k in config.keys():\n",
    "        if k.endswith('_p'): config[k] *= drop_mult\n",
    "    n_hidden = config[meta['hid_name']]\n",
    "    config.pop('output_p')\n",
    "    init = config.pop('init') if 'init' in config else None\n",
    "    encoder = AttentiveSentenceEncoder(seq_len, arch(vocab_sz, **config), pad_idx=pad_idx, max_len=max_len)\n",
    "    decoder = LabelAttentionClassifier(n_hidden, n_class, y_range=y_range)\n",
    "    model = SequentialRNN(encoder, decoder)\n",
    "    return model if init is None else model.apply(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346c62b1-c29b-432a-9b8f-9ef7ca978bcf",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294269ac-6a2f-4bc0-8160-7e4ea3cab241",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepK",
   "language": "python",
   "name": "deepk"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
