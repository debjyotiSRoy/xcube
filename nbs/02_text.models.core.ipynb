{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea3b7cc-8ead-4343-a437-bc0fbfc28ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "! [ -e /content ] && pip install -Uqq xcube  # upgrade fastai on colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c1d2dc-4959-4a79-b27a-77acbc54e5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "from fastai.data.all import *\n",
    "from fastai.text.models.core import *\n",
    "from fastai.text.models.awdlstm import *\n",
    "from xcube.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b4e9f7-bc4b-454a-abb3-d7af066b7321",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp text.models.core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ff76fb-51c8-48eb-9a56-0820d20b2312",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c319081-5bc4-4dca-a277-e725860f2258",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "_model_meta = {AWD_LSTM: {'hid_name':'emb_sz', 'url':URLs.WT103_FWD, 'url_bwd':URLs.WT103_BWD,\n",
    "                          'config_lm':awd_lstm_lm_config, 'split_lm': awd_lstm_lm_split,\n",
    "                          'config_clas':awd_lstm_clas_config, 'split_clas': awd_lstm_clas_split},}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f138b28-457d-4c8c-b2a5-5a620c60c992",
   "metadata": {},
   "source": [
    "# Core XML Text Modules\n",
    "> Contain the modules needed to build different XML architectures and the generic functions to get those models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9247354-c15b-453e-9a60-5f5ba0336391",
   "metadata": {},
   "source": [
    "The models provided here are variations of the ones provided by [fastai](https://docs.fast.ai/text.models.core.html) with modifications tailored for XML."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d811afdb-22d7-4e86-b15a-77b63e1a511e",
   "metadata": {},
   "source": [
    "## Basic Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1c5748-ef36-401c-bc6b-72f0fcb4dbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SequentialRNN(nn.Sequential):\n",
    "    \"A sequential pytorch module that passes the reset call to its children.\"\n",
    "    def reset(self):\n",
    "        for c in self.children(): getattr(c, 'reset', noop)()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8b84e3-067f-4110-9546-3500b69ceff2",
   "metadata": {},
   "source": [
    "## Classification Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e098324-afd1-41ed-9d3e-a4f040f7a239",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _pad_tensor(t, bs):\n",
    "    if t.size(0) < bs: return torch.cat([t, t.new_zeros(bs-t.size(0), *t.shape[1:])])\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff116dfc-1388-4dd6-81f1-3e618ccac43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SentenceEncoder(Module):\n",
    "    \"Create an encoder over `module` that can process a full sentence.\"\n",
    "    def __init__(self, bptt, module, pad_idx=1, max_len=None): store_attr('bptt,module,pad_idx,max_len')\n",
    "    def reset(self): getattr(self.module, 'reset', noop)()\n",
    "\n",
    "    def forward(self, input):\n",
    "        bs,sl = input.size()\n",
    "        self.reset()\n",
    "        mask = input == self.pad_idx\n",
    "        outs,masks = [],[]\n",
    "        for i in range(0, sl, self.bptt):\n",
    "            #Note: this expects that sequence really begins on a round multiple of bptt\n",
    "            real_bs = (input[:,i] != self.pad_idx).long().sum()\n",
    "            o = self.module(input[:real_bs,i: min(i+self.bptt, sl)])\n",
    "            if self.max_len is None or sl-i <= self.max_len:\n",
    "                outs.append(o)\n",
    "                masks.append(mask[:,i: min(i+self.bptt, sl)])\n",
    "        outs = torch.cat([_pad_tensor(o, bs) for o in outs], dim=1)\n",
    "        mask = torch.cat(masks, dim=1)\n",
    "        return outs,mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d355c2-825a-4bb9-aae9-da6b3d56bb78",
   "metadata": {},
   "source": [
    ":::{.callout-warning}\n",
    "\n",
    "This module expects the inputs padded with most of the padding first, with the sequence beginning at a round multiple of bptt (and the rest of the padding at the end). Use `pad_input_chunk` to get your data in a suitable format.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c5fb20-0f83-467e-a9f7-61d83a300ade",
   "metadata": {},
   "source": [
    "Under DEV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fd05e3-f3ee-4080-a4a8-d22c93d2a83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class AttentiveSentenceEncoder(Module):\n",
    "    \"Create an encoder over `module` that can process a full sentence.\"\n",
    "    def __init__(self, bptt, module, pad_idx=1, max_len=None): store_attr('bptt,module,pad_idx,max_len')\n",
    "    def reset(self): getattr(self.module, 'reset', noop)()\n",
    "\n",
    "    def forward(self, input):\n",
    "        bs,sl = input.size()\n",
    "        self.reset()\n",
    "        mask = input == self.pad_idx\n",
    "        outs,masks = [],[]\n",
    "        for i in range(0, sl, self.bptt):\n",
    "            #Note: this expects that sequence really begins on a round multiple of bptt\n",
    "            real_bs = (input[:,i] != self.pad_idx).long().sum()\n",
    "            o = self.module(input[:real_bs,i: min(i+self.bptt, sl)])\n",
    "            if self.max_len is None or sl-i <= self.max_len:\n",
    "                outs.append(o)\n",
    "                masks.append(mask[:,i: min(i+self.bptt, sl)])\n",
    "        outs = torch.cat([_pad_tensor(o, bs) for o in outs], dim=1)\n",
    "        mask = torch.cat(masks, dim=1)\n",
    "        return outs,mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12c0410-fe06-471b-a8f2-97045891bfe5",
   "metadata": {},
   "source": [
    "Examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9ba1f4-06fd-47b7-8c4f-86b9cd291bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "config = awd_lstm_clas_config.copy()\n",
    "del config['output_p']\n",
    "config\n",
    "encoder = SentenceEncoder(72, AWD_LSTM(vocab_sz=100, **config), pad_idx=1, max_len=72*20)\n",
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cbbcff-6828-4e9e-9fed-2fe7ec1ab0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def masked_concat_pool(output, mask, bptt):\n",
    "    \"Pool `MultiBatchEncoder` outputs into one vector [last_hidden, max_pool, avg_pool]\"\n",
    "    lens = output.shape[1] - mask.long().sum(dim=1)\n",
    "    last_lens = mask[:,-bptt:].long().sum(dim=1)\n",
    "    avg_pool = output.masked_fill(mask[:, :, None], 0).sum(dim=1)\n",
    "    avg_pool.div_(lens.type(avg_pool.dtype)[:,None])\n",
    "    max_pool = output.masked_fill(mask[:,:,None], -float('inf')).max(dim=1)[0]\n",
    "    x = torch.cat([output[torch.arange(0, output.size(0)),-last_lens-1], max_pool, avg_pool], 1) #Concat pooling.\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f031cf-23de-47ed-8aac-fabf33f41716",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "# x = to_device(torch.randint(low=0, high=100, size=(128, 85))) # if you want to send it to gpu\n",
    "x = torch.randint(low=0, high=100, size=(128, 85)) \n",
    "x.device\n",
    "out, mask = encoder(x)\n",
    "out.shape, mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94171767-3926-4a8b-b13b-a304b366918b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class PoolingLinearClassifier(Module):\n",
    "    \"Create a linear classifier with pooling\"\n",
    "    def __init__(self, dims, ps, bptt, y_range=None):\n",
    "        if len(ps) != len(dims)-1: raise ValueError(\"Number of layers and dropout values do not match.\")\n",
    "        acts = [nn.ReLU(inplace=True)] * (len(dims) - 2) + [None]\n",
    "        layers = [LinBnDrop(i, o, p=p, act=a) for i,o,p,a in zip(dims[:-1], dims[1:], ps, acts)]\n",
    "        if y_range is not None: layers.append(SigmoidRange(*y_range))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        self.bptt = bptt\n",
    "\n",
    "    def forward(self, input):\n",
    "        out,mask = input\n",
    "        x = masked_concat_pool(out, mask, self.bptt)\n",
    "        x = self.layers(x)\n",
    "        return x, out, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709afd48-96e6-474d-847a-03bbeab7ac9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "x = masked_concat_pool(out, mask, bptt=72)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f1f904-1006-488c-990f-136915d6a235",
   "metadata": {},
   "source": [
    "The output of `masked_concat_pool` is fed into the decoder. So Let's now check out the decoder which compresses the incoming features (in this case 1200) to 50 linear features and then outputs the number of classes (in this example 6594)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebf19a9-b5be-45fa-88c2-2687293c6501",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "layers = [1200, 50, 6594]\n",
    "ps = [0.04, 0.1]\n",
    "# decoder = PoolingLinearClassifier(layers, ps, bptt=72).cuda() # if gpu available\n",
    "decoder = PoolingLinearClassifier(layers, ps, bptt=72)\n",
    "decoder\n",
    "\n",
    "preds, *_ = decoder((out, mask))\n",
    "\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3d32cc-82d6-417c-9d63-7bfeba1a5263",
   "metadata": {},
   "source": [
    "Breaking down the `PoolingLinearClassifier.__init__`:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d473ae-1eb1-463a-ad01-a1390c23e7e6",
   "metadata": {},
   "source": [
    "Note that in the `__init__` while creating `PoolingLinearClassifier` `dims` is `layers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89baa7d0-c77b-45c1-abc5-218ee29e364c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "dims = layers\n",
    "print(f\"{dims = }\")\n",
    "\n",
    "print(f\"{ps = }\")\n",
    "\n",
    "# Also note that `bptt` is `seq_len`\n",
    "\n",
    "bptt = 72\n",
    "print(f\"{bptt = }\")\n",
    "\n",
    "y_range = None\n",
    "\n",
    "if len(ps) != len(dims) - 1: raise ValueError(\"Number of layers and dopout values do not match.\")\n",
    "\n",
    "acts = [nn.ReLU(inplace=True)] * (len(dims) - 2) + [None]\n",
    "acts\n",
    "\n",
    "for i, o, p, a in zip(dims[:-1], dims[1:], ps, acts):\n",
    "    print(f\"{i = }, {o = }, {p = }, {a = }\")\n",
    "\n",
    "layers = [LinBnDrop(i, o, p=p, act=a) for i, o, p, a in zip(dims[:-1], dims[1:], ps, acts)]\n",
    "layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f56bd99-2f56-4053-9792-7555b7893064",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class OurPoolingLinearClassifier(Module):\n",
    "    def __init__(self, dims, ps, bptt, y_range=None):\n",
    "        self.layer = LinBnDrop(dims[0], dims[1], p=ps, act=None)\n",
    "        self.bptt = bptt\n",
    "\n",
    "    def forward(self, input):\n",
    "        out, mask = input\n",
    "        x = masked_concat_pool(out, mask, self.bptt)\n",
    "        x = self.layer(x)\n",
    "        return x, out, out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae922f0-c0cc-4842-a2f9-23670ace4ade",
   "metadata": {},
   "source": [
    "Note that `OurPoolingLinearClassifier` is exactly same as fastai's [`PoolingLinearClassifier`](https://docs.fast.ai/text.models.core.html#poolinglinearclassifier) except that we do not do the feature compression from 1200 to 50 linear features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146a30f1-fe87-49e3-993f-ef57fcf63a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "decoder = OurPoolingLinearClassifier(dims=[1200, 6594], ps=0.04, bptt=72)\n",
    "\n",
    "decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba9d961-79ea-4342-874e-f9e10fb60daf",
   "metadata": {},
   "source": [
    "Note: Also try `OurPoolingLinearClassifier` w/o dropouts and batch normalization (Verify this, but as far as what I found it does not work well as compared to /w batch normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c85678-04c1-4a62-83f4-f112d34c5f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class LabelAttentionClassifier(Module):\n",
    "    def __init__(self, dims, ps, bptt, y_range=None):\n",
    "        self.fts = dims[0]\n",
    "        self.lbs = dims[-1] \n",
    "        self.layers = LinBnDrop(self.lbs, ln=False, p=ps, act=None) # deb\n",
    "        self.bptt = bptt\n",
    "        self.emb_label = Embedding(self.lbs, self.fts) # deb: note that size of the label embeddings need not be same as nh \n",
    "        self.final_lin = nn.Linear(self.fts, self.lbs) \n",
    "\n",
    "    def forward(self, input):\n",
    "        out, _ = input\n",
    "        attn_wgts = out @ self.emb_label.weight.transpose(0, 1) # deb\n",
    "        attn_wgts = F.softmax(attn_wgts, 1) # deb\n",
    "        ctx = attn_wgts.transpose(1,2) @ out # deb\n",
    "        x = self.layers(ctx)\n",
    "        x = (self.final_lin.weight * x).sum(dim=2)\n",
    "        return x, out, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45c3964-e0f5-4708-8107-11dc426d58df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class LabelAttentionClassifier2(Module):\n",
    "    initrange=0.1\n",
    "    def __init__(self, dims, ps, bptt, y_range=None):\n",
    "        self.fts = dims[0]\n",
    "        self.lbs = dims[-1] \n",
    "        \n",
    "        # ps = 0.1 # deb\n",
    "        self.layers = LinBnDrop(self.lbs, ln=False, p=ps, act=None) # deb\n",
    "        self.bptt = bptt\n",
    "        # self.emb_label = Embedding(self.lbs, self.fts) # deb: note that size of the label embeddings need not be same as nh \n",
    "        self.emb_label = self._init_param(self.lbs, self.fts) # deb: note that size of the label embeddings need not be same as nh \n",
    "        self.final_lin = nn.Linear(self.fts, self.lbs) \n",
    "        self.final_lin.weight.data.uniform_(-self.initrange, self.initrange)\n",
    "        self.final_lin.bias.data.zero_()\n",
    "    \n",
    "    def _init_param(self, *sz): return nn.Parameter(torch.zeros(sz).normal_(0, 0.01))\n",
    "\n",
    "    def forward(self, input):\n",
    "        out, _ = input\n",
    "        # x = masked_concat_pool(out, mask, self.bptt)\n",
    "        \n",
    "        # bs = out.shape[0]\n",
    "        # ctx = out.new_zeros((bs, self.lbs, self.fts))\n",
    "        # for out_split in torch.split(out, 1, dim=1):\n",
    "        # self.emb_label = nn.Parameter(self.emb_label * self.m1)\n",
    "        attn_wgts = out @ self.emb_label.transpose(0, 1) # deb\n",
    "        # attn_wgts = sigmoid_range(attn_wgts, 0, 5.5) # did not help\n",
    "        attn_wgts = F.softmax(attn_wgts, 1) # deb\n",
    "        # attn_wgts = torch.nn.functional.log_softmax(attn_wgts, 1) # deb\n",
    "        # attn_wgts = torch.log(attn_wgts)/(attn_wgts.sum(dim=1, keepdim=True) + 1e-12)\n",
    "        # attn_wgts[torch.isnan(attn_wgts)] = tensor(0.)\n",
    "        # attn_wgts = torch.nn.functional.normalize(torch.log(attn_wgts), dim=1)\n",
    "        ctx = attn_wgts.transpose(1,2) @ out # deb\n",
    "        \n",
    "\n",
    "        x = self.layers(ctx)\n",
    "        # x = self.final_lin.weight.mul(x).sum(dim=2).add(self.final_lin.bias) #missed_deb\n",
    "        x = (self.final_lin.weight * x).sum(dim=2) + self.final_lin.bias\n",
    "        # x = (self.final_lin.weight * x + self.final_lin.bias.unsqueeze(1)).sum(dim=2)\n",
    "        \n",
    "        # x = x.view(x.shape[0], x.shape[1])\n",
    "        return x, out, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d48768-d9b8-4330-9c15-c30047a0a56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class LabelAttentionClassifier3(Module):\n",
    "    initrange=0.1\n",
    "    def __init__(self, dims, ps, bptt, y_range=None):\n",
    "        self.fts = dims[0]\n",
    "        self.lbs = dims[-1] \n",
    "        \n",
    "        # ps = 0.1 # deb\n",
    "        self.layers = LinBnDrop(self.lbs, ln=False, p=ps, act=None) # deb\n",
    "        self.attn = XMLAttention(self.lbs, self.fts, 0.0)\n",
    "        self.final_lin = nn.Linear(self.fts, self.lbs) \n",
    "        init_default(self.final_lin, \n",
    "                     func=partial(torch.nn.init.uniform_, a=-self.initrange, b=self.initrange))\n",
    "    \n",
    "    def forward(self, input):\n",
    "        out, _ = input\n",
    "        ctx = self.attn(out)\n",
    "        x = self.layers(ctx)\n",
    "        x = (self.final_lin.weight * ctx).sum(dim=2) + self.final_lin.bias\n",
    "        \n",
    "        return x, out, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d776f07-355c-4d47-9545-277304f2c2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| eval:false\n",
    "# decoder = LabelAttentionClassifier([1200, 6594], ps=0.04, bptt=72).cuda() # if gpu available\n",
    "decoder = LabelAttentionClassifier([400, 6594], ps=0.04, bptt=72)\n",
    "decoder\n",
    "\n",
    "preds, *_ = decoder((out, None))\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f4ad25-394c-4c39-aa2b-8f806a27126e",
   "metadata": {},
   "source": [
    "Breaking down `LabelAttentionClassifier` to make sure we understand each line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902eb465-11f9-4731-8fb6-51ee99abbe10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| eval:false\n",
    "decoder.emb_label.weight.shape\n",
    "\n",
    "out.shape, out.device\n",
    "\n",
    "attn_wgts = out @ decoder.emb_label.weight.transpose(0,1)\n",
    "attn_wgts.shape, attn_wgts.device\n",
    "\n",
    "attn_wgts = F.softmax(attn_wgts, 1)\n",
    "\n",
    "# attn_wgts = None\n",
    "# import gc\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "out[:, :, None].shape\n",
    "\n",
    "attn_wgts.transpose(1,2).shape\n",
    "\n",
    "ctx = attn_wgts.transpose(1,2) @ out\n",
    "ctx.shape\n",
    "\n",
    "a = torch.arange(10).reshape(5,2)\n",
    "\n",
    "a, a.shape\n",
    "\n",
    "for a_split in torch.split(a, 2): print(a_split, a_split.shape, end='\\n****\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914f7b3e-f778-4758-bfb1-6eed725de8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_xmltext_classifier(arch, vocab_sz, n_class, seq_len=72, config=None, drop_mult=1., lin_ftrs=None,\n",
    "                       ps=None, pad_idx=1, max_len=72*20, y_range=None):\n",
    "    \"Create a text classifier from `arch` and its `config`, maybe `pretrained`\"\n",
    "    meta = _model_meta[arch]\n",
    "    config = ifnone(config, meta['config_clas']).copy()\n",
    "    for k in config.keys():\n",
    "        if k.endswith('_p'): config[k] *= drop_mult\n",
    "    if lin_ftrs is None: lin_ftrs = [50]\n",
    "    if ps is None: ps = [0.1]*len(lin_ftrs) # not required if not using OurPoolingLinearClasifier\n",
    "#     layers = [config[meta['hid_name']] * 3] + lin_ftrs + [n_class]  # required if using fastai's PoolingLinearClassifier\n",
    "    layers = [config[meta['hid_name']]] + [n_class]\n",
    "#     ps = [config.pop('output_p')] + ps\n",
    "    ps = config.pop('output_p')\n",
    "    init = config.pop('init') if 'init' in config else None\n",
    "    encoder = AttentiveSentenceEncoder(seq_len, arch(vocab_sz, **config), pad_idx=pad_idx, max_len=max_len)\n",
    "    # decoder = OurPoolingLinearClassifier(layers, ps, bptt=seq_len, y_range=y_range)\n",
    "    decoder = LabelAttentionClassifier3(layers, ps, bptt=seq_len, y_range=y_range)\n",
    "    model = SequentialRNN(encoder, decoder)\n",
    "    return model if init is None else model.apply(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346c62b1-c29b-432a-9b8f-9ef7ca978bcf",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294269ac-6a2f-4bc0-8160-7e4ea3cab241",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
