{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d8d823-2303-49fa-ba64-7881bcb70b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "! [ -e /content ] && pip install -Uqq xcube  # upgrade xcube on colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6496c1-fab0-447f-af93-79cae9087894",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from fastai.basics import *\n",
    "from fastai.text.learner import *\n",
    "from fastai.callback.rnn import *\n",
    "from fastai.text.models.awdlstm import *\n",
    "from fastai.text.models.core import get_text_classifier\n",
    "from fastprogress.fastprogress import master_bar, progress_bar\n",
    "from xcube.text.models.core import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de734b9-c91e-42d1-a986-3d9a4f4e8d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6918a5a-ce9c-4222-bf49-fde8e0268244",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp text.learner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259d4897-0be6-47da-bd90-21217e9f184e",
   "metadata": {},
   "source": [
    "# Learner for the XML Text application:\n",
    "\n",
    "> All the functions necessary to build `Learner` suitable for transfer learning in XML text classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6651917d-cd3b-4533-b19d-7ca3feb32873",
   "metadata": {},
   "source": [
    "The most important function of this module is `xmltext_classifier_learner`. This will help you define a `Learner` using a pretrained Language Model for the encoder and a pretrained Learning-to-Rank-Model for the decoder. (Tutorial: Coming Soon!). This module is inspired from [fastai's](https://github.com/fastai/fastai) [TextLearner](https://docs.fast.ai/text.learner.html) based on the paper [ULMFit](https://arxiv.org/pdf/1801.06146.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b331383-0c65-4d88-9689-4eaa2c0eef47",
   "metadata": {},
   "source": [
    "## Loading label embeddings from a pretrained colab model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88dfc32-d2b6-4107-9b5e-5e3f9933380e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _get_text_vocab(dls:DataLoaders) -> list:\n",
    "    \"Get text vocabulary from `DataLoaders`\"\n",
    "    vocab = dls.vocab\n",
    "    if isinstance(vocab, L): vocab = vocab[0]\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bbf616-ed91-4681-8733-ba59703b8d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _get_label_vocab(dls:DataLoaders) -> list:\n",
    "    \"Get label vocabulary from `DataLoaders`\"\n",
    "    vocab = dls.vocab\n",
    "    if isinstance(vocab, L): vocab = vocab[1]\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65e749a-83ac-41ad-b5b8-b7d141bc0143",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def match_collab(\n",
    "    old_wgts:dict, # Embedding weights of the colab model\n",
    "    collab_vocab:dict, # Vocabulary of `token` and `label` used for colab pre-training\n",
    "    lbs_vocab:list # Current labels vocabulary\n",
    ") -> dict:\n",
    "    \"Convert the label embedding in `old_wgts` to go from `old_vocab` in colab to `lbs_vocab`\"\n",
    "    bias, wgts = old_wgts.get('i_bias.weight', None), old_wgts.get('i_weight.weight')\n",
    "    wgts_m = wgts.mean(0)\n",
    "    new_wgts = wgts.new_zeros((len(lbs_vocab), wgts.size(1)))\n",
    "    if bias is not None:\n",
    "        bias_m = bias.mean(0)\n",
    "        new_bias = bias.new_zeros((len(lbs_vocab), 1))\n",
    "    collab_lbs_vocab = collab_vocab['label']\n",
    "    collab_o2i = collab_lbs_vocab.o2i if hasattr(collab_lbs_vocab, 'o2i') else {w:i for i,w in enumerate(collab_lbs_vocab)}\n",
    "    missing = 0\n",
    "    for i,w in enumerate(lbs_vocab):\n",
    "        idx = collab_o2i.get(w, -1)\n",
    "        new_wgts[i] = wgts[idx] if idx>=0 else wgts_m\n",
    "        if bias is not None: new_bias[i] = bias[idx] if idx>=0 else bias_m\n",
    "        if idx == -1: missing = missing + 1\n",
    "    old_wgts['i_weight.weight'] = new_wgts\n",
    "    if bias is not None: old_wgts['i_bias.weight'] = new_bias\n",
    "    return old_wgts, missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf0c5d8-7104-4971-a78f-d17be39abccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "wgts = {'u_weight.weight': torch.randn(3,5), \n",
    "        'i_weight.weight': torch.randn(4,5),\n",
    "        'u_bias.weight'  : torch.randn(3,1),\n",
    "        'i_bias.weight'  : torch.randn(4,1)}\n",
    "collab_vocab = {'token': ['#na#', 'sun', 'moon', 'earth', 'mars'],\n",
    "                'label': ['#na#', 'a', 'c', 'b']}\n",
    "lbs_vocab = ['a', 'b', 'c']\n",
    "new_wgts, missing = match_collab(wgts.copy(), collab_vocab, lbs_vocab)\n",
    "test_eq(missing, 0)\n",
    "test_close(wgts['u_weight.weight'], new_wgts['u_weight.weight'])\n",
    "test_close(wgts['u_bias.weight'], new_wgts['u_bias.weight'])\n",
    "with ExceptionExpected(ex=AssertionError, regex=\"close\"):\n",
    "    test_close(wgts['i_weight.weight'][1:], new_wgts['i_weight.weight'])\n",
    "    test_close(wgts['i_bias.weight'][1:], new_wgts['i_bias.weight'])\n",
    "old_w, new_w = wgts['i_weight.weight'], new_wgts['i_weight.weight']\n",
    "old_b, new_b = wgts['i_bias.weight'], new_wgts['i_bias.weight']\n",
    "for (old_k,old_v), (new_k, new_v) in zip(wgts.items(), new_wgts.items()): \n",
    "    if old_k.startswith('u'): test_eq(old_v.size(), new_v.size())\n",
    "    else: test_ne(old_v.size(), new_v.size());\n",
    "    # print(f\"old: {old_k} = {old_v.size()}, new: {new_k} = {new_v.size()}\")\n",
    "test_eq(new_w[0], old_w[1]); test_eq(new_b[0], old_b[1])\n",
    "test_eq(new_w[1], old_w[3]); test_eq(new_b[1], old_b[3])\n",
    "test_eq(new_w[2], old_w[2]); test_eq(new_b[2], old_b[2])\n",
    "test_shuffled(list(old_b[1:].squeeze().numpy()), list(new_b.squeeze().numpy()))\n",
    "test_eq(torch.sort(old_b[1:], dim=0)[0], torch.sort(new_b, dim=0)[0])\n",
    "test_eq(torch.sort(old_w[1:], dim=0)[0], torch.sort(new_w, dim=0)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341847dd-00a6-414d-98fb-e4922e1eccb4",
   "metadata": {},
   "source": [
    "## Loading Pretrained Information Gain as Attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49138f56-a5a3-4e12-b37c-eae2db651d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xcube.l2r.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e08619-019a-4631-8f59-b46a70c44de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_mimic = untar_xxx(XURLs.MIMIC3)\n",
    "xml_vocab = load_pickle(source_mimic/'mimic3-9k_clas_full_vocab.pkl')\n",
    "xml_vocab = L(xml_vocab).map(listify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ead7eb-6a67-4311-bbb8-871623a5121d",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_l2r = untar_xxx(XURLs.MIMIC3_L2R)\n",
    "boot_path = join_path_file('mimic3-9k_tok_lbl_info', source_l2r, ext='.pkl')\n",
    "bias_path = join_path_file('p_L', source_l2r, ext='.pkl')\n",
    "l2r_bootstrap = torch.load(boot_path, map_location=default_device())\n",
    "brain_bias = torch.load(bias_path, map_location=default_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bcf19c-2416-4d14-8614-8a889fea0a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last two places in brain vocab has ['xxfake', 'xxfake']\n"
     ]
    }
   ],
   "source": [
    "*brain_vocab, brain = mapt(l2r_bootstrap.get, ['toks', 'lbs', 'mutual_info_jaccard'])\n",
    "brain_vocab = L(brain_vocab).map(listify)\n",
    "toks, lbs = brain_vocab\n",
    "print(f\"last two places in brain vocab has {toks[-2:]}\")\n",
    "# toks = CategoryMap(toks, sort=False)\n",
    "brain_bias = brain_bias[:, :, 0].squeeze(-1)\n",
    "lbs_des = load_pickle(source_mimic/'code_desc.pkl')\n",
    "assert isinstance(lbs_des, dict)\n",
    "test_eq(brain.shape, (len(toks), len(lbs))) # last two places has 'xxfake'\n",
    "test_eq(brain_bias.shape, [len(lbs)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbf1375-8dae-4531-99c3-b42947a28393",
   "metadata": {},
   "source": [
    "The tokens which are there in the xml vocab but not in the brain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ddcd95-7b47-496f-8d91-cbebbf73ca96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#20) ['remiained','supicious','sharpio','theses','dissension','unrmarkable','q2day','92k','foi','mhc'...]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_found_in_brain = L(set(xml_vocab[0]).difference(set(brain_vocab[0])))\n",
    "not_found_in_brain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115ba7d8-9d37-4fdc-9045-40ce43d3b842",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fail(lambda : toks.index('cella'), contains='is not in list')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc7d566-7fcd-4542-809f-0591018405bf",
   "metadata": {},
   "source": [
    "The tokens which are in the brain but were not present in the xml vocab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2b1933-81cd-4280-afca-8f009bd7688f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(brain_vocab[0]).difference(xml_vocab[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bb96cc-7c9d-4bad-9137-81e2f302ae21",
   "metadata": {},
   "source": [
    "Thankfully, we have `info` for all the labels in the xml vocab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ef0876-d1ec-40af-b4d0-069a5b024b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert set(brain_vocab[1]).symmetric_difference(brain_vocab[1]) == set()\n",
    "# test_shuffled(xml_vocab[1], mimic_vocab[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5bae0e-536f-47f3-8433-ae7680c62fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _xml2brain(xml_vocab, brain_vocab, parent_bar=None):\n",
    "    \"Creates a mapping between the indices of the xml vocab and the brain vocab\"\n",
    "    pbar = progress_bar(xml_vocab, parent=parent_bar, leave=True)\n",
    "    xml2brain = {i: brain_vocab.index(o) if o in brain_vocab else np.inf  for i,o in enumerate(pbar)}\n",
    "    xml2brain_notfnd = [o for o in xml2brain if xml2brain[o] is np.inf]\n",
    "    return xml2brain, xml2brain_notfnd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89677ad5-60bb-4758-b854-8d87d869b89a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='57376' class='' max='57376' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [57376/57376 00:19&lt;00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "toks_xml2brain, toks_notfnd = _xml2brain(xml_vocab[0], brain_vocab[0])\n",
    "\n",
    "toks_found = set(toks_xml2brain).difference(set(toks_notfnd))\n",
    "test_shuffled(array(xml_vocab[0])[toks_notfnd], not_found_in_brain)\n",
    "some_xml_idxs = np.random.choice(array(L(toks_found)), size=10)\n",
    "some_xml_toks = array(xml_vocab[0])[some_xml_idxs]\n",
    "corres_brain_idxs = L(map(toks_xml2brain.get, some_xml_idxs))\n",
    "corres_brain_toks = array(toks)[corres_brain_idxs]\n",
    "assert all_equal(some_xml_toks, corres_brain_toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61646a6f-4323-4f7f-9ca0-e341aa36b8e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='8922' class='' max='8922' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [8922/8922 00:00&lt;00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lbs_xml2brain, lbs_notfnd = _xml2brain(xml_vocab[1], brain_vocab[1])\n",
    "\n",
    "lbs_found = set(lbs_xml2brain).difference(set(lbs_notfnd))\n",
    "some_xml_idxs = np.random.choice(array(L(lbs_found)), size=10)\n",
    "some_xml_lbs = array(xml_vocab[1])[some_xml_idxs]\n",
    "corres_brain_idxs = L(map(lbs_xml2brain.get, some_xml_idxs))\n",
    "corres_brain_lbs = array(lbs)[corres_brain_idxs]\n",
    "assert all_equal(some_xml_lbs, corres_brain_lbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d95a209-eba2-4e1f-9fbd-605b77393b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def brainsplant(xml_vocab, brain_vocab, brain, brain_bias, device=None):\n",
    "    toks_lbs = 'toks lbs'.split()\n",
    "    mb = master_bar(range(2))\n",
    "    for i in mb:\n",
    "        globals().update(dict(zip((toks_lbs[i]+'_xml2brain', toks_lbs[i]+'_notfnd'), (_xml2brain(xml_vocab[i], brain_vocab[i], parent_bar=mb)))))\n",
    "        mb.write = f\"Finished Loop {i}\"\n",
    "    xml_brain = torch.zeros(*xml_vocab.map(len)).to(default_device() if device is None else device) # initialize empty brain\n",
    "    xml_lbsbias = torch.zeros(len(xml_vocab[1])).to(default_device() if device is None else device)\n",
    "    toks_map = L((xml_idx, brn_idx) for xml_idx, brn_idx in toks_xml2brain.items() if brn_idx is not np.inf) \n",
    "    lbs_map = L((xml_idx, brn_idx) for xml_idx, brn_idx in lbs_xml2brain.items() if brn_idx is not np.inf) \n",
    "    xml_brain[toks_map.itemgot(0)] = brain[toks_map.itemgot(1)][:, lbs_map.itemgot(1)] # permute toks dim to match xml and brain\n",
    "    xml_brain[:, lbs_map.itemgot(0)] = xml_brain.clone() # permute lbs dim to match xml and brain\n",
    "    xml_lbsbias[lbs_map.itemgot(0)] = brain_bias[lbs_map.itemgot(1)].clone() # permute toks dim to match xml and brain\n",
    "    return xml_brain, xml_lbsbias, toks_map, lbs_map, toks_xml2brain, lbs_xml2brain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6655ab82-2986-471d-b628-aaee8fea1473",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xml_brain, xml_lbsbias, toks_map, lbs_map, toks_xml2brain, lbs_xml2brain = brainsplant(xml_vocab, brain_vocab, brain, brain_bias)\n",
    "test_eq(xml_brain.shape, xml_vocab.map(len))\n",
    "test_eq(xml_brain[toks_notfnd], xml_brain.new_zeros(len(toks_notfnd), len(xml_vocab[1])))\n",
    "assert all_equal(array(xml_vocab[0])[toks_map.itemgot(0)], array(brain_vocab[0])[toks_map.itemgot(1)])\n",
    "assert all_equal(array(xml_vocab[1])[lbs_map.itemgot(0)], array(brain_vocab[1])[lbs_map.itemgot(1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d54faff-2352-437e-b72e-b4e1c3d6dca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the lbl 996.87 (Complications of transplanted intestine), the top tokens that needs attention are:\n",
      "+ ('consultued', 0.25548762)\n",
      "+ ('cip', 0.25548762)\n",
      "+ ('parlor', 0.24661502)\n",
      "+ ('transplantations', 0.18601614)\n",
      "+ ('scaffoid', 0.18601614)\n",
      "+ ('epineprine', 0.18601614)\n",
      "+ ('culinary', 0.17232327)\n",
      "+ ('coordinates', 0.1469037)\n",
      "+ ('aminotransferases', 0.12153866)\n",
      "+ ('hydronephroureter', 0.12153866)\n",
      "+ ('27yom', 0.12153866)\n",
      "+ ('27y', 0.103684604)\n",
      "+ ('hardward', 0.090407245)\n",
      "+ ('leukoreduction', 0.08014185)\n",
      "+ ('venting', 0.07831942)\n",
      "+ ('secrete', 0.07196123)\n",
      "+ ('orthogonal', 0.07196123)\n",
      "+ ('naac', 0.06891022)\n",
      "+ ('mgso4', 0.0662555)\n",
      "+ ('septecemia', 0.065286644)\n"
     ]
    }
   ],
   "source": [
    "# tests to ensure `brainsplant` was successful \n",
    "lbl = '642.41'\n",
    "lbl = '38.93'\n",
    "lbl = '51.10'\n",
    "lbl = '996.87'\n",
    "lbl_idx_from_brn = brain_vocab[1].index(lbl)\n",
    "tok_vals_from_brn, top_toks_from_brn= L(brain[:, lbl_idx_from_brn].topk(k=20)).map(Self.cpu())\n",
    "lbl_idx_from_xml = xml_vocab[1].index(lbl)\n",
    "tok_vals_from_xml, top_toks_from_xml = L(xml_brain[:, lbl_idx_from_xml].topk(k=20)).map(Self.cpu())\n",
    "test_eq(lbs_xml2brain[lbl_idx_from_xml], lbl_idx_from_brn)\n",
    "test_eq(tok_vals_from_brn, tok_vals_from_xml)\n",
    "test_eq(array(brain_vocab[0])[top_toks_from_brn], array(xml_vocab[0])[top_toks_from_xml])\n",
    "test_eq(brain_bias[lbl_idx_from_brn], xml_lbsbias[lbl_idx_from_xml])\n",
    "print(f\"For the lbl {lbl} ({lbs_des.get(lbl)}), the top tokens that needs attention are:\")\n",
    "print('\\n'.join(L(array(xml_vocab[0])[top_toks_from_xml], use_list=True).zipwith(L(tok_vals_from_xml.numpy(), use_list=True)).map(str).map(lambda o: \"+ \"+o)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9088612a-0f41-4458-9608-1ef421f145af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "For the token restitched, the top labels that needs attention are:\n",
      "+ ('Other operations on supporting structures of uterus', 0.29102018)\n",
      "+ ('Other proctopexy', 0.29102018)\n",
      "+ ('Other operations on cul-de-sac', 0.18601614)\n",
      "+ (None, 0.07494824)\n",
      "+ ('Intervertebral disc disorder with myelopathy, thoracic region', 0.055331517)\n",
      "+ ('Excision of scapula, clavicle, and thorax [ribs and sternum] for graft', 0.04382947)\n",
      "+ ('Other repair of omentum', 0.028067086)\n",
      "+ ('Chronic lymphocytic thyroiditis', 0.01986737)\n",
      "+ (None, 0.019236181)\n",
      "+ ('Reclosure of postoperative disruption of abdominal wall', 0.016585195)\n",
      "+ ('Other disorders of calcium metabolism', 0.009393147)\n",
      "+ ('Pain in joint involving pelvic region and thigh', 0.008421187)\n",
      "+ ('Exteriorization of small intestine', 0.00817792)\n",
      "+ ('Fusion or refusion of 9 or more vertebrae', 0.00762466)\n",
      "+ ('Kyphosis (acquired) (postural)', 0.0074228523)\n",
      "+ ('Unspecified procedure as the cause of abnormal reaction of patient, or of later complication, without mention of misadventure at time of procedure', 0.0063889036)\n",
      "+ ('Application or administration of adhesion barrier substance', 0.00610513)\n",
      "+ ('Acute osteomyelitis involving other specified sites', 0.0054434645)\n",
      "+ ('Body Mass Index less than 19, adult', 0.004719585)\n",
      "+ ('Dorsal and dorsolumbar fusion, anterior technique', 0.0046444684)\n"
     ]
    }
   ],
   "source": [
    "tok = 'fibrillation'\n",
    "tok = 'colpo'\n",
    "tok = 'amiodarone'\n",
    "tok = 'flagyl'\n",
    "tok = 'nasalilid'\n",
    "tok = 'hemetemesis'\n",
    "tok = 'restitched'\n",
    "tok_idx_from_brn = brain_vocab[0].index(tok)\n",
    "lbs_vals_from_brn, top_lbs_from_brn = L(brain[tok_idx_from_brn].topk(k=20)).map(Self.cpu())\n",
    "tok_idx_from_xml = xml_vocab[0].index(tok)\n",
    "test_eq(tok_idx_from_brn, toks_xml2brain[tok_idx_from_xml])\n",
    "lbs_vals_from_xml, top_lbs_from_xml = L(xml_brain[tok_idx_from_xml].topk(k=20)).map(Self.cpu())\n",
    "test_eq(lbs_vals_from_brn, lbs_vals_from_xml)\n",
    "try: \n",
    "    test_eq(array(brain_vocab[1])[top_lbs_from_brn], array(xml_vocab[1])[top_lbs_from_xml])\n",
    "except AssertionError as e: \n",
    "    print(type(e).__name__, \"due to instability in sorting (nothing to worry!)\");\n",
    "    test_shuffled(array(brain_vocab[1])[top_lbs_from_brn], array(xml_vocab[1])[top_lbs_from_xml])\n",
    "print('')\n",
    "print(f\"For the token {tok}, the top labels that needs attention are:\")\n",
    "print('\\n'.join(L(mapt(lbs_des.get, array(xml_vocab[1])[top_lbs_from_xml])).zipwith(L(lbs_vals_from_xml.numpy(), use_list=True)).map(str).map(lambda o: \"+ \"+o)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e897af-7a67-4c27-aa86-74cdfcd6151b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "some tokens (with repetitions):\n",
      " -ppd1\n",
      "-hosptital\n",
      "-ectasis\n",
      "-hosptital\n",
      "-nephrograms\n",
      "-pnet\n",
      "-hosptital\n",
      "-hosptital\n",
      "-nephrograms\n",
      "-arched\n",
      "-pnet\n",
      "-ppd1\n",
      "-hosptital\n",
      "-ectasis\n",
      "-pnet\n",
      "-nephrograms\n",
      "-nephrograms\n",
      "-entrant\n",
      "-ectasis\n",
      "-2xit\n"
     ]
    }
   ],
   "source": [
    "some_toks = random.sample(toks_map.itemgot(0), 10)\n",
    "counts = [c*6 for c in random.sample(range(10), 10)]\n",
    "some_toks = random.sample(some_toks, 20, counts=counts)\n",
    "# Counter(some_toks)\n",
    "cors_toks_brn = L(mapt(toks_xml2brain.get, some_toks))\n",
    "test_eq(array(brain_vocab[0])[cors_toks_brn], array(xml_vocab[0])[some_toks])\n",
    "print(\"some tokens (with repetitions):\\n\",'\\n'.join(['-'+xml_vocab[0][t]for t in some_toks]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41799697-a3b3-4e0d-a53c-7a39246c218a",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = xml_brain[some_toks]\n",
    "test_eq(attn.shape, (len(some_toks), xml_brain.shape[1]))\n",
    "# semantics of attn\n",
    "# for each token we can compute the attention each label deserves by pulling out all the columns for a label\n",
    "for t, a in zip(some_toks,attn):\n",
    "    test_eq(xml_brain[t], a)\n",
    "# for each label we can compute the attention those tokens deserve by pulling out all rows for a label\n",
    "for lbl in range(xml_brain.shape[1]):\n",
    "    test_eq(xml_brain[:, lbl][some_toks], attn[:, lbl])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728754ad-5181-4d23-bf88-9783ee2753fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>most_relevant_lbl</th>\n",
       "      <th>lbl_attn</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2xit</td>\n",
       "      <td>200.50</td>\n",
       "      <td>0.108871</td>\n",
       "      <td>Primary central nervous system lymphoma, unspecified site, extranodal and solid organ sites</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>hosptital</td>\n",
       "      <td>793.3</td>\n",
       "      <td>0.087126</td>\n",
       "      <td>Nonspecific abnormal findings on radiological and other examination of biliary tract</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hosptital</td>\n",
       "      <td>793.3</td>\n",
       "      <td>0.087126</td>\n",
       "      <td>Nonspecific abnormal findings on radiological and other examination of biliary tract</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>hosptital</td>\n",
       "      <td>793.3</td>\n",
       "      <td>0.087126</td>\n",
       "      <td>Nonspecific abnormal findings on radiological and other examination of biliary tract</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hosptital</td>\n",
       "      <td>793.3</td>\n",
       "      <td>0.087126</td>\n",
       "      <td>Nonspecific abnormal findings on radiological and other examination of biliary tract</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hosptital</td>\n",
       "      <td>793.3</td>\n",
       "      <td>0.087126</td>\n",
       "      <td>Nonspecific abnormal findings on radiological and other examination of biliary tract</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>pnet</td>\n",
       "      <td>40.19</td>\n",
       "      <td>0.080142</td>\n",
       "      <td>Other diagnostic procedures on lymphatic structures</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>pnet</td>\n",
       "      <td>40.19</td>\n",
       "      <td>0.080142</td>\n",
       "      <td>Other diagnostic procedures on lymphatic structures</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>pnet</td>\n",
       "      <td>40.19</td>\n",
       "      <td>0.080142</td>\n",
       "      <td>Other diagnostic procedures on lymphatic structures</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ectasis</td>\n",
       "      <td>172.3</td>\n",
       "      <td>0.063419</td>\n",
       "      <td>Malignant melanoma of skin of other and unspecified parts of face</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ectasis</td>\n",
       "      <td>172.3</td>\n",
       "      <td>0.063419</td>\n",
       "      <td>Malignant melanoma of skin of other and unspecified parts of face</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ectasis</td>\n",
       "      <td>172.3</td>\n",
       "      <td>0.063419</td>\n",
       "      <td>Malignant melanoma of skin of other and unspecified parts of face</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>entrant</td>\n",
       "      <td>E874.8</td>\n",
       "      <td>0.033696</td>\n",
       "      <td>Mechanical failure of instrument or apparatus during other specified procedures</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>arched</td>\n",
       "      <td>759.7</td>\n",
       "      <td>0.033448</td>\n",
       "      <td>Multiple congenital anomalies, so described</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ppd1</td>\n",
       "      <td>607.89</td>\n",
       "      <td>0.030672</td>\n",
       "      <td>Other specified disorders of penis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ppd1</td>\n",
       "      <td>607.89</td>\n",
       "      <td>0.030672</td>\n",
       "      <td>Other specified disorders of penis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>nephrograms</td>\n",
       "      <td>410.10</td>\n",
       "      <td>0.028963</td>\n",
       "      <td>Acute myocardial infarction, of other anterior wall, episode of care unspecified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nephrograms</td>\n",
       "      <td>410.10</td>\n",
       "      <td>0.028963</td>\n",
       "      <td>Acute myocardial infarction, of other anterior wall, episode of care unspecified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>nephrograms</td>\n",
       "      <td>410.10</td>\n",
       "      <td>0.028963</td>\n",
       "      <td>Acute myocardial infarction, of other anterior wall, episode of care unspecified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>nephrograms</td>\n",
       "      <td>410.10</td>\n",
       "      <td>0.028963</td>\n",
       "      <td>Acute myocardial infarction, of other anterior wall, episode of care unspecified</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          token most_relevant_lbl  lbl_attn   \n",
       "19         2xit            200.50  0.108871  \\\n",
       "7     hosptital             793.3  0.087126   \n",
       "3     hosptital             793.3  0.087126   \n",
       "12    hosptital             793.3  0.087126   \n",
       "1     hosptital             793.3  0.087126   \n",
       "6     hosptital             793.3  0.087126   \n",
       "14         pnet             40.19  0.080142   \n",
       "10         pnet             40.19  0.080142   \n",
       "5          pnet             40.19  0.080142   \n",
       "13      ectasis             172.3  0.063419   \n",
       "2       ectasis             172.3  0.063419   \n",
       "18      ectasis             172.3  0.063419   \n",
       "17      entrant            E874.8  0.033696   \n",
       "9        arched             759.7  0.033448   \n",
       "11         ppd1            607.89  0.030672   \n",
       "0          ppd1            607.89  0.030672   \n",
       "8   nephrograms            410.10  0.028963   \n",
       "4   nephrograms            410.10  0.028963   \n",
       "15  nephrograms            410.10  0.028963   \n",
       "16  nephrograms            410.10  0.028963   \n",
       "\n",
       "                                                                                    description  \n",
       "19  Primary central nervous system lymphoma, unspecified site, extranodal and solid organ sites  \n",
       "7          Nonspecific abnormal findings on radiological and other examination of biliary tract  \n",
       "3          Nonspecific abnormal findings on radiological and other examination of biliary tract  \n",
       "12         Nonspecific abnormal findings on radiological and other examination of biliary tract  \n",
       "1          Nonspecific abnormal findings on radiological and other examination of biliary tract  \n",
       "6          Nonspecific abnormal findings on radiological and other examination of biliary tract  \n",
       "14                                          Other diagnostic procedures on lymphatic structures  \n",
       "10                                          Other diagnostic procedures on lymphatic structures  \n",
       "5                                           Other diagnostic procedures on lymphatic structures  \n",
       "13                            Malignant melanoma of skin of other and unspecified parts of face  \n",
       "2                             Malignant melanoma of skin of other and unspecified parts of face  \n",
       "18                            Malignant melanoma of skin of other and unspecified parts of face  \n",
       "17              Mechanical failure of instrument or apparatus during other specified procedures  \n",
       "9                                                   Multiple congenital anomalies, so described  \n",
       "11                                                           Other specified disorders of penis  \n",
       "0                                                            Other specified disorders of penis  \n",
       "8              Acute myocardial infarction, of other anterior wall, episode of care unspecified  \n",
       "4              Acute myocardial infarction, of other anterior wall, episode of care unspecified  \n",
       "15             Acute myocardial infarction, of other anterior wall, episode of care unspecified  \n",
       "16             Acute myocardial infarction, of other anterior wall, episode of care unspecified  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([(xml_vocab[0][t], l:=xml_vocab[1][lbl_idx], val.item(), lbs_des.get(l, 'NF')) for t,lbl_idx,val in zip(some_toks,attn.max(dim=1).indices.cpu(), attn.max(dim=1).values.cpu())],\n",
    "            columns=['token', 'most_relevant_lbl', 'lbl_attn', 'description']).sort_values(by='lbl_attn', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1739ab9-644d-4f65-bbe9-b8c0e1d141f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xcube.layers import inattention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23826717-0b57-43dc-8eba-3ec964956398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define label inattention cutoff\n",
    "k = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592fe1a7-c063-4cb9-9b36-cd2942fd84e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After looking at the tokens ['ppd1', 'hosptital', 'ectasis', 'hosptital', 'nephrograms', 'pnet', 'hosptital', 'hosptital', 'nephrograms', 'arched', 'pnet', 'ppd1', 'hosptital', 'ectasis', 'pnet', 'nephrograms', 'nephrograms', 'entrant', 'ectasis', '2xit'], I am confident about the following labels:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lbl</th>\n",
       "      <th>lbl_cf</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>793.3</td>\n",
       "      <td>0.435630</td>\n",
       "      <td>Nonspecific abnormal findings on radiological and other examination of biliary tract</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>E930.6</td>\n",
       "      <td>0.366395</td>\n",
       "      <td>Antimycobacterial antibiotics causing adverse effects in therapeutic use</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>339.3</td>\n",
       "      <td>0.276665</td>\n",
       "      <td>NF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>40.19</td>\n",
       "      <td>0.240426</td>\n",
       "      <td>Other diagnostic procedures on lymphatic structures</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>89.44</td>\n",
       "      <td>0.217201</td>\n",
       "      <td>Other cardiovascular stress test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>172.3</td>\n",
       "      <td>0.190256</td>\n",
       "      <td>Malignant melanoma of skin of other and unspecified parts of face</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>333.72</td>\n",
       "      <td>0.180402</td>\n",
       "      <td>Acute dystonia due to drugs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>333.6</td>\n",
       "      <td>0.180398</td>\n",
       "      <td>Genetic torsion dystonia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>130.9</td>\n",
       "      <td>0.174414</td>\n",
       "      <td>Toxoplasmosis, unspecified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>008.47</td>\n",
       "      <td>0.115853</td>\n",
       "      <td>Intestinal infection due to other gram-negative bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>127.3</td>\n",
       "      <td>0.115853</td>\n",
       "      <td>Trichuriasis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>803.41</td>\n",
       "      <td>0.115853</td>\n",
       "      <td>Other closed skull fracture with intracranial injury of other and unspecified nature, with no loss of consciousness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>410.10</td>\n",
       "      <td>0.115853</td>\n",
       "      <td>Acute myocardial infarction, of other anterior wall, episode of care unspecified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>200.50</td>\n",
       "      <td>0.108871</td>\n",
       "      <td>Primary central nervous system lymphoma, unspecified site, extranodal and solid organ sites</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>376.01</td>\n",
       "      <td>0.093351</td>\n",
       "      <td>Orbital cellulitis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>27.22</td>\n",
       "      <td>0.090275</td>\n",
       "      <td>Biopsy of uvula and soft palate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>474.8</td>\n",
       "      <td>0.090275</td>\n",
       "      <td>Other chronic disease of tonsils and adenoids</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>E821.0</td>\n",
       "      <td>0.085695</td>\n",
       "      <td>Nontraffic accident involving other off-road motor vehicle injuring driver of motor vehicle other than motorcycle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>607.89</td>\n",
       "      <td>0.061344</td>\n",
       "      <td>Other specified disorders of penis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>V58.62</td>\n",
       "      <td>0.056972</td>\n",
       "      <td>Encounter for long-term (current) use of antibiotics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>253.7</td>\n",
       "      <td>0.056044</td>\n",
       "      <td>Iatrogenic pituitary disorders</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>009.0</td>\n",
       "      <td>0.054827</td>\n",
       "      <td>Infectious colitis, enteritis, and gastroenteritis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>91.0</td>\n",
       "      <td>0.054826</td>\n",
       "      <td>Microscopic examination of specimen from liver, biliary tract, and pancreas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>530.84</td>\n",
       "      <td>0.048355</td>\n",
       "      <td>Tracheoesophageal fistula</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>320.82</td>\n",
       "      <td>0.046552</td>\n",
       "      <td>Meningitis due to gram-negative bacteria, not elsewhere classified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>237.5</td>\n",
       "      <td>0.045013</td>\n",
       "      <td>Neoplasm of uncertain behavior of brain and spinal cord</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>E874.8</td>\n",
       "      <td>0.033696</td>\n",
       "      <td>Mechanical failure of instrument or apparatus during other specified procedures</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>755.69</td>\n",
       "      <td>0.033448</td>\n",
       "      <td>Other congenital anomalies of lower limb, including pelvic girdle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>759.7</td>\n",
       "      <td>0.033448</td>\n",
       "      <td>Multiple congenital anomalies, so described</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>V19.2</td>\n",
       "      <td>0.033448</td>\n",
       "      <td>Family history of deafness or hearing loss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>749.02</td>\n",
       "      <td>0.033448</td>\n",
       "      <td>Cleft palate, unilateral, incomplete</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>524.06</td>\n",
       "      <td>0.033448</td>\n",
       "      <td>Microgenia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>748.5</td>\n",
       "      <td>0.033305</td>\n",
       "      <td>Agenesis, hypoplasia, and dysplasia of lung, congenital</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>906.8</td>\n",
       "      <td>0.029432</td>\n",
       "      <td>Late effect of burns of other specified sites</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>692.82</td>\n",
       "      <td>0.026444</td>\n",
       "      <td>Dermatitis due to other radiation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>434.00</td>\n",
       "      <td>0.024178</td>\n",
       "      <td>Cerebral thrombosis, without mention of cerebral infarction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>790.21</td>\n",
       "      <td>0.024178</td>\n",
       "      <td>Impaired fasting glucose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>47.19</td>\n",
       "      <td>0.020842</td>\n",
       "      <td>Other incidental appendectomy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>V06.8</td>\n",
       "      <td>0.020807</td>\n",
       "      <td>Need for prophylactic vaccination and inoculation against other combinations of diseases</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>275.49</td>\n",
       "      <td>0.017118</td>\n",
       "      <td>Other disorders of calcium metabolism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>197.5</td>\n",
       "      <td>0.015346</td>\n",
       "      <td>Secondary malignant neoplasm of large intestine and rectum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>518.52</td>\n",
       "      <td>0.013704</td>\n",
       "      <td>NF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>427.69</td>\n",
       "      <td>0.010883</td>\n",
       "      <td>Other premature beats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>707.20</td>\n",
       "      <td>0.007563</td>\n",
       "      <td>NF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>715.96</td>\n",
       "      <td>0.006654</td>\n",
       "      <td>Osteoarthrosis, unspecified whether generalized or localized, involving lower leg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>780.97</td>\n",
       "      <td>0.003610</td>\n",
       "      <td>Altered mental status</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>707.05</td>\n",
       "      <td>0.002747</td>\n",
       "      <td>Decubitus ulcer, buttock</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       lbl    lbl_cf   \n",
       "36   793.3  0.435630  \\\n",
       "20  E930.6  0.366395   \n",
       "22   339.3  0.276665   \n",
       "34   40.19  0.240426   \n",
       "30   89.44  0.217201   \n",
       "33   172.3  0.190256   \n",
       "37  333.72  0.180402   \n",
       "25   333.6  0.180398   \n",
       "38   130.9  0.174414   \n",
       "46  008.47  0.115853   \n",
       "45   127.3  0.115853   \n",
       "44  803.41  0.115853   \n",
       "43  410.10  0.115853   \n",
       "24  200.50  0.108871   \n",
       "15  376.01  0.093351   \n",
       "11   27.22  0.090275   \n",
       "35   474.8  0.090275   \n",
       "5   E821.0  0.085695   \n",
       "1   607.89  0.061344   \n",
       "7   V58.62  0.056972   \n",
       "16   253.7  0.056044   \n",
       "12   009.0  0.054827   \n",
       "13    91.0  0.054826   \n",
       "21  530.84  0.048355   \n",
       "18  320.82  0.046552   \n",
       "23   237.5  0.045013   \n",
       "42  E874.8  0.033696   \n",
       "41  755.69  0.033448   \n",
       "28   759.7  0.033448   \n",
       "31   V19.2  0.033448   \n",
       "39  749.02  0.033448   \n",
       "40  524.06  0.033448   \n",
       "27   748.5  0.033305   \n",
       "26   906.8  0.029432   \n",
       "17  692.82  0.026444   \n",
       "32  434.00  0.024178   \n",
       "10  790.21  0.024178   \n",
       "19   47.19  0.020842   \n",
       "29   V06.8  0.020807   \n",
       "3   275.49  0.017118   \n",
       "14   197.5  0.015346   \n",
       "0   518.52  0.013704   \n",
       "6   427.69  0.010883   \n",
       "2   707.20  0.007563   \n",
       "9   715.96  0.006654   \n",
       "4   780.97  0.003610   \n",
       "8   707.05  0.002747   \n",
       "\n",
       "                                                                                                            description  \n",
       "36                                 Nonspecific abnormal findings on radiological and other examination of biliary tract  \n",
       "20                                             Antimycobacterial antibiotics causing adverse effects in therapeutic use  \n",
       "22                                                                                                                   NF  \n",
       "34                                                                  Other diagnostic procedures on lymphatic structures  \n",
       "30                                                                                     Other cardiovascular stress test  \n",
       "33                                                    Malignant melanoma of skin of other and unspecified parts of face  \n",
       "37                                                                                          Acute dystonia due to drugs  \n",
       "25                                                                                             Genetic torsion dystonia  \n",
       "38                                                                                           Toxoplasmosis, unspecified  \n",
       "46                                                             Intestinal infection due to other gram-negative bacteria  \n",
       "45                                                                                                         Trichuriasis  \n",
       "44  Other closed skull fracture with intracranial injury of other and unspecified nature, with no loss of consciousness  \n",
       "43                                     Acute myocardial infarction, of other anterior wall, episode of care unspecified  \n",
       "24                          Primary central nervous system lymphoma, unspecified site, extranodal and solid organ sites  \n",
       "15                                                                                                   Orbital cellulitis  \n",
       "11                                                                                      Biopsy of uvula and soft palate  \n",
       "35                                                                        Other chronic disease of tonsils and adenoids  \n",
       "5     Nontraffic accident involving other off-road motor vehicle injuring driver of motor vehicle other than motorcycle  \n",
       "1                                                                                    Other specified disorders of penis  \n",
       "7                                                                  Encounter for long-term (current) use of antibiotics  \n",
       "16                                                                                       Iatrogenic pituitary disorders  \n",
       "12                                                                   Infectious colitis, enteritis, and gastroenteritis  \n",
       "13                                          Microscopic examination of specimen from liver, biliary tract, and pancreas  \n",
       "21                                                                                            Tracheoesophageal fistula  \n",
       "18                                                   Meningitis due to gram-negative bacteria, not elsewhere classified  \n",
       "23                                                              Neoplasm of uncertain behavior of brain and spinal cord  \n",
       "42                                      Mechanical failure of instrument or apparatus during other specified procedures  \n",
       "41                                                    Other congenital anomalies of lower limb, including pelvic girdle  \n",
       "28                                                                          Multiple congenital anomalies, so described  \n",
       "31                                                                           Family history of deafness or hearing loss  \n",
       "39                                                                                 Cleft palate, unilateral, incomplete  \n",
       "40                                                                                                           Microgenia  \n",
       "27                                                              Agenesis, hypoplasia, and dysplasia of lung, congenital  \n",
       "26                                                                        Late effect of burns of other specified sites  \n",
       "17                                                                                    Dermatitis due to other radiation  \n",
       "32                                                          Cerebral thrombosis, without mention of cerebral infarction  \n",
       "10                                                                                             Impaired fasting glucose  \n",
       "19                                                                                        Other incidental appendectomy  \n",
       "29                             Need for prophylactic vaccination and inoculation against other combinations of diseases  \n",
       "3                                                                                 Other disorders of calcium metabolism  \n",
       "14                                                           Secondary malignant neoplasm of large intestine and rectum  \n",
       "0                                                                                                                    NF  \n",
       "6                                                                                                 Other premature beats  \n",
       "2                                                                                                                    NF  \n",
       "9                                     Osteoarthrosis, unspecified whether generalized or localized, involving lower leg  \n",
       "4                                                                                                 Altered mental status  \n",
       "8                                                                                              Decubitus ulcer, buttock  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_lbs_attn = attn.clone().unsqueeze(0).permute(0,2,1).inattention(k=k).permute(0,2,1).squeeze(0).contiguous() # applying `inattention` across the lbs dim\n",
    "test_eq(top_lbs_attn.shape, (len(some_toks), xml_brain.shape[1]))\n",
    "test_ne(attn, top_lbs_attn)\n",
    "test_eq(top_lbs_attn.argmax(dim=1), attn.argmax(dim=1))\n",
    "lbs_cf = top_lbs_attn.sum(dim=0)\n",
    "test_eq(lbs_cf.shape, [top_lbs_attn.shape[1]])\n",
    "idxs = lbs_cf.nonzero().flatten().cpu()\n",
    "print(f\"After looking at the tokens {[xml_vocab[0][t]for t in some_toks]}, I am confident about the following labels:\")\n",
    "pd.DataFrame([(l:=xml_vocab[1][idx], val.item(), lbs_des.get(l, 'NF')) for idx,val in zip(idxs,lbs_cf[idxs])],\n",
    "            columns=['lbl', 'lbl_cf', 'description']).sort_values(by='lbl_cf', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9ec9bc-8d5c-498b-a868-3d5e43faf89e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| t: tensor([[0, 0, 0],\n",
      "               [0, 0, 0],\n",
      "               [0, 0, 0],\n",
      "               [0, 0, 0]])\n",
      "    s: tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9],\n",
      "               [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]])\n",
      "ic| s[row_perm.itemgot(1)]: tensor([[10, 11, 12, 13, 14, 15, 16, 17, 18, 19],\n",
      "                                    [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9]])\n",
      "ic| s[row_perm.itemgot(1)][:, col_perm.itemgot(1)]: tensor([[11, 13, 19],\n",
      "                                                            [ 1,  3,  9]])\n",
      "ic| t: tensor([[13, 19, 11],\n",
      "               [ 0,  0,  0],\n",
      "               [ 0,  0,  0],\n",
      "               [ 3,  9,  1]])\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "# semantics: `s` pulling out its 1st and 0th row is equivalent to `t` pulling out its 0th and 3rd row respectively (i.e., the data residing in the 1st and 0th row of the s matrix is same as the data residing at the 0th and the 3rd row of t's matrix)\n",
    "t = torch.zeros(4, 3).long()\n",
    "s = torch.arange(20).view(2, 10).long()\n",
    "# s = torch.arange(6).view(2,3).long()\n",
    "row_perm = L((0, 1), (3, 0)) # \n",
    "col_perm = L((2, 1), (0, 3), (1, -1))\n",
    "# col_perm = L((0,2), (1,0), (2,1))\n",
    "ic(t,s);\n",
    "ic(s[row_perm.itemgot(1)]); # pull out relevant rows from s\n",
    "ic(s[row_perm.itemgot(1)][:, col_perm.itemgot(1)]); # pull out relevant cols from s\n",
    "t[row_perm.itemgot(0)] = s[row_perm.itemgot(1)][:, col_perm.itemgot(1)] # permute rows\n",
    "t[:, col_perm.itemgot(0)] = t.clone() # permute cols\n",
    "# t[row_perm.itemgot(0)] = s[row_perm.itemgot(1)][:, col_perm.itemgot(1)][:, col_perm.itemgot(0)]\n",
    "ic(t);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2530f3-f6e2-427d-9712-461e25b60fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "l2r_wgts = torch.load(join_path_file('lin_lambdarank_full', source_l2r, ext='.pth'), map_location=default_device())\n",
    "if 'model' in l2r_wgts: l2r_wgts = l2r_wgts['model']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b62294c-fe30-4c66-832f-32f017def64d",
   "metadata": {},
   "source": [
    "Need to match the wgts in xml and brain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc99996-3591-47fb-8fe1-23aebad6121d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def brainsplant_diffntble(xml_vocab, brain_vocab, l2r_wgts, device=None):\n",
    "    toks_lbs = 'toks lbs'.split()\n",
    "    mb = master_bar(range(2))\n",
    "    for i in mb:\n",
    "        globals().update(dict(zip((toks_lbs[i]+'_xml2brain', toks_lbs[i]+'_notfnd'), (_xml2brain(xml_vocab[i], brain_vocab[i], parent_bar=mb)))))\n",
    "        mb.write = f\"Finished Loop {i}\" \n",
    "    toks_map = L((xml_idx, brn_idx) for xml_idx, brn_idx in toks_xml2brain.items() if brn_idx is not np.inf) \n",
    "    lbs_map = L((xml_idx, brn_idx) for xml_idx, brn_idx in lbs_xml2brain.items() if brn_idx is not np.inf) \n",
    "    tf_xml = torch.zeros(len(xml_vocab[0]), 200).to(default_device() if device is None else device) \n",
    "    tb_xml = torch.zeros(len(xml_vocab[0]), 1).to(default_device() if device is None else device) \n",
    "    lf_xml = torch.zeros(len(xml_vocab[1]), 200).to(default_device() if device is None else device) \n",
    "    lb_xml = torch.zeros(len(xml_vocab[1]), 1).to(default_device() if device is None else device) \n",
    "    tf_l2r, tb_l2r, lf_l2r, lb_l2r = list(l2r_wgts.values())\n",
    "    tf_xml[toks_map.itemgot(0)] = tf_l2r[toks_map.itemgot(1)].clone()\n",
    "    tb_xml[toks_map.itemgot(0)] = tb_l2r[toks_map.itemgot(1)].clone()\n",
    "    lf_xml[lbs_map.itemgot(0)] = lf_l2r[lbs_map.itemgot(1)].clone()\n",
    "    lb_xml[lbs_map.itemgot(0)] = lb_l2r[lbs_map.itemgot(1)].clone()\n",
    "    # import pdb; pdb.set_trace()\n",
    "    xml_wgts = {k: xml_val for k, xml_val in zip(l2r_wgts.keys(), (tf_xml, tb_xml, lf_xml, lb_xml))}\n",
    "    mod_dict = nn.ModuleDict({k.split('.')[0]: nn.Embedding(*v.size()) for k,v in xml_wgts.items()}).to(default_device() if device is None else device) \n",
    "    mod_dict.load_state_dict(xml_wgts)\n",
    "    return mod_dict, toks_map, lbs_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a718c477-718b-4043-a123-9194e94d9aa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mod_dict, toks_map, lbs_map = brainsplant_diffntble(xml_vocab, brain_vocab, l2r_wgts)\n",
    "assert isinstance(mod_dict, nn.Module)\n",
    "assert nn.Module in mod_dict.__class__.__mro__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cba9729-5fdd-4418-9b9b-6685b264fc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(mod_dict['token_factors'].weight.data[toks_map.itemgot(0)], l2r_wgts['token_factors.weight'][toks_map.itemgot(1)])\n",
    "test_eq(mod_dict['token_bias'].weight.data[toks_map.itemgot(0)], l2r_wgts['token_bias.weight'][toks_map.itemgot(1)])\n",
    "test_eq(mod_dict['label_factors'].weight.data[lbs_map.itemgot(0)], l2r_wgts['label_factors.weight'][lbs_map.itemgot(1)])\n",
    "test_eq(mod_dict['label_bias'].weight.data[lbs_map.itemgot(0)], l2r_wgts['label_bias.weight'][lbs_map.itemgot(1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c861c18-839f-4423-972c-1632fc3289e6",
   "metadata": {},
   "source": [
    "## Base `Learner` for NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4e89ee-1737-4069-9627-8a5c6b31db48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def load_collab_keys(\n",
    "    model, # Model architecture\n",
    "    wgts:dict # Model weights\n",
    ") -> tuple:\n",
    "    \"Load only collab `wgts` (`i_weight` and `i_bias`) in `model`, keeping the rest as is\"\n",
    "    sd = model.state_dict()\n",
    "    lbs_weight, i_weight = sd.get('1.attn.lbs_weight.weight', None), wgts.get('i_weight.weight', None)\n",
    "    lbs_bias, i_bias = sd.get('1.attn.lbs_weight.bias', None), wgts.get('i_bias.weight', None) \n",
    "    if lbs_weight is not None and i_weight is not None: lbs_weight.data = i_weight.data\n",
    "    if lbs_bias is not None and i_bias is not None: lbs_bias.data = i_bias.data\n",
    "    if '1.attn.lbs_weight_dp.emb.weight' in sd:\n",
    "        sd['1.attn.lbs_weight_dp.emb.weight'] = i_weight.data.clone()\n",
    "    return model.load_state_dict(sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81941404-34e8-4a65-b369-7fa293b97b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.pay_attn.lbs.weight\n",
      "1.boost_attn.lin.weight\n",
      "1.boost_attn.lin.bias\n"
     ]
    }
   ],
   "source": [
    "config = awd_lstm_clas_config.copy()\n",
    "config.update({'n_hid': 10, 'emb_sz': 5})\n",
    "# tst = get_text_classifier(AWD_LSTM, 100, 3, config=config)\n",
    "tst = get_xmltext_classifier(AWD_LSTM, 100, 3, config=config)\n",
    "old_sd = tst.state_dict().copy()\n",
    "r = re.compile(\".*attn.*\")\n",
    "test_eq([key for key in old_sd if 'attn' in key], list(filter(r.match, old_sd)))\n",
    "print(\"\\n\".join(list(filter(r.match, old_sd))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af00554-0fba-4e8d-b75e-a01e5fec4b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53294433-b75e-4d54-a574-6af80e705b03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_sd = copy.deepcopy(tst.state_dict())\n",
    "load_collab_keys(tst, new_wgts)\n",
    "# <TODO: Deb> fix the following tests later\n",
    "# test_ne(old_sd['1.attn.lbs_weight.weight'], tst.state_dict()['1.attn.lbs_weight.weight'])\n",
    "# test_eq(tst.state_dict()['1.pay_attn.lbs_weight.weight'], new_wgts['i_weight.weight'])\n",
    "# test_ne(old_sd['1.attn.lbs_weight_dp.emb.weight'], tst.state_dict()['1.attn.lbs_weight_dp.emb.weight'])\n",
    "# test_eq(tst.state_dict()['1.attn.lbs_weight_dp.emb.weight'], new_wgts['i_weight.weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1e5ac5-9a18-4139-9cf1-030977f84eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from xcube.layers import *\n",
    "from xcube.layers import _planted_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af50343-383d-4284-b11e-d3016d54e19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@delegates(Learner.__init__)\n",
    "class TextLearner(Learner):\n",
    "    \"Basic class for a `Learner` in NLP.\"\n",
    "    def __init__(self, \n",
    "        dls:DataLoaders, # Text `DataLoaders`\n",
    "        model, # A standard PyTorch model\n",
    "        alpha:float=2., # Param for `RNNRegularizer`\n",
    "        beta:float=1., # Param for `RNNRegularizer`\n",
    "        moms:tuple=(0.8,0.7,0.8), # Momentum for `Cosine Annealing Scheduler`\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(dls, model, moms=moms, **kwargs)\n",
    "        self.add_cbs(rnn_cbs())\n",
    "\n",
    "    def save_encoder(self, \n",
    "        file:str # Filename for `Encoder` \n",
    "    ):\n",
    "        \"Save the encoder to `file` in the model directory\"\n",
    "        if rank_distrib(): return # don't save if child proc\n",
    "        encoder = get_model(self.model)[0]\n",
    "        if hasattr(encoder, 'module'): encoder = encoder.module\n",
    "        torch.save(encoder.state_dict(), join_path_file(file, self.path/self.model_dir, ext='.pth'))\n",
    "    \n",
    "    @delegates(save_model)\n",
    "    def save(self,\n",
    "        file:str, # Filename for the state_directory of the model\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Save model and optimizer state (if `with_opt`) to `self.path/self.model_dir/file`\n",
    "        Save `self.dls.vocab` to `self.path/self.model_dir/clas_vocab.pkl`\n",
    "        \"\"\"\n",
    "        model_file = join_path_file(file, self.path/self.model_dir, ext='.pth')\n",
    "        vocab_file = join_path_file(file+'_vocab', self.path/self.model_dir, ext='.pkl')\n",
    "        save_model(model_file, self.model, getattr(self, 'opt', None), **kwargs)\n",
    "        save_pickle(vocab_file, self.dls.vocab)\n",
    "        return model_file\n",
    "\n",
    "    def load_encoder(self, \n",
    "        file:str, # Filename of the saved encoder \n",
    "        device:(int,str,torch.device)=None # Device used to load, defaults to `dls` device\n",
    "    ):\n",
    "        \"Load the encoder `file` from the model directory, optionally ensuring it's on `device`\"\n",
    "        encoder = get_model(self.model)[0]\n",
    "        if device is None: device = self.dls.device\n",
    "        if hasattr(encoder, 'module'): encoder = encoder.module\n",
    "        distrib_barrier()\n",
    "        wgts = torch.load(join_path_file(file,self.path/self.model_dir, ext='.pth'), map_location=device)\n",
    "        encoder.load_state_dict(clean_raw_keys(wgts))\n",
    "        self.freeze()\n",
    "        return self\n",
    "    \n",
    "    def load_brain(self,\n",
    "        file_wgts: str, # Filename of the saved attention wgts\n",
    "        file_bias: str, # Filename of the saved label bias\n",
    "        device:(int,str,torch.device)=None # Device used to load, defaults to `dls` device\n",
    "    ):\n",
    "        \"\"\"Load the pre-learnt label specific attention weights for each token from `file` located in the \n",
    "        model directory, optionally ensuring it's one `device`\n",
    "        \"\"\"\n",
    "        brain_path = join_path_file(file_wgts, self.path/self.model_dir, ext='.pkl')\n",
    "        bias_path = join_path_file(file_bias, self.path/self.model_dir, ext='.pkl')\n",
    "        brain_bootstrap = torch.load(brain_path, map_location=default_device() if device is None else device)\n",
    "        *brain_vocab, brain = mapt(brain_bootstrap.get, ['toks', 'lbs', 'mutual_info_jaccard'])\n",
    "        brain_vocab = L(brain_vocab).map(listify)\n",
    "        vocab = L(_get_text_vocab(self.dls), _get_label_vocab(self.dls)).map(listify)\n",
    "        brain_bias = torch.load(bias_path, map_location=default_device() if device is None else device)\n",
    "        brain_bias = brain_bias[:, :, 0].squeeze(-1)\n",
    "        print(\"Performing brainsplant...\")\n",
    "        self.brain, self.lbsbias, *_ = brainsplant(vocab, brain_vocab, brain, brain_bias)\n",
    "        print(\"Successfull!\")\n",
    "        # import pdb; pdb.set_trace()\n",
    "        plant_attn_layer = Lambda(Planted_Attention(self.brain))\n",
    "        setattr(self.model[1].pay_attn, 'attn', plant_attn_layer)\n",
    "        assert self.model[1].pay_attn.attn.func.f is _planted_attention\n",
    "        return self\n",
    "\n",
    "    def load_pretrained(self, \n",
    "        wgts_fname:str, # Filename of saved weights \n",
    "        vocab_fname:str, # Saved vocabulary filename in pickle format\n",
    "        model=None # Model to load parameters from, defaults to `Learner.model`\n",
    "    ):\n",
    "        \"Load a pretrained model and adapt it to the data vocabulary.\"\n",
    "        old_vocab = load_pickle(vocab_fname)\n",
    "        new_vocab = _get_text_vocab(self.dls)\n",
    "        distrib_barrier()\n",
    "        wgts = torch.load(wgts_fname, map_location = lambda storage,loc: storage)\n",
    "        if 'model' in wgts: wgts = wgts['model'] #Just in case the pretrained model was saved with an optimizer\n",
    "        wgts = match_embeds(wgts, old_vocab, new_vocab)\n",
    "        load_ignore_keys(self.model if model is None else model, clean_raw_keys(wgts))\n",
    "        self.freeze()\n",
    "        return self\n",
    "\n",
    "    #For previous versions compatibility. Remove at release\n",
    "    @delegates(load_model_text)\n",
    "    def load(self, \n",
    "        file:str, # Filename of saved model \n",
    "        with_opt:bool=None, # Enable to load `Optimizer` state\n",
    "        device:(int,str,torch.device)=None, # Device used to load, defaults to `dls` device\n",
    "        **kwargs\n",
    "    ):\n",
    "        if device is None: device = self.dls.device\n",
    "        if self.opt is None: self.create_opt()\n",
    "        file = join_path_file(file, self.path/self.model_dir, ext='.pth')\n",
    "        load_model_text(file, self.model, self.opt, device=device, **kwargs)\n",
    "        return self\n",
    "    \n",
    "    def load_collab(self,\n",
    "        wgts_fname:str, # Filename of the saved collab model\n",
    "        collab_vocab_fname:str, # Saved Vocabulary of collab labels in pickle format \n",
    "        model=None # Model to load parameters from, defaults to `Learner.model`\n",
    "    ):\n",
    "        \"Load the label embeddings learned by collab model`, and adapt it to the label vocabulary.\"\n",
    "        collab_vocab = load_pickle(collab_vocab_fname)\n",
    "        lbs_vocab = _get_label_vocab(self.dls)\n",
    "        distrib_barrier()\n",
    "        wgts = torch.load(wgts_fname, map_location=lambda storage,loc: storage)\n",
    "        if 'model' in wgts: wgts = wgts['model'] #Just in case the pretrained model was saved with an optimizer\n",
    "        wgts, _ = match_collab(wgts, collab_vocab, lbs_vocab)\n",
    "        load_collab_keys(self.model if model is None else model, wgts)\n",
    "        self.freeze()\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c077c1d-1fde-4c45-ae1e-6f6cd3f58b4c",
   "metadata": {},
   "source": [
    "Adds a `ModelResetter` and an `RNNRegularizer` with `alpha` and `beta` to the callbacks, the rest is the same as `Learner` init. \n",
    "\n",
    "This `Learner` adds functionality to the base class:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9001ab92-67ce-4357-b447-0b85c0cbd384",
   "metadata": {},
   "source": [
    "## `Learner` convenience functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a39c937-70c1-438b-84d9-34d7ef4a8c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from xcube.text.models.core import _model_meta "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38916689-ddc4-498f-881d-36ddebd9e9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "@delegates(Learner.__init__)\n",
    "def xmltext_classifier_learner(dls, arch, seq_len=72, config=None, backwards=False, pretrained=True, collab=False, drop_mult=0.5, n_out=None,\n",
    "                           lin_ftrs=None, ps=None, max_len=72*20, y_range=None, splitter=None, running_decoder=True, **kwargs):\n",
    "    \"Create a `Learner` with a text classifier from `dls` and `arch`.\"\n",
    "    vocab = _get_text_vocab(dls)\n",
    "    if n_out is None: n_out = get_c(dls)\n",
    "    assert n_out, \"`n_out` is not defined, and could not be inferred from the data, set `dls.c` or pass `n_out`\"\n",
    "    model = get_xmltext_classifier2(arch, len(vocab), n_out, seq_len=seq_len, config=config, y_range=y_range,\n",
    "                                drop_mult=drop_mult, max_len=max_len, running_decoder=running_decoder)\n",
    "    # model = get_xmltext_classifier(arch, len(vocab), n_out, seq_len=seq_len, config=config, y_range=y_range,\n",
    "                                # drop_mult=drop_mult, max_len=max_len)\n",
    "    meta = _model_meta[arch]\n",
    "    learn = TextLearner(dls, model, splitter=splitter if splitter is not None else meta['split_clas'], **kwargs)\n",
    "    url = 'url_bwd' if backwards else 'url'\n",
    "    if pretrained:\n",
    "        if url not in meta:\n",
    "            warn(\"There are no pretrained weights for that architecture yet!\")\n",
    "            return learn\n",
    "        model_path = untar_data(meta[url], c_key='model')\n",
    "        try: fnames = [list(model_path.glob(f'*.{ext}'))[0] for ext in ['pth', 'pkl']]\n",
    "        except IndexError: print(f'The model in {model_path} is incomplete, download again'); raise\n",
    "        learn = learn.load_pretrained(*fnames, model=learn.model[0])\n",
    "    if collab:\n",
    "        try: fnames = [list(learn.path.glob(f'**/collab/*collab*.{ext}'))[0] for ext in ['pth', 'pkl']]\n",
    "        except IndexError: print(f'The collab model in {learn.path} is incomplete, re-train it!'); raise\n",
    "        learn = learn.load_colab(*fnames, model=learn.model[1])\n",
    "    learn.freeze()\n",
    "    return learn   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e66275-882c-4fce-b021-a0e3cc0c5b69",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2f6c93-6031-40fb-8bc9-e32a9c519643",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83456dda-2b60-464b-905b-d081aeeeeea5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepk",
   "language": "python",
   "name": "deepk"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
