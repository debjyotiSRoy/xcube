{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6eed62a-b076-418b-9293-53db08ae0fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "! [ -e /content ] && pip install -Uqq xcube # upgrade xcube on colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b2ba66-c8bf-4819-8b37-e6fd554bbe20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp l2r.learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f1100f-5d24-4a14-9751-1f0d761722a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "from fastai.torch_imports import *\n",
    "from fastai.learner import *\n",
    "from fastai.optimizer import *\n",
    "from fastai.torch_core import *\n",
    "from fastcore.all import *\n",
    "from xcube.imports import *\n",
    "from xcube.metrics import *\n",
    "from xcube.l2r.gradients import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5a7c93-f167-4f1c-8778-e622969499ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce78bf05-7a73-4f1b-9fef-7cc691299d7f",
   "metadata": {},
   "source": [
    "# L2R Learner\n",
    "\n",
    "> Learner for Learning to Rank Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb9812b-26f5-4252-8597-a751a90d462a",
   "metadata": {},
   "source": [
    "This module contains a specialized version of fastai's full fledged [`Learner`](https://docs.fast.ai/learner.html#learner). Every functionality here can also be achieved with fastai's `Learner`. The purpose of re-creating a learner was purely educational."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c841e8cf-a3cc-495e-9f73-fc8f846b590a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from fastai.callback.core import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62586629",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class SkipToEpoch(Callback):\n",
    "    \"Skip training up to `epoch`\"\n",
    "    order = 70\n",
    "    \n",
    "    def __init__(self, epoch:int):\n",
    "        self._skip_to = epoch\n",
    "\n",
    "    def before_epoch(self):\n",
    "        if self.epoch < self._skip_to:\n",
    "            raise CancelEpochException\n",
    "    \n",
    "    def after_cancel_epoch(self): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06df17bc-d84f-4fdb-ab1a-45c9487c8ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class L2RLearner:\n",
    "    def __init__(self, \n",
    "        model, \n",
    "        dls, \n",
    "        grad_func, \n",
    "        loss_func, \n",
    "        lr, \n",
    "        cbs, \n",
    "        opt_func=SGD, \n",
    "        path=None,\n",
    "        model_dir:str|Path='models', # Subdirectory to save and load models\n",
    "        moms:tuple=(0.95,0.08,0.95)\n",
    "    ):\n",
    "        store_attr(but='cbs')\n",
    "        self.path = Path(path) if path is not None else getattr(dls, 'path', Path('.'))\n",
    "        self.cbs = L()\n",
    "        self.add_cbs(cbs)\n",
    "        self.logger = print\n",
    "\n",
    "    def add_cb(self, cb):\n",
    "        cb.learn = self\n",
    "        setattr(self, cb.name, cb)\n",
    "        self.cbs.append(cb)\n",
    "        return self\n",
    "\n",
    "    def add_cbs(self, cbs):\n",
    "        L(cbs).map(self.add_cb)\n",
    "        return self\n",
    "    \n",
    "    @contextmanager\n",
    "    def added_cbs(self, cbs):\n",
    "        self.add_cbs(cbs)\n",
    "        try: yield\n",
    "        finally: self.remove_cbs(cbs)\n",
    "        \n",
    "    @contextmanager\n",
    "    def removed_cbs(self, cbs):\n",
    "        self.remove_cbs(cbs)\n",
    "        try: yield self\n",
    "        finally: self.add_cbs(cbs)\n",
    "        \n",
    "    def remove_cbs(self, cbs):\n",
    "        L(cbs).map(self.remove_cb)\n",
    "        return self\n",
    "    \n",
    "    def remove_cb(self, cb):\n",
    "        cb.learn = None\n",
    "        if hasattr(self, cb.name): delattr(self, cb.name)\n",
    "        if cb in self.cbs: self.cbs.remove(cb)\n",
    "        return self\n",
    "\n",
    "    def _step(self): self.opt.step()\n",
    "        \n",
    "    def one_batch(self, *args, **kwargs):\n",
    "        # self('before_batch')\n",
    "        self.preds = self.model(self.xb)\n",
    "        self('after_pred')\n",
    "        # if self.model.training: # training\n",
    "        if not self.model.training: return\n",
    "        self.srtd_preds, self.lambda_i = self.grad_func(self.preds, self.xb)\n",
    "        self.lambda_i = self.lambda_i.half()\n",
    "        self('after_loss')\n",
    "        self('before_backward')\n",
    "        self.srtd_preds.backward(self.lambda_i)\n",
    "        self('after_backward')\n",
    "        \n",
    "        # free memory (TODO: Put this in a little callback)\n",
    "        self.lambda_i = None\n",
    "        import gc; gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "            \n",
    "        # self('before_step')\n",
    "        # self.opt.step()\n",
    "        self._with_events(self._step, 'step', CancelStepException)\n",
    "        # self('after_step')\n",
    "        self.opt.zero_grad()\n",
    "            \n",
    "        # self('after_batch')\n",
    "        \n",
    "    def one_epoch(self, train, **kwargs):\n",
    "        self.model.training = train\n",
    "        self.dl = self.dls.train if train else self.dls.valid\n",
    "        (self._do_epoch_validate, self._do_epoch_train)[self.model.training](**kwargs)\n",
    "        \n",
    "    def _do_epoch_train(self, *args, **kwargs):\n",
    "        self._with_events(partial(self._all_batches, *args, **kwargs), 'train', CancelTrainException)\n",
    "        # self('before_train')\n",
    "        # self._all_batches(*args, **kwargs)\n",
    "        # self('after_train')\n",
    "        \n",
    "    def _do_epoch_validate(self, *args, idx=1, dl=None, **kwargs):\n",
    "        if dl is None: dl = self.dls[idx]\n",
    "        self.dl = dl\n",
    "        with torch.no_grad():\n",
    "            self._with_events(partial(self._all_batches, *args, **kwargs), 'validate', CancelValidException)\n",
    "            # self('before_validate')\n",
    "            # self._all_batches(*args, **kwargs)\n",
    "            # self('after_validate')\n",
    "        \n",
    "    def _all_batches(self, *args, **kwargs):\n",
    "        self.n_iter = len(self.dl)\n",
    "        for self.iter_num, self.xb in enumerate(self.dl):\n",
    "            self._with_events(partial(self.one_batch, *args, **kwargs), 'batch', CancelBatchException)\n",
    "    \n",
    "    def create_opt(self):\n",
    "        self.opt = self.opt_func(self.model.parameters(), self.lr)\n",
    "        # self.opt.clear_state()\n",
    "        return self.opt\n",
    "    \n",
    "    def _do_epoch(self, **kwargs):\n",
    "        self.one_epoch(True, **kwargs)\n",
    "        self.one_epoch(False, **kwargs)\n",
    "\n",
    "    def fit(self, n_epochs, cbs=None, start_epoch=0, lr=None, wd=None, reset_opt=False, **kwargs):\n",
    "        if start_epoch != 0:\n",
    "            cbs = L(cbs) + SkipToEpoch(start_epoch)\n",
    "        with self.added_cbs(cbs):\n",
    "            opt = getattr(self, 'opt', None)\n",
    "            if opt is None or reset_opt: self.create_opt()\n",
    "            if wd is not None: self.opt.set_hypers(wd=wd)\n",
    "            self.opt.set_hypers(lr=self.lr if lr is None else lr)\n",
    "            self.n_epochs = n_epochs\n",
    "            self('before_fit')\n",
    "            try:\n",
    "                for self.epoch,_ in enumerate(range(self.n_epochs)):\n",
    "                    self._with_events(partial(self._do_epoch, **kwargs), 'epoch', CancelEpochException)\n",
    "                    # self('before_epoch')\n",
    "                    # self.one_epoch(True, **kwargs)\n",
    "                    # self.one_epoch(False, **kwargs)\n",
    "                    # self('after_epoch')\n",
    "            except CancelFitException: pass \n",
    "            self('after_fit')\n",
    "    \n",
    "    def validate(self, idx=1, dl=None, **kwargs):\n",
    "        try: \n",
    "            self.model.training = False\n",
    "            self._do_epoch_validate(idx, dl, **kwargs)\n",
    "        except CancelFitException: pass\n",
    "    \n",
    "    def __call__(self, name):\n",
    "        for cb in self.cbs: getattr(cb, name, noop)()\n",
    "        \n",
    "    def _with_events(self, f, event_type, ex, final=noop):\n",
    "        try: self(f'before_{event_type}'); f()\n",
    "        except ex: self(f'after_cancel_{event_type}')\n",
    "        self(f'after_{event_type}'); final()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627fd87c-b8e0-4236-bd80-6f1414ef64ef",
   "metadata": {},
   "source": [
    "**Serializing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1059cdef-53f1-4f7c-bfff-87816f9a1640",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "@delegates(save_model)\n",
    "def save(self:L2RLearner, file, **kwargs):\n",
    "    \"Save model and optimizer state (if 'with_opt') to `self.path/file`\"\n",
    "    file = join_path_file(file, self.path/self.model_dir, ext='.pth')\n",
    "    save_model(file, self.model, getattr(self, 'opt', None), **kwargs)\n",
    "    return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ad0bde-478c-49a8-b125-106444ef965c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "@delegates(load_model)\n",
    "def load(self:L2RLearner, file, device=None, **kwargs):\n",
    "    \"Load model and optimizer state (if `with_opt`) from `self.path/file` using `device`\"\n",
    "    if device is None and hasattr(self.dls, 'device'): device = self.dls.device\n",
    "    self.opt = getattr(self, 'opt', None)\n",
    "    if self.opt is None: self.create_opt()\n",
    "    file = join_path_file(file, self.path/self.model_dir, ext='.pth')\n",
    "    load_model(file, self.model, self.opt, device=device, **kwargs)\n",
    "    return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4969fa22-7176-4528-ad47-c21750036a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def show_results(self:L2RLearner, device=None, k=None):\n",
    "    \"Produces the ranking for 100 random labels\"\n",
    "    dataset = to_device(self.dls.train.dataset, device=device)\n",
    "    num_lbs = dataset.shape[0]\n",
    "    idxs = torch.randperm(num_lbs)[:100]\n",
    "    xb = dataset[idxs]\n",
    "    xb = xb.unsqueeze(0)\n",
    "    preds, preds_rank, *_,  _ndcg_at_k = ndcg(self.model(xb), xb, k=k)\n",
    "    if _ndcg_at_k is not None: _ndcg_at_k.squeeze_(0) \n",
    "    # lbs = xb[:, :, :, 1].unique().cpu().numpy()\n",
    "    lbs = idxs.numpy()\n",
    "    cols = pd.MultiIndex.from_product([lbs, ('tok', 'lbl', 'rank', 'score', 'preds', 'model_rank')], names=['label', 'key2'])\n",
    "    data = torch.concat( (xb, preds.unsqueeze(-1), preds_rank.unsqueeze(-1)), dim=-1).squeeze(0).permute(1, 0, 2).contiguous()\n",
    "    data = data.reshape(data.shape[0], -1)\n",
    "    df_results = pd.DataFrame(data, columns=cols)\n",
    "    df_results.index.name = 'toks'\n",
    "    # pd.set_option('display.max_columns', None)\n",
    "    df_ndcg = pd.DataFrame({'labels': lbs, 'ndcg_at_k':_ndcg_at_k.cpu().numpy()})\n",
    "    return df_results, df_ndcg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c74968-6cff-4704-b16b-ccf494472a6d",
   "metadata": {},
   "source": [
    "### Learner convenience functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5360c28a-5f98-4a83-82cd-c4a261756569",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_learner(model, dls, grad_fn=rank_loss3, loss_fn=loss_fn2, lr=1e-5, cbs=None, opt_func=partial(SGD, mom=0.9), lambrank=False, **kwargs):\n",
    "    if lambrank: grad_fn = partial(grad_fn, lambrank=lambrank)\n",
    "    learner = L2RLearner(model, dls, grad_fn, loss_fn, lr, cbs, opt_func=opt_func, **kwargs)\n",
    "    return learner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3f7b38-1e2c-49c5-8231-f4e0a5130e86",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294269ac-6a2f-4bc0-8160-7e4ea3cab241",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
