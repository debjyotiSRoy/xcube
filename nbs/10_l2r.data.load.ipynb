{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b549cba-b245-4c41-ad56-a601fed02972",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "! [ -e /content ] && pip install -Uqq fastai # upgrade fastai on colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3b0b1d-1206-45e8-bb99-bef4260792c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp l2r.data.load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fcac62-a980-40b0-8527-ae7aa0dff3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from fastai.torch_imports import *\n",
    "from fastai.data.load import DataLoader\n",
    "from xcube.imports import *\n",
    "from xcube.torch_imports import *\n",
    "from xcube.fastai_imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf82387-254e-4b20-a306-d23bc3320488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ac225c-b870-47da-bfcc-6d72f7d8ab8b",
   "metadata": {},
   "source": [
    "# L2R DataLoader\n",
    "\n",
    "> DataLoader for L2R applications. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1fd6f6-cdda-4572-812b-59923b3d2c63",
   "metadata": {},
   "source": [
    "This module contains all the classes and supporting fuctions to build a `L2RDataLoader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d8115f-96f9-45ca-9b76-207fb71c6544",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def quantized_score(df, qnts=None, device=None):\n",
    "    \"computes quantize the rank column of `df['rank']` based on `qnts`\"\n",
    "    \n",
    "    if qnts is None:\n",
    "        qnts = torch.concat([torch.Tensor([0]), torch.logspace(torch.log10(torch.tensor(1e-4)), torch.log10(torch.tensor(1)), 100)])\n",
    "    qnts = to_device(qnts)\n",
    "    # import pdb; pdb.set_trace()\n",
    "    # from IPython import embed; embed()\n",
    "    data = qnts.new_tensor(df.to_numpy())\n",
    "    test_eq(data.shape, (len(df), 3)) # dim1 is a 3 tuple (token, label, rank)\n",
    "    # sort by the labels\n",
    "    data = data[data[:, 1].argsort()]\n",
    "    # indices of the unique labels\n",
    "    # import pdb; pdb.set_trace()\n",
    "        # splt_idxs = np.concatenate([torch.as_tensor(np.unique(data[:, 1].cpu().numpy(), return_index=True)[1], device=default_device(), dtype=torch.int).cpu().numpy(), array([len(data)])])\n",
    "        # splt_idxs = list(splt_idxs[1:] - splt_idxs[:-1])\n",
    "    splt_idxs = np.unique(data[:, 1].cpu().numpy(), return_index=True)[1][1:]\n",
    "    # split by the unique labels\n",
    "    data = np.split(data.cpu().numpy(), splt_idxs)\n",
    "    # stacking the 0th dim with label specific data\n",
    "    data = np.stack(data)\n",
    "    # data = torch.as_tensor(data, dtype=qnts.dtype)\n",
    "    data = qnts.new_tensor(data)\n",
    "    # test_eq(data.shape, (8922, 57352, 3))\n",
    "    # computing the bins based on qnts\n",
    "    bins = torch.quantile(data[:, :, -1], qnts, dim=1)\n",
    "    # test_eq(bins.shape, (101, 8922))\n",
    "    # placing relevance scores right next to ranks\n",
    "    relv_scores = bins.shape[0] - torch.searchsorted(bins.T, data[:, :, -1], right=False) # shape (8922, 57352)\n",
    "    data = torch.cat((data, relv_scores.unsqueeze(-1)), dim=-1)\n",
    "    # data[:, :, -1] = relv_scores\n",
    "    return data # dim 0: labels, dim 1: 3 tuple (token, label, rank, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ebe55c-7664-4f1d-a464-1381c3b5bea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class L2RDataLoader(DataLoader):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.sl, self.lbs_chunks = kwargs.pop('sl', None), kwargs.pop('lbs_chunks', None)\n",
    "        if self.sl is None: self.sl = 64\n",
    "        if self.lbs_chunks is None: self.lbs_chunks = 4\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "    def randomize(self):\n",
    "        seed = np.random.default_rng().integers(0, 2**32-1, 1).item()\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "    \n",
    "    def shuffle_fn(self, idxs): return self.rng.permutation(idxs)\n",
    "\n",
    "    def get_idxs(self):\n",
    "        if self.n is not None: idxs = range(self.n)\n",
    "        if self.shuffle: idxs = (idx for idx in self.shuffle_fn(idxs))\n",
    "        return idxs\n",
    "    \n",
    "    def create_batch(self, start_idx):\n",
    "        return self.dset[start_idx: min(start_idx+self.bs, self.dset.shape[0])]\n",
    "        # if self.device: to_device(btch, self.device)\n",
    "        # return btch\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.ceil((np.ceil(self.dataset.shape[1]/self.sl) * self.lbs_chunks)/self.bs))\n",
    "    \n",
    "    def before_iter(self):\n",
    "        # shuffling\n",
    "        randperm = torch.randint(low=0, high=self.dataset.shape[1], size=(self.dataset.shape[1],))\n",
    "        self.dataset = self.dataset[:, randperm]\n",
    "        # self.lbs_chunks = 4\n",
    "        size_of_dim0 = torch.ceil(self.dataset.new_empty(1).fill_(self.dataset.shape[0]/self.lbs_chunks)).item()\n",
    "        pad_len_dim0 = int(self.lbs_chunks * np.floor(self.dataset.shape[0]/self.lbs_chunks) + self.lbs_chunks - self.dataset.shape[0])\n",
    "        self.dataset_pad = F.pad(self.dataset, (0,0,0,0,0,pad_len_dim0), value=-1)\n",
    "\n",
    "        trn_sqs = list(torch.split(self.dataset_pad, split_size_or_sections=self.sl, dim=1))\n",
    "        test_eq(len(trn_sqs), np.ceil(self.dataset_pad.shape[1]/self.sl))\n",
    "        test_eq(trn_sqs[-1].shape, (self.dataset_pad.shape[0], self.dataset_pad.shape[1]%self.sl,4))\n",
    "        deficit = self.sl - trn_sqs[-1].shape[1]\n",
    "        if deficit: \n",
    "            test_eq(trn_sqs[-1].shape, (self.dataset_pad.shape[0], self.dataset_pad.shape[1]%self.sl,4));\n",
    "            # trn_sqs[-1] = torch.concat((trn_sqs[-1], self.dataset_pad.new_empty((trn_sqs[-1].shape[0], deficit,3)).fill_(-1)), dim=1)\n",
    "            trn_sqs[-1] = trn_sqs[-1].repeat_interleave(self.sl//trn_sqs[-1].shape[1], dim=1)\n",
    "        test_eq(trn_sqs[-1].shape, (self.dataset_pad.shape[0], self.sl,4));\n",
    "        # self.dset = torch.concat(trn_sqs)\n",
    "        # self.dset = torch.stack(trn_sqs)\n",
    "        \n",
    "        trn_sqs = map(partial(torch.chunk, chunks=self.lbs_chunks), trn_sqs)\n",
    "        trn_sqs = itertools.chain.from_iterable(trn_sqs)\n",
    "        self.dset = trn_sqs\n",
    "        # test_eq(self.dset.shape, (self.dataset_pad.shape[0]*len(trn_sqs), self.sl, 3))\n",
    "        # test_eq(self.dset.shape, (len(trn_sqs), self.dataset_pad.shape[0], self.sl, 3))\n",
    "        # print(f\"{self.dset.shape=}\")\n",
    "        # yield from (btch for btch in dset.split(self.bs))\n",
    "    \n",
    "    def create_batches(self, samps):\n",
    "            # trn_sqs = list(torch.split(self.dataset, split_size_or_sections=self.sl, dim=1))\n",
    "            # test_eq(len(trn_sqs), np.ceil(self.dataset.shape[1]/self.sl))\n",
    "            # test_eq(trn_sqs[-1].shape, (self.dataset.shape[0], self.dataset.shape[1]%self.sl,3))\n",
    "            # deficit = self.sl - trn_sqs[-1].shape[1]\n",
    "            # if deficit: \n",
    "            #     test_eq(trn_sqs[-1].shape, (self.dataset.shape[0], self.dataset.shape[1]%self.sl,3));\n",
    "            #     trn_sqs[-1] = torch.concat((trn_sqs[-1], self.dataset.new_empty((trn_sqs[-1].shape[0], deficit,3)).fill_(-1)), dim=1)\n",
    "            # test_eq(trn_sqs[-1].shape, (self.dataset.shape[0], self.sl,3));\n",
    "            # # self.dset = torch.concat(trn_sqs)\n",
    "            # self.dset = torch.stack(trn_sqs)\n",
    "            # # test_eq(self.dset.shape, (self.dataset.shape[0]*len(trn_sqs), self.sl, 3))\n",
    "            # test_eq(self.dset.shape, (len(trn_sqs), self.dataset.shape[0], self.sl, 3))\n",
    "            # print(f\"{self.dset.shape=}\")\n",
    "            # # yield from (btch for btch in dset.split(self.bs))\n",
    "        # chunks = range(0, self.dset.shape[0], self.bs)\n",
    "        # with ProcessPoolExecutor(self.n_workers) as ex:\n",
    "        # with Pool(processes=self.num_workers) as pool:\n",
    "        # yield from pool.imap_unordered(self.create_batch, chunks, 16)\n",
    "        # yield from map(self.create_batch, chunks)\n",
    "        # yield from chunked(self.dset, chunk_sz=self.bs)\n",
    "        yield from (torch.stack(btch) for btch in self.chunkify(self.dset))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b1a0ee-56eb-40d9-aafe-6c69d791a922",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cc98c0-b181-48ce-9028-6ee2f18d0947",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
