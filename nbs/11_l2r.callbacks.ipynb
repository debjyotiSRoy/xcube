{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "32b4035f-c7eb-44fb-b460-838432fbdb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "! [ -e /content ] && pip install -Uqq xcube # upgrade xcube on colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d1927335-90e5-4bf8-addd-6d19996bcf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp l2r.callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "88f55fa0-dbb1-446d-a10a-c9e69f688478",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from fastai.torch_imports import *\n",
    "from fastai.torch_core import *\n",
    "from fastai.callback.core import *\n",
    "from fastcore.all import *\n",
    "from xcube.imports import *\n",
    "from xcube.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8eb18b39-1d55-4525-a7c3-a7fa9271d518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a318d28d-0a96-4bee-950f-ea97cb3ef961",
   "metadata": {},
   "source": [
    "# L2R Callbacks\n",
    "\n",
    "> General purpose callbacks needed for L2R learner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328a9b30-1517-4f65-8d07-ee97e9c2889c",
   "metadata": {},
   "source": [
    "## Essential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0955d7fb-dbdb-4cab-80be-89a945981498",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TrainEval(Callback):\n",
    "    \"`Callback` that tracks the number of iterations done and properly sets training/eval mode\"\n",
    "    order, run_valid = -10, False\n",
    "    \n",
    "    def before_fit(self):\n",
    "        \"Set the iter and epoch counters to 0\"\n",
    "        self.learn.epoch = 0\n",
    "        self.learn.train_iter,self.learn.pct_train = 0,0.\n",
    "        \n",
    "    def after_batch(self):\n",
    "        \"Update the iter counter (in training mode)\"\n",
    "        if not self.model.training: return\n",
    "        self.learn.pct_train += 1./(self.n_iter*self.n_epochs)\n",
    "        self.learn.train_iter += 1\n",
    "        \n",
    "    def before_train(self):\n",
    "        \"Set the model to training mode\"\n",
    "        self.learn.pct_train=self.epoch/self.n_epochs\n",
    "        self.model.train()\n",
    "        self.learn.training=True\n",
    "        \n",
    "    def before_validate(self):\n",
    "        \"Set the model to validation mode\"\n",
    "        self.model.eval()\n",
    "        self.learn.training=False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddef4bd7-b8a8-4443-a257-5112a8d8035e",
   "metadata": {},
   "source": [
    "## Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b6e8076a-5c29-428c-8d0a-1de2b9689956",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from fastai.learner import Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "169635c0-1fff-47ff-81ff-99acabd3100c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class AvgSmoothMetric(Metric):\n",
    "    \"Smooth average of the losses (exponentially weighted with `beta`)\"\n",
    "    def __init__(self, beta=0.98): self.beta = beta\n",
    "    def reset(self):               self.count,self.val = 0,tensor(0.)\n",
    "    def accumulate(self, learn):\n",
    "        self.count += 1\n",
    "        self.val = torch.lerp(to_detach(learn.moi.mean()), self.val, self.beta)\n",
    "    @property\n",
    "    def value(self): return self.val/(1-self.beta**self.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "268c8e7c-4aa3-481e-a7e7-8249e7dd1951",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _TstLearner(): pass\n",
    "learn = _TstLearner()\n",
    "_data = torch.randn(24)\n",
    "\n",
    "tst = AvgSmoothMetric()\n",
    "tst.reset()\n",
    "val = tensor(0.)\n",
    "for i,o in enumerate(_data): \n",
    "    learn.moi = o\n",
    "    tst.accumulate(learn)\n",
    "    val = val*0.98 + o*(1-0.98)\n",
    "    test_close(val/(1-0.98**(i+1)), tst.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "from fastprogress.core import format_time  \n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "453b888a-d274-4bb2-b9e1-0ce310a5367d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TrackResults(Callback):\n",
    "    def __init__(self, train_metrics=False, beta=0.98): \n",
    "        store_attr()\n",
    "        self.names = ['loss', 'ndcg', 'ndcg_at_6', 'acc']\n",
    "        self.metric_names = L(self.names).map(lambda o: 'val_'+o)\n",
    "        if train_metrics: self.metric_names = L(self.names).map(lambda o: 'train_'+o) + self.metric_names\n",
    "        self.metric_names.append('time')\n",
    "        self.metric_names = L('epoch') + self.metric_names\n",
    "        self.smooth_moi = AvgSmoothMetric(beta=beta) # to maintain weighted avg of metric of interest \n",
    "    \n",
    "    def before_fit(self): \n",
    "        self.lrs, self.mois, self.smooth_mois, self.losses_full, self.grads_full, self.metrics_full = [], [], [], [], defaultdict(list), defaultdict(list) \n",
    "        self.ioi = self.names.index(self.learn.save_call_back.monitor\n",
    "                                  if hasattr(self.learn, 'save_call_back') else 'loss')\n",
    "        self.smooth_moi.reset()\n",
    "    \n",
    "    def before_train(self): self._initialise_metrics()\n",
    "    \n",
    "    def before_validate(self): self._initialise_metrics()\n",
    "        \n",
    "    def after_train(self):\n",
    "        self.losses_full.extend(self.losses)\n",
    "        log = self._compute_epoch_mean()\n",
    "        if self.train_metrics:\n",
    "            self.metrics_full['trn'].append(log)\n",
    "        # print(self.epoch, self.model.training, *log)\n",
    "        self.log+=log\n",
    "                \n",
    "    def after_validate(self):\n",
    "        log = self._compute_epoch_mean()\n",
    "        if hasattr(self, 'metrics_full'):\n",
    "            self.metrics_full['val'].append(log)\n",
    "        # print(self.epoch if hasattr(self, 'epoch') else 0, self.model.training, *log)\n",
    "        if getattr(self, 'log', False): self.log+=log\n",
    "\n",
    "    def before_epoch(self):\n",
    "        self.start_epoch = time.time()\n",
    "        self.log = L(getattr(self, 'epoch', 0))\n",
    "\n",
    "    def after_epoch(self): \n",
    "        self.log.append(format_time(time.time() - self.start_epoch))\n",
    "        # import pdb; pdb.set_trace()\n",
    "        self.logger(self.log)\n",
    "            \n",
    "    def _compute_epoch_mean(self):\n",
    "        _li = [self.losses, self.ndcgs, self.ndcgs_at_6, self.accs]\n",
    "        _li = [torch.stack(o) if o else torch.Tensor() for o in _li] \n",
    "        [self.losses, self.ndcgs, self.ndcgs_at_6, self.accs] = _li\n",
    "        log = [round(o.mean().item(), 4) if o.sum() else \"NA\" for o in _li]\n",
    "        return log\n",
    "    \n",
    "    def _initialise_metrics(self): self.losses, self.ndcgs, self.ndcgs_at_6, self.accs = [], [], [], []\n",
    "\n",
    "    \n",
    "    def after_batch(self):\n",
    "        with torch.no_grad():\n",
    "            loss = self.loss_func(self.preds, self.xb)\n",
    "            if loss.mean().isnan(): \n",
    "                import pdb; pdb.set_trace()\n",
    "            self.losses.append(loss.mean())\n",
    "            if self.model.training:\n",
    "                if self.train_metrics: \n",
    "                    self._compute_metrics()\n",
    "                    # grab moi\n",
    "                    self.learn.moi = getattr(self, \"losses ndcgs ndcgs_at_6 accs\".split()[self.ioi])[-1] # -1 is to get the value for the current batch\n",
    "                    # do weighted avg\n",
    "                    self.smooth_moi.accumulate(self.learn)\n",
    "                    # record `learn` attribute\n",
    "                    self.learn.smooth_moi = self.smooth_moi.value\n",
    "                    # record moi attribute in `self`\n",
    "                    self.mois.append(self.learn.moi)\n",
    "                    # record smooth_moi attribute in `self`\n",
    "                    self.smooth_mois.append(self.learn.smooth_moi)\n",
    "                    # record lr in `self`\n",
    "                    self.lrs.append(self.opt.hypers[-1]['lr'])\n",
    "            else: self._compute_metrics()\n",
    "                        \n",
    "    def _compute_metrics(self):\n",
    "        *_, _ndcg, _ndcg_at_k = ndcg(self.preds, self.xb, k=6)\n",
    "        self.ndcgs.append(_ndcg.mean())\n",
    "        self.ndcgs_at_6.append(_ndcg_at_k.mean())\n",
    "        acc = accuracy(self.xb, self.model).mean()\n",
    "        self.accs.append(acc.mean())\n",
    "        \n",
    "    def after_backward(self):\n",
    "        for name,param in self.model.named_parameters():\n",
    "            grad = param.grad.data.detach().clone()\n",
    "            self.grads_full[name].append(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "824872d8-8fc0-4d09-8571-b6ac9681289c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Monitor(Callback):\n",
    "    order = 60\n",
    "    def __init__(self, monitor='ndcg_at_6', comp=None, min_delta=0., reset_on_fit=False):\n",
    "        if comp is None: comp = np.greater\n",
    "        if comp == np.less: min_delta *= -1\n",
    "        # store_attr()\n",
    "        self.monitor,self.comp,self.min_delta,self.reset_on_fit,self.best= monitor,comp,min_delta,reset_on_fit,None\n",
    "       \n",
    "    def before_fit(self):\n",
    "        if self.reset_on_fit or self.best is None: self.best = float('inf') if self.comp == np.less else -float('inf')\n",
    "        assert self.monitor in self.track_results.names\n",
    "        self.idx = list(self.track_results.names).index(self.monitor)\n",
    "        \n",
    "    def after_epoch(self):\n",
    "        if self.track_results.metrics_full.get('val') is None: return \n",
    "        val = self.track_results.metrics_full.get('val')[-1][self.idx]\n",
    "        if self.comp(val - self.min_delta if not isinstance(val, str) else self.best, self.best): self.best, self.new_best, = val, True\n",
    "        else: self.new_best = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f58b91-7034-4827-ab08-dc758185c922",
   "metadata": {},
   "source": [
    "## ParamScheduler -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b2fdc9a7-f87c-43eb-b988-338ed317faab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@docs\n",
    "class XParamScheduler(Callback):\n",
    "    \"Schedule hyper-parameters according to `scheds`\"\n",
    "    order,run_valid = 60,False\n",
    "\n",
    "    def __init__(self, scheds): self.scheds = scheds\n",
    "    def before_fit(self): self.hps = {p:[] for p in self.scheds.keys()}\n",
    "    def before_batch(self): \n",
    "        if not self.model.training: return\n",
    "        self._update_val(self.pct_train)\n",
    "\n",
    "    def _update_val(self, pct):\n",
    "        for n,f in self.scheds.items(): self.opt.set_hyper(n, f(pct))\n",
    "\n",
    "    def after_batch(self):\n",
    "        if not self.model.training: return\n",
    "        for p in self.scheds.keys(): self.hps[p].append(self.opt.hypers[-1][p])\n",
    "\n",
    "    def after_fit(self):\n",
    "        if hasattr(self.learn, 'track_results') and hasattr(self, 'hps'): self.track_results.hps = self.hps\n",
    "\n",
    "    _docs = {\"before_fit\": \"Initialize container for hyper-parameters\",\n",
    "             \"before_batch\": \"Set the proper hyper-parameters in the optimizer\",\n",
    "             \"after_batch\": \"Record hyper-parameters of this batch\",\n",
    "             \"after_fit\": \"Save the hyper-parameters in the track_results if there is one\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216f054b-a981-48fe-aa30-d5dcc87dc102",
   "metadata": {},
   "source": [
    "`scheds` is a dictionary with one key for each hyper-parameter you want to schedule, with either a scheduler or a list of schedulers as values (in the second case, the list must have the same length as the the number of parameters groups of the optimizer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f438e04a-18f3-4a76-a669-bcea0493f94e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/debjyotiSRoy/xcube/blob/main/xcube/l2r/callbacks.py#L161){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### XParamScheduler.before_fit\n",
       "\n",
       ">      XParamScheduler.before_fit ()\n",
       "\n",
       "Initialize container for hyper-parameters"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/debjyotiSRoy/xcube/blob/main/xcube/l2r/callbacks.py#L161){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### XParamScheduler.before_fit\n",
       "\n",
       ">      XParamScheduler.before_fit ()\n",
       "\n",
       "Initialize container for hyper-parameters"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(XParamScheduler.before_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c205b264-e804-42f1-8e0f-04b631751463",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/debjyotiSRoy/xcube/blob/main/xcube/l2r/callbacks.py#L162){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### XParamScheduler.before_batch\n",
       "\n",
       ">      XParamScheduler.before_batch ()\n",
       "\n",
       "Set the proper hyper-parameters in the optimizer"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/debjyotiSRoy/xcube/blob/main/xcube/l2r/callbacks.py#L162){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### XParamScheduler.before_batch\n",
       "\n",
       ">      XParamScheduler.before_batch ()\n",
       "\n",
       "Set the proper hyper-parameters in the optimizer"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(XParamScheduler.before_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9e21d6ad-402b-400b-8f4d-44392575822d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/debjyotiSRoy/xcube/blob/main/xcube/l2r/callbacks.py#L169){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### XParamScheduler.after_batch\n",
       "\n",
       ">      XParamScheduler.after_batch ()\n",
       "\n",
       "Record hyper-parameters of this batch"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/debjyotiSRoy/xcube/blob/main/xcube/l2r/callbacks.py#L169){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### XParamScheduler.after_batch\n",
       "\n",
       ">      XParamScheduler.after_batch ()\n",
       "\n",
       "Record hyper-parameters of this batch"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(XParamScheduler.after_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bd9b0e74-89d6-4952-a3c6-837b25ede827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/debjyotiSRoy/xcube/blob/main/xcube/l2r/callbacks.py#L173){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### XParamScheduler.after_fit\n",
       "\n",
       ">      XParamScheduler.after_fit ()\n",
       "\n",
       "Save the hyper-parameters in the track_results if there is one"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/debjyotiSRoy/xcube/blob/main/xcube/l2r/callbacks.py#L173){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### XParamScheduler.after_fit\n",
       "\n",
       ">      XParamScheduler.after_fit ()\n",
       "\n",
       "Save the hyper-parameters in the track_results if there is one"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(XParamScheduler.after_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d4b046df-51e2-400d-8cb7-12ee7a795640",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from xcube.l2r.learner import L2RLearner\n",
    "from fastai.callback.schedule import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "91226f9d-a5ad-4e0d-95d6-1aa06d2a0185",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@patch\n",
    "def fit_one_cycle(self:L2RLearner, n_epoch, lr_max=None, div=25., div_final=1e5, pct_start=0.25, moms=None, cbs=None, start_epoch=0):\n",
    "    \"Fit `self.model` for `n_epoch` using the 1cycle policy.\"\n",
    "    self.lr_max = lr_max\n",
    "    self.opt = getattr(self, 'opt', None)\n",
    "    if self.opt is None: self.create_opt()\n",
    "    self.opt.set_hyper('lr', self.lr if lr_max is None else lr_max)\n",
    "    lr_max = np.array([h['lr'] for h in self.opt.hypers])\n",
    "    scheds = {'lr': combined_cos(pct_start, lr_max/div, lr_max, lr_max/div_final),\n",
    "              'mom': combined_cos(pct_start, *(self.moms if moms is None else moms))}\n",
    "    # self.add_cb(XParamScheduler(scheds)) \n",
    "    # pdb.set_trace()\n",
    "    self.fit(n_epoch, cbs=XParamScheduler(scheds)+L(cbs), start_epoch=start_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68dd9503-93d5-45d6-9ce4-5fa2c5be9b1e",
   "metadata": {},
   "source": [
    "The 1cycle policy was introduced by Leslie N. Smith et al. in [Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates](https://arxiv.org/abs/1708.07120). It schedules the learning rate with a cosine annealing from `lr_max/div` to `lr_max` then `lr_max/div_final` (pass an array to `lr_max` if you want to use differential learning rates) and the momentum with cosine annealing according to the values in `moms`. The first phase takes `pct_start` of the training. You can optionally pass additional `cbs` and `reset_opt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4ecdb3bc-7473-4b26-b805-dfaf128f41f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@patch\n",
    "def plot_sched(self:TrackResults, keys=None, figsize=None):\n",
    "    keys = self.hps.keys() if keys is None else L(keys)\n",
    "    rows,cols = (len(keys)+1)//2, min(2, len(keys))\n",
    "    figsize = figsize or (6*cols,4*rows)\n",
    "    _, axs = plt.subplots(rows, cols, figsize=figsize)\n",
    "    axs = axs.flatten() if len(keys) > 1 else L(axs)\n",
    "    for p,ax in zip(keys, axs):\n",
    "        ax.plot(self.hps[p])\n",
    "        ax.set_ylabel(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ff6c3dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@patch\n",
    "def fit_sgdr(self:L2RLearner, n_cycles, cycle_len, lr_max=None, cycle_mult=2, cbs=None, reset_opt=False, wd=None,\n",
    "             start_epoch=0):\n",
    "    \"Fit `self.model` for `n_cycles` of `cycle_len` using SGDR.\"\n",
    "    self.lr_max = lr_max\n",
    "    self.opt = getattr(self, 'opt', None)\n",
    "    if self.opt is None: self.create_opt()\n",
    "    self.opt.set_hyper('lr', self.lr if lr_max is None else lr_max)\n",
    "    lr_max = np.array([h['lr'] for h in self.opt.hypers])\n",
    "    n_epoch = cycle_len * (cycle_mult**n_cycles-1)//(cycle_mult-1)\n",
    "    pcts = [cycle_len * cycle_mult**i / n_epoch for i in range(n_cycles)]\n",
    "    scheds = [SchedCos(lr_max, 0) for _ in range(n_cycles)]\n",
    "    scheds = {'lr': combine_scheds(pcts, scheds)}\n",
    "    self.fit(n_epoch, cbs=ParamScheduler(scheds)+L(cbs), reset_opt=reset_opt, wd=wd, start_epoch=start_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9957eda0",
   "metadata": {},
   "source": [
    "This schedule was introduced by Ilya Loshchilov et al. in [SGDR: Stochastic Gradient Descent with Warm Restarts](https://arxiv.org/abs/1608.03983). It consists of `n_cycles` that are cosine annealings from `lr_max` (defaults to the `Learner` lr) to 0, with a length of `cycle_len * cycle_mult**i` for the `i`-th cycle (first one is `cycle_len`-long, then we multiply the length by `cycle_mult` at each epoch). You can optionally pass additional `cbs` and `reset_opt`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c424e0-ad13-43ef-877a-719db5e526ef",
   "metadata": {},
   "source": [
    "## XLRFind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d81560c0-66bc-4396-8d35-f1fdca6cd2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@docs\n",
    "class XLRFinder(XParamScheduler):\n",
    "    \"Training with exponentially growing learning rate\"\n",
    "    run_after=TrackResults\n",
    "    def __init__(self, start_lr=1e-7, end_lr=10, num_it=100, stop_div=True):\n",
    "        if num_it < 6: num_it = 6\n",
    "        self.scheds = {'lr': [SchedExp(s, e) for (s,e) in zip(start_lr,end_lr)\n",
    "                             ] if is_listy(start_lr) else SchedExp(start_lr, end_lr)}\n",
    "        self.num_it,self.stop_div = num_it,stop_div\n",
    "\n",
    "    def before_fit(self):\n",
    "        if hasattr(self, 'track_results'):\n",
    "            moi = self.track_results.names[self.track_results.ioi]\n",
    "            print(f'Smoothing {moi}')\n",
    "        super().before_fit()\n",
    "        path = self.path\n",
    "        path.mkdir(parents=True, exist_ok=True)\n",
    "        self.tmp_d = tempfile.TemporaryDirectory(dir=path)\n",
    "        self.tmp_p = Path(self.tmp_d.name).stem\n",
    "        self.learn.save(f'{self.tmp_p}/_tmp')\n",
    "        self.best_moi = -float('inf')\n",
    "\n",
    "    def before_batch(self): self._update_val(self.train_iter/self.num_it)\n",
    "\n",
    "    def after_batch(self):\n",
    "        super().after_batch()\n",
    "        if self.smooth_moi > self.best_moi: self.best_moi = self.smooth_moi\n",
    "        if 4*self.smooth_moi < self.best_moi and self.stop_div: raise CancelFitException()\n",
    "        if self.train_iter >= self.num_it: raise CancelFitException()\n",
    "\n",
    "    def before_validate(self): raise CancelValidException()\n",
    "\n",
    "    def after_fit(self):\n",
    "        self.learn.opt.zero_grad() # Needed before detaching the optimizer for future fits\n",
    "        tmp_f = self.path/self.tmp_p/'_tmp.pth'\n",
    "        if tmp_f.exists():\n",
    "            self.learn.load(f'{self.tmp_p}/_tmp', with_opt=True)\n",
    "            self.tmp_d.cleanup()\n",
    "            \n",
    "    def after_cancel_validate(self): pass\n",
    "\n",
    "    _docs = {\"before_fit\": \"Initialize container for hyper-parameters and save the model\",\n",
    "             \"before_batch\": \"Set the proper hyper-parameters in the optimizer\",\n",
    "             \"after_batch\": \"Record hyper-parameters of this batch and potentially stop training\",\n",
    "             \"after_fit\": \"Save the hyper-parameters in the recorder if there is one and load the original model\",\n",
    "             \"before_validate\": \"Skip the validation part of training\",\n",
    "             \"after_cancel_validate\": \"pass `CancelValidException`\",\n",
    "             \"run_after\":\"\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "821b3070-6d95-4a36-aa0f-1812e9e016f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def plot_xlr_find(self:TrackResults, skip_end=5, return_fig=True, suggestions=None, nms=None, **kwargs):\n",
    "    \"Plot the result of an LR Finder test (won't work if you didn't do `learn.xlr_find()` before)\"\n",
    "    lrs  = self.lrs  if skip_end==0 else self.lrs[:-skip_end]\n",
    "    mois = L(self.mois if skip_end==0 else self.mois[:-skip_end]).map(Tensor.cpu)\n",
    "    smooth_mois = L(self.smooth_mois if skip_end==0 else self.smooth_mois[:-skip_end]).map(Tensor.cpu)\n",
    "    fig, (ax1, ax2) = plt.subplots(1,2)\n",
    "    ax1.plot(lrs, smooth_mois)\n",
    "    ax1.set_ylabel(\"Smoothened MOI\")\n",
    "    ax1.set_xlabel(\"Learning Rate\")\n",
    "    ax1.set_xscale('log')\n",
    "    if suggestions:\n",
    "        colors = plt.rcParams['axes.prop_cycle'].by_key()['color'][1:]\n",
    "        for (val, idx), nm, color in zip(suggestions, nms, colors):\n",
    "            ax1.plot(val, idx, 'o', label=nm, c=color)\n",
    "        ax1.legend(loc='best')\n",
    "    ax2.plot(range(len(mois)), mois, label='MOI')\n",
    "    ax2.plot(range(len(smooth_mois)), smooth_mois, label='SMOI')\n",
    "    ax2.legend(loc='best')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f2aab344-52bd-467f-a5c7-2e59c00199d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def xrl_find(self:L2RLearner, start_lr=1e-5, end_lr=1e-1, num_it=400, stop_div=True, show_plot=True, suggest_funcs=(valley,)):\n",
    "    \"Launch a mock training to find a good learning rate and return suggestions based on `suggest_funcs` as a named tuple\"\n",
    "    n_epochs = num_it//len(self.dls.train) + 1\n",
    "    cb=XLRFinder(start_lr=start_lr, end_lr=end_lr, num_it=num_it, stop_div=stop_div)\n",
    "    self.fit(n_epochs, cbs=cb)\n",
    "    if suggest_funcs is not None:\n",
    "        lrs, smooth_mois = tensor(self.track_results.lrs[num_it//10:-5]),tensor(self.track_results.smooth_mois[num_it//10:-5])\n",
    "        nan_idxs = torch.nonzero(torch.isnan(smooth_mois.view(-1)))\n",
    "        if len(nan_idxs):\n",
    "            drop_idx = min(nan_idxs)\n",
    "            lrs = lrs[:drop_idx]\n",
    "            smooth_mois = smooth_mois[:drop_idx]\n",
    "        _suggestions, nms = [], []\n",
    "        for func in tuplify(suggest_funcs):\n",
    "            nms.append(func.__name__ if not isinstance(func, partial) else func.func.__name__) # deal with partials\n",
    "            _suggestions.append(func(lrs, smooth_mois, num_it))\n",
    "            \n",
    "        # pdb.set_trace()\n",
    "        SuggestedLRs = collections.namedtuple('SuggestedLRs', nms)\n",
    "        lrs, pnts = [], []\n",
    "        for lr, pnt in _suggestions:\n",
    "            lrs.append(lr)\n",
    "            pnts.append(pnt)   \n",
    "        if show_plot: self.track_results.plot_xlr_find(suggestions=pnts, nms=nms)\n",
    "        return SuggestedLRs(*lrs)\n",
    "    \n",
    "    elif show_plot: self.track_results.plot_xlr_find()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16df8fce-77c7-4f8b-bfa6-d3dd6fb2650e",
   "metadata": {},
   "source": [
    "## Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "739fe6b6-c568-4a12-8aac-a628ddb772e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ProgressBarCallback(Callback):\n",
    "    order = 70\n",
    "    \n",
    "    def before_fit(self):\n",
    "        self.mbar = master_bar(range(self.n_epochs))\n",
    "        if self.learn.logger != noop:\n",
    "            self.old_logger,self.learn.logger = self.logger,self._write_stats\n",
    "            # metric_names = L(self.track_results.names).map(lambda o: 'train_'+o) + L(self.track_results.names).map(lambda o: 'val_'+o)\n",
    "            # metric_names.append('time')\n",
    "            # metric_names = L('epoch') + metric_names\n",
    "            self._write_stats(self.track_results.metric_names)\n",
    "        \n",
    "    def before_epoch(self):\n",
    "        if getattr(self, 'mbar', False): self.mbar.update(self.epoch)\n",
    "        \n",
    "    def before_train(self): self._launch_pbar()\n",
    "    \n",
    "    def before_validate(self): self._launch_pbar()\n",
    "        \n",
    "    def _launch_pbar(self):\n",
    "        self.pbar = progress_bar(self.dl, parent=getattr(self, 'mbar', None), leave=False)\n",
    "        self.pbar.update(0)\n",
    "        \n",
    "    def after_batch(self):\n",
    "        self.pbar.update(self.iter_num+1)\n",
    "    \n",
    "    def after_train(self):\n",
    "        self.pbar.on_iter_end()\n",
    "        \n",
    "    def after_validate(self):\n",
    "        self.pbar.on_iter_end()\n",
    "        \n",
    "    def after_fit(self):\n",
    "        if getattr(self, 'mbar', False):\n",
    "            self.mbar.on_iter_end()\n",
    "            delattr(self, 'mbar')\n",
    "        if hasattr(self, 'old_logger'): self.learn.logger = self.old_logger\n",
    "\n",
    "    def _write_stats(self, log):\n",
    "        if getattr(self, 'mbar', False): self.mbar.write([f'{l:.6f}' if isinstance(l, float) else str(l) for l in log], table=True)\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0fc213-020e-4650-acd9-9e026b4b4a38",
   "metadata": {},
   "source": [
    "## Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "384ef72f-0b72-4959-b329-bc78962b8a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SaveCallBack(Monitor):\n",
    "    order = Monitor.order+1\n",
    "    def __init__(self, \n",
    "        fname, \n",
    "        monitor='ndcg_at_6', \n",
    "        comp=None, \n",
    "        min_delta=0., \n",
    "        reset_on_fit=False,\n",
    "    ):\n",
    "        super().__init__(monitor=monitor, comp=comp, min_delta=min_delta, reset_on_fit=reset_on_fit)\n",
    "        self.last_saved_path = None\n",
    "        store_attr('fname')\n",
    "        \n",
    "    @property\n",
    "    def best(self): return self._best\n",
    "    @best.setter    \n",
    "    def best(self, b): self._best = b\n",
    "    \n",
    "    def after_epoch(self):\n",
    "        super().after_epoch()\n",
    "        if self.new_best:\n",
    "            print(f'Better model found at epoch {self.epoch} with {self.monitor} value: {self.best}.')\n",
    "            self.learn.save(self.fname)\n",
    "    \n",
    "    # def after_fit(self):\n",
    "        # if self.best_at_end: self.learn.load(self.fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resume Training From Checkpoint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class InterruptCallback(Callback):\n",
    "    def __init__(self, epoch):\n",
    "        self._interupt_before = epoch\n",
    "    def before_epoch(self):\n",
    "        if self.epoch == self._interupt_before:\n",
    "            raise CancelFitException"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bab2f2",
   "metadata": {},
   "source": [
    "## CSVLogging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "330f0298",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class CSVLog(Callback):\n",
    "    \"Log the results displayed in `learn.path/fname`\"\n",
    "    order=60\n",
    "    def __init__(self, fname='history.csv', append=False):\n",
    "        self.fname,self.append = Path(fname),append\n",
    "\n",
    "    def read_log(self):\n",
    "        \"Convenience method to quickly access the log.\"\n",
    "        return pd.read_csv(self.path/self.fname)\n",
    "\n",
    "    def before_fit(self):\n",
    "        \"Prepare file with metric names.\"\n",
    "        self.path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        self.file = (self.path/self.fname).open('a' if self.append else 'w')\n",
    "        self.file.write(','.join(self.track_results.metric_names) + '\\n')\n",
    "        self.old_logger,self.learn.logger = self.logger,self._write_line\n",
    "\n",
    "    def _write_line(self, log):\n",
    "        \"Write a line with `log` and call the old logger.\"\n",
    "        self.file.write(','.join([str(t) for t in log]) + '\\n')\n",
    "        self.file.flush()\n",
    "        os.fsync(self.file.fileno())\n",
    "        self.old_logger(log)\n",
    "\n",
    "    def after_fit(self):\n",
    "        \"Close the file and clean up.\"\n",
    "        self.file.close()\n",
    "        self.learn.logger = self.old_logger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0245cc6",
   "metadata": {},
   "source": [
    "## Mixed Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "616b7a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from torch.cuda.amp import GradScaler, autocast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0ceec180",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class AMPMode(Enum):\n",
    "    \"Automatic mixed precision modes for ease of completion\"\n",
    "    FP16 = 'fp16'\n",
    "    BF16 = 'bf16'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TestCallback(Callback):\n",
    "    order = 1000\n",
    "    def before_batch(self):\n",
    "        # import pdb; pdb.set_trace()\n",
    "        ic(self.opt.hypers)\n",
    "    # def after_epoch(self):\n",
    "        # ic(self.opt.hypers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MemoryErrorTest(Callback):\n",
    "    order=SaveCallBack.order-1\n",
    "    def after_batch(self):\n",
    "        if self.iter_num == 4:\n",
    "            # raise torch.cuda.OutOfMemoryError(\"Memory !!!\")\n",
    "            raise CancelTrainException()\n",
    "    def after_cancel_train(self): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8d0a94f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@delegates(GradScaler)\n",
    "class MixedPrecision(Callback):\n",
    "    \"Mixed precision training using Pytorch's Automatic Mixed Precision (AMP)\"\n",
    "    order = 10\n",
    "    def __init__(self,\n",
    "        amp_mode:str|AMPMode=AMPMode.FP16, # Mixed Precision training mode. Supports fp16 and bf16.\n",
    "        **kwargs\n",
    "    ):\n",
    "        amp_mode = AMPMode(amp_mode)\n",
    "        store_attr(names='amp_mode')\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "    def before_fit(self):\n",
    "        if self.amp_mode == AMPMode.BF16:\n",
    "            if torch.cuda.is_available() and not torch.cuda.is_bf16_supported():\n",
    "                raise ValueError(\"Unsupported GPU for bfloat16 mixed precision training\")\n",
    "            dtype = torch.bfloat16\n",
    "        elif self.amp_mode == AMPMode.FP16:\n",
    "            dtype = torch.float16\n",
    "        else:\n",
    "            raise ValueError(f\"Unrecognized precision: {self.amp_mode}\")\n",
    "        # `GradScaler` is not needed for bfloat16 as fp32 and bf16 have the same range\n",
    "        self.kwargs['enabled'] = dtype == torch.float16\n",
    "        self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype), GradScaler(**self.kwargs),L()\n",
    "\n",
    "    def before_batch(self): \n",
    "        if self.model.training: \n",
    "            self.autocast.__enter__()\n",
    "    def after_pred(self):\n",
    "        pass\n",
    "        # assert self.learn.preds.dtype == torch.float16\n",
    "        # self.learn.preds = to_float(self.preds)\n",
    "    def after_loss(self): self.autocast.__exit__(None, None, None)\n",
    "    def before_backward(self): \n",
    "        # pass\n",
    "        # import pdb; pdb.set_trace()\n",
    "        # self.learn.preds = self.scaler.scale(self.preds)\n",
    "        assert self.learn.preds.dtype == torch.float16\n",
    "        # assert self.learn.lambda_i.dtype == torch.float16\n",
    "        assert self.learn.srtd_preds.dtype == torch.float16\n",
    "        self.learn.srtd_preds = self.scaler.scale(self.srtd_preds)\n",
    "        # self.learn.lambda_i = self.scaler.scale(self.lambda_i)\n",
    "    # def after_backward(self):\n",
    "    #     print(self.track_results.grads_full['token_factors.weight']\n",
    "    #     import pdb; pdb.set_trace()\n",
    "    def before_step(self):\n",
    "        \"Use `self` as a fake optimizer. `self.skipped` will be set to True `after_step` if gradients overflow.\"\n",
    "        self.skipped=True\n",
    "        self.scaler.step(self)\n",
    "        if self.skipped: raise CancelStepException()\n",
    "        self.scales.append(self.scaler.get_scale())\n",
    "        # pass\n",
    "    def after_step(self): self.learn.scaler.update()\n",
    "    def after_cancel_step(self): pass\n",
    "    def after_fit(self): self.autocast,self.learn.scaler,self.scales = None,None,None\n",
    "\n",
    "    @property\n",
    "    def param_groups(self):\n",
    "        \"Pretend to be an optimizer for `GradScaler`\"\n",
    "        return self.opt.param_groups\n",
    "    def step(self, *args, **kwargs):\n",
    "        \"Fake optimizer step to detect whether this batch was skipped from `GradScaler`\"\n",
    "        self.skipped=False\n",
    "    def after_epoch(self): \n",
    "        torch.cuda.empty_cache()\n",
    "    def after_batch(self): \n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7d8a932e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates some tensors in default dtype (here assumed to be float32)\n",
    "a_float32 = torch.rand((2, 8, 8), device=\"cuda\")\n",
    "b_float32 = torch.rand((2, 8, 8), device=\"cuda\")\n",
    "c_float32 = torch.rand((8, 8), device=\"cuda\")\n",
    "d_float32 = torch.rand((8, 8), device=\"cuda\")\n",
    "\n",
    "with torch.autocast(device_type=\"cuda\"):\n",
    "    # torch.mm is on autocast's list of ops that should run in float16.\n",
    "    # Inputs are float32, but the op runs in float16 and produces float16 output.\n",
    "    # No manual casts are required.\n",
    "    e_float16 = torch.bmm(a_float32, b_float32)\n",
    "    # Also handles mixed input types\n",
    "    # f_float16 = torch.mm(d_float32, e_float16)\n",
    "\n",
    "# After exiting autocast, calls f_float16.float() to use with d_float32\n",
    "# g_float32 = torch.mm(d_float32, f_float16.float())\n",
    "# h_float32 = torch.mm(c_float32, d_float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GradientClip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class GradientClip(Callback):\n",
    "    \"Clip norm of gradients\"\n",
    "    order=MixedPrecision.order+1\n",
    "    def __init__(self,max_norm:float=1., norm_type:float=2.0): store_attr()\n",
    "    def before_step(self): \n",
    "        # import pdb; pdb.set_trace()\n",
    "        nn.utils.clip_grad_norm_(self.model.parameters(), self.max_norm, self.norm_type)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f9428d-6d4b-4f18-a1a2-e4a963e011b4",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "294269ac-6a2f-4bc0-8160-7e4ea3cab241",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
