{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d488bfb2-194f-4ee5-bc10-e174a54421a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "! [ -e /content ] && pip install -Uqq xcube # upgrade xcube on colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64cdf2d-31dc-489f-9e65-1f660ce61285",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp l2r.data.info_gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c762a07-0c49-4317-a3e9-9161bf2c52b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from fastcore.basics import *\n",
    "from fastai.torch_core import *\n",
    "from fastai.data.core import *\n",
    "from fastai.data.transforms import *\n",
    "from fastai.text.core import *\n",
    "from fastai.text.data import *\n",
    "from xcube.imports import *\n",
    "from xcube.torch_imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358429e4-97df-483b-afe6-a33dc142962a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00877f46-6b01-410b-8cce-5b05f3ca7dae",
   "metadata": {},
   "source": [
    "# Information Gain\n",
    "\n",
    "> Computation of mutual information gain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2885474a-39cf-4f8a-981b-5ea302712cea",
   "metadata": {},
   "source": [
    "This module contains the all classes and functions needed to compute mutual information gain for the tokens and labels. This mutual information is then used to bootstrap a L2R model from xml text data. Please follow the [booting tutorial](14_tutorial.boot_l2r.ipynb) to understand how this module is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e7f27f-8c36-431e-8221-9d38536261f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BatchLbsChunkify(ItemTransform):\n",
    "    order = 100\n",
    "    def __init__(self, chnk_st, chnk_end): store_attr('chnk_st,chnk_end')\n",
    "    def encodes(self, x): \n",
    "        return (x[0], x[1][:, self.chnk_st:self.chnk_end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c992082a-2dae-41d7-8de7-886e1d59ffd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MutualInfoGain:\n",
    "    def __init__(self, df, bs=8, chnk_sz=200, device=None): store_attr('df,bs,chnk_sz,device')\n",
    "    \n",
    "    def onehotify(self):\n",
    "        x_tfms = [Tokenizer.from_df('text', n_workers=num_cpus()), attrgetter(\"text\"), Numericalize(), OneHotEncode()]\n",
    "        y_tfms = [ColReader('labels', label_delim=';'), MultiCategorize(), OneHotEncode()]\n",
    "        tfms = [x_tfms, y_tfms]\n",
    "        self.dsets = Datasets(self.df, tfms=[x_tfms, y_tfms], )\n",
    "        self.toksize, self.lblsize = self.dsets.vocab.map(len)\n",
    "        return self.dsets\n",
    "        \n",
    "    def lbs_chunked(self):\n",
    "        lbs = self.dsets.vocab[1]\n",
    "        self.dls = []\n",
    "        for chnk_st in range(0, len(lbs), self.chnk_sz):\n",
    "            self.dls.append(TfmdDL(self.dsets, bs=self.bs, \n",
    "                              after_batch=[BatchLbsChunkify(chnk_st, min(chnk_st+self.chnk_sz, len(lbs)))], \n",
    "                              device=default_device() if self.device is None else self.device))\n",
    "        return self.dls\n",
    "    \n",
    "    def _mutual_info_gain(self, dl):\n",
    "        \"\"\"\n",
    "        Computes [mutual information gain](https://en.wikipedia.org/wiki/Mutual_information) for each token label pair\n",
    "        `dl` is (bag-of-words text, one-hot encoded targets)\n",
    "        \"\"\"\n",
    "        xb, yb = dl.one_batch() \n",
    "        toksize, lblsize = xb.size(1), yb.size(1)\n",
    "        p_TL = torch.zeros(toksize, lblsize, 4, dtype=torch.float, device=default_device())\n",
    "        eps = p_TL.new_empty(1).fill_(1e-8)\n",
    "        for x,y in dl:\n",
    "            test_eq(x.shape, (dl.bs, toksize)); test_eq(y.shape, (dl.bs, lblsize))\n",
    "            t = x.unsqueeze(-1).expand(-1, -1, lblsize) ; test_eq(t.shape, (dl.bs, toksize, lblsize))\n",
    "            l = y.unsqueeze(1).expand(-1, toksize, -1) ; test_eq(l.shape, (dl.bs, toksize, lblsize))\n",
    "            tl = torch.stack((t,l), dim=-1) ; test_eq(tl.shape, (dl.bs, toksize, lblsize, 2)) \n",
    "            p_TL_tt = tl[...,0].logical_and(tl[...,1]) ; test_eq(p_TL_tt.shape, (dl.bs, toksize, lblsize)) \n",
    "            p_TL_tf = tl[...,0].logical_and(tl[...,1].logical_not()) ; test_eq(p_TL_tf.shape, (dl.bs, toksize, lblsize)) \n",
    "            p_TL_ft = tl[...,0].logical_not().logical_and(tl[...,1]) ; test_eq(p_TL_ft.shape, (dl.bs, toksize, lblsize))\n",
    "            p_TL_ff = tl[...,0].logical_not().logical_and(tl[...,1].logical_not()) ; test_eq(p_TL_ff.shape, (dl.bs, toksize, lblsize)) \n",
    "            p_TL = p_TL + torch.stack((p_TL_tt, p_TL_tf, p_TL_ft, p_TL_ff), dim=-1).float().sum(dim=0)\n",
    "        p_TL = p_TL / tensor(len(self.dsets)).float()\n",
    "        p_TL = p_TL.view(toksize, lblsize, 2, 2) ; test_eq(p_TL.shape, (toksize, lblsize, 2, 2))# last axis: lbl axis, 2nd last axis: token axis\n",
    "        return p_TL\n",
    "    \n",
    "    def joint_pmf(self):\n",
    "        self.p_TL_full = [] \n",
    "        for dl in progress_bar(self.dls):\n",
    "            p_TL = self._mutual_info_gain(dl)\n",
    "            self.p_TL_full.append(p_TL)\n",
    "            del p_TL; #del p_T; del p_L; del p_TxL; del I_TL; torch.cuda.empty_cache()\n",
    "        self.p_TL_full = torch.cat(self.p_TL_full, dim=1); test_eq(self.p_TL_full.shape, (self.toksize, self.lblsize, 2, 2))\n",
    "        return self.p_TL_full\n",
    "    \n",
    "    def compute(self):\n",
    "        eps = self.p_TL_full.new_empty(1).fill_(1e-15)\n",
    "        toksize, lblsize = self.p_TL_full.size(0), self.p_TL_full.size(1)\n",
    "        p_T = self.p_TL_full[:,0].sum(-1, keepdim=True); test_eq(p_T.shape, (toksize, 2, 1))# 0 because we can pick any label and apply total prob law\n",
    "        p_L = self.p_TL_full[0,:].sum(-2, keepdim=True); test_eq(p_L.shape, (lblsize, 1, 2)) # 0 becuase we can pick any token and apply total prob law\n",
    "        p_TxL = self.p_TL_full.sum(-1, keepdim=True) @ self.p_TL_full.sum(-2, keepdim=True); test_eq(p_TxL.shape, (toksize, lblsize, 2, 2))\n",
    "        H_T = -(p_T * torch.log(p_T+eps)).sum(-2).squeeze(); test_eq(H_T.shape, [toksize])\n",
    "        H_L = -(p_L * torch.log(p_L+eps)).sum(-1).squeeze(); test_eq(H_L.shape, [lblsize])\n",
    "        I_TL = (self.p_TL_full * torch.log((self.p_TL_full + eps)/(p_TxL + eps))).flatten(start_dim=-2).sum(-1); test_eq(I_TL.shape, (toksize, lblsize))\n",
    "        return p_T, p_L, p_TxL, H_T, H_L, I_TL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e9138a-a876-420c-bd43-500d34b02863",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@property\n",
    "@patch\n",
    "def lbs_frqs(self:MutualInfoGain):\n",
    "    f = ColReader('labels', label_delim=';')\n",
    "    self._frqs = Counter()\n",
    "    for o in self.df.itertuples(): self._frqs.update(f(o))\n",
    "    return self._frqs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f009e81-e4d9-4b7f-99ca-4bdf506c823c",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bbd3f1-d655-4e48-acd8-041886ee97b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepK",
   "language": "python",
   "name": "deepk"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
