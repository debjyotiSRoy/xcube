{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e250b634-6386-4092-8cd7-6a061998ee99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "! [ -e /content ] && pip install -Uqq xcube # upgrade xcube on colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900fde3d-895d-4701-8c56-d91e5bc3d581",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xcube.l2r.all import *\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0461bd5-4bd4-4324-970b-a18b664d9d6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_CudaDeviceProperties(name='NVIDIA GeForce RTX 3060 Laptop GPU', major=8, minor=6, total_memory=6143MB, multi_processor_count=30)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_properties(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c7c456-ca6e-4e26-8391-3912fe2f401d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce86749-b1cd-4bbb-99a5-1761e9ab4695",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#11) [Path('/home/deb/.xcube/data/mimic3_l2r/info.pkl'),Path('/home/deb/.xcube/data/mimic3_l2r/code_descriptions.csv'),Path('/home/deb/.xcube/data/mimic3_l2r/mimic3-9k_tok_lbl_info.pkl'),Path('/home/deb/.xcube/data/mimic3_l2r/code_desc.pkl'),Path('/home/deb/.xcube/data/mimic3_l2r/p_TL.pkl'),Path('/home/deb/.xcube/data/mimic3_l2r/trn_val_split.pkl'),Path('/home/deb/.xcube/data/mimic3_l2r/mimic3-9k_tok.ft'),Path('/home/deb/.xcube/data/mimic3_l2r/mimic3-9k_lbl.ft'),Path('/home/deb/.xcube/data/mimic3_l2r/mimic3-9k.csv'),Path('/home/deb/.xcube/data/mimic3_l2r/scored_tokens.pth')...]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source = untar_xxx(XURLs.MIMIC3_L2R)\n",
    "source.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace60196-6fae-4649-ba93-3da39f43e136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths = make_paths(Path.cwd(), 'mimic3-9k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23b219d-e10a-4214-bd1a-b589f77b236a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ic(torch.cuda.get_device_name(default_device()));\n",
    "# test_eq(torch.cuda.get_device_name(0), torch.cuda.get_device_name(default_device()))\n",
    "# test_eq(default_device(), torch.device(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0bd96f-9315-4746-84ee-8ed5ac643d45",
   "metadata": {},
   "source": [
    "Setting some environment variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad35ab3-ce3d-44be-9f12-a679bc7770b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32dbb09-2578-46ac-8c99-6449d4ae8320",
   "metadata": {},
   "source": [
    "# L2R Training\n",
    "\n",
    "> Training a learning-to-rank model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4f5a0a-6618-48c6-9bf6-6a394c6c1953",
   "metadata": {},
   "source": [
    "In this tutorial we will train a l2r model. We will bootstrap the model using the data we prepared in tutorial [booting L2R](14_tutorial.boot_l2r.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739e4121-7903-495d-95df-610af53a42ad",
   "metadata": {},
   "source": [
    "## Getting ready"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52d88be-94e7-4dac-a69e-72d0eb02ca0e",
   "metadata": {},
   "source": [
    "Prepping l2r data for xcube's `L2RDataLoader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f8dc37-55f6-46ed-9ab7-d0092f715421",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#9) [Path('/home/deb/.xcube/data/mimic3_l2r/info.pkl'),Path('/home/deb/.xcube/data/mimic3_l2r/code_descriptions.csv'),Path('/home/deb/.xcube/data/mimic3_l2r/mimic3-9k_tok_lbl_info.pkl'),Path('/home/deb/.xcube/data/mimic3_l2r/code_desc.pkl'),Path('/home/deb/.xcube/data/mimic3_l2r/p_TL.pkl'),Path('/home/deb/.xcube/data/mimic3_l2r/mimic3-9k_tok.ft'),Path('/home/deb/.xcube/data/mimic3_l2r/mimic3-9k_lbl.ft'),Path('/home/deb/.xcube/data/mimic3_l2r/mimic3-9k.csv'),Path('/home/deb/.xcube/data/mimic3_l2r/mimic3-9k_tok_lbl.ft')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source = untar_xxx(XURLs.MIMIC3_L2R)\n",
    "source.ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cd9a41-dec1-47f5-959d-1198d6beedf6",
   "metadata": {},
   "source": [
    "Note: If you don't have enough GPU/CPU memory just run the last cell of this section to load the pregenerated ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a78d710-aee4-48ac-a628-aec26002488f",
   "metadata": {},
   "source": [
    "Here we can just load the file which contains the relevant information about the tokens, labels and their mutual-information-gain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c40d00-979b-4b2b-ab25-a6b6344fd15a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not Enough GPU Memory (just 5.99951171875 GB), we'll use cpu\n"
     ]
    }
   ],
   "source": [
    "# Cheking if you have enough memory to set device\n",
    "cuda_memory = torch.cuda.get_device_properties(torch.cuda.current_device()).total_memory/1024**3\n",
    "if cuda_memory < 10.: print(f\"Not Enough GPU Memory (just {cuda_memory} GB), we'll use {default_device(use=False)}\")\n",
    "l2r_bootstrap = torch.load(source/'mimic3-9k_tok_lbl_info.pkl', map_location=default_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3332f1ff-6a23-4ad6-bc42-d52b4e9bc38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(l2r_bootstrap.keys(), ['toks', 'lbs', 'mut_info_lbl_entropy', 'mutual_info_jaccard'])\n",
    "toks = l2r_bootstrap.get('toks', None)\n",
    "lbs = l2r_bootstrap.get('lbs', None)\n",
    "info = l2r_bootstrap.get('mutual_info_jaccard', None)\n",
    "for o in (toks, lbs, info): assert o is not None\n",
    "test_eq(info.shape, (len(toks), len(lbs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e019c3e2-f092-43f2-8f6a-31c58e4fddf6",
   "metadata": {},
   "source": [
    "`info` contains the mutual-information-gain values for the tokens and labels. In what follows we'll toss in some pandas to take a good hard look at the data before we proceed towards making xcube's `L2RDataLoader`: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3321d66-0f78-4a81-9807-b218a872a440",
   "metadata": {},
   "source": [
    "*Note:* Storing the tokens and the labels in a dataframe as `object` will take up a lot of RAM space when we prepare that `DataLoader`. So we are going to store the corresponding token and label indices instead in a dataframe called `df_l2r`. We are also going to store the tokens and the labels with their corresponding indices in seperate dataframes (this will help in quick merging for analysis)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d63c53-d5f0-4d79-9536-fcbec6f35f9e",
   "metadata": {},
   "source": [
    "Here we will rank the tokens for each label based on the decreasing values of the mutual-info and stack them up with mutual-info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c28fb66-3532-456d-b702-b528e3165d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked = info.argsort(descending=True, dim=0).argsort(dim=0)\n",
    "info_ranked =torch.stack((info, ranked), dim=2).flatten(start_dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5020a427-05b5-47ff-b018-405384ef5362",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = pd.MultiIndex.from_product([range(len(lbs)), ['mutual_info', 'rank']], names=['label', 'key2'])\n",
    "df_l2r = pd.DataFrame(info_ranked, index=range(len(toks)), columns=cols)\n",
    "df_l2r.index.name='token'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894efcff-9187-4620-9778-bfc827851e75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th colspan=\"2\" halign=\"left\">0</th>\n",
       "      <th colspan=\"2\" halign=\"left\">1</th>\n",
       "      <th colspan=\"2\" halign=\"left\">2</th>\n",
       "      <th colspan=\"2\" halign=\"left\">3</th>\n",
       "      <th colspan=\"2\" halign=\"left\">4</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"2\" halign=\"left\">8917</th>\n",
       "      <th colspan=\"2\" halign=\"left\">8918</th>\n",
       "      <th colspan=\"2\" halign=\"left\">8919</th>\n",
       "      <th colspan=\"2\" halign=\"left\">8920</th>\n",
       "      <th colspan=\"2\" halign=\"left\">8921</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>key2</th>\n",
       "      <th>mutual_info</th>\n",
       "      <th>rank</th>\n",
       "      <th>mutual_info</th>\n",
       "      <th>rank</th>\n",
       "      <th>mutual_info</th>\n",
       "      <th>rank</th>\n",
       "      <th>mutual_info</th>\n",
       "      <th>rank</th>\n",
       "      <th>mutual_info</th>\n",
       "      <th>rank</th>\n",
       "      <th>...</th>\n",
       "      <th>mutual_info</th>\n",
       "      <th>rank</th>\n",
       "      <th>mutual_info</th>\n",
       "      <th>rank</th>\n",
       "      <th>mutual_info</th>\n",
       "      <th>rank</th>\n",
       "      <th>mutual_info</th>\n",
       "      <th>rank</th>\n",
       "      <th>mutual_info</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000022</td>\n",
       "      <td>866.0</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>1022.0</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>1156.0</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>823.0</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>984.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>850.0</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>944.0</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>960.0</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>771.0</td>\n",
       "      <td>6.888287e-07</td>\n",
       "      <td>31821.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>56856.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41420.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>56855.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41412.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>22838.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41423.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>22863.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41425.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41414.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>32387.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>56857.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41427.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>56856.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41413.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>22843.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41430.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>22870.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41432.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41421.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>32390.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 17844 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "label        0                    1                    2              \\\n",
       "key2  mutual_info     rank mutual_info     rank mutual_info     rank   \n",
       "token                                                                  \n",
       "0        0.000022    866.0    0.000011   1022.0    0.000022   1156.0   \n",
       "1        0.000000  56856.0    0.000000  41420.0    0.000000  56855.0   \n",
       "2        0.000000  56857.0    0.000000  41427.0    0.000000  56856.0   \n",
       "\n",
       "label        3                    4              ...        8917           \\\n",
       "key2  mutual_info     rank mutual_info     rank  ... mutual_info     rank   \n",
       "token                                            ...                        \n",
       "0        0.000011    823.0    0.000033    984.0  ...    0.000011    850.0   \n",
       "1        0.000000  41412.0    0.000000  22838.0  ...    0.000000  41423.0   \n",
       "2        0.000000  41413.0    0.000000  22843.0  ...    0.000000  41430.0   \n",
       "\n",
       "label        8918                 8919                 8920           \\\n",
       "key2  mutual_info     rank mutual_info     rank mutual_info     rank   \n",
       "token                                                                  \n",
       "0        0.000033    944.0    0.000011    960.0    0.000011    771.0   \n",
       "1        0.000000  22863.0    0.000000  41425.0    0.000000  41414.0   \n",
       "2        0.000000  22870.0    0.000000  41432.0    0.000000  41421.0   \n",
       "\n",
       "label          8921           \n",
       "key2    mutual_info     rank  \n",
       "token                         \n",
       "0      6.888287e-07  31821.0  \n",
       "1      0.000000e+00  32387.0  \n",
       "2      0.000000e+00  32390.0  \n",
       "\n",
       "[3 rows x 17844 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_l2r.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a799b04c-a6ab-4349-9f13-cf5af76e3e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_l2r = df_l2r.stack(level=0).reset_index().rename_axis(None, axis=1)\n",
    "df_l2r[['token', 'label']] = df_l2r[['token', 'label']].astype(np.int32) \n",
    "test_eq(len(df_l2r), len(toks) * len(lbs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf93bf9-dd9c-42fd-ab0b-4e8ea0ae2f7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>label</th>\n",
       "      <th>mutual_info</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>866.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>1022.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>1156.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   token  label  mutual_info    rank\n",
       "0      0      0     0.000022   866.0\n",
       "1      0      1     0.000011  1022.0\n",
       "2      0      2     0.000022  1156.0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_l2r.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560a3bc4-159b-4cf4-a43b-dd5d7cf8d133",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index          1.192093e-07\n",
       "token          1.906211e+00\n",
       "label          1.906211e+00\n",
       "mutual_info    1.906211e+00\n",
       "rank           1.906211e+00\n",
       "dtype: float64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_l2r.memory_usage()/1024**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49baf2b-ada3-4638-ade9-e9e3ba9fbb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_toks = pd.DataFrame([(i, w) for i,w in enumerate(toks)], columns=['token', 'tok_val'])\n",
    "df_lbs = pd.DataFrame([(i,w) for i, w in enumerate(lbs)], columns=['lbl', 'lbl_val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb455422-8e1e-4921-b1d1-e998b6bed63a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>tok_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>xxunk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>xxpad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>xxbos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   token tok_val\n",
       "0      0   xxunk\n",
       "1      1   xxpad\n",
       "2      2   xxbos"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_toks.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991db1e5-20ce-4593-8766-11acd6450db8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lbl</th>\n",
       "      <th>lbl_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>003.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>003.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>003.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   lbl lbl_val\n",
       "0    0   003.0\n",
       "1    1   003.1\n",
       "2    2   003.8"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lbs.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d03d0e-7844-428c-b1f1-298c437a5ade",
   "metadata": {},
   "source": [
    "You can save `df_l2r`, `df_toks` and `df_lbs` if you are working on your own dataset. In this case though `untar_xxx` has already downloaded those for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8929b4-3d64-40ca-95ad-98d47a6579f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#3) [Path('/home/deb/.xcube/data/mimic3_l2r/mimic3-9k_tok.ft'),Path('/home/deb/.xcube/data/mimic3_l2r/mimic3-9k_lbl.ft'),Path('/home/deb/.xcube/data/mimic3_l2r/mimic3-9k_tok_lbl.ft')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L(source.glob(\"**/*.ft\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8e5c61-8cd9-4611-ad82-f2449bb94d6b",
   "metadata": {},
   "source": [
    "## Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f851bd31-4a56-4e36-a537-31399294c017",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_l2r = pd.read_feather(source/'mimic3-9k_tok_lbl.ft')\n",
    "test_eq(df_l2r.dtypes.mutual_info, np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7492acac-37c7-4952-b3d4-9de1b74d8cb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>label</th>\n",
       "      <th>mutual_info</th>\n",
       "      <th>rank</th>\n",
       "      <th>bcx_mutual_info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>866.0</td>\n",
       "      <td>-6.530356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>1022.0</td>\n",
       "      <td>-6.753679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>1156.0</td>\n",
       "      <td>-6.530356</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   token  label  mutual_info    rank  bcx_mutual_info\n",
       "0      0      0     0.000022   866.0        -6.530356\n",
       "1      0      1     0.000011  1022.0        -6.753679\n",
       "2      0      2     0.000022  1156.0        -6.530356"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_l2r.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff35265b-d65c-4293-ad7c-7ff3fec82f9e",
   "metadata": {},
   "source": [
    "If you loaded the pregenerated `df_l2r` then you will see the column \"bcx_mutual_info\". It is a box-cox transformation of the \"mutual-info\". In this section we'll justify that transformation. So let's perform some statistical analysis of that `mutual_info` column before we build the `L2RDataLoader` in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ea16e4-c03c-4bee-a005-5bf02e15c902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc; gc.collect()\n",
    "# df_l2r.info()\n",
    "# ic(df_l2r.memory_usage().sum()/1024**3)\n",
    "# ic(sys.getsizeof(df_l2r)/1024**3);\n",
    "# df_collab.token.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b1e6d3-1d9e-4ba6-a311-3a7fda646896",
   "metadata": {},
   "outputs": [],
   "source": [
    "mut_infos = df_l2r['mutual_info'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69637d68-eeeb-4976-aa0c-23bc5e09479f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 0.99999636, 7.697003e-05)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mut_infos.min(), mut_infos.max(), mut_infos.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72b9869-841f-4b84-ad3b-2d4dbec36f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.43 s, sys: 0 ns, total: 3.43 s\n",
      "Wall time: 3.44 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "142.83643748058765"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "skew(mut_infos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ba4636-48af-49af-8932-2bd2d1dfc84c",
   "metadata": {},
   "source": [
    "The mutual-info values are incredibly skewed. So we need to apply some transformation. Sometimes `mut_infos` might contain negs, we need to convert those to eps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fc5dee-26ff-4ff5-b23e-a877faf23860",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| np.sum(where_negs): 0\n",
      "ic| np.min(mut_infos): 0.0\n",
      "    np.max(mut_infos): 0.99999636\n",
      "    np.mean(mut_infos): 7.697003e-05\n"
     ]
    }
   ],
   "source": [
    "# np.where(mut_infos<0, 1, 0).sum() # or, better yet\n",
    "where_negs = mut_infos < 0\n",
    "ic(np.sum(where_negs))\n",
    "eps = np.float32(1e-20)\n",
    "mut_infos[where_negs] = eps\n",
    "test_eq(np.sum(mut_infos<0), 0)\n",
    "ic(np.min(mut_infos), np.max(mut_infos), np.mean(mut_infos));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79078145-b16f-4ebe-8cc4-3f2661e6e3e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hist, bins, _ = plt.hist(mut_infos, bins=50)\n",
    "# plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6faa68-092f-43f5-9ef5-789f89f3812f",
   "metadata": {},
   "source": [
    "**Applying log transform:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d57c44a-86a5-47e5-8fe5-604f66ded5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_mut_infos = np.log(mut_infos + eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353df9c8-f6dd-4dcf-a6a0-d00887d25eda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0, 0)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isnan(log_mut_infos).sum(), np.isneginf(log_mut_infos).sum(), np.isinf(log_mut_infos).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dd795e-95ae-42e1-a995-c16ffd1b5d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.42 s, sys: 0 ns, total: 3.42 s\n",
      "Wall time: 3.43 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1.3383214188674972"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time skew(log_mut_infos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bfc376-1324-425c-b165-a18ee3569cbf",
   "metadata": {},
   "source": [
    "A little better skewness than before!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f112b0-3eb5-45ae-8590-bb8a7ff6d8f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hist, bins, _ = plt.hist(log_mut_infos, bins=50,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c711dfb-a826-4128-82c8-38f9a4fc63e1",
   "metadata": {},
   "source": [
    "**Applying sqrt transform:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a3976b-bcc4-4428-84ba-ea8f17e48318",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqrt_mut_infos = np.sqrt(mut_infos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace595e8-365c-451e-8590-f367a87b5c22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0, 0)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isnan(sqrt_mut_infos).sum(), np.isinf(sqrt_mut_infos).sum(), np.isneginf(sqrt_mut_infos).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31d6551-876b-4846-bedf-a34d41e09660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.39 s, sys: 913 ms, total: 3.3 s\n",
      "Wall time: 3.3 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16.40865608826817"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time skew(sqrt_mut_infos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18750bdc-4806-4330-8aaf-8db703868953",
   "metadata": {},
   "source": [
    "Worse than log transform!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f977fa6-d197-4838-b135-5cd45bb9c6d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hist, bins, _ = plt.hist(sqrt_mut_infos, bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170762c1-7839-45ac-8e39-573ff29efd05",
   "metadata": {},
   "source": [
    "**Apply box-cox transfrom:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6a39ff-7560-445d-8fb8-47f59ea3510c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deb/miniconda3/envs/deep/lib/python3.10/site-packages/scipy/stats/_morestats.py:933: RuntimeWarning: overflow encountered in power\n",
      "  variance = np.var(data**lmb / lmb, axis=0)\n",
      "/home/deb/miniconda3/envs/deep/lib/python3.10/site-packages/numpy/core/_methods.py:233: RuntimeWarning: invalid value encountered in subtract\n",
      "  x = asanyarray(arr - arrmean)\n"
     ]
    }
   ],
   "source": [
    "bcx_mut_infos, *_ = boxcox(mut_infos+eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2154ec-607f-4ae6-b66a-f1464cc0aab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0, 0)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isnan(bcx_mut_infos).sum(), np.isinf(bcx_mut_infos).sum(), np.isneginf(bcx_mut_infos).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514afd2e-baa1-4b7f-9b2a-245724e00924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.39 s, sys: 991 ms, total: 3.39 s\n",
      "Wall time: 3.4 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.885981418331696"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time skew(bcx_mut_infos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9aa969-ae69-4a91-b3ba-2b0c7e31a7f5",
   "metadata": {},
   "source": [
    "This is the best skew so we'll go with boxcox."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56efcffe-dde6-44db-a9fc-420bbe909b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_l2r['bcx_mutual_info'] = bcx_mut_infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd640f46-f7d4-41e2-867f-3dbd89d4a7ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hist, bins, _ = plt.hist(bcx_mut_infos, bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6696f8e-f7d9-4f1f-89a1-5497e2f09559",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-9.734209, -3.6358892e-06, -7.381837, -6.9605794)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(bcx_mut_infos), np.max(bcx_mut_infos), np.mean(bcx_mut_infos), np.median(bcx_mut_infos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46b4204-0109-4ccf-b37d-147ef65ede6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import clear_output\n",
    "\n",
    "# clear_output(wait=True)\n",
    "\n",
    "# from tqdm import tqdm\n",
    "# from time import sleep\n",
    "# import psutil\n",
    "\n",
    "# with tqdm(total=100, desc='cpu%', position=1) as cpubar, tqdm(total=100, desc='ram%', position=0) as rambar:\n",
    "#     while True:\n",
    "#         rambar.n=psutil.virtual_memory().percent\n",
    "#         cpubar.n=psutil.cpu_percent()\n",
    "#         rambar.refresh()\n",
    "#         cpubar.refresh()\n",
    "#         sleep(0.5)\n",
    "#         clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ede15bd-422e-4e92-a8f7-45a9480832f6",
   "metadata": {},
   "source": [
    "Box plots using matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a663d859-53c1-4d60-9a53-fe57fda8ffe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 34s, sys: 2.57 s, total: 1min 36s\n",
      "Wall time: 1min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "ax1 = fig.add_subplot(1, 2, 1)\n",
    "ax2 = fig.add_subplot(1, 2, 2)\n",
    "\n",
    "ax1.boxplot(mut_infos, vert=0, notch=True, patch_artist=True)\n",
    "ax1.set_xscale('log')\n",
    "ax1.set_xlabel('Mutual Information')\n",
    "\n",
    "ax2.boxplot(bcx_mut_infos, vert=0, notch=True, patch_artist=True)\n",
    "# ax2.set_xscale('symlog')\n",
    "ax2.set_xlabel('Box-Cox Mutual Information')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4baa4e24-53b1-43c4-ac33-80baadb270ec",
   "metadata": {},
   "source": [
    "Histograms and kde using matplotlib:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5237212a-21ff-40ff-a0f9-5fc3270420ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# hist, bins, pathches = ax1.hist(df_l2r['mutual_info'])\n",
    "hist, bins, pathches = ax1.hist(mut_infos)\n",
    "ax1.set_xlabel('Mutual Information')\n",
    "ax1.set_ylabel('frequency')\n",
    "ax1.grid(axis='y', color='black')\n",
    "ax1.set_yscale('log')\n",
    "\n",
    "# ax2.hist(df_l2r['bcx_mutual_info'])\n",
    "ax2.hist(bcx_mut_infos)\n",
    "ax2.set_xlabel('Boxcox Mutual Information')\n",
    "ax2.set_ylabel('frequency')\n",
    "ax2.grid(axis='y', color='black')\n",
    "ax2.set_yscale('log')\n",
    "\n",
    "fig.suptitle('Histograms of with and without BoxCox of Mutual Information')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197837b6-6e3e-4bee-a92c-463e2c81e567",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mut_infos bdrs</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.0, 0.09999963641166687]</td>\n",
       "      <td>511675599.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.09999963641166687, 0.19999927282333374]</td>\n",
       "      <td>14848.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.19999927282333374, 0.2999989092350006]</td>\n",
       "      <td>3323.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.2999989092350006, 0.3999985456466675]</td>\n",
       "      <td>191.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.3999985456466675, 0.49999818205833435]</td>\n",
       "      <td>454.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.49999818205833435, 0.5999978184700012]</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.5999978184700012, 0.6999974250793457]</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.6999974250793457, 0.799997091293335]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[0.799997091293335, 0.8999967575073242]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.8999967575073242, 0.9999963641166687]</td>\n",
       "      <td>94.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               mut_infos bdrs       counts\n",
       "0                  [0.0, 0.09999963641166687]  511675599.0\n",
       "1  [0.09999963641166687, 0.19999927282333374]      14848.0\n",
       "2   [0.19999927282333374, 0.2999989092350006]       3323.0\n",
       "3    [0.2999989092350006, 0.3999985456466675]        191.0\n",
       "4   [0.3999985456466675, 0.49999818205833435]        454.0\n",
       "5   [0.49999818205833435, 0.5999978184700012]         30.0\n",
       "6    [0.5999978184700012, 0.6999974250793457]          5.0\n",
       "7     [0.6999974250793457, 0.799997091293335]          0.0\n",
       "8     [0.799997091293335, 0.8999967575073242]          0.0\n",
       "9    [0.8999967575073242, 0.9999963641166687]         94.0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bdrs = [bins[i:i+2] for i in range(0, len(bins)-1)]\n",
    "pd.DataFrame({'mut_infos bdrs': bdrs, 'counts': hist})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76920535-91bd-4dac-8c5b-7878b640c227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.stats import gaussian_kde\n",
    "# density = gaussian_kde(df_l2r['mutual_info'])\n",
    "# xs = np.linspace(0, 1, 200)\n",
    "# density.covariance_factor = lambda : .25\n",
    "# density._compute_covariance()\n",
    "# plt.plot(xs, density(xs))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719e2f38-bbb6-4a17-a5f3-97e79bc68c2d",
   "metadata": {},
   "source": [
    "We can now build the `Dataloaders` object from this dataframe `df_collab`, by defaultit takes the first column as the user (in our case the token) and the second column as the item (in our case the label), and the third column as the ratings (in our case the frequency):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b2b499-7ee9-4fd5-9070-0ce6a419507d",
   "metadata": {},
   "source": [
    "## Build `L2RDataloader` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480117ba-1933-4f56-bf0a-9929013c5fa1",
   "metadata": {},
   "source": [
    "In this section we'll build `L2RDataLoader` for [Learning to Rank (L2R)](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/MSR-TR-2010-82.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d3dc9f-0fae-4a37-91cc-aca1447c71b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mimic3_l2r/\n",
      "    info.pkl\n",
      "    code_descriptions.csv\n",
      "    mimic3-9k_tok_lbl_info.pkl\n",
      "    code_desc.pkl\n",
      "    p_TL.pkl\n",
      "    trn_val_split.pkl\n",
      "    mimic3-9k_tok.ft\n",
      "    mimic3-9k_lbl.ft\n",
      "    mimic3-9k.csv\n",
      "    scored_tokens.pth\n",
      "    mimic3-9k_tok_lbl.ft\n"
     ]
    }
   ],
   "source": [
    "list_files(str(source))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a319ca5-8416-4de6-a831-9b9b73a2d159",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>label</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>866.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1022.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1156.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   token  label    rank\n",
       "0      0      0   866.0\n",
       "1      0      1  1022.0\n",
       "2      0      2  1156.0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_l2r = pd.read_feather(source/'mimic3-9k_tok_lbl.ft')\n",
    "df_l2r = df_l2r.drop(['mutual_info', 'bcx_mutual_info'], axis=1)\n",
    "df_l2r.token.nunique(), df_l2r.label.nunique()\n",
    "df_l2r.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e839c71e-8e37-430d-91a3-2b5e0ee59bff",
   "metadata": {},
   "source": [
    "**`df_tiny`**: If we need a smaller dataset for quick iterations\n",
    "\n",
    "Note: For technical reasons behind building a `L2RDataloader` the number of tokens should be $x (mod 64) \\equiv 8$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74270936-6e31-4136-8544-54cd300a8d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_toks, num_lbs = 8 + 5*64, 104"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e953f105-a959-4ad4-9c64-3f2dbbf433af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# might have to repeat this a few times until the cell asserst true\n",
    "np.random.seed(101)\n",
    "rnd_toks = np.random.randint(0, len(df_l2r.token.unique()), size=(num_toks,) )\n",
    "np.random.seed(101)\n",
    "rnd_lbs = np.random.randint(0, len(df_l2r.label.unique()), size=(num_lbs,) )\n",
    "mask = df_l2r.token.isin(rnd_toks) & df_l2r.label.isin(rnd_lbs)\n",
    "df_tiny = df_l2r[mask].reset_index(drop=True)\n",
    "test_eq(df_tiny.token.nunique(), num_toks) \n",
    "test_eq(df_tiny.label.nunique(), num_lbs) \n",
    "# df_tiny.apply(lambda x: x.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e492de0-8ddd-423a-aafb-301ba954b13b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>label</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22</td>\n",
       "      <td>49</td>\n",
       "      <td>1877.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22</td>\n",
       "      <td>239</td>\n",
       "      <td>21308.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>394</td>\n",
       "      <td>39854.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22</td>\n",
       "      <td>436</td>\n",
       "      <td>8618.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22</td>\n",
       "      <td>561</td>\n",
       "      <td>1646.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   token  label     rank\n",
       "0     22     49   1877.0\n",
       "1     22    239  21308.0\n",
       "2     22    394  39854.0\n",
       "3     22    436   8618.0\n",
       "4     22    561   1646.0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tiny.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d946375e-1cfb-4b6b-a26d-983e398b66dc",
   "metadata": {},
   "source": [
    "Let's just delete the `df_l2r` to free up RAM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed7e13c-0c14-45e3-aae8-7d8fa7bba6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_l2r = pd.DataFrame()\n",
    "# lst = [df_l2r]\n",
    "# del lst\n",
    "# del df_l2r\n",
    "# import gc; gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28fe0a7-7c34-4f2c-bfc9-d6a8df861da8",
   "metadata": {},
   "source": [
    "**Only for `df_tiny`**:\n",
    "\n",
    "Due to random sampling the rankings are not uniform i.e., not from 0 to `num_toks`. A litte preprocessing to make sure that we have uniform rankings for all labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a204ef0c-5417-4fce-a634-5e8eeeccad98",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df_tiny.groupby('label', group_keys=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef6dd9c-6748-40b2-9f44-aaa28301a81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_rerank(df, column='rank'):\n",
    "    df = df.sort_values(by=column)\n",
    "    df['rank'] = range(len(df))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90c6aed-0c97-494c-8dc9-b8e48826c87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tiny = grouped.apply(sort_rerank)\n",
    "dict_grouped = dict(list(df_tiny.groupby('label')))\n",
    "# checking a random label has ranks 0 thru `num_toks`\n",
    "a_lbl = random.choice(list(dict_grouped.keys()))\n",
    "test_eq(range(num_toks), dict_grouped[a_lbl]['rank'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a566e771-9e4e-4f5d-81b0-8f421dcf4dbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>label</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9282</th>\n",
       "      <td>16188</td>\n",
       "      <td>2265</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5434</th>\n",
       "      <td>9501</td>\n",
       "      <td>2265</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2938</th>\n",
       "      <td>4608</td>\n",
       "      <td>2265</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3562</th>\n",
       "      <td>5700</td>\n",
       "      <td>2265</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>858</th>\n",
       "      <td>1944</td>\n",
       "      <td>2265</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      token  label  rank\n",
       "9282  16188   2265     0\n",
       "5434   9501   2265     1\n",
       "2938   4608   2265     2\n",
       "3562   5700   2265     3\n",
       "858    1944   2265     4"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_grouped[a_lbl].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48faa4c9-7182-4ca1-9435-f6d55089c79b",
   "metadata": {},
   "source": [
    "Using Pandas `groupby` to add quantized *relevance scores* to each token-label pair based on the corresponding ranks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00071b3-00b1-4619-b53d-688f0727a8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df_tiny.groupby('label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f113934-8854-4d27-9280-61ec9413ecc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_grouped = dict(list(grouped))\n",
    "# _tmp = dict_grouped[16].copy()\n",
    "# _tmp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9794f044-e72c-4c89-9fc2-dd5158a2bf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut(df, qnts, column='rank'):\n",
    "    num = df.to_numpy()\n",
    "    bins = np.quantile(num[:, -1], qnts)\n",
    "    num[:, -1] = len(bins) - np.digitize(num[:, -1], bins)\n",
    "    # bins = np.quantile(df['rank'], qnts)\n",
    "    # df[column] = len(bins) - np.digitize(df['rank'], bins)\n",
    "    # df[column] = pd.qcut(df[column], qnts, labels=labels)\n",
    "    return num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08133b4-0397-48ea-b604-6b1bc186090e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.6 ms ± 344 µs per loop (mean ± std. dev. of 15 runs, 50 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 50 -r 15\n",
    "qnts = np.concatenate([array([0]), np.geomspace(1e-2, 1, 10)])\n",
    "scored = grouped.apply(cut, qnts) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303eac1f-fc7f-4420-aef4-28d6d84be453",
   "metadata": {},
   "source": [
    "Pandas `groupby` was just to illustrate how we do the scoring. In reality we are going to use tensorized implemnetation. Follow along:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdeba64-1ba7-45a9-90ea-80dcd00ccb43",
   "metadata": {},
   "source": [
    "**[Simulate Pandas `groupby` using `Numpy/PyTorch`](https://stackoverflow.com/questions/38013778/is-there-any-numpy-group-by-function/38015063#38015063):** (Why? `Pandas` are cute but speed thrills! More importantly: \"Memory\"). If interested read  sourcecode of [`PreLoadTrans.quantized_score`](10_l2r.data.load):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d340d8b-241f-4551-ae92-7623f65e210a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdl = PreLoadTrans(df_tiny, device=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fc2838-93cb-4400-b119-e4d0d609f0e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deb/xcube/xcube/l2r/data/load.py:55: UserWarning: torch.searchsorted(): input value tensor is non-contiguous, this will lower the performance due to extra data copy when converting non-contiguous tensor to contiguous, please use contiguous input value tensor if possible. This message will only appear once per program. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525541990/work/aten/src/ATen/native/BucketizationUtils.h:33.)\n",
      "  relv_scores = bins.shape[0] - torch.searchsorted(bins.T, data[:, :, -1], right=False) # shape (8922, 57352)\n"
     ]
    }
   ],
   "source": [
    "# %%timeit -n 50 -r 15\n",
    "scored_toks = pdl.quantized_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5eaf8d-212b-4ac8-962f-e0fae9ee8950",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eqs(scored_toks.shape, \n",
    "         (df_tiny.label.nunique(), df_tiny.token.nunique(), 4), \n",
    "         (pdl.num_lbs, pdl.num_toks, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be0be51-5ee4-4895-b45c-c39981bbe386",
   "metadata": {},
   "source": [
    "Save if you want to! BTW `untar_xxx` has got the one for the full dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5ea9e8-adc0-4375-a87a-b08749b505ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#1) [Path('/home/deb/.xcube/data/mimic3_l2r/scored_tokens.pth')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L(source.glob(\"**/*scored*.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ac7e45-0957-4904-9811-e7dcaa1cb53e",
   "metadata": {},
   "source": [
    "Create training and validation split:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e07c50-c3e9-4f44-a147-5d68a443558c",
   "metadata": {},
   "source": [
    "**Remember**: In `scored_toks` dim 0: labels, dim 1: 4 tuple (token, label, rank, score). Below is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f72833-c73a-4c24-bc8d-1d02c174003c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41514.0, 8124.0, 234.0, 4.0)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok, lbl, rank, score = L(scored_toks[97, 32], use_list=True).map(Tensor.item)\n",
    "tok, lbl, rank, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4984e8d3-852f-4935-92fa-15b4b3df4ea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>label</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23393</th>\n",
       "      <td>41514</td>\n",
       "      <td>8124</td>\n",
       "      <td>234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       token  label  rank\n",
       "23393  41514   8124   234"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tiny[(df_tiny.token == tok)  & (df_tiny.label == lbl)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c122b8-9a4d-4672-b782-e173329c7e44",
   "metadata": {},
   "source": [
    "**Remember:** For each label the tokens are ranked 0 through `num_toks`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e9d519-683f-45b9-b0d6-62e5e53de67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = scored_toks[:, :, 2].unique(dim=1).sort(-1)[0]\n",
    "ranks_shouldbe = torch.arange(scored_toks.shape[1], dtype=torch.float).expand(scored_toks.shape[0], -1)\n",
    "test_eq(ranks, ranks_shouldbe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692b20ce-f8bb-4b0b-b1b6-2daa06217949",
   "metadata": {},
   "source": [
    "**Remember:** For each label `quantized_score` scores the tokens on a log scale based on their ranks. The score scale is 1-101: 101 being the highest score (assigned to most relevant  token), and 1 is the lowest score (assigned to least relevant tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564d1b8a-3b5c-45aa-aa29-a081104b6566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  1.,   1.,   1.,   1.,   2.,   2.,   2.,   2.,   2.,   2.,   3.,   3.,\n",
       "          3.,   3.,   4.,   4.,   4.,   4.,   4.,   4.,   5.,   5.,   5.,   6.,\n",
       "          6.,   6.,   6.,   6.,   6.,   7.,   7.,   7.,   7.,   7.,   8.,   8.,\n",
       "          8.,   8.,   8.,   8.,   8.,   8.,   9.,   9.,   9.,   9.,   9.,   9.,\n",
       "         10.,  10.,  10.,  10.,  11.,  11.,  11.,  11.,  11.,  11.,  12.,  12.,\n",
       "         12.,  12.,  12.,  13.,  13.,  13.,  13.,  13.,  13.,  14.,  14.,  14.,\n",
       "         14.,  15.,  15.,  15.,  16.,  16.,  16.,  17.,  17.,  17.,  17.,  17.,\n",
       "         18.,  18.,  18.,  19.,  19.,  19.,  19.,  20.,  20.,  20.,  20.,  21.,\n",
       "         21.,  21.,  21.,  22.,  22.,  22.,  22.,  23.,  23.,  23.,  23.,  24.,\n",
       "         24.,  24.,  25.,  25.,  25.,  26.,  26.,  27.,  27.,  27.,  28.,  28.,\n",
       "         29.,  29.,  30.,  30.,  31.,  31.,  32.,  32.,  33.,  34.,  34.,  35.,\n",
       "         36.,  37.,  38.,  39.,  40.,  42.,  43.,  45.,  48.,  51.,  55.,  63.,\n",
       "        101.])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = scored_toks[:, :, -1].unique(dim=1).sort(-1)[0]\n",
    "scores[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101c67f1-f803-47f6-b0a9-fa2abc01ed74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.9 ms, sys: 3.01 ms, total: 15.9 ms\n",
      "Wall time: 2.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "scored_toks, binned_toks, probs, is_valid, bin_size, bin_bds = pdl.train_val_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0337e514-b221-4c8f-9095-0c85e2d97399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_sl=16\n",
      "CPU times: user 98.8 ms, sys: 4.17 ms, total: 103 ms\n",
      "Wall time: 13.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "val_sl = pdl.pad_split()\n",
    "test_eq(is_valid.sum(dim=-1).unique().item(), val_sl)\n",
    "print(f\"{val_sl=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc53aa7-cbd3-4537-940d-567ccbe0bc51",
   "metadata": {},
   "source": [
    "Taking a look at the train/valid split for some labels (just to make sure we ticked all boxes!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e17d8d-64d8-4c0c-a28b-656311154cdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>label</th>\n",
       "      <th>rank</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>28488.0</td>\n",
       "      <td>6820.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>101.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>2274.0</td>\n",
       "      <td>6820.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>63.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>50503.0</td>\n",
       "      <td>6820.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>55.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>56935.0</td>\n",
       "      <td>6820.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>51.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>20945.0</td>\n",
       "      <td>6820.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>48.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>57124.0</td>\n",
       "      <td>6820.0</td>\n",
       "      <td>320.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>9772.0</td>\n",
       "      <td>6820.0</td>\n",
       "      <td>318.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>907.0</td>\n",
       "      <td>6820.0</td>\n",
       "      <td>317.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>4924.0</td>\n",
       "      <td>6820.0</td>\n",
       "      <td>316.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>56756.0</td>\n",
       "      <td>6820.0</td>\n",
       "      <td>314.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>328 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       token   label   rank  score\n",
       "226  28488.0  6820.0    0.0  101.0\n",
       "225   2274.0  6820.0    1.0   63.0\n",
       "224  50503.0  6820.0    2.0   55.0\n",
       "223  56935.0  6820.0    3.0   51.0\n",
       "222  20945.0  6820.0    4.0   48.0\n",
       "..       ...     ...    ...    ...\n",
       "112  57124.0  6820.0  320.0    1.0\n",
       "114   9772.0  6820.0  318.0    1.0\n",
       "115    907.0  6820.0  317.0    1.0\n",
       "116   4924.0  6820.0  316.0    1.0\n",
       "118  56756.0  6820.0  314.0    1.0\n",
       "\n",
       "[328 rows x 4 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.DataFrame(scored_toks[89], columns=['token', 'label', 'rank', 'score']).sort_values(by='score', ascending=False)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7951e3-9197-46e3-85bd-36c5660b3cae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shape</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>scored_toks</th>\n",
       "      <td>(104, 328, 4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>binned_toks</th>\n",
       "      <td>(104, 328)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>probs</th>\n",
       "      <td>(104, 328)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_valid</th>\n",
       "      <td>(104, 328)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bin_size</th>\n",
       "      <td>(11,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bin_bds</th>\n",
       "      <td>(8, 2)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     shape\n",
       "scored_toks  (104, 328, 4)\n",
       "binned_toks     (104, 328)\n",
       "probs           (104, 328)\n",
       "is_valid        (104, 328)\n",
       "bin_size             (11,)\n",
       "bin_bds             (8, 2)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = partial(namestr, namespace=globals())\n",
    "row_vals = apply(torch.Tensor.size, (scored_toks, binned_toks, probs, is_valid, bin_size, bin_bds))\n",
    "pd.DataFrame(index = list(itertools.chain.from_iterable(apply(name, [scored_toks, binned_toks, probs, is_valid, bin_size, bin_bds]))), columns=['shape'], data={'shape': row_vals})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb14e95-6969-41db-a38e-6b2d4a74c03d",
   "metadata": {},
   "source": [
    "Lowest numbered bin contains irrelevant tokens for a label, and the highest numbered bin contains most relevant tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f36a730-4635-45ef-8bc2-73c456f2b8ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bin #</th>\n",
       "      <th>bin_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    bin #  bin_size\n",
       "0       0        30\n",
       "1       1       180\n",
       "2       2        75\n",
       "3       3        27\n",
       "4       4        10\n",
       "5       5         4\n",
       "6       6         1\n",
       "7       7         0\n",
       "8       8         0\n",
       "9       9         0\n",
       "10     10         1"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.DataFrame({'bin #': range(len(bin_size)), \n",
    "                    #'bin_bds': list(bin_bds.numpy()), \n",
    "                    'bin_size': bin_size})\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6a647f-8d27-424c-b60c-1f6f368381e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lbl</th>\n",
       "      <th>lbl_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>52</td>\n",
       "      <td>018.95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    lbl lbl_val\n",
       "52   52  018.95"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_toks = pd.read_feather(source/'mimic3-9k_tok.ft')\n",
    "df_lbs = pd.read_feather(source/'mimic3-9k_lbl.ft')\n",
    "\n",
    "a_lbl = np.random.choice(pdl.num_lbs)\n",
    "df_lbs.iloc[[a_lbl]]\n",
    "# df_lbs.loc[[a_lbl]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41768f8a-0052-4bc0-9fa9-92f77783c472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>tok_val</th>\n",
       "      <th>score</th>\n",
       "      <th>probs</th>\n",
       "      <th>binned_toks</th>\n",
       "      <th>size</th>\n",
       "      <th>is_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>4924</td>\n",
       "      <td>distance</td>\n",
       "      <td>101.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>4056</td>\n",
       "      <td>defined</td>\n",
       "      <td>63.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2100</td>\n",
       "      <td>wnl</td>\n",
       "      <td>55.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>436</td>\n",
       "      <td>change</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>591</td>\n",
       "      <td>motion</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1327</td>\n",
       "      <td>residual</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22</td>\n",
       "      <td>patient</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1788</td>\n",
       "      <td>thursday</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1699</td>\n",
       "      <td>clopidogrel</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2021</td>\n",
       "      <td>saturations</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1944</td>\n",
       "      <td>4l</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2265</td>\n",
       "      <td>hemodynamics</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2467</td>\n",
       "      <td>according</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2274</td>\n",
       "      <td>medial</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2998</td>\n",
       "      <td>synagis</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3159</td>\n",
       "      <td>film</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3748</td>\n",
       "      <td>dispo</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.024691</td>\n",
       "      <td>3</td>\n",
       "      <td>27</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3401</td>\n",
       "      <td>twenty</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.024691</td>\n",
       "      <td>3</td>\n",
       "      <td>27</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3575</td>\n",
       "      <td>attached</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.024691</td>\n",
       "      <td>3</td>\n",
       "      <td>27</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4598</td>\n",
       "      <td>stopping</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.024691</td>\n",
       "      <td>3</td>\n",
       "      <td>27</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    token       tok_val  score     probs  binned_toks  size  is_valid\n",
       "30   4924      distance  101.0  0.333333           10     1       0.0\n",
       "25   4056       defined   63.0  0.333333            6     1       0.0\n",
       "10   2100           wnl   55.0  0.166667            5     4       1.0\n",
       "2     436        change   51.0  0.166667            5     4       0.0\n",
       "3     591        motion   48.0  0.166667            5     4       1.0\n",
       "5    1327      residual   45.0  0.166667            5     4       0.0\n",
       "0      22       patient   43.0  0.066667            4    10       0.0\n",
       "7    1788      thursday   42.0  0.066667            4    10       0.0\n",
       "6    1699   clopidogrel   40.0  0.066667            4    10       0.0\n",
       "9    2021   saturations   39.0  0.066667            4    10       0.0\n",
       "8    1944            4l   38.0  0.066667            4    10       0.0\n",
       "11   2265  hemodynamics   37.0  0.066667            4    10       0.0\n",
       "13   2467     according   36.0  0.066667            4    10       0.0\n",
       "12   2274        medial   35.0  0.066667            4    10       0.0\n",
       "14   2998       synagis   34.0  0.066667            4    10       0.0\n",
       "15   3159          film   34.0  0.066667            4    10       0.0\n",
       "20   3748         dispo   33.0  0.024691            3    27       0.0\n",
       "16   3401        twenty   32.0  0.024691            3    27       0.0\n",
       "18   3575      attached   32.0  0.024691            3    27       0.0\n",
       "27   4598      stopping   31.0  0.024691            3    27       0.0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3 = pd.DataFrame({'token': scored_toks[a_lbl, :, 0] ,'score': scored_toks[a_lbl, :, -1], 'probs': probs[a_lbl], \n",
    "                    'binned_toks': binned_toks[a_lbl], \n",
    "                    #'bds': list(bin_bds[binned_toks[a_lbl]].numpy()), \n",
    "                    'size': bin_size[binned_toks[a_lbl]], \n",
    "                    'is_valid': is_valid[a_lbl]})\n",
    "df3 = df_toks.merge(df3, on='token')\n",
    "df3.sort_values(by='score', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2f27a3-5b48-40d7-9f68-ce28234dc0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eqs(is_valid[a_lbl].sum(), df3['is_valid'].sum(), pdl.val_sl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62528572-38e2-426f-b7f4-aa048eac6705",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>tok_val</th>\n",
       "      <th>score</th>\n",
       "      <th>probs</th>\n",
       "      <th>binned_toks</th>\n",
       "      <th>size</th>\n",
       "      <th>is_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2100</td>\n",
       "      <td>wnl</td>\n",
       "      <td>55.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>591</td>\n",
       "      <td>motion</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3469</td>\n",
       "      <td>brace</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.024691</td>\n",
       "      <td>3</td>\n",
       "      <td>27</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>6460</td>\n",
       "      <td>arriving</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.024691</td>\n",
       "      <td>3</td>\n",
       "      <td>27</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>36874</td>\n",
       "      <td>discrepant</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.008889</td>\n",
       "      <td>2</td>\n",
       "      <td>75</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>11782</td>\n",
       "      <td>paraseptal</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.008889</td>\n",
       "      <td>2</td>\n",
       "      <td>75</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>12787</td>\n",
       "      <td>microangiopathy</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.008889</td>\n",
       "      <td>2</td>\n",
       "      <td>75</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>12108</td>\n",
       "      <td>ascities</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.003704</td>\n",
       "      <td>1</td>\n",
       "      <td>180</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>12119</td>\n",
       "      <td>mso4</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.003704</td>\n",
       "      <td>1</td>\n",
       "      <td>180</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>20191</td>\n",
       "      <td>svts</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.003704</td>\n",
       "      <td>1</td>\n",
       "      <td>180</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>18491</td>\n",
       "      <td>traumas</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.003704</td>\n",
       "      <td>1</td>\n",
       "      <td>180</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>24673</td>\n",
       "      <td>pupilary</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.003704</td>\n",
       "      <td>1</td>\n",
       "      <td>180</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>35391</td>\n",
       "      <td>bender</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.003704</td>\n",
       "      <td>1</td>\n",
       "      <td>180</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>39731</td>\n",
       "      <td>statble</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.003704</td>\n",
       "      <td>1</td>\n",
       "      <td>180</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>48054</td>\n",
       "      <td>tattooes</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.003704</td>\n",
       "      <td>1</td>\n",
       "      <td>180</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>49147</td>\n",
       "      <td>peridiverticulitis</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.003704</td>\n",
       "      <td>1</td>\n",
       "      <td>180</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     token             tok_val  score     probs  binned_toks  size  is_valid\n",
       "10    2100                 wnl   55.0  0.166667            5     4       1.0\n",
       "3      591              motion   48.0  0.166667            5     4       1.0\n",
       "17    3469               brace   28.0  0.024691            3    27       1.0\n",
       "40    6460            arriving   24.0  0.024691            3    27       1.0\n",
       "195  36874          discrepant   18.0  0.008889            2    75       1.0\n",
       "63   11782          paraseptal   12.0  0.008889            2    75       1.0\n",
       "72   12787     microangiopathy   12.0  0.008889            2    75       1.0\n",
       "67   12108            ascities   11.0  0.003704            1   180       1.0\n",
       "68   12119                mso4   11.0  0.003704            1   180       1.0\n",
       "110  20191                svts   10.0  0.003704            1   180       1.0\n",
       "105  18491             traumas    8.0  0.003704            1   180       1.0\n",
       "130  24673            pupilary    8.0  0.003704            1   180       1.0\n",
       "183  35391              bender    5.0  0.003704            1   180       1.0\n",
       "215  39731             statble    5.0  0.003704            1   180       1.0\n",
       "270  48054            tattooes    2.0  0.003704            1   180       1.0\n",
       "274  49147  peridiverticulitis    2.0  0.003704            1   180       1.0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3[df3['is_valid'] == 1].sort_values(by='score', ascending=False)#.groupby('binned_toks').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b89591-dff1-4f7e-8c72-13981ba6fbbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 63 labels out of total 104, in the validation set we have at least one top 2 bin\n"
     ]
    }
   ],
   "source": [
    "top_lens = pdl.count_topbins()\n",
    "test_eq(top_lens.shape, [pdl.num_lbs])\n",
    "print(f\"For {torch.where(top_lens >= 1)[0].numel()} labels out of total {pdl.num_lbs}, in the validation set we have at least one top 2 bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33897661-b022-4031-97c7-6532c01783a1",
   "metadata": {},
   "source": [
    "Prepare the train/val dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c071e1-57fe-4f11-89d0-4ba4418ea461",
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_dset, val_dset = pdl.datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd95cfb-1b40-4711-acbd-336b16931114",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| trn_dset.shape: torch.Size([104, 328, 4])\n",
      "    val_dset.shape: torch.Size([104, 16, 4])\n"
     ]
    }
   ],
   "source": [
    "test_eq(val_dset.shape, (scored_toks.shape[0], val_sl, scored_toks.shape[2]))\n",
    "test_eq(trn_dset.shape, scored_toks.shape) # Use this if you don't want to remove the validation tokens\n",
    "ic(trn_dset.shape, val_dset.shape);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ab10db-03fe-4d9a-814d-7f635521075e",
   "metadata": {},
   "source": [
    "Again, `untar_xxx` has got the trn/val split for the full dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a8a4b1-0f79-46a7-92e7-ef390d64dfea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#1) [Path('/home/deb/.xcube/data/mimic3_l2r/trn_val_split.pkl')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L(source.glob(\"**/*split*.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7964660a-4bcb-476b-b81e-e95e76a5e595",
   "metadata": {},
   "source": [
    "Now that we have prepared the train/valid split we can delete `scored_toks` and `is_valid` to reclaim some memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6945e1-7ab3-4e76-902d-1851ec99c3cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "376"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scored_toks, is_valid  = None, None\n",
    "import gc; gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17620c93-3687-4fed-be47-cad5f1a234d1",
   "metadata": {},
   "source": [
    "This stuff goes inside the custom `DataLoader`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af7278b-6ade-43fa-9137-fd278ea33cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 deb deb 8191681843 Dec  2 18:02 /home/deb/xcube/nbs/examples/mimic/sample/trn_val_split.pkl\n",
      "-rw-r--r-- 1 deb deb 573459 Dec  8 14:49 /home/deb/xcube/nbs/examples/mimic/sample/trn_val_split_tiny.pkl\n"
     ]
    }
   ],
   "source": [
    "# datetime.fromtimestamp((path/'trn_val_split.pkl').stat().st_ctime)\n",
    "!ls -la {path/'trn_val_split.pkl'}\n",
    "!ls -la {path/'trn_val_split_tiny.pkl'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb68d88-81a2-4523-a941-ec2d8a63f867",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| trn.shape: torch.Size([104, 328, 4])\n",
      "    val_dset.shape: torch.Size([1, 104, 16, 4])\n"
     ]
    }
   ],
   "source": [
    "# trn, val_dset = torch.load('trn_val_split.pkl')\n",
    "trn, val_dset = torch.load('trn_val_split_tiny.pkl')\n",
    "val_dset = val_dset.unsqueeze(0)\n",
    "ic(trn.shape, val_dset.shape);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f687f3-3383-4b04-a31c-fed459410d89",
   "metadata": {},
   "source": [
    "1. Shuffle the training dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5e2438-2dca-405f-910e-b8ef2a37a8d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| randperm.shape: torch.Size([57352])\n"
     ]
    }
   ],
   "source": [
    "randperm = torch.randint(low=0, high=trn.shape[1], size=(trn.shape[1],))\n",
    "ic(randperm.shape);\n",
    "trn = trn[:, randperm]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adca3dc9-af0a-4c17-827a-2aea31fb679b",
   "metadata": {},
   "source": [
    "2. Split the training set into sqs that make the data and batching them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dab3198-9e00-4647-9d46-e2b7409d3ead",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGBA size=500x250>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = Image.open(path/'LRdataloader.png').resize((500,250))\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c2695a-63b3-4736-a8f7-24f24c8cec8c",
   "metadata": {},
   "source": [
    "Parameters you need to select: `lbs_chunks`, `tok_sl`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91eade6a-bc93-40c3-be49-e39826a2e4da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| size_of_dim0: 2231.0, pad_len_dim0: 2\n",
      "ic| trn_pad.shape: torch.Size([8924, 57352, 4])\n"
     ]
    }
   ],
   "source": [
    "lbs_chunks = 4\n",
    "# lbs_chunks = 32 # for tiny\n",
    "size_of_dim0 = torch.ceil(trn.new_empty(1).fill_(trn.shape[0]/lbs_chunks)).item()\n",
    "pad_len_dim0 = int(lbs_chunks * np.floor(trn.shape[0]/lbs_chunks) + lbs_chunks - trn.shape[0]) # smallest number greater than `trn.shape[0]` divisible by `lbs_chunks`:\n",
    "ic(size_of_dim0, pad_len_dim0);\n",
    "trn_pad = F.pad(trn, (0,0,0,0,0,pad_len_dim0), value=-1)\n",
    "ic(trn_pad.shape);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6ffa3d-b894-4ee7-aeda-7a072a06df5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| len(trn_sqs): 897\n",
      "    trn_sqs[0].shape: torch.Size([8924, 64, 4])\n",
      "    trn_sqs[1].shape: torch.Size([8924, 64, 4])\n",
      "    trn_sqs[-1].shape: torch.Size([8924, 8, 4])\n",
      "ic| deficit: 56\n"
     ]
    }
   ],
   "source": [
    "trn_sl = 64\n",
    "trn_sqs = list(torch.split(trn_pad, split_size_or_sections=trn_sl, dim=1))\n",
    "ic(len(trn_sqs), trn_sqs[0].shape, trn_sqs[1].shape, trn_sqs[-1].shape);\n",
    "test_eq(len(trn_sqs), np.ceil(trn.shape[1]/trn_sl))\n",
    "test_eq(trn_sqs[-1].shape, (trn_pad.shape[0], trn_pad.shape[1]%trn_sl,4));\n",
    "deficit = trn_sl - trn_sqs[-1].shape[1]\n",
    "ic(deficit);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4e2ff7-3990-4531-99c8-05654e5e2acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| len(trn_sqs): 897\n",
      "    trn_sqs[0].shape: torch.Size([8924, 64, 4])\n",
      "    trn_sqs[1].shape: torch.Size([8924, 64, 4])\n",
      "    trn_sqs[-1].shape: torch.Size([8924, 64, 4])\n"
     ]
    }
   ],
   "source": [
    "if deficit: \n",
    "    test_eq(trn_sqs[-1].shape, (trn_pad.shape[0], trn_pad.shape[1]%trn_sl,4));\n",
    "    # trn_sqs[-1] = torch.concat((trn_sqs[-1],trn.new_empty((trn_sqs[-1].shape[0], deficit,3)).fill_(-1)), dim=1)\n",
    "    trn_sqs[-1] = trn_sqs[-1].repeat_interleave(trn_sl//trn_sqs[-1].shape[1], dim=1)\n",
    "test_eq(trn_sqs[-1].shape, (trn_pad.shape[0], trn_sl,4));\n",
    "ic(len(trn_sqs), trn_sqs[0].shape, trn_sqs[1].shape, trn_sqs[-1].shape);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28b00c6-d60a-4ad2-a8c2-8d91ad7d44d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_sqs = map(partial(torch.chunk, chunks=lbs_chunks), trn_sqs)\n",
    "trn_sqs = itertools.chain.from_iterable(trn_sqs)\n",
    "trn_sqs, trn_sqs1, trn_sqs2, trn_sqs3 = itertools.tee(trn_sqs, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087e1854-eecb-4659-8fbf-105689fee879",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((#1) [torch.Size([2231, 64, 4])], 3588)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L(trn_sqs1).map(Tensor.size).unique(), len(L(trn_sqs2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b77d689-fbce-46cb-acb3-7e81e37fe9d2",
   "metadata": {},
   "source": [
    "This is where the number 3588 is coming from:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f32cc8f-dba3-4aab-bbf2-925dd1e0f0e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.125"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(328/64) # tok_sl 64 (#tok_chunks = #toks/trn_sl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdefcc55-60fc-4b41-bf17-70ce9b5bb391",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "896.125"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "57352/64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b614710c-184b-408d-84ea-e7a4eafb161e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6694c62e-d292-4363-be98-068ff9e24b0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32.0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(128/4) # labels chunk size is 4 (#lbs_chunk_sz = #lbs/lbs_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3f5b70-88aa-4a6b-b3e0-b1fa90d6ff96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2231.0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "8924/4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a904c79-d7a6-43d1-a877-1d46f4b4bb38",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1191a4b8-0ad9-4d42-88f4-7379df980860",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "6*32 #(tok_chunks * lbs_chunks) This is basically the # of datapoints, which gets batched. So #mini-batches in an epoch = #datapoints/bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6c6254-a774-4f43-be55-7321e25c4ced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3588"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "897*4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f31d946-19af-47de-9b6f-d782517b1059",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213ccb7b-7007-4e72-8407-4c4d7ff3def0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = trn_sqs\n",
    "# dset_it = iter(dset)\n",
    "# n = next(dset_it)\n",
    "# ic(n.shape);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe0abae-0d79-42bb-8399-0ddc6ddb2af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 64\n",
    "bs_tiny = 8\n",
    "# btchs = chunked(dset, bs_tiny)\n",
    "# btch_it = iter(btchs)\n",
    "# xb = next(btch_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9068b9b2-eb39-442a-b74e-8aed19d16841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ic(type(xb), len(xb), xb[0].shape)\n",
    "# btch = torch.stack(xb)\n",
    "# ic(btch.shape);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adde24e4-fbec-46b1-bff3-018781db7e6e",
   "metadata": {},
   "source": [
    "The number of mini-batches in an epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a0ed4c-8c95-48a8-b704-c4455a327e9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56.0625, 24.0)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3588/bs, 192/bs_tiny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd55159-e347-4104-86a4-3ad22bd2cf4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of mini-batches in an epoch\n",
    "int(np.ceil((np.ceil(trn.shape[1]/trn_sl) * lbs_chunks)/bs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793d52ae-4e4d-4e56-870e-7490ef42a23c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xb.shape = torch.Size([64, 2231, 64, 4]), lbs.shape = torch.Size([64, 2231, 1])\n",
      "xb.shape = torch.Size([64, 2231, 64, 4]), lbs.shape = torch.Size([64, 2231, 1])\n",
      "xb.shape = torch.Size([64, 2231, 64, 4]), lbs.shape = torch.Size([64, 2231, 1])\n",
      "xb.shape = torch.Size([64, 2231, 64, 4]), lbs.shape = torch.Size([64, 2231, 1])\n",
      "xb.shape = torch.Size([64, 2231, 64, 4]), lbs.shape = torch.Size([64, 2231, 1])\n",
      "xb.shape = torch.Size([64, 2231, 64, 4]), lbs.shape = torch.Size([64, 2231, 1])\n",
      "xb.shape = torch.Size([64, 2231, 64, 4]), lbs.shape = torch.Size([64, 2231, 1])\n",
      "xb.shape = torch.Size([64, 2231, 64, 4]), lbs.shape = torch.Size([64, 2231, 1])\n",
      "xb.shape = torch.Size([64, 2231, 64, 4]), lbs.shape = torch.Size([64, 2231, 1])\n",
      "xb.shape = torch.Size([64, 2231, 64, 4]), lbs.shape = torch.Size([64, 2231, 1])\n",
      "xb.shape = torch.Size([64, 2231, 64, 4]), lbs.shape = torch.Size([64, 2231, 1])\n",
      "xb.shape = torch.Size([64, 2231, 64, 4]), lbs.shape = torch.Size([64, 2231, 1])\n",
      "xb.shape = torch.Size([64, 2231, 64, 4]), lbs.shape = torch.Size([64, 2231, 1])\n",
      "xb.shape = torch.Size([64, 2231, 64, 4]), lbs.shape = torch.Size([64, 2231, 1])\n",
      "xb.shape = torch.Size([64, 2231, 64, 4]), lbs.shape = torch.Size([64, 2231, 1])\n",
      "xb.shape = torch.Size([64, 2231, 64, 4]), lbs.shape = torch.Size([64, 2231, 1])\n",
      "xb.shape = torch.Size([64, 2231, 64, 4]), lbs.shape = torch.Size([64, 2231, 1])\n",
      "xb.shape = torch.Size([64, 2231, 64, 4]), lbs.shape = torch.Size([64, 2231, 1])\n",
      "xb.shape = torch.Size([64, 2231, 64, 4]), lbs.shape = torch.Size([64, 2231, 1])\n",
      "xb.shape = torch.Size([64, 2231, 64, 4]), lbs.shape = torch.Size([64, 2231, 1])\n",
      "xb.shape = torch.Size([64, 2231, 64, 4]), lbs.shape = torch.Size([64, 2231, 1])\n",
      "xb.shape = torch.Size([64, 2231, 64, 4]), lbs.shape = torch.Size([64, 2231, 1])\n",
      "xb.shape = torch.Size([64, 2231, 64, 4]), lbs.shape = torch.Size([64, 2231, 1])\n",
      "xb.shape = torch.Size([64, 2231, 64, 4]), lbs.shape = torch.Size([64, 2231, 1])\n",
      "xb.shape = torch.Size([64, 2231, 64, 4]), lbs.shape = torch.Size([64, 2231, 1])\n",
      "xb.shape = torch.Size([64, 2231, 64, 4]), lbs.shape = torch.Size([64, 2231, 1])\n",
      "xb.shape = torch.Size([64, 2231, 64, 4]), lbs.shape = torch.Size([64, 2231, 1])\n",
      "xb.shape = torch.Size([64, 2231, 64, 4]), lbs.shape = torch.Size([64, 2231, 1])\n",
      "xb.shape = torch.Size([64, 2231, 64, 4]), lbs.shape = torch.Size([64, 2231, 1])\n",
      "xb.shape = torch.Size([64, 2231, 64, 4]), lbs.shape = torch.Size([64, 2231, 1])\n",
      "xb.shape = torch.Size([64, 2231, 64, 4]), lbs.shape = torch.Size([64, 2231, 1])\n",
      "xb.shape = torch.Size([64, 2231, 64, 4]), lbs.shape = torch.Size([64, 2231, 1])\n",
      "xb.shape = torch.Size([64, 2231, 64, 4]), lbs.shape = torch.Size([64, 2231, 1])\n",
      "xb.shape = torch.Size([64, 2231, 64, 4]), lbs.shape = torch.Size([64, 2231, 1])\n",
      "xb.shape = torch.Size([64, 2231, 64, 4]), lbs.shape = torch.Size([64, 2231, 1])\n",
      "xb.shape = torch.Size([64, 2231, 64, 4]), lbs.shape = torch.Size([64, 2231, 1])\n",
      "xb.shape = torch.Size([64, 2231, 64, 4]), lbs.shape = torch.Size([64, 2231, 1])\n",
      "xb.shape = torch.Size([64, 2231, 64, 4]), lbs.shape = torch.Size([64, 2231, 1])\n",
      "xb.shape = torch.Size([64, 2231, 64, 4]), lbs.shape = torch.Size([64, 2231, 1])\n",
      "xb.shape = torch.Size([64, 2231, 64, 4]), lbs.shape = torch.Size([64, 2231, 1])\n",
      "xb.shape = torch.Size([64, 2231, 64, 4]), lbs.shape = torch.Size([64, 2231, 1])\n",
      "xb.shape = torch.Size([64, 2231, 64, 4]), lbs.shape = torch.Size([64, 2231, 1])\n",
      "xb.shape = torch.Size([64, 2231, 64, 4]), lbs.shape = torch.Size([64, 2231, 1])\n",
      "xb.shape = torch.Size([64, 2231, 64, 4]), lbs.shape = torch.Size([64, 2231, 1])\n",
      "xb.shape = torch.Size([64, 2231, 64, 4]), lbs.shape = torch.Size([64, 2231, 1])\n",
      "xb.shape = torch.Size([64, 2231, 64, 4]), lbs.shape = torch.Size([64, 2231, 1])\n",
      "xb.shape = torch.Size([64, 2231, 64, 4]), lbs.shape = torch.Size([64, 2231, 1])\n",
      "xb.shape = torch.Size([64, 2231, 64, 4]), lbs.shape = torch.Size([64, 2231, 1])\n",
      "xb.shape = torch.Size([64, 2231, 64, 4]), lbs.shape = torch.Size([64, 2231, 1])\n",
      "xb.shape = torch.Size([64, 2231, 64, 4]), lbs.shape = torch.Size([64, 2231, 1])\n",
      "xb.shape = torch.Size([64, 2231, 64, 4]), lbs.shape = torch.Size([64, 2231, 1])\n",
      "xb.shape = torch.Size([64, 2231, 64, 4]), lbs.shape = torch.Size([64, 2231, 1])\n",
      "xb.shape = torch.Size([64, 2231, 64, 4]), lbs.shape = torch.Size([64, 2231, 1])\n",
      "xb.shape = torch.Size([64, 2231, 64, 4]), lbs.shape = torch.Size([64, 2231, 1])\n",
      "xb.shape = torch.Size([64, 2231, 64, 4]), lbs.shape = torch.Size([64, 2231, 1])\n",
      "xb.shape = torch.Size([64, 2231, 64, 4]), lbs.shape = torch.Size([64, 2231, 1])\n",
      "xb.shape = torch.Size([4, 2231, 64, 4]), lbs.shape = torch.Size([4, 2231, 1])\n",
      "The number of minibatches = 57\n"
     ]
    }
   ],
   "source": [
    "c = 0\n",
    "for btch in chunked(dset, bs):\n",
    "    xb = torch.stack(btch)\n",
    "    lbs = torch.unique(xb[:,:,:,1], dim=-1)\n",
    "    print(f\"{xb.shape = }, {lbs.shape = }\")\n",
    "    c += 1\n",
    "print(f\"The number of minibatches = {c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e202570f-57bc-427d-af89-1365925a8034",
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_dset = torch.concat(trn_sqs)\n",
    "test_eq(trn_dset.shape, (trn.shape[0]*len(trn_sqs), trn_sl, 3))\n",
    "ic(trn_dset.shape, val_dset.shape);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac16792e-2f21-4bb1-a31e-d9fa7a111bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| xb.shape: torch.Size([4, 2231, 64, 4])\n",
      "    torch.unique(xb[:,:,:,1], dim=-1).shape: torch.Size([4, 2231, 1])\n"
     ]
    }
   ],
   "source": [
    "ic(xb.shape, torch.unique(xb[:,:,:,1], dim=-1).shape);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80280950-eb4a-4748-8e81-f4227e608b1d",
   "metadata": {},
   "source": [
    "Showtime: Writing our custom `DataLoader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f4184c-8843-4617-8c6f-d64b48809410",
   "metadata": {},
   "outputs": [],
   "source": [
    "%less {inspect.getsourcefile(DataLoader)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3734e0bb-85b7-4930-ae1e-ab74faa17461",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrnDataLoader(DataLoader):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.sl, self.lbs_chunks = kwargs.pop('sl', None), kwargs.pop('lbs_chunks', None)\n",
    "        if self.sl is None: self.sl = 64\n",
    "        if self.lbs_chunks is None: self.lbs_chunks = 4\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "    def randomize(self):\n",
    "        seed = np.random.default_rng().integers(0, 2**32-1, 1).item()\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "    \n",
    "    def shuffle_fn(self, idxs): return self.rng.permutation(idxs)\n",
    "\n",
    "    def get_idxs(self):\n",
    "        if self.n is not None: idxs = range(self.n)\n",
    "        if self.shuffle: idxs = (idx for idx in self.shuffle_fn(idxs))\n",
    "        return idxs\n",
    "    \n",
    "    def create_batch(self, start_idx):\n",
    "        return self.dset[start_idx: min(start_idx+self.bs, self.dset.shape[0])]\n",
    "        # if self.device: to_device(btch, self.device)\n",
    "        # return btch\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.ceil((np.ceil(self.dataset.shape[1]/self.sl) * self.lbs_chunks)/self.bs))\n",
    "    \n",
    "    def before_iter(self):\n",
    "        # shuffling\n",
    "        randperm = torch.randint(low=0, high=self.dataset.shape[1], size=(self.dataset.shape[1],))\n",
    "        self.dataset = self.dataset[:, randperm]\n",
    "        # self.lbs_chunks = 4\n",
    "        size_of_dim0 = torch.ceil(self.dataset.new_empty(1).fill_(self.dataset.shape[0]/self.lbs_chunks)).item()\n",
    "        pad_len_dim0 = int(self.lbs_chunks * np.floor(self.dataset.shape[0]/self.lbs_chunks) + self.lbs_chunks - self.dataset.shape[0])\n",
    "        self.dataset_pad = F.pad(self.dataset, (0,0,0,0,0,pad_len_dim0), value=-1)\n",
    "\n",
    "        trn_sqs = list(torch.split(self.dataset_pad, split_size_or_sections=self.sl, dim=1))\n",
    "        test_eq(len(trn_sqs), np.ceil(self.dataset_pad.shape[1]/self.sl))\n",
    "        test_eq(trn_sqs[-1].shape, (self.dataset_pad.shape[0], self.dataset_pad.shape[1]%self.sl,4))\n",
    "        deficit = self.sl - trn_sqs[-1].shape[1]\n",
    "        if deficit: \n",
    "            test_eq(trn_sqs[-1].shape, (self.dataset_pad.shape[0], self.dataset_pad.shape[1]%self.sl,4));\n",
    "            # trn_sqs[-1] = torch.concat((trn_sqs[-1], self.dataset_pad.new_empty((trn_sqs[-1].shape[0], deficit,3)).fill_(-1)), dim=1)\n",
    "            trn_sqs[-1] = trn_sqs[-1].repeat_interleave(self.sl//trn_sqs[-1].shape[1], dim=1)\n",
    "        test_eq(trn_sqs[-1].shape, (self.dataset_pad.shape[0], self.sl,4));\n",
    "        # self.dset = torch.concat(trn_sqs)\n",
    "        # self.dset = torch.stack(trn_sqs)\n",
    "        \n",
    "        trn_sqs = map(partial(torch.chunk, chunks=self.lbs_chunks), trn_sqs)\n",
    "        trn_sqs = itertools.chain.from_iterable(trn_sqs)\n",
    "        self.dset = trn_sqs\n",
    "        # test_eq(self.dset.shape, (self.dataset_pad.shape[0]*len(trn_sqs), self.sl, 3))\n",
    "        # test_eq(self.dset.shape, (len(trn_sqs), self.dataset_pad.shape[0], self.sl, 3))\n",
    "        # print(f\"{self.dset.shape=}\")\n",
    "        # yield from (btch for btch in dset.split(self.bs))\n",
    "    \n",
    "    def create_batches(self, samps):\n",
    "            # trn_sqs = list(torch.split(self.dataset, split_size_or_sections=self.sl, dim=1))\n",
    "            # test_eq(len(trn_sqs), np.ceil(self.dataset.shape[1]/self.sl))\n",
    "            # test_eq(trn_sqs[-1].shape, (self.dataset.shape[0], self.dataset.shape[1]%self.sl,3))\n",
    "            # deficit = self.sl - trn_sqs[-1].shape[1]\n",
    "            # if deficit: \n",
    "            #     test_eq(trn_sqs[-1].shape, (self.dataset.shape[0], self.dataset.shape[1]%self.sl,3));\n",
    "            #     trn_sqs[-1] = torch.concat((trn_sqs[-1], self.dataset.new_empty((trn_sqs[-1].shape[0], deficit,3)).fill_(-1)), dim=1)\n",
    "            # test_eq(trn_sqs[-1].shape, (self.dataset.shape[0], self.sl,3));\n",
    "            # # self.dset = torch.concat(trn_sqs)\n",
    "            # self.dset = torch.stack(trn_sqs)\n",
    "            # # test_eq(self.dset.shape, (self.dataset.shape[0]*len(trn_sqs), self.sl, 3))\n",
    "            # test_eq(self.dset.shape, (len(trn_sqs), self.dataset.shape[0], self.sl, 3))\n",
    "            # print(f\"{self.dset.shape=}\")\n",
    "            # # yield from (btch for btch in dset.split(self.bs))\n",
    "        # chunks = range(0, self.dset.shape[0], self.bs)\n",
    "        # with ProcessPoolExecutor(self.n_workers) as ex:\n",
    "        # with Pool(processes=self.num_workers) as pool:\n",
    "        # yield from pool.imap_unordered(self.create_batch, chunks, 16)\n",
    "        # yield from map(self.create_batch, chunks)\n",
    "        # yield from chunked(self.dset, chunk_sz=self.bs)\n",
    "        yield from (torch.stack(btch) for btch in self.chunkify(self.dset))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d705b54a-e5fd-42b5-aa9e-c8be6d9a75da",
   "metadata": {},
   "outputs": [],
   "source": [
    "%save dataloader.py _i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5459296f-32e5-41b4-b754-6fe8b1f478e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "class FDataLoader:\n",
    "    def __init__(self, dataset, sl=48, bs=128, n_workers=1, device=None):\n",
    "        store_attr('dataset,sl,bs,n_workers,device')\n",
    "        \n",
    "    def create_batch(self, start_idx):\n",
    "        btch = self.dset[start_idx: min(start_idx+self.bs, self.dset.shape[0])]\n",
    "        if self.device: to_device(btch, self.device)\n",
    "        return btch\n",
    "    \n",
    "    def __iter__(self):\n",
    "        trn_sqs = list(torch.split(self.dataset, split_size_or_sections=self.sl, dim=1))\n",
    "        test_eq(len(trn_sqs), np.ceil(self.dataset.shape[1]/self.sl))\n",
    "        test_eq(trn_sqs[-1].shape, (self.dataset.shape[0], self.dataset.shape[1]%self.sl,3))\n",
    "        deficit = self.sl - trn_sqs[-1].shape[1]\n",
    "        if deficit: \n",
    "            test_eq(trn_sqs[-1].shape, (self.dataset.shape[0], self.dataset.shape[1]%self.sl,3));\n",
    "            trn_sqs[-1] = torch.concat((trn_sqs[-1], self.dataset.new_empty((trn_sqs[-1].shape[0], deficit,3)).fill_(-1)), dim=1)\n",
    "        test_eq(trn_sqs[-1].shape, (self.dataset.shape[0], self.sl,3));\n",
    "        # self.dset = torch.concat(trn_sqs)\n",
    "        self.dset = torch.stack(trn_sqs)\n",
    "        # test_eq(self.dset.shape, (self.dataset.shape[0]*len(trn_sqs), self.sl, 3))\n",
    "        test_eq(self.dset.shape, (len(trn_sqs), self.dataset.shape[0], self.sl, 3))\n",
    "        print(f\"{self.dset.shape=}\")\n",
    "        # yield from (btch for btch in dset.split(self.bs))\n",
    "        chunks = np.arange(0, self.dset.shape[0], self.bs)\n",
    "        # with ProcessPoolExecutor(self.n_workers) as ex:\n",
    "        with Pool(processes=self.n_workers) as pool:\n",
    "            yield from pool.imap_unordered(self.create_batch, chunks, 16)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da52ccb-9d48-416f-94fe-f3ab17aaa024",
   "metadata": {},
   "source": [
    "Let's create the train/valid `DataLoaders`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5172ee3-b61e-4fff-91b6-06fb643714f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| trn.shape: torch.Size([8922, 57352, 4])\n",
      "    val_dset.shape: torch.Size([1, 8922, 32, 4])\n"
     ]
    }
   ],
   "source": [
    "trn, val_dset = torch.load('trn_val_split.pkl')\n",
    "# trn, val_dset = torch.load('trn_val_split_tiny.pkl')\n",
    "val_dset = val_dset.unsqueeze(0)\n",
    "ic(trn.shape, val_dset.shape);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0d0367-1032-4184-bb26-4b97b503c943",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_full = 32\n",
    "bs_tiny = 8\n",
    "sl = 64\n",
    "lbs_chunks_full = 4\n",
    "lbs_chunks_tiny = 32\n",
    "trn_dl = L2RDataLoader(dataset=trn, sl=sl, bs=bs_full, lbs_chunks=lbs_chunks_full, shuffle=False, after_batch=partial(to_device, device=default_device()), num_workers=0)\n",
    "# trn_dl = TrnDataLoader(dataset=trn, sl=sl_tiny, lbs_chunks=lbs_chunks_tiny, bs=bs_tiny, shuffle=False, after_batch=partial(to_device, device=default_device()), num_workers=0)\n",
    "# trn_dl = L2RDataLoader(dataset=trn, sl=sl, lbs_chunks=lbs_chunks_tiny, bs=bs_tiny, shuffle=False, after_batch=partial(to_device, device=default_device()), num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172fbd5f-0e5e-4cbe-a963-d18a0d5c12e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%less {inspect.getsourcefile(L2RDataLoader)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c69264-0018-4cbe-95f9-a88a026d68bc",
   "metadata": {},
   "source": [
    "Don't forget to check the length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23b6088-e120-4e47-a526-6559e8febc76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trn_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85cc802-3456-4113-bb10-82a55a98f372",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| trn_dl.device: None\n",
      "    trn_dl.num_workers: 1\n",
      "    trn_dl.fake_l.num_workers: 0\n"
     ]
    }
   ],
   "source": [
    "ic(trn_dl.device, trn_dl.num_workers, trn_dl.fake_l.num_workers);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3432f5d3-cf81-45a2-81ae-c96a4b0cf4f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| xb.shape: torch.Size([32, 2231, 64, 4])\n",
      "    xb.device: device(type='cuda', index=0)\n"
     ]
    }
   ],
   "source": [
    "xb = trn_dl.one_batch()\n",
    "ic(xb.shape, xb.device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30566a96-1115-41f8-a43b-d4c3c0759f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 46s, sys: 1min 45s, total: 3min 31s\n",
      "Wall time: 26.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for xb in trn_dl:\n",
    "    time.sleep(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d897214c-2a42-4c5e-b986-ff21cb43729f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| val_dl.device: None\n",
      "    val_dl.num_workers: 1\n",
      "    val_dl.fake_l.num_workers: 0\n"
     ]
    }
   ],
   "source": [
    "val_dl = DataLoader(val_dset, bs=1, shuffle=False, after_batch=partial(to_device, device=default_device()), num_workers=0)\n",
    "ic(val_dl.device, val_dl.num_workers, val_dl.fake_l.num_workers);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74e0a70-1925-4906-9ff9-589691b35fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| xb.shape: torch.Size([1, 8922, 32, 4])\n",
      "    xb.device: device(type='cuda', index=0)\n"
     ]
    }
   ],
   "source": [
    "xb = val_dl.one_batch()\n",
    "ic(xb.shape, xb.device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def96b13-c243-4d2c-ad0a-160691e66bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 145 ms, sys: 0 ns, total: 145 ms\n",
      "Wall time: 20.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for xb in val_dl:\n",
    "    time.sleep(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664b7352-7fe2-4dad-8066-cc98af0518e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = DataLoaders(trn_dl, val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abbd200-7581-4e45-a02e-59944297e564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dls_tiny = DataLoaders(trn_dl, val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882dd795-1b70-4c9f-8e98-755279555716",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(dls, dls_learn_rank_path, pickle_protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb30f44-5c5a-47da-9f57-1d19239da817",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('/home/deb/xcube/nbs/examples/mimic/sample/models/mimic3-9k_dls_learn_rank_tiny2.pkl')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name, ext = dls_learn_rank_tiny_path.name.split('.')\n",
    "new_name = name + '2.' + ext\n",
    "new_name = dls_learn_rank_tiny_path.parent/new_name\n",
    "new_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2dd3ec-21ae-4c98-915c-46bcc8b2d855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(dls_tiny, new_name, pickle_protocol=4)\n",
    "# torch.save(dls_tiny, dls_learn_rank_tiny_path, pickle_protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22e905a-0ba0-40a8-bb1c-f61e558065ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dls_learn_rank_path = dls_collab_path.parent/'mimic3-9k_dls_learn_rank.pkl'\n",
    "# dls_learn_rank_path\n",
    "\n",
    "# %%time\n",
    "# torch.save(dls_learn_rank, dls_learn_rank_path , pickle_protocol=4)\n",
    "\n",
    "# %%time\n",
    "# dls_learn_rank = torch.load(dls_learn_rank_path, map_location=lambda storage, loc: storage.cuda(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d5ac43-8c22-4722-a68b-630d089ffd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vals, cnts = torch.unique(xb[:, :, 1][:, 0].int(), return_counts=True)\n",
    "# pair = torch.concat((vals[...,None], cnts[...,None]), dim =-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6d4277-0daf-4dd4-a5b3-6185f58c1eb6",
   "metadata": {},
   "source": [
    "## Bloodshed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58aad4e5-08b9-49c0-947e-5e8ffee9269c",
   "metadata": {},
   "source": [
    "A little hack to circumvent custom pickle deserialization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807b5d64-4541-49b0-a46a-7ae5d0dfe27c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.TrnDataLoader"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('dataloader.py', 'r') as f: lines = f.readlines()\n",
    "exec(''.join(lines))\n",
    "TrnDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816053dd-a3aa-4010-91fe-3f97fc83d819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# dls_learn_rank = torch.load(dls_learn_rank_path, map_location=lambda storage, loc: storage.cuda(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5fa1c7-d97d-463d-bc8e-fdab5a1ab2ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.84 ms, sys: 275 µs, total: 2.11 ms\n",
      "Wall time: 1.94 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "dls = torch.load(dls_learn_rank_tiny_path)\n",
    "len(dls.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f773f817-5193-46ee-832f-332ddafbd5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# dls = torch.load(dls_learn_rank_tiny_path)\n",
    "# len(dls.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd2ff77-3df8-4ac6-94fc-0b0bf106f997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.29 s, sys: 0 ns, total: 7.29 s\n",
      "Wall time: 1.47 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for _ in range(5):\n",
    "    for xb in dls.train: time.sleep(0.01)\n",
    "    for xb in dls.valid: time.sleep(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161a4e2d-28b0-44e4-bd85-f4a523d01880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(dls.train).__name__, dls.train.__class__.__name__\n",
    "# TrnDataLoader.mro(), TrnDataLoader.__mro__\n",
    "# inspect.getmro(TrnDataLoader)\n",
    "# import fastai\n",
    "# %ls {fastai.__path__[0]}\n",
    "# coll_repr(object.__subclasses__(), max_n = 20)\n",
    "# dict.__module__\n",
    "# sys.modules['fastai']\n",
    "# L(pkgutil.iter_modules(fastai.__path__))[8]\n",
    "# inspect.getmembers(fastai)[8]\n",
    "# inspect.ismodule(fastai.data.load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7aa34a-9489-4c3d-8772-7f8da1bdb4ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| xb.shape: torch.Size([8, 4, 64, 4])\n",
      "    xb.device: device(type='cuda', index=0)\n"
     ]
    }
   ],
   "source": [
    "xb = dls.train.one_batch()\n",
    "ic(xb.shape, xb.device);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc8c4bc-c043-4214-9c53-88453f007428",
   "metadata": {},
   "source": [
    "**The L2R Models:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6b4183-8d88-4d0e-8db7-a7aad6736d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class L2R_DotProductBias(nn.Module):\n",
    "    def __init__(self, num_lbs, num_toks, num_factors, y_range=None):\n",
    "        super().__init__()\n",
    "        self.num_toks, self.num_lbs = num_toks+1, num_lbs+1 # +1 for the `padding_idx` \n",
    "        self.token_factors = nn.Embedding(self.num_toks, num_factors, padding_idx=-1)\n",
    "        self.token_bias = nn.Embedding(self.num_toks, 1, padding_idx=-1)\n",
    "        self.label_factors = nn.Embedding(self.num_lbs, num_factors, padding_idx=-1)\n",
    "        self.label_bias = nn.Embedding(self.num_lbs, 1, padding_idx=-1)\n",
    "        self.y_range = y_range\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        # import pdb; pdb.set_trace()\n",
    "        xb_toks = xb[:, :, :, 0].long() # xb[...,0] # shape (64, 2233, 64)\n",
    "        xb_lbs = torch.unique(xb[:, :, :, 1], dim=-1).flatten(start_dim=1).long() # shape (64, 2233, )\n",
    "        # To convert -1 which is the padding index to the last index:\n",
    "        xb_toks, xb_lbs= xb_toks%(num_toks+1), xb_lbs%(num_lbs+1)\n",
    "        \n",
    "        toks_embs = self.token_factors(xb_toks) # shape (64, 2233, 64, 400)\n",
    "        toks_shape = toks_embs.shape\n",
    "        toks_embs = toks_embs.view(-1, *toks_shape[2:]) # shape (64*2233, 64, 400)\n",
    "\n",
    "        lbs_embs = self.label_factors(xb_lbs) # shape (64, 2233, 400)\n",
    "        lbs_shape = lbs_embs.shape\n",
    "        lbs_embs = lbs_embs.view(-1, *lbs_shape[2:]).unsqueeze(dim=-1) # shape (64*2233, 400, 1)\n",
    "        \n",
    "        res = torch.bmm(toks_embs, lbs_embs) # shape (64*2233, 64, 1)\n",
    "        # res = torch.matmul(toks_embs, lbs_embs)\n",
    "        res = res.view(toks_shape[0], toks_shape[1], *res.shape[1:]) + self.token_bias(xb_toks) + self.label_bias(xb_lbs).unsqueeze(2) # shape (64, 2233, 64, 1)\n",
    "        \n",
    "        return sigmoid_range(res, *self.y_range) if self.y_range is not None else res\n",
    "        # return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d48df0a-d92e-4442-bdac-c2e7a553ea37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class L2R_NN(nn.Module):\n",
    "    def __init__(self, num_lbs, num_toks, num_factors, n_act = 200, y_range=None):\n",
    "        super().__init__()\n",
    "        self.num_toks, self.num_lbs = num_toks+1, num_lbs+1 # +1 for the `padding_idx` \n",
    "        self.token_factors = nn.Embedding(self.num_toks, num_factors, padding_idx=-1)\n",
    "        self.label_factors = nn.Embedding(self.num_lbs, num_factors, padding_idx=-1)\n",
    "        self.y_range = y_range\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(num_factors*2, n_act),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_act, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        # import pdb; pdb.set_trace()\n",
    "        xb_toks = xb[:, :, :, 0].long() # xb[...,0] # shape (64, 2233, 64)\n",
    "        xb_lbs = torch.unique(xb[:, :, :, 1], dim=-1).flatten(start_dim=1).long() # shape (64, 2233, )\n",
    "        # To convert -1 which is the padding index to the last index:\n",
    "        xb_toks, xb_lbs= xb_toks%(num_toks+1), xb_lbs%(num_lbs+1)\n",
    "        \n",
    "        toks_embs = self.token_factors(xb_toks) # shape (64, 2233, 64, 200)\n",
    "\n",
    "        lbs_embs = self.label_factors(xb_lbs) # shape (64, 2233, 200)\n",
    "        lbs_embs = lbs_embs.unsqueeze(2) # shape (64, 2233, 1, 200)\n",
    "        lbs_embs = lbs_embs.expand(-1, -1, xb.shape[2], -1)\n",
    "        \n",
    "        embs = torch.cat((toks_embs, lbs_embs), dim=-1) # shape (64, 2233, 64, 400)\n",
    "        res = self.layers(embs)\n",
    "        \n",
    "        return sigmoid_range(res, *self.y_range) if self.y_range is not None else res\n",
    "        # return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b832c2-5480-41dc-b529-1e47bf483672",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = model.parameters()\n",
    "dev_gen = (p.device for p in params)\n",
    "L(dev_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6230da1-80d8-4825-8cfb-64403f89d237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_factors, token_bias, label_factors, label_bias = map(partial(to_device, device=torch.device(\"cpu\")), model.parameters())\n",
    "# L(map(Tensor.size, (token_factors, token_bias, label_factors, label_bias)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f1d137-e6bc-4fb3-be64-8e1729aced5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| xb_toks.shape: torch.Size([64, 2242, 48])\n",
      "    xb_toks.device: device(type='cuda', index=0)\n",
      "ic| xb_lbs.shape: torch.Size([64, 2242])\n"
     ]
    }
   ],
   "source": [
    "xb_toks = xb[:, :, :, 0].long() # xb[...,0]\n",
    "ic(xb_toks.shape, xb_toks.device);\n",
    "xb_lbs = torch.unique(xb[:, :, :, 1], dim=-1).flatten(start_dim=1).long()\n",
    "ic(xb_lbs.shape);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54a5f3a-1575-418d-a9d5-ee574749d017",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.equal(xb_lbs[56, 890:1050], torch.unique(xb[56, 890:1050, :, 1], dim=-1).squeeze())\n",
    "torch.equal(torch.unique(xb_toks, dim=-1, sorted=False), xb_toks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cdc2eb-9f40-46b3-8c61-b6c4e8f3e433",
   "metadata": {},
   "source": [
    "To convert -1 to the last index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900dadbc-06f1-4d0f-9a70-2fe2dbef6e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "xb_toks = xb_toks % (num_toks+1)\n",
    "xb_lbs = xb_lbs % (num_lbs+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077baa38-c617-4ef9-a03e-eb63d25615d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| toks_embs.shape: torch.Size([64, 2242, 48, 400])\n",
      "ic| toks_embs.shape: torch.Size([143488, 48, 400])\n"
     ]
    }
   ],
   "source": [
    "toks_embs = model.token_factors(xb_toks)\n",
    "ic(toks_embs.shape);\n",
    "toks_shape = toks_embs.shape\n",
    "toks_embs = toks_embs.view(-1, *toks_shape[2:])\n",
    "ic(toks_embs.shape);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ec1ed6-6450-49eb-ac6d-49457a7cca39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| lbs_embs.shape: torch.Size([64, 2242, 400])\n",
      "ic| lbs_embs.shape: torch.Size([143488, 400])\n"
     ]
    }
   ],
   "source": [
    "lbs_embs = model.label_factors(xb_lbs)\n",
    "ic(lbs_embs.shape);\n",
    "lbs_shape = lbs_embs.shape\n",
    "lbs_embs = lbs_embs.view(-1, *lbs_shape[2:])\n",
    "ic(lbs_embs.shape);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dff85b1-e75a-4e14-bcb3-04eb26df0931",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| lbs_embs.shape: torch.Size([143488, 400, 1])\n"
     ]
    }
   ],
   "source": [
    "lbs_embs = lbs_embs.unsqueeze(dim=-1)\n",
    "ic(lbs_embs.shape);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c46541-758b-4d85-8616-c16427d1900d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#2) [(torch.Size([143488, 400, 1]), device(type='cuda', index=0)),(torch.Size([143488, 48, 400]), device(type='cuda', index=0))]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L(lbs_embs, toks_embs).map(lambda t: (t.shape, t.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b375e09-0763-453a-ab5b-ea32e78f2e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = torch.bmm(toks_embs, lbs_embs)\n",
    "# res = torch.matmul(toks_embs, lbs_embs)\n",
    "\n",
    "res = res.view(toks_shape[0], toks_shape[1], *res.shape[1:])\n",
    "ic(res.shape);\n",
    "\n",
    "res = res + model.token_bias(xb_toks) + model.label_bias(xb_lbs).unsqueeze(2)\n",
    "ic(res.shape);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606eb0d4-8264-4014-b161-013749629f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "xb_iter = iter(dls.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693a1f1f-e174-41c3-8036-fff4457f810c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| xb.shape: torch.Size([32, 2231, 64, 4])\n",
      "    xb.device: device(type='cuda', index=0)\n"
     ]
    }
   ],
   "source": [
    "xb = next(xb_iter)\n",
    "ic(xb.shape, xb.device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232b33ab-979c-44ec-ae1b-1f405dbc608d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| preds.shape: torch.Size([32, 2231, 64, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 2231, 64, 1])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = rank_model(xb)\n",
    "ic(preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cfc7ee-b6c6-4fcd-882c-76559876ea13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| preds.shape: torch.Size([32, 2231, 64, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 2231, 64, 1])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = rank_model_NN(xb)\n",
    "ic(preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644bcecc-4827-461d-abef-13f5ba797b4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='113' class='' max='113' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [113/113 01:05&lt;00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([32, 2231, 64, 4]), preds.shape = torch.Size([32, 2231, 64, 1])\n",
      "xb.shape =torch.Size([4, 2231, 64, 4]), preds.shape = torch.Size([4, 2231, 64, 1])\n"
     ]
    }
   ],
   "source": [
    "for xb in progress_bar(dls.train):\n",
    "    preds = rank_model(xb)\n",
    "    print(f\"{xb.shape =}, {preds.shape = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928b446c-36c8-4311-aaf8-cd7dadcc480d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xb.shape =torch.Size([1, 8922, 32, 4]), preds.shape = torch.Size([1, 8922, 32, 1])\n"
     ]
    }
   ],
   "source": [
    "for xb in dls.valid:\n",
    "    preds = rank_model(xb)\n",
    "    print(f\"{xb.shape =}, {preds.shape = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb28dd8-2e4d-43f5-a3eb-d3427eb51bfd",
   "metadata": {},
   "source": [
    "The following notations are borrowed from [From RankNet to LambdaRank to LambdaMART: An Overview](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/MSR-TR-2010-82.pdf)\n",
    "\n",
    "\n",
    "Let $I$ denote the pair of indices $\\{i, j\\}$, for which we desire token_i to be ranked differently from token_j (for a given label group). Since we must include each pair just once, so it is convenient to consider pairs of indices $\\{i, j\\}$ for which token_i is more relevant than token_j.\n",
    "\n",
    "$$\\lambda_{ij} = \\sigma \\left\\{ \\frac{1}{2}(1 - S_{ij}) - \\frac{1}{1+e^{ \\sigma(p_i - p_j)}} \\right\\}  \\textsf{  Eq: 3},$$ where $\\sigma$ is a hyper-parameter which controls the shape of the sigmoid and $p_i, p_j$ are predictions made by the model for token_i and token_j respectively, and\n",
    "\n",
    "$$S_{ij} = \\begin{cases} \n",
    "                1, & \\text{if token_i is more relevant} \\\\ \n",
    "                0, & \\text{if token_i is as relevant as token_j} \\\\\n",
    "                -1, & \\text{if token_j is ore relevant} \n",
    "            \\end{cases}$$\n",
    "\n",
    "The weight update rule in gradient descent is given by:\n",
    "$$\\delta w_k = \\eta \\sum_{\\{i,j\\} \\in I} (\\lambda_{ij} \\frac{\\partial p_i}{\\partial w_k} - \\lambda_{ij} \\frac{\\partial p_j}{\\partial w_k}) = -\\eta \\sum_i \\lambda_i \\frac{\\partial p_i}{\\partial w_k},$$ where\n",
    "\n",
    "$$\\lambda_i = \\sum_{j: \\{i,j\\} \\in I} \\lambda_{ij} - \\sum_{j: \\{j,i\\} \\in I} \\lambda_{ji} \\textsf{  Eq: 4}.$$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd50279-279e-4567-b773-cc670cbe8a06",
   "metadata": {},
   "source": [
    "**Implementing the above equations:**\n",
    "\n",
    "(Handcrfted Gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcffa15-dc36-41ce-9c69-b2958255ee4f",
   "metadata": {},
   "source": [
    "We can think of the tensor returned by `_summation` as essentially the summation notation in eq:4 above. It has three dimension. The length of the zeroth dim is the number of tokens. And each token contains a 2d tensor. For each token the zeroth and the first dim of 2d tensor has the following interpretation.\n",
    "\n",
    "For each token in a sequence (i.e. the i's) it contains the information about the other tokens (i.e. the j's) that  \n",
    "1. The first column value tells us the row num we got to index in the pairs array.\n",
    "2. The last column value tells us whether i is more relevant or less relevant than j. In other words, it determines the sign while computing $\\lambda_i$ in eq: 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e508170-ec11-45a7-8807-f64f4a0b117b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_loss(preds, xb):\n",
    "    eps = preds.new_empty(1).fill_(1e-15)\n",
    "    ideal_rank = xb[:, :, :, -1].argsort(dim=-1, descending=True).argsort(dim=-1) # ranking by the scores, highest score gets rank 0\n",
    "    ideal_discnt_fac = torch.log2(ideal_rank+2)\n",
    "    ideal_discntd_gain = (torch.pow(2, xb[:, :, :, -1]) - 1)  / (ideal_discnt_fac + eps)\n",
    "    idcg = ideal_discntd_gain.sum(dim=-1)\n",
    "    \n",
    "    # Sort the tokens by the relevance scores so that we can compute the set $I$ defined above:\n",
    "    srtd_relvs, srtd_idxs = xb[:,:,:,-1].sort(descending=True)\n",
    "    srtd_toks  = torch.take_along_dim(xb[:, :, :, 0], srtd_idxs, dim=-1)\n",
    "    # srtd_ranks = torch.take_along_dim(xb[:,:,:,-2], srtd_idxs, dim=-1) # pulling out the original ranks from the dataset\n",
    "    srtd_ranks = torch.take_along_dim(ideal_rank, srtd_idxs, dim=-1) # pulling out the ranking by scores for this batch \n",
    "    srtd_preds = torch.take_along_dim(preds[:, :, :, 0], srtd_idxs, dim=-1)\n",
    "    # srtd_preds = preds[:, :, :, 0].take_along_dim(srtd_idxs, dim=-1)\n",
    "    \n",
    "    # In the following `ij` is essentially the set $I$\n",
    "    sl = xb.shape[2]\n",
    "    ij = torch.as_tensor(np.fromiter(itertools.combinations(np.arange(sl), 2), dtype=np.dtype((int,2))),\n",
    "                                device=xb.device)#.expand(xb.shape[0], xb.shape[1], -1, -1)\n",
    "    \n",
    "    toki_tokj = srtd_toks[:, :, ij] # these are the i,j token indices - i more relevant thatn j  \n",
    "    pi_pj = srtd_preds[:, :, ij] # these are p_i and p_j \n",
    "    si_sj = srtd_relvs[:, :, ij] # these are the relevance scores for token_i and token_j\n",
    "    ri_rj = srtd_ranks[:, :, ij] # these are the ranks for token_i and token_j\n",
    "    max_rank = ri_rj.max(); ri_rj = ri_rj % (max_rank + 2) # convert any -1s from padding to highest rank\n",
    "    dfi_dfj = 1 / torch.log2(ri_rj + 2)\n",
    "    \n",
    "    # `sigma` is from eq:3 to compute $\\lambda_{ij}$\n",
    "    sigma = 0.5\n",
    "    si, sj= si_sj[:, :, :, 0], si_sj[:, :, :, 1]\n",
    "    signs = torch.sign(si - sj)\n",
    "    pow_si, pow_sj = torch.pow(2, si), torch.pow(2, sj)\n",
    "    delta_dcg = torch.abs((pow_si - pow_sj) * (dfi_dfj[:,:,:,0] - dfi_dfj[:,:,:,1]))\n",
    "    delta_ndcg = delta_dcg / idcg.unsqueeze(-1)\n",
    "    \n",
    "    pi, pj = pi_pj[:, :, :, 0], pi_pj[:, :, :, 1]\n",
    "    exp_ij = torch.exp(sigma * (pi - pj))\n",
    "    lambda_ij = sigma * (  0.5 * (1 - signs) -  1/(1 + exp_ij) )\n",
    "    # lambda_ij = sigma * (  0.5 * (1 - signs) -  torch.sigmoid(pj-pi) )\n",
    "    \n",
    "    # lambda_ij = lambda_ij * delta_ndcg # use this for Lambda-Rank\n",
    "    \n",
    "    # from IPython import embed; embed()\n",
    "    \n",
    "    sumer = _summation(sl, ij)\n",
    "    idxr, signs = sumer[:, :, 0], sumer[:, :, -1]\n",
    "    # Now we can compute $\\lambda_i$ from eq: 4,\n",
    "    lambda_i = (lambda_ij[:, :, idxr] * signs).sum(dim=-1)\n",
    "    \n",
    "    return srtd_preds, lambda_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad996485-9aeb-43f6-8614-98644c1d9f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _summation(sl, ij):\n",
    "    sumer = []\n",
    "    for i in range(sl):\n",
    "        _x = torch.nonzero(ij == i, as_tuple=False)\n",
    "        _x[:, -1] = torch.pow(-1, _x[:, 1])\n",
    "        sumer.append(_x)\n",
    "    return torch.stack(sumer, dim=0)\n",
    "\n",
    "def _idcg(xb, k=None, gain_fn=None):\n",
    "    # pdb.set_trace()\n",
    "    x = xb[:, :, :, -1]\n",
    "    ranks = x.argsort(dim=-1, descending=True).argsort(dim=-1) # ranking by the scores, highest score gets rank 0\n",
    "    dfs = 1/torch.log2(ranks + 2)\n",
    "    gains = torch.pow(2, x) if gain_fn == 'exp' else torch.pow(x, 3)\n",
    "    idg = gains * dfs\n",
    "    idcg = idg.sum(dim=-1)\n",
    "    \n",
    "    idcg_at_k = None\n",
    "    if k is not None:\n",
    "        topk, topk_idxs = torch.topk(x, k=k, dim=-1, largest=True)\n",
    "        # topk_relvs = torch.take_along_dim(x, topk_idxs, dim=-1)\n",
    "        dfs_at_k = 1/torch.log2(2 + torch.arange(k)).cuda()\n",
    "        gains_at_k = torch.pow(2, topk) if gain_fn == 'exp' else torch.pow(topk, 3)\n",
    "        idg_at_k = gains_at_k * dfs_at_k\n",
    "        idcg_at_k = idg_at_k.sum(-1)\n",
    "    \n",
    "    return idcg, idcg_at_k\n",
    "\n",
    "def rank_loss2(preds, xb, sigma=0.5, lambrank=False, gain_fn=None):\n",
    "    # In the following `ij` is essentially the set $I$\n",
    "    sl = xb.shape[2]\n",
    "    ij = torch.as_tensor(np.fromiter(itertools.combinations(np.arange(sl), 2), dtype=np.dtype((int,2))),\n",
    "                                device=xb.device)#.expand(xb.shape[0], xb.shape[1], -1, -1)\n",
    "    \n",
    "    # Sort the tokens by the model prediction scores so that we can compute the set $I$ defined above:\n",
    "    srtd_preds, srtd_idxs = preds[:, :, :,  0].sort(descending=True)\n",
    "    \n",
    "    srtd_ranks = srtd_preds.new_empty(srtd_preds.size())#srtd_idxs.argsort()\n",
    "    srtd_ranks[:,:] = torch.arange(preds.shape[2])\n",
    "    ri_rj = srtd_ranks[:, :, ij] # these are the ranks for token_i and token_j\n",
    "    dfi_dfj = 1.0 / torch.log2(ri_rj + 2)\n",
    "    dfi = dfi_dfj[:,:,:,0]\n",
    "    dfj = dfi_dfj[:,:,:,1]\n",
    "        \n",
    "    srtd_relvs = torch.take_along_dim(xb[:, :, :, -1], srtd_idxs, dim=-1)\n",
    "    pi_pj = srtd_preds[:, :, ij] # these are p_i and p_j \n",
    "    pi, pj = pi_pj[:, :, :, 0], pi_pj[:, :, :, 1]\n",
    "    exp_ij = torch.exp(sigma * (pi - pj))\n",
    "    si_sj = srtd_relvs[:, :, ij] # these are the relevance scores for token_i and token_j\n",
    "    si, sj= si_sj[:, :, :, 0], si_sj[:, :, :, 1]\n",
    "    gain_i, gain_j = ( torch.pow(2.0, si), torch.pow(2.0, sj) ) if gain_fn == 'exp' else ( torch.pow(si, 3.0), torch.pow(sj, 3.0) ) # cubic\n",
    "    signs = torch.sign(si - sj)\n",
    "    delta_dcg = torch.abs((gain_i - gain_j) * (dfi - dfj))\n",
    "    idcg, idcg_at_k = _idcg(xb, k=6, gain_fn=gain_fn)\n",
    "    delta_ndcg_at_k = delta_dcg / idcg_at_k.unsqueeze(-1)\n",
    "    \n",
    "    lambda_ij = sigma * (  0.5 * (1 - signs) -  1/(1 + exp_ij) )\n",
    "    if lambrank: lambda_ij *= delta_ndcg_at_k # use this for Lambda-Rank\n",
    "    \n",
    "    sumer = _summation(sl, ij)\n",
    "    idxr, signs = sumer[:, :, 0], sumer[:, :, -1]\n",
    "    # Now we can compute $\\lambda_i$ from eq: 4,\n",
    "    lambda_i = (lambda_ij[:, :, idxr] * signs).sum(dim=-1)\n",
    "    \n",
    "    return srtd_preds, lambda_i\n",
    "\n",
    "def rank_loss3(preds, xb, sigma=0.5, lambrank=False, gain_fn=None):\n",
    "    with torch.no_grad():\n",
    "        # pdb.set_trace()\n",
    "        x = xb[:, :, :, -1, None]\n",
    "        x_t = xb[:, :, :, -1, None].transpose(-1,-2)\n",
    "        preds_t = preds.transpose(-1,-2)\n",
    "        preds_rank = preds[:, :, :, 0].argsort(dim=-1, descending=True).argsort(dim=-1).unsqueeze(-1)\n",
    "        preds_rank_t = preds_rank.transpose(-1,-2)\n",
    "        \n",
    "        exp_ij= 1.0 + torch.exp(sigma* (preds - preds_t))\n",
    "        rel_diff = x - x_t\n",
    "        gain_diff = torch.pow(2.0, x) - torch.pow(2.0, x_t) if gain_fn == 'exp' else torch.pow(x, 3.0) - torch.pow(x_t, 3.0)\n",
    "        decay_diff = 1.0/torch.log2(preds_rank + 2.0) - 1.0/torch.log2(preds_rank_t  + 2.0)\n",
    "        idcg, idcg_at_k = _idcg(xb, k=6, gain_fn=gain_fn)\n",
    "        idcg_at_k = idcg_at_k[..., None, None]\n",
    "        # pdb.set_trace()\n",
    "        delta_ndcg_at_k = torch.abs(gain_diff * decay_diff * 1/idcg_at_k)\n",
    "        pos_pairs = (rel_diff > 0).float()\n",
    "        neg_pairs = (rel_diff < 0).float()\n",
    "        S_ij = pos_pairs - neg_pairs\n",
    "        lambda_update = sigma * (  0.5 * (1 - S_ij) -  1/exp_ij )\n",
    "        if lambrank: lambda_update *= delta_ndcg_at_k \n",
    "        lambda_update = lambda_update.sum(dim=-1, keepdim=True)\n",
    "    return preds, lambda_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bad8923-adde-4e30-ae82-23f0f8b1f9d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| xb.shape: torch.Size([32, 2231, 64, 4])\n",
      "    preds.shape: torch.Size([32, 2231, 64, 1])\n"
     ]
    }
   ],
   "source": [
    "xb = dls.one_batch()\n",
    "preds = model(xb)\n",
    "ic(xb.shape, preds.shape);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82cd38e-6cde-4d28-b23a-ed0a08e8813e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,xb in enumerate(dls.train):\n",
    "    print(i, xb.shape)\n",
    "    bd[i] =  xb[:, :, :, 0].view(-1, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc2c7cd-265c-4d9d-a9ac-4af8e366395b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.set_printoptions(profile=\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181defb9-1f0d-4745-adad-f7c4bfe24a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(x.shape[0]):\n",
    "    for j in range(x.shape[1]):\n",
    "        print(f\"{x[i,j].cpu().numpy() = }\")\n",
    "        print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd74fb8b-6ec8-45fd-96c1-c955e9becc58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([27., 27., 21., 21., 21., 21., 18., 18., 18., 18., 18., 18., 18., 18., 18., 18., 17., 17., 16., 14., 14., 13., 13., 13., 13., 13., 12., 12., 12., 11., 10., 10., 10., 10.,  9.,  9.,  9.,  8.,\n",
       "         6.,  5.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  3.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], device='cuda:0')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb[:,:,:,-1][0,3].sort(descending=True).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d4514c-4dd0-4baf-8682-e5fd6c5aec61",
   "metadata": {},
   "outputs": [],
   "source": [
    "btch_lbl = (6,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb26edd1-9ed6-467d-bfc2-01c15e205283",
   "metadata": {},
   "source": [
    "These are the relevances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dc821c-6723-4b81-8932-06edc2032873",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([43., 32., 30., 30., 29., 27., 26., 26., 25., 22., 21., 20., 18., 18., 16., 15., 15., 14., 13., 13., 13., 13., 11., 11., 10., 10., 10.,  9.,  9.,  9.,  9.,  8.,  8.,  8.,  7.,  6.,  6.,  6.,\n",
       "         6.,  6.,  5.,  5.,  5.,  5.,  4.,  4.,  4.,  4.,  4.,  3.,  3.,  2.,  2.,  2.,  2.,  2.,  2.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], device='cuda:0')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relvs = xb[btch_lbl][:, -1].sort(descending=True).values\n",
    "relvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b23959-2aa4-4f5d-90af-bd077355dacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, lambda_update = rank_loss3(preds, xb, lambrank=True, gain_fn='exp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831480c1-674b-4801-80f9-fbab0daf79d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 2231, 64, 1]), torch.Size([32, 2231, 64, 1]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape, lambda_update.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ecbcb8-ca64-487e-be6e-76b037d150ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "vals, idxs = preds[:, :, :, -1].sort(dim=-1, descending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e715cb68-6dd9-4c38-9cce-62a7c56a5564",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2.6919e+01,  1.7313e+01,  1.5528e+01,  1.3897e+01,  1.3603e+01,  1.3230e+01,  1.2256e+01,  1.2162e+01,  1.1968e+01,  1.1541e+01,  1.0806e+01,  8.7038e+00,  8.1869e+00,  8.1249e+00,\n",
       "         7.4311e+00,  6.8465e+00,  5.5709e+00,  5.3884e+00,  5.3050e+00,  4.5804e+00,  3.9161e+00,  2.8923e+00,  2.8743e+00,  2.6756e+00,  2.6228e+00,  2.5613e+00,  2.5344e+00,  1.2879e+00,\n",
       "         7.9447e-01,  7.5437e-01,  7.4869e-01,  7.1853e-01,  6.0639e-01,  4.7160e-02,  1.0306e-02, -3.7450e-01, -1.2874e+00, -2.3287e+00, -2.5765e+00, -3.3385e+00, -3.6131e+00, -3.9631e+00,\n",
       "        -4.2888e+00, -4.5948e+00, -4.8598e+00, -5.0398e+00, -5.3674e+00, -5.6183e+00, -5.8409e+00, -7.0450e+00, -7.4644e+00, -7.6236e+00, -8.0989e+00, -8.5302e+00, -8.5709e+00, -9.1744e+00,\n",
       "        -9.4538e+00, -1.0686e+01, -1.3591e+01, -1.4217e+01, -1.4981e+01, -1.5726e+01, -1.6596e+01, -1.9282e+01], device='cuda:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vals[btch_lbl].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0213a6c0-6658-43d6-bb1a-e74a24343108",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3.7216e-01,  1.8570e-01,  1.1915e-01,  8.2802e-02,  6.1525e-02,  4.6548e-02,  3.4320e-02,  2.6303e-02,  1.9694e-02,  1.4048e-02,  9.1416e-03,  4.1144e-03,  1.7195e-03, -1.0041e+00,\n",
       "         1.2586e-03,  1.9755e-03,  1.7810e-03,  2.1037e-03,  2.2783e-03,  2.0710e-03,  1.7387e-03,  1.2027e-03,  1.2919e-03,  1.2627e-03,  1.3090e-03,  1.3423e-03,  1.3909e-03,  8.0485e-04,\n",
       "         6.6036e-04,  6.7093e-04,  6.9112e-04,  7.0177e-04,  6.8337e-04,  5.3464e-04,  5.3772e-04,  4.5585e-04,  2.9819e-04,  1.8305e-04,  1.6503e-04,  1.1513e-04,  1.0289e-04,  6.1926e-05,\n",
       "        -1.4166e-04,  6.6397e-05,  5.9048e-05,  5.4659e-05,  4.7077e-05,  2.8312e-05,  3.8010e-05,  2.1393e-05,  1.7522e-05,  1.6259e-05,  1.2917e-05,  1.0459e-05,  1.0233e-05,  7.5760e-06,\n",
       "        -1.0269e-03,  3.6402e-06,  8.8572e-07,  6.6940e-07,  4.7333e-07,  3.2430e-07,  2.2206e-07, -7.8613e-06], device='cuda:0')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "their_lambdas = torch.take_along_dim(lambda_update[:, :, :, 0], idxs, dim=-1)\n",
    "their_lambdas[btch_lbl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3a1812-8ad9-4276-a0d2-61dd11023c7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(7.5670e-10, device='cuda:0'), tensor(0.1386, device='cuda:0'))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean, std = their_lambdas[btch_lbl].mean(), their_lambdas[btch_lbl].std()\n",
    "mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aef2824-4d3f-467a-9543-6415d0eaa84c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.8429e-08, device='cuda:0')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "their_lambdas[btch_lbl].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96fbc67-a91c-450b-b462-edfe8ee73dbb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7d6489-df09-41e1-a558-926c73484a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "srtd_preds, my_lambdas = rank_loss2(preds, xb, lambrank=True, gain_fn='exp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc76e027-e16f-44bc-82d7-836949c003d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 2231, 64]), torch.Size([32, 2231, 64]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "srtd_preds.shape, my_lambdas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2dab0e-173b-4fcf-9a9e-4549d4de6ebe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2.6919e+01,  1.7313e+01,  1.5528e+01,  1.3897e+01,  1.3603e+01,  1.3230e+01,  1.2256e+01,  1.2162e+01,  1.1968e+01,  1.1541e+01,  1.0806e+01,  8.7038e+00,  8.1869e+00,  8.1249e+00,\n",
       "         7.4311e+00,  6.8465e+00,  5.5709e+00,  5.3884e+00,  5.3050e+00,  4.5804e+00,  3.9161e+00,  2.8923e+00,  2.8743e+00,  2.6756e+00,  2.6228e+00,  2.5613e+00,  2.5344e+00,  1.2879e+00,\n",
       "         7.9447e-01,  7.5437e-01,  7.4869e-01,  7.1853e-01,  6.0639e-01,  4.7160e-02,  1.0306e-02, -3.7450e-01, -1.2874e+00, -2.3287e+00, -2.5765e+00, -3.3385e+00, -3.6131e+00, -3.9631e+00,\n",
       "        -4.2888e+00, -4.5948e+00, -4.8598e+00, -5.0398e+00, -5.3674e+00, -5.6183e+00, -5.8409e+00, -7.0450e+00, -7.4644e+00, -7.6236e+00, -8.0989e+00, -8.5302e+00, -8.5709e+00, -9.1744e+00,\n",
       "        -9.4538e+00, -1.0686e+01, -1.3591e+01, -1.4217e+01, -1.4981e+01, -1.5726e+01, -1.6596e+01, -1.9282e+01], device='cuda:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "srtd_preds[btch_lbl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dab6c50-7104-4ed1-ad86-44a565cdd3e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3.7216e-01,  1.8570e-01,  1.1915e-01,  8.2802e-02,  6.1525e-02,  4.6548e-02,  3.4320e-02,  2.6303e-02,  1.9694e-02,  1.4048e-02,  9.1416e-03,  4.1144e-03,  1.7195e-03, -1.0041e+00,\n",
       "         1.2586e-03,  1.9755e-03,  1.7810e-03,  2.1037e-03,  2.2783e-03,  2.0710e-03,  1.7387e-03,  1.2027e-03,  1.2919e-03,  1.2627e-03,  1.3090e-03,  1.3423e-03,  1.3909e-03,  8.0485e-04,\n",
       "         6.6036e-04,  6.7093e-04,  6.9112e-04,  7.0177e-04,  6.8337e-04,  5.3464e-04,  5.3772e-04,  4.5585e-04,  2.9819e-04,  1.8305e-04,  1.6503e-04,  1.1513e-04,  1.0289e-04,  6.1924e-05,\n",
       "        -1.4166e-04,  6.6396e-05,  5.9049e-05,  5.4658e-05,  4.7076e-05,  2.8314e-05,  3.8009e-05,  2.1391e-05,  1.7525e-05,  1.6259e-05,  1.2919e-05,  1.0459e-05,  1.0234e-05,  7.5749e-06,\n",
       "        -1.0269e-03,  3.6403e-06,  8.8820e-07,  6.6970e-07,  4.7114e-07,  3.2185e-07,  2.2176e-07, -7.8592e-06], device='cuda:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_lambdas[btch_lbl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7462b00-d0c6-4118-9212-ace34107cc7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(6.9849e-10, device='cuda:0', grad_fn=<MeanBackward0>),\n",
       " tensor(0.1386, device='cuda:0', grad_fn=<StdBackward0>))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = my_lambdas[btch_lbl].mean()\n",
    "std = my_lambdas[btch_lbl].std()\n",
    "\n",
    "mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96897097-4dd4-4fc2-857b-8c59d79bba57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.7684e-07, device='cuda:0', grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs = torch.abs(their_lambdas-my_lambdas).max()\n",
    "abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3334757d-e502-4eaa-ba4a-f4b2db5e0e07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(their_lambdas, my_lambdas, atol=float(abs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdb30c6-db1f-48cc-b5a1-61b1e8b0e2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "i, ik = _idcg(xb, k=6, gain_fn='exp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b33d9d-7923-4db2-9908-7e0f7eaf6a5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 2231])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ik.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cd5b5c-ee5a-49c3-a62c-2c2aacbd3f7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.1259e+15, 5.6090e+11, 5.5252e+11,  ..., 1.8014e+16, 1.1806e+21, 1.5112e+23],\n",
       "        [9.4447e+21, 5.6090e+11, 2.8148e+14,  ..., 2.2213e+12, 2.2030e+12, 2.2213e+12],\n",
       "        [1.1529e+18, 1.1215e+12, 1.1215e+12,  ..., 1.1215e+12, 4.4031e+12, 1.8020e+16],\n",
       "        ...,\n",
       "        [1.1529e+18, 2.3697e+16, 2.0404e+13,  ..., 9.2617e+13, 4.6324e+13, 8.1517e+13],\n",
       "        [9.6715e+24, 4.6316e+13, 2.3174e+13,  ..., 1.4412e+17, 3.2595e+14, 3.7042e+14],\n",
       "        [2.3612e+21, 6.5189e+14, 6.5189e+14,  ..., 1.2379e+27, 7.6423e+00, 7.6423e+00]], device='cuda:0')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba694d7-6b27-4e12-864c-6c5de3252f00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.1259e+15, 5.6089e+11, 5.5245e+11,  ..., 1.8014e+16, 1.1806e+21, 1.5112e+23],\n",
       "        [9.4447e+21, 5.6089e+11, 2.8148e+14,  ..., 2.2213e+12, 2.2030e+12, 2.2213e+12],\n",
       "        [1.1529e+18, 1.1215e+12, 1.1215e+12,  ..., 1.1215e+12, 4.4030e+12, 1.8020e+16],\n",
       "        ...,\n",
       "        [1.1529e+18, 2.3697e+16, 2.0404e+13,  ..., 9.2617e+13, 4.6324e+13, 8.1517e+13],\n",
       "        [9.6715e+24, 4.6316e+13, 2.3174e+13,  ..., 1.4412e+17, 3.2595e+14, 3.7042e+14],\n",
       "        [2.3612e+21, 6.5189e+14, 6.5189e+14,  ..., 1.2379e+27, 1.6523e+00, 1.6523e+00]], device='cuda:0')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62539840-45cc-472a-8daa-4e41b0e5ea03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.equal(i, ik)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cca7b9-0bba-4eb5-8b36-de7f3e2666cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.4540e+14, device='cuda:0')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.abs(i-ik).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8727b1-0cd7-452e-8576-032c945a0c83",
   "metadata": {},
   "source": [
    "If we were to use a loss fuunction instead of hand creafted gradients:\n",
    "\n",
    "$$C = \\sum_{\\{i,j\\} \\in I} \\frac{1}{2}(1 - S_{ij})\\sigma(p_i-p_j) + \\log(1 + e^{-\\sigma(p_i - p_j)})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba8e995-0fa0-4d7c-879e-b256a0f78314",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(preds, xb, sigma=0.5):\n",
    "    \n",
    "    srtd_relvs, srtd_idxs = xb[:, :, :, -1].sort(descending=True)\n",
    "    srtd_preds = torch.take_along_dim(preds[:,:,:,0], srtd_idxs, dim=-1)\n",
    "\n",
    "    sl = torch.arange(xb.shape[2], device=xb.device)\n",
    "    ij = torch.cartesian_prod(sl, sl)\n",
    "    idxs, = torch.nonzero(ij[:, 0] < ij[:, 1], as_tuple=True)\n",
    "    ij = ij[idxs]\n",
    "    \n",
    "    si_sj = srtd_relvs[:, :, ij] # these are the relevance scores for token_i and token_j\n",
    "    si, sj= si_sj[:, :, :, 0], si_sj[:, :, :, 1]\n",
    "    signs = torch.sign(si - sj)\n",
    "    pi_pj = srtd_preds[:, :, ij]\n",
    "    pi, pj = pi_pj[:,:,:,0], pi_pj[:,:,:,1]\n",
    "    exp_ij = torch.exp(-sigma*(pi -pj))\n",
    "    exp_ij[exp_ij==torch.inf] = tensor(1e6)\n",
    "    C = ( 0.5*(1 - signs)*sigma*(pi -pj) + torch.log(1 + exp_ij) ) #shape (64, 2234, 64)\n",
    "    # C = C.sum(dim=-1) # shape (64, 2234)\n",
    "    C = C.mean(dim=-1)\n",
    "    return C#.mean()\n",
    "\n",
    "def loss_fn2(preds, xb, sigma=.5):\n",
    "    \"Computes average pairwise cross-entropy loss\"\n",
    "    sl = xb.shape[2]\n",
    "    rel_diff = xb[:, :, :, -1, None] - xb[:, :, :, -1, None].transpose(-1, -2)\n",
    "    pos_pairs = (rel_diff > 0).float()\n",
    "    neg_pairs = (rel_diff < 0).float()\n",
    "    S_ij = pos_pairs - neg_pairs\n",
    "    preds_diff = preds - preds.transpose(-1, -2)\n",
    "    C = .5 * (1 - S_ij) * sigma * preds_diff - F.logsigmoid(sigma * preds_diff)\n",
    "    C = torch.triu(C, diagonal=1) # to take each pair only once\n",
    "    C = C.sum((-1,-2)) / (C.new_ones(C.shape[-2:]).triu(diagonal=1).sum())\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89d8b42-b650-466d-b02e-80f7d40d2141",
   "metadata": {},
   "outputs": [],
   "source": [
    "xb = dls.one_batch()\n",
    "preds = model(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dce71a-f437-45c3-8be5-184e4432fc16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| L(xb, preds).map(Tensor.size): [torch.Size([8, 4, 64, 4]), torch.Size([8, 4, 64, 1])]\n"
     ]
    }
   ],
   "source": [
    "ic(L(xb, preds).map(Tensor.size));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e785de67-c940-4d7b-a252-af4961a4592c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = (preds, xb)\n",
    "C_deb = loss_fn(*inp)\n",
    "C_them = loss_fn2(*inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71aca793-367d-434d-b8df-55e113ecd4d2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb9908c-3f7e-465d-9b4d-f242640c072a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| xb.shape: torch.Size([8, 4, 64, 4])\n",
      "    preds.shape: torch.Size([8, 4, 64, 1])\n",
      "    C.shape: torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "C = loss_fn(preds, xb)\n",
    "ic(xb.shape, preds.shape, C.shape);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191178a2-b76f-4648-be8e-d19bbdbec3ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1399.1204, device='cuda:0', grad_fn=<MeanBackward0>),\n",
       " tensor(34.6843, device='cuda:0', grad_fn=<StdBackward0>))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C.mean(), C.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409c51ad-bdbe-4e1c-86a8-05f0c5728f9d",
   "metadata": {},
   "source": [
    "Good luck traing with this kinda loss! In other words, we need handcrafted gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be331d3-bc45-475b-9dd4-c338b004bcba",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829b1d1e-382d-415a-b572-b5edd79e7b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| len(params_list): 6\n"
     ]
    }
   ],
   "source": [
    "params_list = L(rank_model_NN.parameters())\n",
    "ic(len(params_list));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fafce6d-1fd4-47ae-aa97-53f48812c350",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert params_list.map(lambda o: o.grad) == [None]*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84795a05-5922-400c-bd93-8b87cfa81898",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| srtd_preds.shape: torch.Size([32, 2231, 64])\n",
      "    lambda_i.shape: torch.Size([32, 2231, 64])\n"
     ]
    }
   ],
   "source": [
    "srtd_preds, lambda_i = rank_loss(preds, xb)\n",
    "ic(srtd_preds.shape, lambda_i.shape);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c1d748-5636-400b-99e3-c60e6dfde41d",
   "metadata": {},
   "source": [
    "Now we need to compute $\\sum_i \\lambda_i \\frac{\\partial p_i}{w_k}$ to be able to do the weight update in sgd:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94304211-2a3e-4269-85e3-8632cf178bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "srtd_preds.backward(lambda_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddb6d75-ffa5-4626-8cd9-465198176910",
   "metadata": {},
   "source": [
    "Check if the gradients were populated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e143da50-73cb-4425-838c-de7d88eb3d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradIsNone(param_list):\n",
    "    # import pdb; pdb.set_trace()\n",
    "    assert param_list.map(lambda o: o.grad) == [None]*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f3a36f-8778-44dd-96be-3d9eb29aae64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# params_list[5].grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf49ae8-71eb-4a10-a318-9700aa7d900a",
   "metadata": {},
   "source": [
    "Check if the the `grad` attributes of the params got populated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b82381b-d0d0-45aa-9112-dcd7172800a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fail(gradIsNone, args=(params_list,), contains='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6dd9943-205b-4200-b8d5-072e27f03da1",
   "metadata": {},
   "source": [
    "---\n",
    "Sort the tokens by the relevance scores so that we can compute the set $I$ defined above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfeed3e3-500c-4d3e-9bad-224606994d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| btch_num: 61, lbl_num: 1023\n"
     ]
    }
   ],
   "source": [
    "btch_num, lbl_num, = 61, 1023\n",
    "ic(btch_num, lbl_num);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e953a2-81c6-4460-ac1c-1c56a9ac1285",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| xb.shape: torch.Size([64, 2233, 48, 3])\n",
      "    preds.shape: torch.Size([64, 2233, 48, 1])\n",
      "    xb[:,:,:,-1].shape: torch.Size([64, 2233, 48])\n",
      "ic| srtd_relvs.shape: torch.Size([64, 2233, 48])\n",
      "    srtd_idxs.shape: torch.Size([64, 2233, 48])\n"
     ]
    }
   ],
   "source": [
    "ic(xb.shape, preds.shape, xb[:,:,:,-1].shape);\n",
    "srtd_relvs, srtd_idxs = xb[:,:,:,-1].sort(descending=True)\n",
    "ic(srtd_relvs.shape, srtd_idxs.shape);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cc192d-d76f-4dab-9ad8-23ed29dbd105",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| srtd_toks.shape: torch.Size([64, 2233, 48])\n",
      "    srtd_preds.shape: torch.Size([64, 2233, 48])\n"
     ]
    }
   ],
   "source": [
    "srtd_toks  = torch.take_along_dim(xb[:, :, :, 0], srtd_idxs, dim=-1)\n",
    "srtd_preds = torch.take_along_dim(preds[:, :, :, 0], srtd_idxs, dim=-1)\n",
    "ic(srtd_toks.shape, srtd_preds.shape);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95895629-3848-4038-95e4-9b0c7a8f86df",
   "metadata": {},
   "source": [
    "In the following `ij` is essentially the set `I`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d4a846-56fe-4603-80a7-4ae0c5dc463d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| xb.shape: torch.Size([64, 2233, 48, 3]), sl: 48\n",
      "ic| ij.shape: torch.Size([1128, 2])\n"
     ]
    }
   ],
   "source": [
    "sl = xb.shape[2]\n",
    "ic(xb.shape, sl);\n",
    "ij = torch.as_tensor(np.fromiter(itertools.combinations(np.arange(sl), 2), dtype=np.dtype((int,2))),\n",
    "                            device=xb.device)#.expand(xb.shape[0], xb.shape[1], -1, -1)\n",
    "ic(ij.shape);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045f7f7c-4755-4f6c-a791-a8d383d75ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.6 s, sys: 349 ms, total: 11 s\n",
      "Wall time: 11 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 2238, 1128, 2])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "btchs, lbs = list(xb.shape[:2])\n",
    "btchs, lbs\n",
    "btch_lbs_srtd_toks_combs = []\n",
    "for btch_idx in range(btchs):\n",
    "    for lbl_idx in range(lbs):\n",
    "        # btch_lbs_srtd_toks_combs.append(torch.take(btch_lbs_srtd_toks[btch_idx, lbl_idx], tok_combs))\n",
    "        btch_lbs_srtd_toks_combs.append(srtd_toks[btch_idx, lbl_idx][tok_combs])\n",
    "btch_lbs_srtd_toks_combs = torch.stack(btch_lbs_srtd_toks_combs).view(btchs, lbs, *tok_combs.shape)\n",
    "btch_lbs_srtd_toks_combs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d97a442-e0c2-4781-a957-eed3dbefbfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "sumer = _summation(sl, ij)\n",
    "test_eq(sumer.shape, (sl, sl-1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b616ca-4070-4bd9-8bbb-9050598b1bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "_df = pd.DataFrame(ij, columns=['i', 'j'])\n",
    "_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71df8a93-9989-406c-87be-8609bb9d5f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_df.groupby('i').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3448e4-759d-4e66-a08c-e2948aba8b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 252 ms, sys: 0 ns, total: 252 ms\n",
      "Wall time: 283 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "toki_tokj = srtd_toks[:, :, ij] # these are the i,j token indices - i more relevant thatn j  \n",
    "si_sj = srtd_preds[:, :, ij] # these are s_i and s_j "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24185ea8-b7b7-431d-9462-f82012c5acba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| toki_tokj.shape: torch.Size([64, 2233, 1128, 2])\n",
      "    si_sj.shape: torch.Size([64, 2233, 1128, 2])\n"
     ]
    }
   ],
   "source": [
    "ic(toki_tokj.shape, si_sj.shape);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c6d7b2-668b-4dd7-abf7-b258185d7bc3",
   "metadata": {},
   "source": [
    "`sigma` is from eq:3 to compute $\\lambda_{ij}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abc89af-5fb1-4f5f-84e6-62f0f7d64d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| lambda_ij.shape: torch.Size([64, 2233, 1128])\n"
     ]
    }
   ],
   "source": [
    "sigma = 0.5\n",
    "lambda_ij = -sigma/(1 + torch.exp(sigma * si_sj[:, :, :, 0] - si_sj[:, :, :, 1]))\n",
    "ic(lambda_ij.shape);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133df706-5e95-4171-accd-c7c7a82385b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.equal(btch_lbs_srtd_toks_combs[btch_num, lbl_num], btch_lbs_srtd_toks_combs_1[btch_num, lbl_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9132915e-97db-484a-8392-04e82af03209",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| idxr.shape: torch.Size([48, 47])\n",
      "    signs.shape: torch.Size([48, 47])\n"
     ]
    }
   ],
   "source": [
    "idxr, signs = sumer[:, :, 0], sumer[:, :, -1]\n",
    "ic(idxr.shape, signs.shape);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19e4be5-a5a1-4c9e-89f9-90317dd1dbbd",
   "metadata": {},
   "source": [
    "Now we can compute $\\lambda_i$ from eq: 4,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2899f56b-28bf-4a18-8c3f-fe0c100a2a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| lambda_i.shape: torch.Size([64, 2233, 48, 47])\n",
      "ic| lambda_i.shape: torch.Size([64, 2233, 48])\n"
     ]
    }
   ],
   "source": [
    "lambda_i = (lambda_ij[:, :, idxr] * signs)\n",
    "ic(lambda_i.shape);\n",
    "lambda_i = lambda_i.sum(dim=-1)\n",
    "ic(lambda_i.shape);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0103432-9a80-47c6-b1e3-c28104bba81a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe0ece5-ae9b-4ba3-b8b7-ac62cfefae21",
   "metadata": {},
   "source": [
    "Tensor Gradients and Jacobian Products:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c27c89-ee0d-4a8c-be3b-18d1a6036b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.eye(4, 5, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e157f39b-3ba4-41b6-80f0-ddb285bc34a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 1., 0.]], requires_grad=True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb3e16b-2497-4012-adf7-4438772c9b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = (inp+1).pow(2).t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33551b5c-f189-4531-b9dd-1a66b74008b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4., 1., 1., 1.],\n",
       "        [1., 4., 1., 1.],\n",
       "        [1., 1., 4., 1.],\n",
       "        [1., 1., 1., 4.],\n",
       "        [1., 1., 1., 1.]], grad_fn=<TBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae79128-c4af-4421-a5c3-22a12797e8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# error\n",
    "out.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a01d6a-61d5-4570-8ce4-457e01168544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones_like(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9c3c9e-2596-4a2e-89b4-886a0d6af0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "out.backward(torch.ones_like(out), retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781309f7-ee92-4b53-be59-1e467f39c1af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First call\n",
      "tensor([[4., 2., 2., 2., 2.],\n",
      "        [2., 4., 2., 2., 2.],\n",
      "        [2., 2., 4., 2., 2.],\n",
      "        [2., 2., 2., 4., 2.]])\n"
     ]
    }
   ],
   "source": [
    "print(f\"First call\\n{inp.grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31e478c-d6f7-473a-9887-b25e25c2173b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Second call\n",
      "tensor([[8., 4., 4., 4., 4.],\n",
      "        [4., 8., 4., 4., 4.],\n",
      "        [4., 4., 8., 4., 4.],\n",
      "        [4., 4., 4., 8., 4.]])\n"
     ]
    }
   ],
   "source": [
    "out.backward(torch.ones_like(out), retain_graph=True)\n",
    "print(f\"\\nSecond call\\n{inp.grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496126f3-d251-48ed-9f27-366a38c750c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Call after zeroing gradients\n",
      "tensor([[4., 2., 2., 2., 2.],\n",
      "        [2., 4., 2., 2., 2.],\n",
      "        [2., 2., 4., 2., 2.],\n",
      "        [2., 2., 2., 4., 2.]])\n"
     ]
    }
   ],
   "source": [
    "inp.grad.zero_()\n",
    "out.backward(torch.ones_like(out), retain_graph=True)\n",
    "print(f\"\\nCall after zeroing gradients\\n{inp.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeff19ad-9cf4-440d-a49a-f8d008a96603",
   "metadata": {},
   "source": [
    "Another example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52709ee9-9fc1-479e-8ce4-76a29fe0aaae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8., 2., 0.], requires_grad=True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randint(low=0, high=10, size=(3,)).float().requires_grad_()\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40ef9ca-c7e6-4d8e-ac54-e380949098de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([64.,  4.,  0.], grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x**2\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb4d672-1dd5-4819-8bf9-268597776c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward(torch.ones_like(y), retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9061400-aa9d-4361-a5da-e231ef16ee88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([16.,  4.,  0.])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b2beaa-95f0-41fa-8c08-d5d54cb05e73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0.])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1049173-1681-4421-bcd2-e18e4be11085",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward(torch.ones_like(y), retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe17964-4356-4416-987b-0a79a4677774",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([16.,  4.,  0.])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2f3f3d-db26-434c-a374-d8309fb60eb6",
   "metadata": {},
   "source": [
    "Another example where we will compute the jacobian:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4761fd13-8d48-44e1-959c-7d80fa10e750",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd.functional import jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6eead3-eb5c-4867-a0c4-e999da65f157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jacobian?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84037c54-02d4-42ad-8fb4-9da2b35b4262",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    # import pdb; pdb.set_trace()\n",
    "    print(\"\")\n",
    "    return (x[0]+x[1], x[2]*x[0], x[1]**3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81555248-ee77-4908-8e01-33c7b581a07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tensor([3., 4., 5.]).requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0689b92-a75f-4e41-9f81-a2e734d6c9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert x.grad is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde195f7-468d-45e9-b6d2-c13046889d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(7., grad_fn=<AddBackward0>),\n",
       " tensor(15., grad_fn=<MulBackward0>),\n",
       " tensor(64., grad_fn=<PowBackward0>))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0042c966-b5ff-4cb0-b778-825828a6b40e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1., 0.]), tensor([5., 0., 3.]), tensor([ 0., 48.,  0.]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jacobian(f, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9979dd91-b350-4391-83f5-4aebd73a9be8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(7., grad_fn=<AddBackward0>),\n",
       " tensor(15., grad_fn=<MulBackward0>),\n",
       " tensor(64., grad_fn=<PowBackward0>))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = x[0]+x[1], x[2]*x[0], x[1]**3\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f276497-dc36-4f24-a99e-b43ecc2d151a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'backward'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [159], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mz\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m(torch\u001b[38;5;241m.\u001b[39mones_like(z))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'backward'"
     ]
    }
   ],
   "source": [
    "z.backward(torch.ones_like(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ef5e12-68ce-47e4-8778-17f6f320428e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(50., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = (x**2).sum()\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771f43ff-5054-4ce4-938d-05f0b1d4b292",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7296d57f-8463-4d43-8ace-3e1d5bce804e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6.,  8., 10.])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84933e57-0fdd-4ebf-b865-3f5a08395c58",
   "metadata": {},
   "source": [
    "Now we need an Optimizer for the training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcc0071-5fba-413a-9084-b0b17f96a5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicOptimizer:\n",
    "    def __init__(self, params, lr): self.params, self.lr = list(params), lr\n",
    "    \n",
    "    def step(self, *args, **kwargs):\n",
    "        for p in self.params:\n",
    "            p.data.add_(-lr, p.grad.data)\n",
    "            \n",
    "    def zero_grad(self, *args, **kwargs):\n",
    "        for p in self.params:\n",
    "            p.grad.detach_()\n",
    "            p.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f683f599-c027-4f02-9d29-e5f0577eb8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-5\n",
    "opt = BasicOptimizer(rank_model.parameters(), lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5974fb23-8362-4a4b-b950-3b432b2e020c",
   "metadata": {},
   "source": [
    "Replacing our Optimizer with fastai's so that we can use callbacks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a5f6f7-246d-481a-a4a7-7a48adca0031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer?\n",
    "# SGD?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d352fb-168b-4303-9ce2-f37943628731",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-5\n",
    "opt = SGD(rank_model.parameters(), lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6144a9b2-4eef-477b-940d-c6db23c775c7",
   "metadata": {},
   "source": [
    "The above is equivalent to the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ffc9f0-a908-4081-9c8a-fdc0f9bf67a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mysgd_step(p, lr, **kwargs):\n",
    "    p.data.add_(p.grad.data, alpha=-lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d993e7ed-34ff-4d86-a112-acefe92043c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-5\n",
    "opt_func = partial(Optimizer, cbs=[mysgd_step])\n",
    "opt = opt_func(rank_model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf261e1a-11db-422f-80ca-1a82bb092806",
   "metadata": {},
   "source": [
    "Now if we want to add **weight decay:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bbfb2e-5f07-4e1d-b58b-708016e04a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-4\n",
    "opt_func = partial(Optimizer, cbs=[weight_decay, sgd_step])\n",
    "opt = opt_func(rank_model.parameters(), lr=lr, wd=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94098cea-9d4b-4c51-9d6b-1cd3f442e9e2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec72a8bc-b58e-4cff-81d6-0f63908369b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-5\n",
    "opt = SGD(rank_model_NN.parameters(), lr=lr, mom=0.0, wd=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b904f406-8ccd-4ad3-80f2-2591cbd2276b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-5\n",
    "opt = RMSProp(rank_model.parameters(), lr=lr, )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a1c0d2-8167-4a09-a218-936aaae0ab3d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3a5c51-5935-4e9f-9514-d43f0c5aeaf9",
   "metadata": {},
   "source": [
    "The `train_epoch` and `validate_epoch` are no longer needed now as we have a fastai like `Learner` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3b836d-a41f-40a5-886a-e89c6ece82da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_epoch(model, mb=None, metric_logger=None, **kwargs):\n",
    "    # import pdb; pdb.set_trace()\n",
    "    losses, ndcgs, ndcgs_at_6, accs = [], [], [], []\n",
    "    pb = progress_bar(dls.valid, parent=mb)\n",
    "    for xb in pb:\n",
    "        preds = model(xb)\n",
    "        loss = loss_fn(preds, xb)\n",
    "        losses.append(loss.mean())\n",
    "        *_, _ndcg, _ndcg_at_k = ndcg(model(xb), xb, k=6)\n",
    "        ndcgs.append(_ndcg.mean())\n",
    "        ndcgs_at_6.append(_ndcg_at_k.mean())\n",
    "        # acc = order_accuracy(xb.squeeze(0))\n",
    "        acc = accuracy(xb, model)\n",
    "        accs.append(acc.mean())\n",
    "        pb.comment = f'second bar stat'\n",
    "    losses = torch.stack(losses)\n",
    "    ndcgs = torch.stack(ndcgs)\n",
    "    ndcgs_at_6 = torch.stack(ndcgs_at_6)\n",
    "    accs = torch.stack(accs)\n",
    "    logger = [round(losses.mean().item(), 4), round(ndcgs.mean().item(), 4), round(ndcgs_at_6.mean().item(), 4), round(accs.mean().item(), 4)]\n",
    "    if metric_logger is not None: metric_logger.append(logger)\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae5e3e5-52af-4bf7-942c-ee0e7962f614",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, mb, track_trn=True, logger=None, grad_logger=None, **kwargs):\n",
    "    ndcgs, accs = [], []\n",
    "    c = 1\n",
    "    for xb in progress_bar(dls.train, parent=mb):\n",
    "        preds = model(xb)\n",
    "        \n",
    "        # import pdb; pdb.set_trace()\n",
    "        \n",
    "        ## handcrafted gradients\n",
    "        srtd_preds, lambda_i = rank_loss(preds, xb)\n",
    "        srtd_preds.backward(lambda_i)\n",
    "        \n",
    "        ## tracking gradients\n",
    "        for name,param in model.named_parameters():\n",
    "            # import pdb; pdb.set_trace()\n",
    "            # from IPython import embed; embed()\n",
    "            grad = param.grad.data.detach().clone()\n",
    "            grad_logger[name].append(grad)\n",
    "        \n",
    "        ## autograd gradients\n",
    "        if logger is not None:\n",
    "            with torch.no_grad():\n",
    "                loss = loss_fn(preds, xb)\n",
    "                logger.append(loss.mean())\n",
    "        # loss.backward()\n",
    "        \n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        if track_trn:\n",
    "            with torch.no_grad():\n",
    "                *_, _ndcg, _ = ndcg(preds, xb)\n",
    "                btch_ndcg_mean = _ndcg.mean()\n",
    "                ndcgs.append(btch_ndcg_mean)\n",
    "                btch_acc_mean = accuracy(xb, model).mean()\n",
    "                accs.append(btch_acc_mean)\n",
    "        mb.child.commment = f'batch ndcg mean: {btch_ndcg_mean if track_trn else \"NA\"}'\n",
    "    ndcgs, accs = (torch.stack(ndcgs), torch.stack(accs)) if track_trn else (torch.Tensor(), torch.Tensor())\n",
    "    return round(ndcgs.mean().item(), 4), round(accs.mean().item(), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc153b4e-b57c-420e-9370-cd3888a1db70",
   "metadata": {},
   "source": [
    "We want to compute a metric which measures how many orderings did the model get right:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0416d253-f34d-4ab9-b83a-5b6393883fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_lbs_accuracy(preds, xb, len=1000, resamps=10, threshold=.5):\n",
    "    preds = preds.squeeze(-1)\n",
    "    tok_sl = xb.shape[2]\n",
    "    acc = 0\n",
    "    for _ in range(resamps):\n",
    "        rnd_idxs = torch.randperm(tok_sl)[:len]\n",
    "        rnd_xb = xb[:, :, rnd_idxs]\n",
    "        rnd_preds = preds[:, :, rnd_idxs] \n",
    "        srtd_relv, srtd_idxs = rnd_xb[:, :, :, -1].sort(descending=True)\n",
    "        srtd_preds = torch.take_along_dim(rnd_preds, srtd_idxs, dim=-1)\n",
    "        sl = torch.arange(len if tok_sl > len else tok_sl, device=xb.device)\n",
    "        ij = torch.cartesian_prod(sl, sl)\n",
    "        idxs, = torch.nonzero(ij[:, 0] < ij[:, 1], as_tuple=True)\n",
    "        ij = ij[idxs]\n",
    "        # si_sj = srtd_relv[:, :, ij]\n",
    "        # (si_sj[:, :, :, 0] >= si_sj[:, :, :, 1]).shape, (*srtd_relv.shape[:2], 49950)\n",
    "        # torch.equal(si_sj.new_ones(*si_sj.shape[:-1]), (si_sj[:, :, :, 0] >= si_sj[:, :, :, 1]))\n",
    "        pi_pj = srtd_preds[:, :, ij]\n",
    "        probs_hat = torch.sigmoid(pi_pj[:, :, :, 0] - pi_pj[:, :, :, 1])\n",
    "        probs_hat = (probs_hat > threshold).float()\n",
    "        # acc += (pi_pj[:, :, :, 0] > pi_pj[:, :, :, 1]).float().mean(dim=-1) # earlier this was wrong\n",
    "        acc += probs_hat.mean(-1) # the last axis is the token pair (more relevant, less relevant)\n",
    "    return acc/resamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c13d5d-db2e-4ea9-bdde-8acd9a2d52f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(xb, model):\n",
    "    if len(xb.shape) != 4: xb = xb.unsqueeze(0) # add the batch dim if it is not there (0: batch, 1: lbs, 2: toks, 3: tok_id,lbl_id,score)\n",
    "    btch_acc = []\n",
    "    for btch_splt in torch.split(xb, 4, dim=0):\n",
    "        lbs_acc = []\n",
    "        for lbs_splt in torch.split(btch_splt, 100, dim=1):\n",
    "            lbs_acc.append(batch_lbs_accuracy(model(lbs_splt), lbs_splt))\n",
    "        # import pdb; pdb.set_trace()\n",
    "        lbs_acc = torch.cat(lbs_acc, dim=-1)\n",
    "        btch_acc.append(lbs_acc)\n",
    "    btch_acc = torch.cat(btch_acc, dim=0)\n",
    "    return btch_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ceaa2e-72f4-4b69-8a5d-14aa51ae51cb",
   "metadata": {},
   "source": [
    "<mark>NOTE: The following `ndcg` only used on a batch: </mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cf16cb-0751-46ac-87f5-442f60935c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ndcg_temp(preds, xb, k=None):\n",
    "    preds = preds.squeeze(-1)\n",
    "    preds_rank = preds.argsort(dim=-1, descending=True).argsort(dim=-1)\n",
    "    ideal_rank = xb[:, :, :, -1].argsort(dim=-1, descending=True).argsort(dim=-1)\n",
    "    discnt_fac = torch.log2(preds_rank+2)\n",
    "    ideal_discnt_fac = torch.log2(ideal_rank+2)\n",
    "    # eps = preds.new_empty(1).fill_(1e-15)\n",
    "    discntd_gain = torch.pow(2, xb[:, :, :, -1])  / (discnt_fac)\n",
    "    ideal_discntd_gain = torch.pow(2, xb[:, :, :, -1])  / (ideal_discnt_fac)\n",
    "    dcg = discntd_gain.sum(dim=-1)#.flatten()\n",
    "    idcg = ideal_discntd_gain.sum(dim=-1)#.flatten()\n",
    "    ndcg = dcg/idcg\n",
    "    \n",
    "    ndcg_at_k = None\n",
    "    \n",
    "    if k is not None:\n",
    "        # pdb.set_trace()\n",
    "        topk_preds, topk_preds_idxs = torch.topk(preds, k=k, dim=-1, largest=True)\n",
    "        topk_preds_relv = torch.take_along_dim(xb[:, :, :, -1], topk_preds_idxs, dim=-1)\n",
    "        topk_df = torch.log2(2 + torch.arange(k)).cuda()# torch.take_along_dim(discnt_fac, topk_preds_idxs, dim=-1)\n",
    "        dg_at_k = torch.pow(2, topk_preds_relv) / (topk_df) # changed\n",
    "        dcg_at_k = dg_at_k.sum(dim=-1)\n",
    "\n",
    "        topk, topk_idxs = torch.topk(xb[:, :, :, -1], k=k, dim=-1, largest=True)\n",
    "        idg_at_k = torch.pow(2, topk) / (topk_df) # changed\n",
    "        idcg_at_k = idg_at_k.sum(dim=-1)\n",
    "        \n",
    "        ndcg_at_k = dcg_at_k / idcg_at_k\n",
    "\n",
    "    return preds, preds_rank, ideal_rank, discnt_fac, ideal_discnt_fac, discntd_gain, ideal_discntd_gain, dcg, idcg, ndcg, ndcg_at_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4790ca-9de8-4801-b1ed-724a63029f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/deb/miniconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3433, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_14902/881185037.py\", line 1, in <module>\n",
      "    acc = batch_lbs_accuracy(preds, xb)\n",
      "NameError: name 'preds' is not defined\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/deb/miniconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2052, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/home/deb/miniconda3/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1112, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/home/deb/miniconda3/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1006, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/home/deb/miniconda3/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 859, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/home/deb/miniconda3/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 812, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(r))\n",
      "  File \"/home/deb/miniconda3/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 730, in format_record\n",
      "    result += ''.join(_format_traceback_lines(frame_info.lines, Colors, self.has_colors, lvals))\n",
      "  File \"/home/deb/miniconda3/lib/python3.10/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home/deb/miniconda3/lib/python3.10/site-packages/stack_data/core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"/home/deb/miniconda3/lib/python3.10/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home/deb/miniconda3/lib/python3.10/site-packages/stack_data/core.py\", line 677, in included_pieces\n",
      "    scope_pieces = self.scope_pieces\n",
      "  File \"/home/deb/miniconda3/lib/python3.10/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home/deb/miniconda3/lib/python3.10/site-packages/stack_data/core.py\", line 614, in scope_pieces\n",
      "    scope_start, scope_end = self.source.line_range(self.scope)\n",
      "  File \"/home/deb/miniconda3/lib/python3.10/site-packages/stack_data/core.py\", line 178, in line_range\n",
      "    return line_range(self.asttext(), node)\n",
      "AttributeError: 'Source' object has no attribute 'asttext'\n"
     ]
    }
   ],
   "source": [
    "acc = batch_lbs_accuracy(preds, xb)\n",
    "ic(xb.shape, preds.shape, acc.shape, acc.mean());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e2bc05-d029-45f4-a7a7-fd04ac790a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| srtd_idxs.shape: torch.Size([64, 2243, 64])\n",
      "ic| srtd_preds.shape: torch.Size([64, 2243, 64])\n",
      "ic| ij.shape: torch.Size([2016, 2]), tok_sl: 64\n"
     ]
    }
   ],
   "source": [
    "tok_sl = xb.shape[2]\n",
    "srtd_relv, srtd_idxs = xb[:, :, :, -1].sort(descending=True)\n",
    "ic(srtd_idxs.shape)\n",
    "srtd_preds = torch.take_along_dim(preds[:,:,:,0], srtd_idxs, dim=-1)\n",
    "ic(srtd_preds.shape)\n",
    "sl = torch.arange(tok_sl, device=xb.device)\n",
    "ij = torch.cartesian_prod(sl, sl)\n",
    "idxs, = torch.nonzero(ij[:, 0] < ij[:, 1], as_tuple=True)\n",
    "ij = ij[idxs]\n",
    "ic(ij.shape, tok_sl);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb5cd35-6edd-4499-94b3-6e7bfe5b2e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| si_sj.shape: torch.Size([64, 2243, 2016, 2])\n",
      "ic| (si_sj[:, :, :, 0] >= si_sj[:, :, :, 1]).shape: torch.Size([64, 2243, 2016])\n",
      "ic| probs.shape: torch.Size([64, 2243, 2016])\n",
      "    probs.mean(): tensor(0.9381, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 2243, 2016]), tensor(0.9381, device='cuda:0'))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "si_sj = srtd_relv[:, :, ij]\n",
    "ic(si_sj.shape);\n",
    "ic((si_sj[:, :, :, 0] >= si_sj[:, :, :, 1]).shape)\n",
    "probs = torch.sigmoid(si_sj[:, :, :, 0] - si_sj[:, :, :, 1])\n",
    "assert torch.equal(si_sj.new_ones(*si_sj.shape[:-1]), (si_sj[:, :, :, 0] >= si_sj[:, :, :, 1]))\n",
    "ic(probs.shape, probs.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8c1d2e-4706-4544-8564-23c7ecb94c1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 2243, 2016])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signs = torch.sign(si_sj[:, :, :, 0] - si_sj[:, :, :, 1])\n",
    "signs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d0febc-2f60-4c71-baac-5fb4e843ba99",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = torch.where(signs[0][0] == 1)[0].numel()\n",
    "eq = torch.where(signs[0][0] == 0)[0].numel()\n",
    "neg = torch.where(signs[0][0] == -1)[0].numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bba419-a432-4068-ace6-abe972fda66e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1934, 82, 0, 2016)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos, eq, neg, (pos+neg+eq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d13ade4-9758-4f5d-b9c6-601f5c881b8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1934., device='cuda:0')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signs[0,0].float().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06402bdf-4d32-4640-bc45-36c904f736b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| pi_pj.shape: torch.Size([64, 2233, 2016, 2])\n",
      "ic| probs_hat.shape: torch.Size([64, 2233, 2016])\n"
     ]
    }
   ],
   "source": [
    "pi_pj = srtd_preds[:, :, ij]\n",
    "ic(pi_pj.shape);\n",
    "probs_hat = torch.sigmoid(pi_pj[:, :, :, 0] - pi_pj[:, :, :, 1])\n",
    "# probs2 =  1/(1 + torch.exp( -0.5 * (pi_pj[:, :, :, 0] - pi_pj[:, :, :, 1])))\n",
    "ic(probs_hat.shape);\n",
    "# .float().mean(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddb0255-93d1-4b43-9472-6d46f25e3d7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4998, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs_hat.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9573a0c6-40a8-4010-8ad3-e4d60c75ba37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_accuracy(xb):\n",
    "    btch_acc = []\n",
    "    for xb_splt in torch.split(xb, 4 ,dim=0):\n",
    "        btch_acc.append(batch_lbs_accuracy(rank_model(xb_splt), xb_splt))\n",
    "    btch_acc = torch.cat(btch_acc, dim=0)\n",
    "    return btch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31690ceb-c6bd-4e27-a1ca-4b22ba6b2e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_accuracy(dset):\n",
    "    dset = dset.unsqueeze(0)\n",
    "    dset_chnked = torch.split(dset, 100, dim=1)\n",
    "    acc = []\n",
    "    for chunk in  dset_chnked:\n",
    "        btch_acc = batch_lbs_accuracy(rank_model(chunk), chunk)\n",
    "        acc.append(btch_acc)\n",
    "    acc = torch.cat(acc, dim=-1)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f411c8-b244-4705-b510-dd670992c453",
   "metadata": {},
   "source": [
    "<mark>NOTE: The following `ndcg_at_k` only used on the entite dataset: </mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314d4192-dbc3-418c-89d3-11b21947f407",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ndcg_at_k(dset, model, k=20):\n",
    "    dset = dset.unsqueeze(0)\n",
    "    dset_chnked = torch.split(dset, 100, dim=1)\n",
    "    ndcg_at_k_list = []\n",
    "    for chunk in  dset_chnked:\n",
    "        *_, ndcg_at_k = ndcg(model(chunk), chunk, k=k)\n",
    "        ndcg_at_k_list.append(ndcg_at_k)\n",
    "    ndcg_at_k_all = torch.cat(ndcg_at_k_list, dim=-1)\n",
    "    return ndcg_at_k_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6f87a3-c0ac-4d97-99eb-949291f4ba69",
   "metadata": {},
   "source": [
    "Now let's write our training loop:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f67eae7-fbe0-4eae-9737-8271cecca844",
   "metadata": {},
   "source": [
    "`train_model` and `validate_model` no longer needed now as we have a fastai like `Learner`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3f3d9f-9929-41b9-8b9f-ea08cf9c1f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, epochs, **kwargs):\n",
    "    mb = master_bar(range(epochs))\n",
    "    for i,_  in enumerate(mb):\n",
    "        print(f\"epoch: {i}\")\n",
    "        print(f\"training ndcg, training accuracy = {train_epoch(model, mb, **kwargs)}\")\n",
    "        mb.main_bar.comment = f'first bar stat'\n",
    "        print(f\"validation loss, validation ndcg, validation ndcg_at_6(candidate: 32), validation accuracy = {validate_epoch(model, mb, **kwargs)}\", end='\\n----\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb08ad4-0d7d-4272-89f7-f3647cf201f6",
   "metadata": {},
   "source": [
    "Let's also write our validation loop in case we just need to validate the model and not train it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68f22fe-1ee5-473c-8b42-9cc522ff8e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model):\n",
    "    return validate_epoch(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa53f8dc-dc20-443a-adc9-a4d7f58f5398",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner:\n",
    "    def __init__(self, model, dls, grad_func, loss_func, lr, opt_func=SGD, path=None):\n",
    "        store_attr()\n",
    "        self.path = Path(path) if path is not None else getattr(dls, 'path', Path('.'))\n",
    "   \n",
    "    def one_batch(self, losses, ndcgs, ndcgs_at_6, accs, track_trn=True, logger=None, grad_logger=None, metric_logger=None, **kwargs):\n",
    "        # import pdb; pdb.set_trace()\n",
    "        self.preds = self.model(self.xb)\n",
    "        if self.model.training: # training\n",
    "            srtd_preds, lambda_i = self.grad_func(self.preds, self.xb)\n",
    "            srtd_preds.backward(lambda_i)\n",
    "            \n",
    "            ## tracking gradients\n",
    "            for name,param in self.model.named_parameters():\n",
    "            # import pdb; pdb.set_trace()\n",
    "            # from IPython import embed; embed()\n",
    "                grad = param.grad.data.detach().clone()\n",
    "                grad_logger[name].append(grad)\n",
    "            \n",
    "            # tracking loss\n",
    "            if logger is not None:\n",
    "                with torch.no_grad():\n",
    "                    loss = self.loss_func(self.preds, self.xb)\n",
    "                    logger.append(loss.mean())\n",
    "                    losses.append(loss.mean())\n",
    "            \n",
    "            ## stepping the params\n",
    "            self.opt.step()\n",
    "            ## zeroing the grad before next batch\n",
    "            self.opt.zero_grad()\n",
    "            \n",
    "            # tracking metrics during training\n",
    "            if track_trn:\n",
    "                with torch.no_grad():\n",
    "                    *_, _ndcg, _ = ndcg(self.preds, self.xb)\n",
    "                    btch_ndcg_mean = _ndcg.mean()\n",
    "                    ndcgs.append(btch_ndcg_mean)\n",
    "                    btch_acc_mean = accuracy(self.xb, self.model).mean()\n",
    "                    accs.append(btch_acc_mean)\n",
    "            \n",
    "        else: # validation\n",
    "            loss = self.loss_func(self.preds, self.xb)\n",
    "            losses.append(loss.mean())\n",
    "            *_, _ndcg, _ndcg_at_k = ndcg(self.preds, self.xb, k=6)\n",
    "            ndcgs.append(_ndcg.mean())\n",
    "            ndcgs_at_6.append(_ndcg_at_k.mean())\n",
    "            # acc = order_accuracy(xb.squeeze(0))\n",
    "            acc = accuracy(self.xb, self.model)\n",
    "            accs.append(acc.mean())\n",
    "            \n",
    "        return losses, ndcgs, ndcgs_at_6, accs    \n",
    "        \n",
    "    def one_epoch(self, train, mb, metric_logger=None, **kwargs):\n",
    "        # import pdb; pdb.set_trace()\n",
    "        losses, ndcgs, ndcgs_at_6, accs = [], [], [], []\n",
    "        self.model.training = train\n",
    "        dl = self.dls.train if train else self.dls.valid\n",
    "        for self.num, self.xb in enumerate(progress_bar(dl, parent=mb, leave=False)):\n",
    "            losses, ndcgs, ndcgs_at_6, accs = self.one_batch(losses, ndcgs, ndcgs_at_6, accs, **kwargs)\n",
    "        _li = [losses, ndcgs, ndcgs_at_6, accs]\n",
    "        _li = [torch.stack(o) if o else torch.Tensor() for o in _li] \n",
    "        [losses, ndcgs, ndcgs_at_6, accs] = _li\n",
    "        # import pdb; pdb.set_trace()\n",
    "        logger = [round(o.mean().item(), 4) if o.sum() else \"NA\" for o in _li]\n",
    "        # logger = [round(losses.mean().item(), 4), round(ndcgs.mean().item(), 4), round(ndcgs_at_6.mean().item(), 4), round(accs.mean().item(), 4)]\n",
    "        if not self.model.training and metric_logger is not None: metric_logger.append(logger)\n",
    "        return logger\n",
    "    \n",
    "    def create_opt(self):\n",
    "        self.opt = self.opt_func(self.model.parameters(), self.lr)\n",
    "        self.opt.clear_state()\n",
    "        return self.opt\n",
    "    \n",
    "    def fit(self, n_epochs, best=None, **kwargs):\n",
    "        self.create_opt()\n",
    "        self.n_epochs = n_epochs\n",
    "        mb = master_bar(range(self.n_epochs))\n",
    "        columns=['train_loss', 'train_ndcg', 'train_ndcg@6', 'train_acc', 'val_loss', 'val_ndcg (candi. 32)', 'val ndcg@6 (candi. 32)', 'val_acc']\n",
    "        # index = range(self.n_epochs)\n",
    "        pdf = pd.DataFrame(columns=columns)#, index=index)\n",
    "        pdf.index.name = 'epoch'\n",
    "        if best is not None and best[0] not in columns: raise NameError(best[0]+'metric is not trackable, please check name!')\n",
    "        try:\n",
    "            for self.epoch,_ in enumerate(mb):\n",
    "                pdf.loc[self.epoch] = pd.Series(dict(zip(columns, self.one_epoch(True, mb, **kwargs) + self.one_epoch(False, mb, **kwargs))))\n",
    "                current = pdf.loc[self.epoch][best[0]]\n",
    "                if best is not None and current > best[1]: \n",
    "                    best[1] = current\n",
    "                    self.save(best[2])\n",
    "                # clear_output(wait=True)\n",
    "                # pdb.set_trace()\n",
    "                display_df(pdf.iloc[[self.epoch]])\n",
    "                # print(f\"train loss, train ndcg, train ndcg@6, train accuracy = {self.one_epoch(True, mb, **kwargs)}\")\n",
    "                # print(f\"validation loss, validation ndcg, validation ndcg_at_6(candidate: 32), validation accuracy = {self.one_epoch(False, mb, **kwargs)}\", end='\\n----\\n')\n",
    "            clear_output(wait=True)\n",
    "            display_df(pdf)\n",
    "        except CancelFitException: pass \n",
    "    \n",
    "    def validate(self, **kwargs):\n",
    "        columns=['val_loss', 'val_ndcg (candi. 32)', 'val ndcg@6 (candi. 32)', 'val_acc']\n",
    "        pdf = pd.DataFrame(columns=columns)\n",
    "        pdf.index.name = 'epoch'\n",
    "        try: \n",
    "            val = dict(zip(columns, self.one_epoch(False, None, **kwargs)))\n",
    "            pdf = pd.DataFrame([val])\n",
    "            display_df(pdf)\n",
    "        except CancelFitException: pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fc532f-8024-4b00-b8d8-a22543069d4f",
   "metadata": {},
   "source": [
    "**Serializing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235a6dbc-6c17-49ae-8f5d-c390b04d51c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "@delegates(save_model)\n",
    "def save(self:Learner, file, **kwargs):\n",
    "    \"Save model and optimizer state (if 'with_opt') to `self.path/file`\"\n",
    "    file = join_path_file(file, self.path, ext='.pth')\n",
    "    save_model(file, self.model, getattr(self, 'opt', None), **kwargs)\n",
    "    return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45c0cc4-5105-427a-8fb3-617458d2515c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "@delegates(load_model)\n",
    "def load(self:Learner, file, device=None, **kwargs):\n",
    "    \"Load model and optimizer state (if `with_opt`) from `self.path/file` using `device`\"\n",
    "    if device is None and hasattr(self.dls, 'device'): device = self.dls.device\n",
    "    self.opt = getattr(self, 'opt', None)\n",
    "    if self.opt is None: self.create_opt()\n",
    "    file = join_path_file(file, self.path, ext='.pth')\n",
    "    load_model(file, self.model, self.opt, device=device, **kwargs)\n",
    "    return self"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1001e884-f747-4c3b-bedd-22280d11a550",
   "metadata": {},
   "source": [
    "## Peace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0426335b-cb7e-4379-8354-090e2dd8f963",
   "metadata": {},
   "source": [
    "#### **Keeping records:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb41ddca-6a21-4fe6-863c-dacc028ae50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = ['lin', 'nn']\n",
    "algos = ['ranknet', 'lambda-rank']\n",
    "idx = pd.Index(['mimic-tiny', 'mimic-full'], name='dataset')\n",
    "cols = pd.MultiIndex.from_product([m, algos], names = ['model', 'algo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67854c21-21cc-40ac-b2be-58a8ca0ab10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=cols, index=idx)\n",
    "df[:] = 'TBD'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29edf896-b531-4870-a482-912f0c27fe38",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc['mimic-tiny']['nn']['ranknet'] = {'lr': 1e-4, 'opt': 'partial(RMSProp, mom=0.9, wd=0.0)', 'best': 70.67, 'epochs': 15, 'seed': 1, 'gain': 'cubic', 'factors': 100}\n",
    "df.loc['mimic-tiny']['lin']['ranknet'] = {'lr': 1e-4, 'opt': 'partial(SGD, mom=0.9, wd=0.0)', 'best': 65.69, 'epochs': 15, 'seed': 1, 'gain': 'cubic', 'factors': 100}\n",
    "df.loc['mimic-tiny']['nn']['lambda-rank'] = {'lr': [1e-2, 1e-3, 1e-4], 'opt': 'partial(Adam, mom=0.9, wd=0.4)', 'best': 62.75, 'epochs': [15, 15], 'seed': [1, str(8667)+' (L2RDataLoader)'], 'gain': 'exp', 'factors': 200}\n",
    "df.loc['mimic-tiny']['lin']['lambda-rank'] = {'lr': [7e-3, 7e-4], 'opt': 'partial(SGD, mom=0.9, wd=0.0)', 'best': 61.7, 'epochs': [15, 15], 'seed': 1, 'gain': 'exp', 'factors': 200}\n",
    "\n",
    "df.loc['mimic-full']['nn']['ranknet'] = 'TBD'\n",
    "df.loc['mimic-full']['lin']['ranknet'] = {'lr': 1e-5, 'opt': 'partial(SGD, mom=0.9, wd=0.0)', 'best': 63.73, 'epochs': 3, 'seed': 1, 'gain': 'cubic', 'factors': 100}\n",
    "df.loc['mimic-full']['nn']['lambda-rank'] = 'TBD'\n",
    "df.loc['mimic-full']['lin']['lambda-rank'] = {'lr': [7e-4, 7e-4, 7e-4], 'opt': 'partial(RMSProp, mom=0.9, wd=0.0)', 'best': 12.85, 'epochs': [4, 2, 4, 4], 'seed': 1, 'gain': 'exp', 'factors': 200}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b01acf-4d87-4fb2-ac26-1f4fb81494e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th colspan=\"2\" halign=\"left\">lin</th>\n",
       "      <th colspan=\"2\" halign=\"left\">nn</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>algo</th>\n",
       "      <th>ranknet</th>\n",
       "      <th>lambda-rank</th>\n",
       "      <th>ranknet</th>\n",
       "      <th>lambda-rank</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mimic-tiny</th>\n",
       "      <td>{'lr': 0.0001, 'opt': 'partial(SGD, mom=0.9, wd=0.0)', 'best': 65.69, 'epochs': 15, 'seed': 1, 'gain': 'cubic', 'factors': 100}</td>\n",
       "      <td>{'lr': [0.007, 0.0007], 'opt': 'partial(SGD, mom=0.9, wd=0.0)', 'best': 61.7, 'epochs': [15, 15], 'seed': 1, 'gain': 'exp', 'factors': 200}</td>\n",
       "      <td>{'lr': 0.0001, 'opt': 'partial(RMSProp, mom=0.9, wd=0.0)', 'best': 70.67, 'epochs': 15, 'seed': 1, 'gain': 'cubic', 'factors': 100}</td>\n",
       "      <td>{'lr': [0.01, 0.001, 0.0001], 'opt': 'partial(Adam, mom=0.9, wd=0.4)', 'best': 62.75, 'epochs': [15, 15], 'seed': [1, '8667 (L2RDataLoader)'], 'gain': 'exp', 'factors': 200}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mimic-full</th>\n",
       "      <td>{'lr': 1e-05, 'opt': 'partial(SGD, mom=0.9, wd=0.0)', 'best': 63.73, 'epochs': 3, 'seed': 1, 'gain': 'cubic', 'factors': 100}</td>\n",
       "      <td>{'lr': [0.0007, 0.0007, 0.0007], 'opt': 'partial(RMSProp, mom=0.9, wd=0.0)', 'best': 12.85, 'epochs': [4, 2, 4, 4], 'seed': 1, 'gain': 'exp', 'factors': 200}</td>\n",
       "      <td>TBD</td>\n",
       "      <td>TBD</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "model                                                                                                                                   lin  \\\n",
       "algo                                                                                                                                ranknet   \n",
       "dataset                                                                                                                                       \n",
       "mimic-tiny  {'lr': 0.0001, 'opt': 'partial(SGD, mom=0.9, wd=0.0)', 'best': 65.69, 'epochs': 15, 'seed': 1, 'gain': 'cubic', 'factors': 100}   \n",
       "mimic-full    {'lr': 1e-05, 'opt': 'partial(SGD, mom=0.9, wd=0.0)', 'best': 63.73, 'epochs': 3, 'seed': 1, 'gain': 'cubic', 'factors': 100}   \n",
       "\n",
       "model                                                                                                                                                                      \\\n",
       "algo                                                                                                                                                          lambda-rank   \n",
       "dataset                                                                                                                                                                     \n",
       "mimic-tiny                    {'lr': [0.007, 0.0007], 'opt': 'partial(SGD, mom=0.9, wd=0.0)', 'best': 61.7, 'epochs': [15, 15], 'seed': 1, 'gain': 'exp', 'factors': 200}   \n",
       "mimic-full  {'lr': [0.0007, 0.0007, 0.0007], 'opt': 'partial(RMSProp, mom=0.9, wd=0.0)', 'best': 12.85, 'epochs': [4, 2, 4, 4], 'seed': 1, 'gain': 'exp', 'factors': 200}   \n",
       "\n",
       "model                                                                                                                                        nn  \\\n",
       "algo                                                                                                                                    ranknet   \n",
       "dataset                                                                                                                                           \n",
       "mimic-tiny  {'lr': 0.0001, 'opt': 'partial(RMSProp, mom=0.9, wd=0.0)', 'best': 70.67, 'epochs': 15, 'seed': 1, 'gain': 'cubic', 'factors': 100}   \n",
       "mimic-full                                                                                                                                  TBD   \n",
       "\n",
       "model                                                                                                                                                                                      \n",
       "algo                                                                                                                                                                          lambda-rank  \n",
       "dataset                                                                                                                                                                                    \n",
       "mimic-tiny  {'lr': [0.01, 0.001, 0.0001], 'opt': 'partial(Adam, mom=0.9, wd=0.4)', 'best': 62.75, 'epochs': [15, 15], 'seed': [1, '8667 (L2RDataLoader)'], 'gain': 'exp', 'factors': 200}  \n",
       "mimic-full                                                                                                                                                                            TBD  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28a6492-b9e2-4ed9-8235-7f36a1a90f7e",
   "metadata": {},
   "source": [
    "#### **Get the `DataLoaders`:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de44cf0-9d03-43c0-8106-6dca28e908fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae027e49-b355-4f41-8d4f-dfdc8f4834b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.initial_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a1529b-527c-42e7-aa15-1399b910dca2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.TrnDataLoader"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('dataloader.py', 'r') as f: lines = f.readlines()\n",
    "exec(''.join(lines))\n",
    "TrnDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84384540-f4b5-4b34-b344-eb655a77b99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.69 ms, sys: 0 ns, total: 2.69 ms\n",
      "Wall time: 38.4 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "dls = torch.load(dls_learn_rank_tiny_path)\n",
    "len(dls.train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e14d24b-85ac-4374-9316-1ab42dd4224a",
   "metadata": {},
   "source": [
    "Based on the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c7412c-361e-46a2-917c-9bc7d2902fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_lbs, num_toks, num_factors = 8922, 57352, 200\n",
    "num_lbs, num_toks, num_factors = 104, 328, 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029e84d8-f175-4254-a7a7-ff7750c1b379",
   "metadata": {},
   "source": [
    "#### **Make the Model:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fe1ef4-7b00-4f1b-8e6b-dc5fff0bb07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = L2R_DotProductBias(num_lbs, num_toks, num_factors, y_range=None).cuda() #.to(default_device())\n",
    "# model = L2R_NN(num_lbs, num_toks, num_factors, n_act=100, y_range=None).cuda() #.to(default_device())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770252a4-f881-4cd3-802d-d96d25ba4ddc",
   "metadata": {},
   "source": [
    "**Create the `Learner` and train:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e157ec-1e88-4f86-8622-59237bf98334",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbs = [TrainEval(), TrackResults(train_metrics=True), ProgressBarCallback(), Monitor(), SaveCallBack('ranknet-tiny', monitor='acc')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb917a46-0561-499c-9c7b-dce068894566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grad_fn = partial(rank_loss3, gain_fn='exp', k=6)\n",
    "# learner = get_learner(model, dls, grad_fn=grad_fn, opt_func=partial(SGD, mom=0.9, wd=0.0), lambrank=True, lr=7e-4) \n",
    "# learner = get_learner(model, dls, lr=1e-5) # lin # ranknet # full\n",
    "# learner = get_learner(model, dls, grad_fn=grad_fn, opt_func=partial(RMSProp, mom=0.9, wd=0.0), lambrank=True, lr=7e-2) # lin # lambdarank # full\n",
    "# learner = get_learner(model, dls, lr=1e-4, opt_func=partial(RMSProp, mom=0.9, wd=0.0)) \n",
    "# learner = get_learner(model, dls, lr=1e-2, grad_fn=grad_fn, lambrank=True, opt_func=partial(Adam, mom=0.9, wd=0.4))  # nn lambdarank tiny\n",
    "# learner = get_learner(model, dls, lr=1e-2, grad_fn=grad_fn, lambrank=True, opt_func=partial(Adam, mom=0.9, wd=0.1))  # nn lambdarank full\n",
    "learner = get_learner(model, dls, lr=1e-4, cbs=cbs) #lin #ranknet #tiny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99179939-e5b9-41b5-b01a-142e058d1e31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<function xcube.l2r.gradients.rank_loss3(preds, xb, sigma=0.5, lambrank=False, gain_fn=None, k=6)>,\n",
       " functools.partial(<function SGD>, mom=0.9),\n",
       " None,\n",
       " 0.0001,\n",
       " <function xcube.l2r.gradients.loss_fn2(preds, xb, sigma=0.5)>]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getattrs(learner, 'grad_func', 'opt_func', 'opt', 'lr', 'loss_func')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325b627a-3658-40ce-bac0-d625ea340bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best = ['val ndcg@6 (candi. 32)', 0.6275, 'lambda-rank-tiny']\n",
    "# best = ['val ndcg@6 (candi. 32)', 0.3495, 'lambda-rank-full']\n",
    "# best = ['val_acc', 0, 'ranknet-full']\n",
    "best = ['val_acc', 0.0, 'ranknet-tiny']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4715d07b-7fae-4838-a400-d42a935d47d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture --no-stderr cap\n",
    "loss_logger = []\n",
    "metric_logger = []\n",
    "grad_logger = defaultdict(list)\n",
    "# with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "            # profile_memory=True, record_shapes=True) as prof:\n",
    "learner.fit(1, best=best, logger=loss_logger, metric_logger=metric_logger, grad_logger=grad_logger, track_trn=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5aea3ce-722a-4065-b641-8f88cb497f12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 True 1.6014 0.4479 0.2691 0.4815\n",
      "0 False 1.438 0.4402 0.3158 0.6004\n",
      "Better model found at epoch 0 with acc value: 0.6004.\n",
      "1 True 0.6697 0.5408 0.3749 0.606\n",
      "1 False 1.143 0.5226 0.4193 0.6389\n",
      "Better model found at epoch 1 with acc value: 0.6389.\n",
      "2 True 0.5086 0.6191 0.4789 0.6476\n",
      "2 False 1.0935 0.5261 0.4395 0.6507\n",
      "Better model found at epoch 2 with acc value: 0.6507.\n",
      "3 True 0.4394 0.6108 0.4691 0.6708\n",
      "3 False 1.0886 0.5418 0.4482 0.655\n",
      "Better model found at epoch 3 with acc value: 0.655.\n"
     ]
    }
   ],
   "source": [
    "learner.fit(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbdab59-0135-47d7-8581-ccd8ff4a218b",
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = learner.load('ranknet-tiny', device=default_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f780c3e-b9ca-4731-94ee-430a57ab4644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 False 1.0886 0.5418 0.4482 0.6548\n"
     ]
    }
   ],
   "source": [
    "learner.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91be6248-0894-4122-83c7-c32ea5a3b9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.cbs[-1].best = 0.68"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fd9cd0-3d03-4185-b268-cf2bd7fcaa28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('l2r_mini.log', 'w') as f: f.write(cap.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdad635e-fa5a-4c68-8369-cf054c5a899a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cat 'l2r_mini.log'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ca525f-c69e-4ef6-a3bd-e18c4fc64de8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('lambda-rank-full.pth')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.save('lambda-rank-full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907c07a3-164a-4b71-86d2-4770e99a5ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = learner.load('lambda-rank-full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0eaec3-6613-4dae-a3fb-85b1bf76fd3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_ndcg (candi. 32)</th>\n",
       "      <th>val ndcg@6 (candi. 32)</th>\n",
       "      <th>val_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.8274</td>\n",
       "      <td>0.4883</td>\n",
       "      <td>0.3495</td>\n",
       "      <td>0.6054</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learner.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5c1ea1-a9c9-40a4-a169-b14925e70643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 104, 16, 4])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb = dls.valid.one_batch()\n",
    "xb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947265da-d022-44c1-b5a3-ba5981ef01e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a325a05e-6512-4ce2-a355-c18da81ae0a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 104, 16, 1])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edcd397-aa8d-4576-9349-0e578251d02f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(3.0154, device='cuda:0', grad_fn=<MeanBackward0>),\n",
       " tensor(5.6613, device='cuda:0', grad_fn=<StdBackward0>))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.mean(), preds.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe2754e-ed13-4ed0-a090-486bb2ef6def",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1555])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ba5a02-d2a7-4998-a9ba-1dc4f66c8068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5017.6982, device='cuda:0', grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421f6854-9c9e-4a4a-b609-0bcd770461c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ndcg_temp(preds, xb, k=None):\n",
    "    preds = preds.squeeze(-1)\n",
    "    preds_rank = preds.argsort(dim=-1, descending=True).argsort(dim=-1)\n",
    "    ideal_rank = xb[:, :, :, -1].argsort(dim=-1, descending=True).argsort(dim=-1)\n",
    "    discnt_fac = torch.log2(preds_rank+2)\n",
    "    ideal_discnt_fac = torch.log2(ideal_rank+2)\n",
    "    # eps = preds.new_empty(1).fill_(1e-15)\n",
    "    discntd_gain = torch.pow(2, xb[:, :, :, -1])  / (discnt_fac)\n",
    "    ideal_discntd_gain = torch.pow(2, xb[:, :, :, -1])  / (ideal_discnt_fac)\n",
    "    dcg = discntd_gain.sum(dim=-1)#.flatten()\n",
    "    idcg = ideal_discntd_gain.sum(dim=-1)#.flatten()\n",
    "    ndcg = dcg/idcg\n",
    "    \n",
    "    ndcg_at_k = None\n",
    "    \n",
    "    if k is not None:\n",
    "        # pdb.set_trace()\n",
    "        topk_preds, topk_preds_idxs = torch.topk(preds, k=k, dim=-1, largest=True)\n",
    "        topk_preds_relv = torch.take_along_dim(xb[:, :, :, -1], topk_preds_idxs, dim=-1)\n",
    "        topk_df = torch.log2(2 + torch.arange(k)).cuda()# torch.take_along_dim(discnt_fac, topk_preds_idxs, dim=-1)\n",
    "        dg_at_k = torch.pow(2, topk_preds_relv) / (topk_df) # changed\n",
    "        dcg_at_k = dg_at_k.sum(dim=-1)\n",
    "\n",
    "        topk, topk_idxs = torch.topk(xb[:, :, :, -1], k=k, dim=-1, largest=True)\n",
    "        idg_at_k = torch.pow(2, topk) / (topk_df) # changed\n",
    "        idcg_at_k = idg_at_k.sum(dim=-1)\n",
    "        \n",
    "        ndcg_at_k = dcg_at_k / idcg_at_k\n",
    "\n",
    "    return preds, preds_rank, ideal_rank, discnt_fac, ideal_discnt_fac, discntd_gain, ideal_discntd_gain, dcg, idcg, ndcg, ndcg_at_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9706d47c-59f7-4fb2-84fd-2240a942a1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, *_, n_at_6 = ndcg(preds, xb, k=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe4793f-1831-43eb-ace2-88d5afffa4e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 104, 16]), torch.Size([1, 104]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape, n_at_6.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e92079-0df2-4109-8b6c-0f9f082ee29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_at_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368431a7-00eb-4bf7-8c29-de050a5a85e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = (btch, lbl) = 7,1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11da023d-1294-4a2b-af53-914fc853c940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9999, device='cuda:0')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_at_6[inp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4335a1c6-60a5-46d0-b4b6-ccc74e56a577",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = preds[inp].cpu()\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866758d5-8840-439e-886e-df3126ef83c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([21.,  1.,  3.,  1.,  3.,  5., 14., 12.,  1., 24.,  1.,  8.,  4., 12., 22.,  6.,  6., 27.,  3., 22.,  1.,  2.,  1.,  3.,  9., 22.,  3.,  6., 11.,  7.,  1.,  2., 27.,  3.,  4., 11., 48., 26.,\n",
       "        34.,  1.,  3.,  3.,  1.,  9.,  1.,  2., 12., 10., 34.,  6., 19.,  4., 28., 11., 26.,  6.,  2., 12.,  3.,  4.,  2.,  6., 13.,  1.])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = xb[inp][:, -1].cpu()\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e933f74-4467-4505-b9a9-0406db51fac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, topk_idxs = torch.topk(y, k=20, dim=-1, largest=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455e7d3a-bd17-4290-80e6-962325d6d49a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([56, 41, 42, 47, 19, 31, 46, 53, 10, 63, 39, 37, 48, 23,  4, 28, 62, 26, 49, 32])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topk_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c1e54c-8439-46f0-a079-17b2411c4d05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.,  1.,  1.,  1., 25., 19.,  5., 17., 17., 15., 15., 15., 48.,  2., 42.,  4., 11., 11., 28., 22.])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gains = x[topk_idxs]\n",
    "gains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f8b64e-c4a8-4a10-baf9-af78d15e4aa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 0.6309, 0.5000, 0.4307, 0.3869, 0.3562, 0.3333, 0.3155, 0.3010, 0.2891, 0.2789, 0.2702, 0.2626, 0.2560, 0.2500, 0.2447, 0.2398, 0.2354, 0.2314, 0.2277])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs = 1/torch.log2(2.0 + torch.arange(20))\n",
    "dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e824a067-1a32-4007-a414-a322ed56a8dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.0000e+00, 1.2619e+00, 1.0000e+00, 8.6135e-01, 1.2981e+07, 1.8676e+05, 1.0667e+01, 4.1349e+04, 3.9457e+04, 9.4721e+03, 9.1404e+03, 8.8552e+03, 7.3929e+13, 1.0238e+00, 1.0995e+12, 3.9144e+00,\n",
       "        4.9114e+02, 4.8212e+02, 6.2110e+07, 9.5492e+05])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dcg = torch.pow(2, gains) * dfs\n",
    "dcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca341f3-8839-441d-ae8a-6d9571e371c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([48., 42., 34., 30., 29., 28., 25., 22., 22., 22., 19., 17., 17., 17., 17., 16., 15., 15., 15., 14.])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topk_x, _ = torch.topk(x, k=20, dim=-1, largest=True)\n",
    "topk_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308fb352-8b10-48fe-8335-f78b6fb7c984",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.8147e+14, 4.3980e+12, 1.7180e+10, 1.0737e+09, 5.3687e+08, 2.6844e+08, 3.3554e+07, 4.1943e+06, 4.1943e+06, 4.1943e+06, 5.2429e+05, 1.3107e+05, 1.3107e+05, 1.3107e+05, 1.3107e+05, 6.5536e+04,\n",
       "        3.2768e+04, 3.2768e+04, 3.2768e+04, 1.6384e+04])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i_gains = torch.pow(2, topk_x)\n",
    "i_gains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906fc5ae-1462-4447-9bed-face3746c53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "idcg = i_gains*dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8791c94-8704-43c6-884e-a54237a0ef1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.8147e+14, 2.7749e+12, 8.5899e+09, 4.6244e+08, 2.0769e+08, 9.5619e+07, 1.1185e+07, 1.3232e+06, 1.2626e+06, 1.2124e+06, 1.4625e+05, 3.5421e+04, 3.4426e+04, 3.3549e+04, 3.2768e+04, 1.6033e+04,\n",
       "        7.8582e+03, 7.7139e+03, 7.5818e+03, 3.7301e+03])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6bfe95-1722-4ed6-9f49-542471498f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_at_k = dcg.sum()/idcg.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7025b1-150f-4b3f-890c-a2bbe7198899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2639)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_at_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ae8b6b-56bf-4506-b6dc-3468d1647189",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| trn.shape: torch.Size([8922, 57352, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([8922, 57352, 4])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trn, _ = torch.load('trn_val_split_tiny.pkl')\n",
    "trn, _ = torch.load('trn_val_split.pkl')\n",
    "trn = trn.to(\"cuda:0\")\n",
    "# trn_data = dls.train.dataset.to(\"cuda:0\")\n",
    "ic(trn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c2a27c-34fe-4b79-9470-9451530c0a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.37 s, sys: 0 ns, total: 5.37 s\n",
      "Wall time: 5.37 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "_ndcg_at_k = ndcg_at_k(trn, model, k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fe32e8-9a43-436b-8176-23f3b4af674a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| _ndcg_at_k.shape: torch.Size([1, 8922])\n",
      "    _ndcg_at_k.min(): tensor(4.7923e-20, device='cuda:0')\n",
      "    _ndcg_at_k.mean(): tensor(0.1285, device='cuda:0')\n",
      "    _ndcg_at_k.max(): tensor(0.9074, device='cuda:0')\n",
      "    _ndcg_at_k.median(): tensor(0.0614, device='cuda:0')\n",
      "    _ndcg_at_k.std(): tensor(0.1649, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "ic(_ndcg_at_k.shape, _ndcg_at_k.min(), _ndcg_at_k.mean(), _ndcg_at_k.max(), _ndcg_at_k.median(), _ndcg_at_k.std());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305c212d-84c0-461b-a3ef-8319eb0246e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = accuracy(trn, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecc5052-77ae-4bcd-9fe1-de5ff5aba79f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.5699, device='cuda:0'), tensor(0.0170, device='cuda:0'))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.mean(), a.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b75e44-bc1a-44ed-9ac3-6f89d7fcc2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = trn.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17630d19-edd6-43a9-9d19-fbd620258cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = learner.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c53296-bddd-4ba1-b1ee-54a5a72afb68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 104, 328, 1])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca3fe51-e476-4aaf-ac36-1a97905cac4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y, *_, test = ndcg(y, x, k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fead452-64e7-4166-96fc-09b7524036df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 104, 328]), torch.Size([1, 104]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68800681-6217-434a-a11d-fad5fa8981c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.4002, device='cuda:0'), tensor(0.3429, device='cuda:0'))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.mean(), test.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd1b466-2656-4d0b-8212-7abacdbc5762",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22732760906219482"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.percentile(_ndcg_at_k.cpu().numpy(), 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ec7893-c8df-47d3-b680-75d26ecb60a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff3ab87-26bd-4efd-91a3-4928739fc98a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "68787200-f373-4bd7-85ba-d12c0b9d270a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b790be-4c6f-48f7-a2ad-bc25d77f2fe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['token_factors.weight', 'label_factors.weight', 'layers.0.weight', 'layers.0.bias', 'layers.2.weight', 'layers.2.bias'])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_logger.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3244bc1f-4795-4d3a-bf68-bdebbb3f8448",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.stack(loss_logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484b18e8-0541-479e-a8be-788b0c5e68b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = dict()\n",
    "for k,v in grad_logger.items():\n",
    "    grads[k] = torch.stack(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20076f7d-7bbb-4b8f-810e-ae7279138ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "grads_tw = grads['token_factors.weight'].mean((1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1abd968-3e11-4593-8dbb-fe246b19c032",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frobenius_norm(t):\n",
    "    return t.square().mean().sqrt()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b036a15b-29a2-4aa5-add3-e0098fa978a9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909215b2-f39a-4253-b726-8706568a90de",
   "metadata": {},
   "source": [
    "Manually running grad updates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995bc45b-5d01-40e2-8c1f-7490ae24a29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "xb_iter= iter(dls.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744a15e2-43c5-4a3d-bb41-b80d8fca4932",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 2231, 64, 4])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb = next(xb_iter)\n",
    "xb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed94bd64-4f2e-45f4-b51f-088047df631d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| preds.shape: torch.Size([32, 2231, 64, 1])\n"
     ]
    }
   ],
   "source": [
    "preds = model(xb)\n",
    "ic(preds.shape);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabcb407-cda1-4c4c-9be5-4b5ebe6cc13f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| preds.min(): tensor(2.8571, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "    preds.max(): tensor(8.1702, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "    preds.mean(): tensor(5.4884, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "    preds.std(): tensor(0.5131, device='cuda:0', grad_fn=<StdBackward0>)\n"
     ]
    }
   ],
   "source": [
    "ic(preds.min(), preds.max(), preds.mean(), preds.std());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2c69d6-e0dd-4589-bdd0-fffa9292220f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%debug\n",
    "srtd_preds, lambda_i = rank_loss(preds, xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8989ba-8132-46b8-8136-734136c21d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| lambda_i.min(): tensor(-20.7688, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "    lambda_i.max(): tensor(20.7013, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "    lambda_i.mean(): tensor(1.7099e-09, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "    lambda_i.std(): tensor(9.3547, device='cuda:0', grad_fn=<StdBackward0>)\n"
     ]
    }
   ],
   "source": [
    "ic(lambda_i.min(), lambda_i.max(), lambda_i.mean(), lambda_i.std());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71164ee8-2841-4529-a96d-44f8b1cc80d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyzing\n",
    "btch_num = 0; lbl_num = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc32632f-d955-417f-9ccc-2a23b3583d18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([58., 51., 41., 35., 34., 33., 32., 29., 27., 26., 26., 23., 17., 17., 16., 16., 15., 15., 14., 13., 13., 13., 12., 12., 11., 11., 10.,  9.,  9.,  9.,  9.,  8.,  7.,  7.,  7.,  6.,  6.,  5.,\n",
       "         4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  3.,  3.,  3.,  3.,  2.,  2.,  2.,  2.,  1.,  1.,  1.,  1.,  1.], device='cuda:0')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "srtd_relvs[btch_num, lbl_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f4eac1-98ca-4203-adc9-2c3286682c67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5.0115, 5.4024, 5.5242, 6.1972, 5.0738, 5.3549, 6.2103, 5.1479, 5.6495, 4.7952, 4.4402, 5.8002, 5.6312, 5.5360, 6.3023, 4.5941, 5.2382, 5.4365, 5.5020, 4.8909, 5.0563, 5.9787, 4.7818, 4.4187,\n",
       "        4.8232, 5.6167, 5.8534, 4.9102, 5.7103, 5.4111, 4.7474, 5.6442, 5.2318, 5.3944, 5.3537, 5.1176, 5.5286, 5.1726, 4.5416, 5.6754, 5.7729, 5.0517, 5.7603, 5.0673, 5.0939, 4.6403, 5.8132, 5.7837,\n",
       "        4.6614, 5.5111, 5.6013, 4.7823, 4.2443, 5.3036, 4.8616, 6.1921, 6.2745, 5.9529, 5.3485, 5.5093, 5.5865, 4.6753, 5.6905, 4.8105], device='cuda:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "srtd_preds[btch_num, lbl_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca151df7-b912-4cd1-af17-0fb4a495a59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (ij, pi_pj[btch_num, lbl_num],  (pi-pj)[btch_num, lbl_num].unsqueeze(-1), exp_ij[btch_num, lbl_num].unsqueeze(-1), torch.sign(si-sj)[btch_num, lbl_num].unsqueeze(-1), lambda_ij[btch_num, lbl_num].unsqueeze(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980fa5f4-9396-4069-9688-97e3e01a2959",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.concat(data, dim=-1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666c9f77-3f5f-4879-8131-f41a10ceee49",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data, columns=['i', 'j', 'pi', 'pj', 'pi-pj', 'exp_ij', 'signs', 'lambda_ij'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092aec08-a20d-4267-80ca-2272abc51991",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['c_ij'] = 0.5 * ( 1 - df['signs']) * .5 * df['pi-pj'] + np.log(1 + np.exp(-.5*(df['pi-pj'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bbcd35-6488-4499-a2a4-46beebdb18d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c84807-3db6-4903-ba93-ef5109ae679a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.921943349842791"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(.5*-0.162543)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbec8da-8ef8-4375-98fb-f5085171b822",
   "metadata": {},
   "source": [
    "This is where 32 is more relevant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a722102c-114a-42ef-8f2c-a74940e79d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.i == 32]#.lambda_ij.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde04bea-dbb1-4fd7-a7e1-10102044ce26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-7.4099374"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add = df[df.i == 32].lambda_ij.sum()\n",
    "add"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf1ad1c-8550-431e-890f-8c708262625a",
   "metadata": {},
   "source": [
    "This is where 32 is less relevant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a4730c-964a-4a4f-b8c2-a7f74f740e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.j == 32]#.lambda_ij.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c3bcfa-6738-44cd-a268-1e718a629526",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-7.798852"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub = df[df.j == 32].lambda_ij.sum()\n",
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4878714-7428-40bf-9d99-2b4d2ebfc253",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3889146"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add - sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70a7dcf-87eb-4654-b4f3-1b78eb0055bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_i[btch_num, lbl_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65e50e5-53da-4fc5-9685-035dd147c8ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.18950351590560352, 0.4764355204802511)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ".5 * -1/(1+np.exp(0.5*.987530)), np.log(1 + np.exp(-.5*.987530))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2a678b-e9fe-4914-b737-41101e41f619",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lambda_i = pd.DataFrame(lambda_i[btch_num, lbl_num], columns=['lambda_i'])\n",
    "df_lambda_i['i'] = range(len(df_lambda_i))\n",
    "cols = list(df_lambda_i.columns)\n",
    "df_lambda_i = df_lambda_i[cols[::-1]]\n",
    "df_lambda_i['preds'] = srtd_preds[btch_num, lbl_num].detach().cpu().numpy()\n",
    "df_lambda_i['relvs'] = srtd_relvs[btch_num, lbl_num].detach().cpu().numpy()\n",
    "ranks = df_lambda_i.preds.to_numpy().argsort()[::-1].argsort()\n",
    "df_lambda_i['preds_rank'] = ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aff587d-d275-42dd-b0f6-f4e8ccfcb2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lambda_i\n",
    "# df_lambda_i.sort_values(by='preds', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f561821-3072-4159-be85-dfd3ecd34298",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fde1768a-e203-4258-97ee-d1c2fa63633d",
   "metadata": {},
   "source": [
    "#### Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b022a7-625f-4b22-b9f1-5385951d7334",
   "metadata": {},
   "source": [
    "**Plotting losses and metrics:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080bfbaa-9bf3-48e5-90a6-d7c222ce6fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(15,8))\n",
    "loss = L(loss_logger).map(torch.Tensor.item)\n",
    "val_loss = L(metric_logger).itemgot(0)\n",
    "val_acc = L(metric_logger).itemgot(-1)\n",
    "val_ndcg = L(metric_logger).itemgot(2)\n",
    "\n",
    "# axes[0,0].scatter(range(len(loss)), loss)\n",
    "axes[0,0].plot(range(len(loss)), loss)\n",
    "axes[0,0].set_xlabel('batches*epochs')\n",
    "axes[0,0].set_ylabel('train loss')\n",
    "\n",
    "axes[0,1].plot(val_loss)\n",
    "axes[0,1].set_xlabel('epochs')\n",
    "axes[0,1].set_ylabel('val loss')\n",
    "\n",
    "axes[1, 0].plot(val_acc)\n",
    "axes[1,0].set_xlabel('epochs')\n",
    "axes[1,0].set_ylabel('val accuracy')\n",
    "\n",
    "axes[1,1].plot(val_ndcg)\n",
    "axes[1,1].set_xlabel('epochs')\n",
    "axes[1,1].set_ylabel('val ndcg@6 (candidate 16)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc266c9-f06c-477f-894b-94e7793b9942",
   "metadata": {},
   "source": [
    "**Plotting Statistics of the Model Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa77b789-c482-4543-b5d1-c26f3f5ecfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2,2, figsize=(15,8), sharex=True)\n",
    "for (k,v), ax in zip(grad_logger.items(), axes.flatten()):\n",
    "    mean_grads = L(v).map(compose(torch.Tensor.square, torch.Tensor.mean, torch.Tensor.sqrt, torch.Tensor.item))\n",
    "    # sparsity = L(v).map(sparsity)\n",
    "    ax.plot(mean_grads, color='r', label='mean')\n",
    "    ax.set_ylabel(k)\n",
    "    # ax_a = ax.twinx()\n",
    "    # ax_a.plot(sparsity, color='b', label='sparsity')\n",
    "    ax.legend(loc='best')\n",
    "    # ax_a.legend(loc='best')\n",
    "fig.suptitle('RMS of the Gradients of Model Parameters')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04955c8f-7e51-4a13-aa14-da92dff6955a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparsity(t): \n",
    "    return 1 - (torch.count_nonzero(t)/t.numel()).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435d0316-4f9d-427f-a9f6-2b6afe5dac46",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2,2, figsize=(15,8), sharex=True)\n",
    "for (k,v), ax in zip(grad_logger.items(), axes.flatten()):\n",
    "    sp = L(v).map(sparsity)\n",
    "    ax.scatter(range(len(sp)), sp, color='r', label='sparsity')\n",
    "    ax.set_ylabel(k)\n",
    "    # ax_a = ax.twinx()\n",
    "    # ax_a.plot(sparsity, color='b', label='sparsity')\n",
    "    ax.legend(loc='best')\n",
    "    # ax_a.legend(loc='best')\n",
    "fig.suptitle('Sparsity of the Model Parameters')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30200208-19e9-4145-a6db-162c949c7a62",
   "metadata": {},
   "source": [
    "#### Debugging why nan/inf?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75790bc0-37ff-4f74-bb67-71a81d000379",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = learner.opt_func(model.parameters(), learner.lr)\n",
    "grad_func = learner.grad_func\n",
    "loss_func = learner.loss_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31eccb9-a3f5-4455-8b36-7ee6556b9ef8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(functools.partial(<function rank_loss3>, gain_fn='exp', k=6, lambrank=True),\n",
       " <function xcube.l2r.gradients.loss_fn2(preds, xb, sigma=0.5)>,\n",
       " 0.01,\n",
       " functools.partial(<function Adam>, mom=0.9, wd=0.1))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_func, loss_func, learner.lr, learner.opt_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9b0282-153d-4461-ae00-b0bfd3e76f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for xb in dls.train:\n",
    "    print(xb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5669cb56-0e72-41a4-a950-6209dd687368",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='113' class='' max='113' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [113/113 04:05&lt;00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mini-batch: 0\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -1.734759280225262e-05, p.std().item() = 1.0002665519714355\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0007847070228308439, p.std().item() = 0.9988294243812561\n",
      "n = 'layers.0.weight', p.mean().item() = 0.00018983049085363746, p.std().item() = 0.028825251385569572\n",
      "n = 'layers.0.bias', p.mean().item() = 0.0010731843067333102, p.std().item() = 0.031683795154094696\n",
      "n = 'layers.2.weight', p.mean().item() = -0.0034287134185433388, p.std().item() = 0.061452098190784454\n",
      "n = 'layers.2.bias', p.mean().item() = 0.09137120097875595, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = 0.012845568358898163, preds.std().item() = 0.20282460749149323\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.6947, device='cuda:0'), loss.std() = tensor(0.0061, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 5.82337511545461e-11, lambda_i.std().item() = 0.2391522228717804\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = 0.0002733718720264733, p.grad.std().item() = 2.4462618827819824\n",
      "n = 'label_factors.weight', p.grad.mean().item() = -0.0012455456890165806, p.grad.std().item() = 0.16901808977127075\n",
      "n = 'layers.0.weight', p.grad.mean().item() = 4.3943610191345215, p.grad.std().item() = 1497.13623046875\n",
      "n = 'layers.0.bias', p.grad.mean().item() = 176.0182647705078, p.grad.std().item() = 1212.4471435546875\n",
      "n = 'layers.2.weight', p.grad.mean().item() = 229.03125, p.grad.std().item() = 16035.4541015625\n",
      "n = 'layers.2.bias', p.grad.mean().item() = -0.00012302398681640625, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -1.979047738132067e-05, p.std().item() = 0.9992666840553284\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0008219213923439384, p.std().item() = 0.9978305101394653\n",
      "n = 'layers.0.weight', p.mean().item() = 0.0001681403664406389, p.std().item() = 0.03046691045165062\n",
      "n = 'layers.0.bias', p.mean().item() = -0.0003278888761997223, p.std().item() = 0.03212932497262955\n",
      "n = 'layers.2.weight', p.mean().item() = -0.004625285975635052, p.std().item() = 0.05998653545975685\n",
      "n = 'layers.2.bias', p.mean().item() = 0.10052809119224548, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 1\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -1.979047738132067e-05, p.std().item() = 0.9992666840553284\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0008219213923439384, p.std().item() = 0.9978305101394653\n",
      "n = 'layers.0.weight', p.mean().item() = 0.0001681403664406389, p.std().item() = 0.03046691045165062\n",
      "n = 'layers.0.bias', p.mean().item() = -0.0003278888761997223, p.std().item() = 0.03212932497262955\n",
      "n = 'layers.2.weight', p.mean().item() = -0.004625285975635052, p.std().item() = 0.05998653545975685\n",
      "n = 'layers.2.bias', p.mean().item() = 0.10052809119224548, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -0.0238512996584177, preds.std().item() = 0.3368228077888489\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.7032, device='cuda:0'), loss.std() = tensor(0.0128, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 8.296743847502341e-11, lambda_i.std().item() = 0.20782262086868286\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = -8.536045788787305e-05, p.grad.std().item() = 5.414724826812744\n",
      "n = 'label_factors.weight', p.grad.mean().item() = -0.0010023872600868344, p.grad.std().item() = 0.19096243381500244\n",
      "n = 'layers.0.weight', p.grad.mean().item() = 30.21754264831543, p.grad.std().item() = 1553.90478515625\n",
      "n = 'layers.0.bias', p.grad.mean().item() = 1344.9471435546875, p.grad.std().item() = 1631.909423828125\n",
      "n = 'layers.2.weight', p.grad.mean().item() = 1388.9776611328125, p.grad.std().item() = 27199.939453125\n",
      "n = 'layers.2.bias', p.grad.mean().item() = 0.0004138946533203125, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -2.4510287403245457e-05, p.std().item() = 0.998268187046051\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0008997046970762312, p.std().item() = 0.9968523383140564\n",
      "n = 'layers.0.weight', p.mean().item() = 5.612354652839713e-05, p.std().item() = 0.03220618516206741\n",
      "n = 'layers.0.bias', p.mean().item() = -0.006230754777789116, p.std().item() = 0.03282256796956062\n",
      "n = 'layers.2.weight', p.mean().item() = -0.004483462776988745, p.std().item() = 0.054828766733407974\n",
      "n = 'layers.2.bias', p.mean().item() = 0.09537748247385025, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 2\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -2.4510287403245457e-05, p.std().item() = 0.998268187046051\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0008997046970762312, p.std().item() = 0.9968523383140564\n",
      "n = 'layers.0.weight', p.mean().item() = 5.612354652839713e-05, p.std().item() = 0.03220618516206741\n",
      "n = 'layers.0.bias', p.mean().item() = -0.006230754777789116, p.std().item() = 0.03282256796956062\n",
      "n = 'layers.2.weight', p.mean().item() = -0.004483462776988745, p.std().item() = 0.054828766733407974\n",
      "n = 'layers.2.bias', p.mean().item() = 0.09537748247385025, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -0.02285429649055004, preds.std().item() = 0.36543673276901245\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.7038, device='cuda:0'), loss.std() = tensor(0.0132, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 9.366449976733193e-11, lambda_i.std().item() = 0.2444065660238266\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = -0.000248988188104704, p.grad.std().item() = 5.883663654327393\n",
      "n = 'label_factors.weight', p.grad.mean().item() = 0.00216209189966321, p.grad.std().item() = 0.1844872236251831\n",
      "n = 'layers.0.weight', p.grad.mean().item() = 11.176956176757812, p.grad.std().item() = 1314.9306640625\n",
      "n = 'layers.0.bias', p.grad.mean().item() = 932.0648803710938, p.grad.std().item() = 1259.3189697265625\n",
      "n = 'layers.2.weight', p.grad.mean().item() = 554.9796142578125, p.grad.std().item() = 22262.0625\n",
      "n = 'layers.2.bias', p.grad.mean().item() = 0.0003662109375, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -3.0107039492577314e-05, p.std().item() = 0.9972711205482483\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0009172670543193817, p.std().item() = 0.9958670139312744\n",
      "n = 'layers.0.weight', p.mean().item() = -4.692780930781737e-05, p.std().item() = 0.033923957496881485\n",
      "n = 'layers.0.bias', p.mean().item() = -0.012285151518881321, p.std().item() = 0.03360461816191673\n",
      "n = 'layers.2.weight', p.mean().item() = -0.0042050061747431755, p.std().item() = 0.049573227763175964\n",
      "n = 'layers.2.bias', p.mean().item() = 0.08829467743635178, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 3\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -3.0107039492577314e-05, p.std().item() = 0.9972711205482483\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0009172670543193817, p.std().item() = 0.9958670139312744\n",
      "n = 'layers.0.weight', p.mean().item() = -4.692780930781737e-05, p.std().item() = 0.033923957496881485\n",
      "n = 'layers.0.bias', p.mean().item() = -0.012285151518881321, p.std().item() = 0.03360461816191673\n",
      "n = 'layers.2.weight', p.mean().item() = -0.0042050061747431755, p.std().item() = 0.049573227763175964\n",
      "n = 'layers.2.bias', p.mean().item() = 0.08829467743635178, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -0.00398308876901865, preds.std().item() = 0.36827531456947327\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.7011, device='cuda:0'), loss.std() = tensor(0.0103, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 4.3101324592731416e-11, lambda_i.std().item() = 0.19687506556510925\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = -0.0009061897289939225, p.grad.std().item() = 7.3087921142578125\n",
      "n = 'label_factors.weight', p.grad.mean().item() = 0.0044909012503921986, p.grad.std().item() = 0.21172818541526794\n",
      "n = 'layers.0.weight', p.grad.mean().item() = -10.048721313476562, p.grad.std().item() = 1496.6190185546875\n",
      "n = 'layers.0.bias', p.grad.mean().item() = 1798.8193359375, p.grad.std().item() = 1847.297119140625\n",
      "n = 'layers.2.weight', p.grad.mean().item() = 6389.28125, p.grad.std().item() = 40917.58984375\n",
      "n = 'layers.2.bias', p.grad.mean().item() = 0.0003662109375, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -3.6259669286664575e-05, p.std().item() = 0.9962751269340515\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0008934364304877818, p.std().item() = 0.9948356747627258\n",
      "n = 'layers.0.weight', p.mean().item() = -0.00014212630048859864, p.std().item() = 0.03538146987557411\n",
      "n = 'layers.0.bias', p.mean().item() = -0.018614616245031357, p.std().item() = 0.034054629504680634\n",
      "n = 'layers.2.weight', p.mean().item() = -0.003899357281625271, p.std().item() = 0.044018663465976715\n",
      "n = 'layers.2.bias', p.mean().item() = 0.08033449947834015, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 4\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -3.6259669286664575e-05, p.std().item() = 0.9962751269340515\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0008934364304877818, p.std().item() = 0.9948356747627258\n",
      "n = 'layers.0.weight', p.mean().item() = -0.00014212630048859864, p.std().item() = 0.03538146987557411\n",
      "n = 'layers.0.bias', p.mean().item() = -0.018614616245031357, p.std().item() = 0.034054629504680634\n",
      "n = 'layers.2.weight', p.mean().item() = -0.003899357281625271, p.std().item() = 0.044018663465976715\n",
      "n = 'layers.2.bias', p.mean().item() = 0.08033449947834015, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -0.01403773669153452, preds.std().item() = 0.3300888240337372\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.6904, device='cuda:0'), loss.std() = tensor(0.0121, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 9.392540217811884e-11, lambda_i.std().item() = 0.22984090447425842\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = -0.0008083858992904425, p.grad.std().item() = 5.105569839477539\n",
      "n = 'label_factors.weight', p.grad.mean().item() = 0.00223007844761014, p.grad.std().item() = 0.1981440782546997\n",
      "n = 'layers.0.weight', p.grad.mean().item() = 2.4402337074279785, p.grad.std().item() = 993.99462890625\n",
      "n = 'layers.0.bias', p.grad.mean().item() = 923.9244384765625, p.grad.std().item() = 1102.767333984375\n",
      "n = 'layers.2.weight', p.grad.mean().item() = 4738.8369140625, p.grad.std().item() = 25735.38671875\n",
      "n = 'layers.2.bias', p.grad.mean().item() = 0.0005950927734375, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -4.335599805926904e-05, p.std().item() = 0.9952811002731323\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0008643674664199352, p.std().item() = 0.9938316345214844\n",
      "n = 'layers.0.weight', p.mean().item() = -0.00024672079598531127, p.std().item() = 0.036858219653367996\n",
      "n = 'layers.0.bias', p.mean().item() = -0.024773763492703438, p.std().item() = 0.03478144109249115\n",
      "n = 'layers.2.weight', p.mean().item() = -0.003774472279474139, p.std().item() = 0.03887547552585602\n",
      "n = 'layers.2.bias', p.mean().item() = 0.07174249738454819, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 5\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -4.335599805926904e-05, p.std().item() = 0.9952811002731323\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0008643674664199352, p.std().item() = 0.9938316345214844\n",
      "n = 'layers.0.weight', p.mean().item() = -0.00024672079598531127, p.std().item() = 0.036858219653367996\n",
      "n = 'layers.0.bias', p.mean().item() = -0.024773763492703438, p.std().item() = 0.03478144109249115\n",
      "n = 'layers.2.weight', p.mean().item() = -0.003774472279474139, p.std().item() = 0.03887547552585602\n",
      "n = 'layers.2.bias', p.mean().item() = 0.07174249738454819, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -0.009866004809737206, preds.std().item() = 0.273455411195755\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.6932, device='cuda:0'), loss.std() = tensor(0.0085, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 6.626959397904031e-11, lambda_i.std().item() = 0.2591142952442169\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = -0.00021730204753112048, p.grad.std().item() = 4.794558525085449\n",
      "n = 'label_factors.weight', p.grad.mean().item() = 0.0021780887618660927, p.grad.std().item() = 0.16672763228416443\n",
      "n = 'layers.0.weight', p.grad.mean().item() = -1.0056942701339722, p.grad.std().item() = 1113.4127197265625\n",
      "n = 'layers.0.bias', p.grad.mean().item() = -50.68861389160156, p.grad.std().item() = 672.2142333984375\n",
      "n = 'layers.2.weight', p.grad.mean().item() = 2886.98583984375, p.grad.std().item() = 22725.99609375\n",
      "n = 'layers.2.bias', p.grad.mean().item() = -0.0006580352783203125, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -5.136263644089922e-05, p.std().item() = 0.9942889213562012\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0008131081704050303, p.std().item() = 0.9928547143936157\n",
      "n = 'layers.0.weight', p.mean().item() = -0.00033119693398475647, p.std().item() = 0.038393303751945496\n",
      "n = 'layers.0.bias', p.mean().item() = -0.03017120249569416, p.std().item() = 0.0360039547085762\n",
      "n = 'layers.2.weight', p.mean().item() = -0.003872215747833252, p.std().item() = 0.03527938574552536\n",
      "n = 'layers.2.bias', p.mean().item() = 0.06874794512987137, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 6\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -5.136263644089922e-05, p.std().item() = 0.9942889213562012\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0008131081704050303, p.std().item() = 0.9928547143936157\n",
      "n = 'layers.0.weight', p.mean().item() = -0.00033119693398475647, p.std().item() = 0.038393303751945496\n",
      "n = 'layers.0.bias', p.mean().item() = -0.03017120249569416, p.std().item() = 0.0360039547085762\n",
      "n = 'layers.2.weight', p.mean().item() = -0.003872215747833252, p.std().item() = 0.03527938574552536\n",
      "n = 'layers.2.bias', p.mean().item() = 0.06874794512987137, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -0.0023910210002213717, preds.std().item() = 0.2315392941236496\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.6921, device='cuda:0'), loss.std() = tensor(0.0096, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 6.731320362218796e-11, lambda_i.std().item() = 0.29426488280296326\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = 0.0003299351956229657, p.grad.std().item() = 3.6763763427734375\n",
      "n = 'label_factors.weight', p.grad.mean().item() = -0.000873811193741858, p.grad.std().item() = 0.14119520783424377\n",
      "n = 'layers.0.weight', p.grad.mean().item() = 6.117428779602051, p.grad.std().item() = 792.339599609375\n",
      "n = 'layers.0.bias', p.grad.mean().item() = -165.1493377685547, p.grad.std().item() = 552.1015014648438\n",
      "n = 'layers.2.weight', p.grad.mean().item() = 5267.7666015625, p.grad.std().item() = 16270.1328125\n",
      "n = 'layers.2.bias', p.grad.mean().item() = 0.0011444091796875, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -5.927454913035035e-05, p.std().item() = 0.9932987093925476\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0007807862712070346, p.std().item() = 0.9918769001960754\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0003999804030172527, p.std().item() = 0.039926961064338684\n",
      "n = 'layers.0.bias', p.mean().item() = -0.034492362290620804, p.std().item() = 0.03738541156053543\n",
      "n = 'layers.2.weight', p.mean().item() = -0.004401594866067171, p.std().item() = 0.033119793981313705\n",
      "n = 'layers.2.bias', p.mean().item() = 0.06335367262363434, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 7\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -5.927454913035035e-05, p.std().item() = 0.9932987093925476\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0007807862712070346, p.std().item() = 0.9918769001960754\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0003999804030172527, p.std().item() = 0.039926961064338684\n",
      "n = 'layers.0.bias', p.mean().item() = -0.034492362290620804, p.std().item() = 0.03738541156053543\n",
      "n = 'layers.2.weight', p.mean().item() = -0.004401594866067171, p.std().item() = 0.033119793981313705\n",
      "n = 'layers.2.bias', p.mean().item() = 0.06335367262363434, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -0.016735423356294632, preds.std().item() = 0.23388023674488068\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.6930, device='cuda:0'), loss.std() = tensor(0.0078, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 6.350400760801733e-11, lambda_i.std().item() = 0.22448565065860748\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = 0.00043026640196330845, p.grad.std().item() = 3.4330861568450928\n",
      "n = 'label_factors.weight', p.grad.mean().item() = -0.002095174277201295, p.grad.std().item() = 0.1476789116859436\n",
      "n = 'layers.0.weight', p.grad.mean().item() = 1.875321388244629, p.grad.std().item() = 787.8773193359375\n",
      "n = 'layers.0.bias', p.grad.mean().item() = 93.98866271972656, p.grad.std().item() = 653.4296875\n",
      "n = 'layers.2.weight', p.grad.mean().item() = 5430.90625, p.grad.std().item() = 18439.2421875\n",
      "n = 'layers.2.bias', p.grad.mean().item() = -0.0001201629638671875, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -6.630102870985866e-05, p.std().item() = 0.9923102855682373\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0007691890350542963, p.std().item() = 0.9909272193908691\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0004519916546996683, p.std().item() = 0.041467759758234024\n",
      "n = 'layers.0.bias', p.mean().item() = -0.038464877754449844, p.std().item() = 0.038503892719745636\n",
      "n = 'layers.2.weight', p.mean().item() = -0.005172367673367262, p.std().item() = 0.03148559108376503\n",
      "n = 'layers.2.bias', p.mean().item() = 0.0589672327041626, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 8\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -6.630102870985866e-05, p.std().item() = 0.9923102855682373\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0007691890350542963, p.std().item() = 0.9909272193908691\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0004519916546996683, p.std().item() = 0.041467759758234024\n",
      "n = 'layers.0.bias', p.mean().item() = -0.038464877754449844, p.std().item() = 0.038503892719745636\n",
      "n = 'layers.2.weight', p.mean().item() = -0.005172367673367262, p.std().item() = 0.03148559108376503\n",
      "n = 'layers.2.bias', p.mean().item() = 0.0589672327041626, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -0.06818319112062454, preds.std().item() = 0.24016143381595612\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.6933, device='cuda:0'), loss.std() = tensor(0.0087, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 8.735062673181915e-11, lambda_i.std().item() = 0.3032383322715759\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = 0.0004962895764037967, p.grad.std().item() = 3.636373519897461\n",
      "n = 'label_factors.weight', p.grad.mean().item() = 0.004691338632255793, p.grad.std().item() = 0.15235140919685364\n",
      "n = 'layers.0.weight', p.grad.mean().item() = 8.215487480163574, p.grad.std().item() = 780.3555908203125\n",
      "n = 'layers.0.bias', p.grad.mean().item() = -371.06536865234375, p.grad.std().item() = 901.2301025390625\n",
      "n = 'layers.2.weight', p.grad.mean().item() = 3105.22265625, p.grad.std().item() = 28268.662109375\n",
      "n = 'layers.2.bias', p.grad.mean().item() = 0.00032806396484375, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -7.179931708378717e-05, p.std().item() = 0.9913239479064941\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0007064162055030465, p.std().item() = 0.989963710308075\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0005037300288677216, p.std().item() = 0.04288439452648163\n",
      "n = 'layers.0.bias', p.mean().item() = -0.04190707206726074, p.std().item() = 0.03960277885198593\n",
      "n = 'layers.2.weight', p.mean().item() = -0.006340104155242443, p.std().item() = 0.031232010573148727\n",
      "n = 'layers.2.bias', p.mean().item() = 0.054184552282094955, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 9\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -7.179931708378717e-05, p.std().item() = 0.9913239479064941\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0007064162055030465, p.std().item() = 0.989963710308075\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0005037300288677216, p.std().item() = 0.04288439452648163\n",
      "n = 'layers.0.bias', p.mean().item() = -0.04190707206726074, p.std().item() = 0.03960277885198593\n",
      "n = 'layers.2.weight', p.mean().item() = -0.006340104155242443, p.std().item() = 0.031232010573148727\n",
      "n = 'layers.2.bias', p.mean().item() = 0.054184552282094955, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -0.09088834375143051, preds.std().item() = 0.24660824239253998\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.6935, device='cuda:0'), loss.std() = tensor(0.0078, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 7.493160258942311e-11, lambda_i.std().item() = 0.31312355399131775\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = 0.00014737877063453197, p.grad.std().item() = 3.828598976135254\n",
      "n = 'label_factors.weight', p.grad.mean().item() = -0.0008374953176826239, p.grad.std().item() = 0.13778898119926453\n",
      "n = 'layers.0.weight', p.grad.mean().item() = -4.7578606605529785, p.grad.std().item() = 811.8342895507812\n",
      "n = 'layers.0.bias', p.grad.mean().item() = -170.13246154785156, p.grad.std().item() = 505.401123046875\n",
      "n = 'layers.2.weight', p.grad.mean().item() = 2536.87744140625, p.grad.std().item() = 17572.666015625\n",
      "n = 'layers.2.bias', p.grad.mean().item() = 0.000263214111328125, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -7.576629286631942e-05, p.std().item() = 0.990339457988739\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0006576450541615486, p.std().item() = 0.9890099167823792\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0005379021749831736, p.std().item() = 0.04428759589791298\n",
      "n = 'layers.0.bias', p.mean().item() = -0.04453577473759651, p.std().item() = 0.04083889722824097\n",
      "n = 'layers.2.weight', p.mean().item() = -0.007510187570005655, p.std().item() = 0.03161762282252312\n",
      "n = 'layers.2.bias', p.mean().item() = 0.04920872673392296, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 10\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -7.576629286631942e-05, p.std().item() = 0.990339457988739\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0006576450541615486, p.std().item() = 0.9890099167823792\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0005379021749831736, p.std().item() = 0.04428759589791298\n",
      "n = 'layers.0.bias', p.mean().item() = -0.04453577473759651, p.std().item() = 0.04083889722824097\n",
      "n = 'layers.2.weight', p.mean().item() = -0.007510187570005655, p.std().item() = 0.03161762282252312\n",
      "n = 'layers.2.bias', p.mean().item() = 0.04920872673392296, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -0.15574133396148682, preds.std().item() = 0.27457931637763977\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.6942, device='cuda:0'), loss.std() = tensor(0.0093, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 6.071233793480957e-11, lambda_i.std().item() = 0.26228341460227966\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = 0.0006477271672338247, p.grad.std().item() = 4.122488975524902\n",
      "n = 'label_factors.weight', p.grad.mean().item() = 0.003667257959023118, p.grad.std().item() = 0.16403594613075256\n",
      "n = 'layers.0.weight', p.grad.mean().item() = -3.4189658164978027, p.grad.std().item() = 759.8529663085938\n",
      "n = 'layers.0.bias', p.grad.mean().item() = 167.7137451171875, p.grad.std().item() = 730.7295532226562\n",
      "n = 'layers.2.weight', p.grad.mean().item() = -1735.4674072265625, p.grad.std().item() = 23047.3828125\n",
      "n = 'layers.2.bias', p.grad.mean().item() = 7.82012939453125e-05, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -7.810651004547253e-05, p.std().item() = 0.9893563985824585\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0005901889526285231, p.std().item() = 0.9880493879318237\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0005816973280161619, p.std().item() = 0.045565444976091385\n",
      "n = 'layers.0.bias', p.mean().item() = -0.047480352222919464, p.std().item() = 0.041656412184238434\n",
      "n = 'layers.2.weight', p.mean().item() = -0.008521482348442078, p.std().item() = 0.03174465894699097\n",
      "n = 'layers.2.bias', p.mean().item() = 0.04452446848154068, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 11\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -7.810651004547253e-05, p.std().item() = 0.9893563985824585\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0005901889526285231, p.std().item() = 0.9880493879318237\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0005816973280161619, p.std().item() = 0.045565444976091385\n",
      "n = 'layers.0.bias', p.mean().item() = -0.047480352222919464, p.std().item() = 0.041656412184238434\n",
      "n = 'layers.2.weight', p.mean().item() = -0.008521482348442078, p.std().item() = 0.03174465894699097\n",
      "n = 'layers.2.bias', p.mean().item() = 0.04452446848154068, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -0.1975356936454773, preds.std().item() = 0.2907639443874359\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.6979, device='cuda:0'), loss.std() = tensor(0.0090, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 6.804373731128521e-11, lambda_i.std().item() = 0.2568388879299164\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = -7.012858259258792e-05, p.grad.std().item() = 5.507950782775879\n",
      "n = 'label_factors.weight', p.grad.mean().item() = -0.0008299019536934793, p.grad.std().item() = 0.1597623974084854\n",
      "n = 'layers.0.weight', p.grad.mean().item() = 3.655200242996216, p.grad.std().item() = 898.6481323242188\n",
      "n = 'layers.0.bias', p.grad.mean().item() = 40.25695037841797, p.grad.std().item() = 498.5871887207031\n",
      "n = 'layers.2.weight', p.grad.mean().item() = -2968.966064453125, p.grad.std().item() = 18247.099609375\n",
      "n = 'layers.2.bias', p.grad.mean().item() = -0.00107574462890625, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -7.925670070108026e-05, p.std().item() = 0.9883748888969421\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0005406553391367197, p.std().item() = 0.9870822429656982\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0006115650176070631, p.std().item() = 0.04678146541118622\n",
      "n = 'layers.0.bias', p.mean().item() = -0.05015413463115692, p.std().item() = 0.04249199479818344\n",
      "n = 'layers.2.weight', p.mean().item() = -0.00927781406790018, p.std().item() = 0.03196251392364502\n",
      "n = 'layers.2.bias', p.mean().item() = 0.043563924729824066, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 12\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -7.925670070108026e-05, p.std().item() = 0.9883748888969421\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0005406553391367197, p.std().item() = 0.9870822429656982\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0006115650176070631, p.std().item() = 0.04678146541118622\n",
      "n = 'layers.0.bias', p.mean().item() = -0.05015413463115692, p.std().item() = 0.04249199479818344\n",
      "n = 'layers.2.weight', p.mean().item() = -0.00927781406790018, p.std().item() = 0.03196251392364502\n",
      "n = 'layers.2.bias', p.mean().item() = 0.043563924729824066, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -0.2372158318758011, preds.std().item() = 0.34540656208992004\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.6964, device='cuda:0'), loss.std() = tensor(0.0122, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 9.473420659045217e-11, lambda_i.std().item() = 0.23631121218204498\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = 0.00056172237964347, p.grad.std().item() = 5.3201003074646\n",
      "n = 'label_factors.weight', p.grad.mean().item() = 0.001134171150624752, p.grad.std().item() = 0.16430844366550446\n",
      "n = 'layers.0.weight', p.grad.mean().item() = 2.7274093627929688, p.grad.std().item() = 796.9984741210938\n",
      "n = 'layers.0.bias', p.grad.mean().item() = 76.27427673339844, p.grad.std().item() = 677.970703125\n",
      "n = 'layers.2.weight', p.grad.mean().item() = 6639.6328125, p.grad.std().item() = 19417.923828125\n",
      "n = 'layers.2.bias', p.grad.mean().item() = -0.00036334991455078125, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -7.950973667902872e-05, p.std().item() = 0.9873952269554138\n",
      "n = 'label_factors.weight', p.mean().item() = 0.000489116064272821, p.std().item() = 0.9861137866973877\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0006483712932094932, p.std().item() = 0.04792465642094612\n",
      "n = 'layers.0.bias', p.mean().item() = -0.052502259612083435, p.std().item() = 0.04318453371524811\n",
      "n = 'layers.2.weight', p.mean().item() = -0.010249032638967037, p.std().item() = 0.032123517245054245\n",
      "n = 'layers.2.bias', p.mean().item() = 0.04356195032596588, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 13\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -7.950973667902872e-05, p.std().item() = 0.9873952269554138\n",
      "n = 'label_factors.weight', p.mean().item() = 0.000489116064272821, p.std().item() = 0.9861137866973877\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0006483712932094932, p.std().item() = 0.04792465642094612\n",
      "n = 'layers.0.bias', p.mean().item() = -0.052502259612083435, p.std().item() = 0.04318453371524811\n",
      "n = 'layers.2.weight', p.mean().item() = -0.010249032638967037, p.std().item() = 0.032123517245054245\n",
      "n = 'layers.2.bias', p.mean().item() = 0.04356195032596588, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -0.2778472304344177, preds.std().item() = 0.36728695034980774\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.6925, device='cuda:0'), loss.std() = tensor(0.0113, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 1.1657186327340696e-10, lambda_i.std().item() = 0.3605416417121887\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = 0.0005267898086458445, p.grad.std().item() = 5.954685688018799\n",
      "n = 'label_factors.weight', p.grad.mean().item() = 0.00011499591346364468, p.grad.std().item() = 0.20413242280483246\n",
      "n = 'layers.0.weight', p.grad.mean().item() = -10.270813941955566, p.grad.std().item() = 987.6343994140625\n",
      "n = 'layers.0.bias', p.grad.mean().item() = -688.900146484375, p.grad.std().item() = 1150.1043701171875\n",
      "n = 'layers.2.weight', p.grad.mean().item() = -4042.53173828125, p.grad.std().item() = 47996.75\n",
      "n = 'layers.2.bias', p.grad.mean().item() = -0.00028896331787109375, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -7.93615254224278e-05, p.std().item() = 0.9864174723625183\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0004382231563795358, p.std().item() = 0.9851678609848022\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0006869324715808034, p.std().item() = 0.04909447580575943\n",
      "n = 'layers.0.bias', p.mean().item() = -0.05394052341580391, p.std().item() = 0.04432586207985878\n",
      "n = 'layers.2.weight', p.mean().item() = -0.011211153119802475, p.std().item() = 0.03345600888133049\n",
      "n = 'layers.2.bias', p.mean().item() = 0.04423169791698456, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 14\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -7.93615254224278e-05, p.std().item() = 0.9864174723625183\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0004382231563795358, p.std().item() = 0.9851678609848022\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0006869324715808034, p.std().item() = 0.04909447580575943\n",
      "n = 'layers.0.bias', p.mean().item() = -0.05394052341580391, p.std().item() = 0.04432586207985878\n",
      "n = 'layers.2.weight', p.mean().item() = -0.011211153119802475, p.std().item() = 0.03345600888133049\n",
      "n = 'layers.2.bias', p.mean().item() = 0.04423169791698456, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -0.32200658321380615, preds.std().item() = 0.4078938663005829\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.7113, device='cuda:0'), loss.std() = tensor(0.0150, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 5.88077364582773e-11, lambda_i.std().item() = 0.21744880080223083\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = -0.0009675712208263576, p.grad.std().item() = 7.393805027008057\n",
      "n = 'label_factors.weight', p.grad.mean().item() = -0.0004723479214590043, p.grad.std().item() = 0.2886631488800049\n",
      "n = 'layers.0.weight', p.grad.mean().item() = -3.2236416339874268, p.grad.std().item() = 977.2217407226562\n",
      "n = 'layers.0.bias', p.grad.mean().item() = 1549.031494140625, p.grad.std().item() = 1487.4178466796875\n",
      "n = 'layers.2.weight', p.grad.mean().item() = -10248.3759765625, p.grad.std().item() = 54872.1640625\n",
      "n = 'layers.2.bias', p.grad.mean().item() = 0.00034236907958984375, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -7.932635344332084e-05, p.std().item() = 0.985440731048584\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0004186993173789233, p.std().item() = 0.9842067956924438\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0007046989630907774, p.std().item() = 0.05008925125002861\n",
      "n = 'layers.0.bias', p.mean().item() = -0.05680171772837639, p.std().item() = 0.04478267952799797\n",
      "n = 'layers.2.weight', p.mean().item() = -0.011278082616627216, p.std().item() = 0.03271773084998131\n",
      "n = 'layers.2.bias', p.mean().item() = 0.04402961954474449, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 15\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -7.932635344332084e-05, p.std().item() = 0.985440731048584\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0004186993173789233, p.std().item() = 0.9842067956924438\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0007046989630907774, p.std().item() = 0.05008925125002861\n",
      "n = 'layers.0.bias', p.mean().item() = -0.05680171772837639, p.std().item() = 0.04478267952799797\n",
      "n = 'layers.2.weight', p.mean().item() = -0.011278082616627216, p.std().item() = 0.03271773084998131\n",
      "n = 'layers.2.bias', p.mean().item() = 0.04402961954474449, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -0.35183560848236084, preds.std().item() = 0.42209070920944214\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.7011, device='cuda:0'), loss.std() = tensor(0.0141, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 4.539727968544405e-11, lambda_i.std().item() = 0.24046844244003296\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = 0.00015684691607020795, p.grad.std().item() = 6.360663414001465\n",
      "n = 'label_factors.weight', p.grad.mean().item() = 0.0034086345694959164, p.grad.std().item() = 0.20368243753910065\n",
      "n = 'layers.0.weight', p.grad.mean().item() = 6.974626541137695, p.grad.std().item() = 869.7683715820312\n",
      "n = 'layers.0.bias', p.grad.mean().item() = 564.2564086914062, p.grad.std().item() = 905.4714965820312\n",
      "n = 'layers.2.weight', p.grad.mean().item() = 1892.388671875, p.grad.std().item() = 31795.046875\n",
      "n = 'layers.2.bias', p.grad.mean().item() = -0.0005950927734375, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -7.94156439951621e-05, p.std().item() = 0.9844654202461243\n",
      "n = 'label_factors.weight', p.mean().item() = 0.00038334119017235935, p.std().item() = 0.9832532405853271\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0007225206354632974, p.std().item() = 0.05104856938123703\n",
      "n = 'layers.0.bias', p.mean().item() = -0.059839680790901184, p.std().item() = 0.04496107995510101\n",
      "n = 'layers.2.weight', p.mean().item() = -0.011283604428172112, p.std().item() = 0.03147054836153984\n",
      "n = 'layers.2.bias', p.mean().item() = 0.04518416151404381, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 16\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -7.94156439951621e-05, p.std().item() = 0.9844654202461243\n",
      "n = 'label_factors.weight', p.mean().item() = 0.00038334119017235935, p.std().item() = 0.9832532405853271\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0007225206354632974, p.std().item() = 0.05104856938123703\n",
      "n = 'layers.0.bias', p.mean().item() = -0.059839680790901184, p.std().item() = 0.04496107995510101\n",
      "n = 'layers.2.weight', p.mean().item() = -0.011283604428172112, p.std().item() = 0.03147054836153984\n",
      "n = 'layers.2.bias', p.mean().item() = 0.04518416151404381, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -0.36531636118888855, preds.std().item() = 0.42687085270881653\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.6995, device='cuda:0'), loss.std() = tensor(0.0146, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 6.887863196469723e-11, lambda_i.std().item() = 0.3665689527988434\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = 0.0005639431765303016, p.grad.std().item() = 6.278127670288086\n",
      "n = 'label_factors.weight', p.grad.mean().item() = -0.001263146405108273, p.grad.std().item() = 0.2422531694173813\n",
      "n = 'layers.0.weight', p.grad.mean().item() = 11.734228134155273, p.grad.std().item() = 935.3468017578125\n",
      "n = 'layers.0.bias', p.grad.mean().item() = -636.6746826171875, p.grad.std().item() = 918.9573364257812\n",
      "n = 'layers.2.weight', p.grad.mean().item() = -16239.912109375, p.grad.std().item() = 53762.0390625\n",
      "n = 'layers.2.bias', p.grad.mean().item() = 0.000209808349609375, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -7.945272955112159e-05, p.std().item() = 0.9834915995597839\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0003568331303540617, p.std().item() = 0.9823163151741028\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0007471769349649549, p.std().item() = 0.052028633654117584\n",
      "n = 'layers.0.bias', p.mean().item() = -0.06203339621424675, p.std().item() = 0.045410238206386566\n",
      "n = 'layers.2.weight', p.mean().item() = -0.010954679921269417, p.std().item() = 0.031407278031110764\n",
      "n = 'layers.2.bias', p.mean().item() = 0.04574957117438316, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 17\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -7.945272955112159e-05, p.std().item() = 0.9834915995597839\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0003568331303540617, p.std().item() = 0.9823163151741028\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0007471769349649549, p.std().item() = 0.052028633654117584\n",
      "n = 'layers.0.bias', p.mean().item() = -0.06203339621424675, p.std().item() = 0.045410238206386566\n",
      "n = 'layers.2.weight', p.mean().item() = -0.010954679921269417, p.std().item() = 0.031407278031110764\n",
      "n = 'layers.2.bias', p.mean().item() = 0.04574957117438316, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -0.3512917757034302, preds.std().item() = 0.41389837861061096\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.7010, device='cuda:0'), loss.std() = tensor(0.0106, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 7.806244539665386e-11, lambda_i.std().item() = 0.2856822907924652\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = 2.0636432964238338e-05, p.grad.std().item() = 6.157074928283691\n",
      "n = 'label_factors.weight', p.grad.mean().item() = -0.000839817977976054, p.grad.std().item() = 0.20478945970535278\n",
      "n = 'layers.0.weight', p.grad.mean().item() = -5.4265851974487305, p.grad.std().item() = 777.1470947265625\n",
      "n = 'layers.0.bias', p.grad.mean().item() = -53.36471939086914, p.grad.std().item() = 585.3323364257812\n",
      "n = 'layers.2.weight', p.grad.mean().item() = -755.8787231445312, p.grad.std().item() = 24207.818359375\n",
      "n = 'layers.2.bias', p.grad.mean().item() = 0.0001983642578125, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -7.907901454018429e-05, p.std().item() = 0.982519268989563\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0003368536417838186, p.std().item() = 0.9813818335533142\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0007862143102101982, p.std().item() = 0.05296649411320686\n",
      "n = 'layers.0.bias', p.mean().item() = -0.06402834504842758, p.std().item() = 0.04594774544239044\n",
      "n = 'layers.2.weight', p.mean().item() = -0.010699601843953133, p.std().item() = 0.03153456747531891\n",
      "n = 'layers.2.bias', p.mean().item() = 0.045806579291820526, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 18\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -7.907901454018429e-05, p.std().item() = 0.982519268989563\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0003368536417838186, p.std().item() = 0.9813818335533142\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0007862143102101982, p.std().item() = 0.05296649411320686\n",
      "n = 'layers.0.bias', p.mean().item() = -0.06402834504842758, p.std().item() = 0.04594774544239044\n",
      "n = 'layers.2.weight', p.mean().item() = -0.010699601843953133, p.std().item() = 0.03153456747531891\n",
      "n = 'layers.2.bias', p.mean().item() = 0.045806579291820526, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -0.355001300573349, preds.std().item() = 0.4185064136981964\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.7012, device='cuda:0'), loss.std() = tensor(0.0116, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 5.1972058601190696e-11, lambda_i.std().item() = 0.31827834248542786\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = 0.0005108172190375626, p.grad.std().item() = 9.174160957336426\n",
      "n = 'label_factors.weight', p.grad.mean().item() = -0.000153714616317302, p.grad.std().item() = 0.21147160232067108\n",
      "n = 'layers.0.weight', p.grad.mean().item() = -7.137170791625977, p.grad.std().item() = 1043.2950439453125\n",
      "n = 'layers.0.bias', p.grad.mean().item() = -16.807981491088867, p.grad.std().item() = 601.5681762695312\n",
      "n = 'layers.2.weight', p.grad.mean().item() = -520.9644165039062, p.grad.std().item() = 25493.544921875\n",
      "n = 'layers.2.bias', p.grad.mean().item() = -0.0005645751953125, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -7.827774243196473e-05, p.std().item() = 0.9815481901168823\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0003200179780833423, p.std().item() = 0.9804419875144958\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0008088047034107149, p.std().item() = 0.05382091924548149\n",
      "n = 'layers.0.bias', p.mean().item() = -0.06589985638856888, p.std().item() = 0.046509094536304474\n",
      "n = 'layers.2.weight', p.mean().item() = -0.010540027171373367, p.std().item() = 0.03182962164282799\n",
      "n = 'layers.2.bias', p.mean().item() = 0.047101981937885284, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 19\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -7.827774243196473e-05, p.std().item() = 0.9815481901168823\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0003200179780833423, p.std().item() = 0.9804419875144958\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0008088047034107149, p.std().item() = 0.05382091924548149\n",
      "n = 'layers.0.bias', p.mean().item() = -0.06589985638856888, p.std().item() = 0.046509094536304474\n",
      "n = 'layers.2.weight', p.mean().item() = -0.010540027171373367, p.std().item() = 0.03182962164282799\n",
      "n = 'layers.2.bias', p.mean().item() = 0.047101981937885284, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -0.34429579973220825, preds.std().item() = 0.44507896900177\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.7004, device='cuda:0'), loss.std() = tensor(0.0137, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 5.7764123345682705e-11, lambda_i.std().item() = 0.2491500824689865\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = 0.00013562291860580444, p.grad.std().item() = 7.260527610778809\n",
      "n = 'label_factors.weight', p.grad.mean().item() = 0.00067877396941185, p.grad.std().item() = 0.23841024935245514\n",
      "n = 'layers.0.weight', p.grad.mean().item() = -26.193674087524414, p.grad.std().item() = 790.0252685546875\n",
      "n = 'layers.0.bias', p.grad.mean().item() = 448.6206970214844, p.grad.std().item() = 685.157470703125\n",
      "n = 'layers.2.weight', p.grad.mean().item() = -1338.6868896484375, p.grad.std().item() = 26541.451171875\n",
      "n = 'layers.2.bias', p.grad.mean().item() = 0.0007305145263671875, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -7.718068809481338e-05, p.std().item() = 0.980578601360321\n",
      "n = 'label_factors.weight', p.mean().item() = 0.00030326598789542913, p.std().item() = 0.9795017838478088\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0008058479870669544, p.std().item() = 0.054621994495391846\n",
      "n = 'layers.0.bias', p.mean().item() = -0.06798593699932098, p.std().item() = 0.04685985669493675\n",
      "n = 'layers.2.weight', p.mean().item() = -0.010204744525253773, p.std().item() = 0.031685564666986465\n",
      "n = 'layers.2.bias', p.mean().item() = 0.046660564839839935, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 20\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -7.718068809481338e-05, p.std().item() = 0.980578601360321\n",
      "n = 'label_factors.weight', p.mean().item() = 0.00030326598789542913, p.std().item() = 0.9795017838478088\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0008058479870669544, p.std().item() = 0.054621994495391846\n",
      "n = 'layers.0.bias', p.mean().item() = -0.06798593699932098, p.std().item() = 0.04685985669493675\n",
      "n = 'layers.2.weight', p.mean().item() = -0.010204744525253773, p.std().item() = 0.031685564666986465\n",
      "n = 'layers.2.bias', p.mean().item() = 0.046660564839839935, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -0.3439292311668396, preds.std().item() = 0.46283969283103943\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.7021, device='cuda:0'), loss.std() = tensor(0.0150, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 5.870337549396254e-11, lambda_i.std().item() = 0.2588682472705841\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = 6.782871787436306e-05, p.grad.std().item() = 6.271938800811768\n",
      "n = 'label_factors.weight', p.grad.mean().item() = -0.00035314157139509916, p.grad.std().item() = 0.23873206973075867\n",
      "n = 'layers.0.weight', p.grad.mean().item() = 10.678568840026855, p.grad.std().item() = 665.3027954101562\n",
      "n = 'layers.0.bias', p.grad.mean().item() = 314.0838317871094, p.grad.std().item() = 583.0945434570312\n",
      "n = 'layers.2.weight', p.grad.mean().item() = 1884.5745849609375, p.grad.std().item() = 20970.283203125\n",
      "n = 'layers.2.bias', p.grad.mean().item() = -0.0003299713134765625, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -7.586333231301978e-05, p.std().item() = 0.9796104431152344\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0002944444422610104, p.std().item() = 0.9785619378089905\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0008115072268992662, p.std().item() = 0.055389709770679474\n",
      "n = 'layers.0.bias', p.mean().item() = -0.07012443244457245, p.std().item() = 0.04704016447067261\n",
      "n = 'layers.2.weight', p.mean().item() = -0.009911973029375076, p.std().item() = 0.03140944242477417\n",
      "n = 'layers.2.bias', p.mean().item() = 0.046960338950157166, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 21\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -7.586333231301978e-05, p.std().item() = 0.9796104431152344\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0002944444422610104, p.std().item() = 0.9785619378089905\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0008115072268992662, p.std().item() = 0.055389709770679474\n",
      "n = 'layers.0.bias', p.mean().item() = -0.07012443244457245, p.std().item() = 0.04704016447067261\n",
      "n = 'layers.2.weight', p.mean().item() = -0.009911973029375076, p.std().item() = 0.03140944242477417\n",
      "n = 'layers.2.bias', p.mean().item() = 0.046960338950157166, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -0.33521121740341187, preds.std().item() = 0.442254900932312\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.7003, device='cuda:0'), loss.std() = tensor(0.0209, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 6.992224854673879e-11, lambda_i.std().item() = 0.2987005412578583\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = 0.00010483441292308271, p.grad.std().item() = 6.024572849273682\n",
      "n = 'label_factors.weight', p.grad.mean().item() = -0.00023342491476796567, p.grad.std().item() = 0.23395957052707672\n",
      "n = 'layers.0.weight', p.grad.mean().item() = 10.196691513061523, p.grad.std().item() = 632.9849243164062\n",
      "n = 'layers.0.bias', p.grad.mean().item() = -213.19882202148438, p.grad.std().item() = 406.73583984375\n",
      "n = 'layers.2.weight', p.grad.mean().item() = 3811.086181640625, p.grad.std().item() = 15096.2099609375\n",
      "n = 'layers.2.bias', p.grad.mean().item() = 0.000762939453125, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -7.451180135831237e-05, p.std().item() = 0.9786438345909119\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0002856570645235479, p.std().item() = 0.9776183366775513\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0008182918536476791, p.std().item() = 0.056133050471544266\n",
      "n = 'layers.0.bias', p.mean().item() = -0.07184911519289017, p.std().item() = 0.04725335165858269\n",
      "n = 'layers.2.weight', p.mean().item() = -0.009853026829659939, p.std().item() = 0.031559057533741\n",
      "n = 'layers.2.bias', p.mean().item() = 0.04565374553203583, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 22\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -7.451180135831237e-05, p.std().item() = 0.9786438345909119\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0002856570645235479, p.std().item() = 0.9776183366775513\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0008182918536476791, p.std().item() = 0.056133050471544266\n",
      "n = 'layers.0.bias', p.mean().item() = -0.07184911519289017, p.std().item() = 0.04725335165858269\n",
      "n = 'layers.2.weight', p.mean().item() = -0.009853026829659939, p.std().item() = 0.031559057533741\n",
      "n = 'layers.2.bias', p.mean().item() = 0.04565374553203583, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -0.3473590314388275, preds.std().item() = 0.47575631737709045\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.6954, device='cuda:0'), loss.std() = tensor(0.0160, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 7.086149722557167e-11, lambda_i.std().item() = 0.28954991698265076\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = 0.0004465179517865181, p.grad.std().item() = 7.126779556274414\n",
      "n = 'label_factors.weight', p.grad.mean().item() = -0.0016394852427765727, p.grad.std().item() = 0.2369024008512497\n",
      "n = 'layers.0.weight', p.grad.mean().item() = 22.499658584594727, p.grad.std().item() = 725.8029174804688\n",
      "n = 'layers.0.bias', p.grad.mean().item() = -188.12237548828125, p.grad.std().item() = 442.9888916015625\n",
      "n = 'layers.2.weight', p.grad.mean().item() = -1168.343994140625, p.grad.std().item() = 25955.1484375\n",
      "n = 'layers.2.bias', p.grad.mean().item() = -0.00032901763916015625, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -7.327827188419178e-05, p.std().item() = 0.9776787161827087\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0002863715053535998, p.std().item() = 0.9766817092895508\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0008352842996828258, p.std().item() = 0.0568571612238884\n",
      "n = 'layers.0.bias', p.mean().item() = -0.07324980199337006, p.std().item() = 0.04764034226536751\n",
      "n = 'layers.2.weight', p.mean().item() = -0.009794547222554684, p.std().item() = 0.03205125778913498\n",
      "n = 'layers.2.bias', p.mean().item() = 0.04514489695429802, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 23\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -7.327827188419178e-05, p.std().item() = 0.9776787161827087\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0002863715053535998, p.std().item() = 0.9766817092895508\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0008352842996828258, p.std().item() = 0.0568571612238884\n",
      "n = 'layers.0.bias', p.mean().item() = -0.07324980199337006, p.std().item() = 0.04764034226536751\n",
      "n = 'layers.2.weight', p.mean().item() = -0.009794547222554684, p.std().item() = 0.03205125778913498\n",
      "n = 'layers.2.bias', p.mean().item() = 0.04514489695429802, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -0.397697389125824, preds.std().item() = 0.4927162826061249\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.6934, device='cuda:0'), loss.std() = tensor(0.0157, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 1.230422846942858e-10, lambda_i.std().item() = 0.30901411175727844\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = -0.0003431496152188629, p.grad.std().item() = 8.44312858581543\n",
      "n = 'label_factors.weight', p.grad.mean().item() = 0.0009051436791196465, p.grad.std().item() = 0.25744467973709106\n",
      "n = 'layers.0.weight', p.grad.mean().item() = -1.165462613105774, p.grad.std().item() = 866.8766479492188\n",
      "n = 'layers.0.bias', p.grad.mean().item() = -488.7823181152344, p.grad.std().item() = 840.372802734375\n",
      "n = 'layers.2.weight', p.grad.mean().item() = -10495.291015625, p.grad.std().item() = 34770.4609375\n",
      "n = 'layers.2.bias', p.grad.mean().item() = -0.0004119873046875, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -7.230104529298842e-05, p.std().item() = 0.9767153263092041\n",
      "n = 'label_factors.weight', p.mean().item() = 0.00028051144909113646, p.std().item() = 0.9757444858551025\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0008375694160349667, p.std().item() = 0.05755593627691269\n",
      "n = 'layers.0.bias', p.mean().item() = -0.07416406273841858, p.std().item() = 0.04833467677235603\n",
      "n = 'layers.2.weight', p.mean().item() = -0.009522169828414917, p.std().item() = 0.03303643316030502\n",
      "n = 'layers.2.bias', p.mean().item() = 0.0455278716981411, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 24\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -7.230104529298842e-05, p.std().item() = 0.9767153263092041\n",
      "n = 'label_factors.weight', p.mean().item() = 0.00028051144909113646, p.std().item() = 0.9757444858551025\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0008375694160349667, p.std().item() = 0.05755593627691269\n",
      "n = 'layers.0.bias', p.mean().item() = -0.07416406273841858, p.std().item() = 0.04833467677235603\n",
      "n = 'layers.2.weight', p.mean().item() = -0.009522169828414917, p.std().item() = 0.03303643316030502\n",
      "n = 'layers.2.bias', p.mean().item() = 0.0455278716981411, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -0.3497125506401062, preds.std().item() = 0.5095140337944031\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.7060, device='cuda:0'), loss.std() = tensor(0.0146, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 1.854504882992103e-10, lambda_i.std().item() = 0.4196665585041046\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = 0.0006816792883910239, p.grad.std().item() = 14.023271560668945\n",
      "n = 'label_factors.weight', p.grad.mean().item() = 0.0021557752043008804, p.grad.std().item() = 0.31635358929634094\n",
      "n = 'layers.0.weight', p.grad.mean().item() = 6.522756099700928, p.grad.std().item() = 1482.489013671875\n",
      "n = 'layers.0.bias', p.grad.mean().item() = -1025.3240966796875, p.grad.std().item() = 1515.3707275390625\n",
      "n = 'layers.2.weight', p.grad.mean().item() = -6867.03466796875, p.grad.std().item() = 64794.984375\n",
      "n = 'layers.2.bias', p.grad.mean().item() = -0.00045680999755859375, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -7.122056558728218e-05, p.std().item() = 0.9757535457611084\n",
      "n = 'label_factors.weight', p.mean().item() = 0.00024995519197545946, p.std().item() = 0.9748157262802124\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0008494149660691619, p.std().item() = 0.05826873704791069\n",
      "n = 'layers.0.bias', p.mean().item() = -0.07417362928390503, p.std().item() = 0.049458906054496765\n",
      "n = 'layers.2.weight', p.mean().item() = -0.009451424703001976, p.std().item() = 0.035210467875003815\n",
      "n = 'layers.2.bias', p.mean().item() = 0.04679949954152107, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 25\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -7.122056558728218e-05, p.std().item() = 0.9757535457611084\n",
      "n = 'label_factors.weight', p.mean().item() = 0.00024995519197545946, p.std().item() = 0.9748157262802124\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0008494149660691619, p.std().item() = 0.05826873704791069\n",
      "n = 'layers.0.bias', p.mean().item() = -0.07417362928390503, p.std().item() = 0.049458906054496765\n",
      "n = 'layers.2.weight', p.mean().item() = -0.009451424703001976, p.std().item() = 0.035210467875003815\n",
      "n = 'layers.2.bias', p.mean().item() = 0.04679949954152107, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -0.34748131036758423, preds.std().item() = 0.5572555065155029\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.7063, device='cuda:0'), loss.std() = tensor(0.0177, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 1.0404847816669616e-10, lambda_i.std().item() = 0.2313310205936432\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = -0.0005991440266370773, p.grad.std().item() = 10.226112365722656\n",
      "n = 'label_factors.weight', p.grad.mean().item() = -0.001586642931215465, p.grad.std().item() = 0.287002295255661\n",
      "n = 'layers.0.weight', p.grad.mean().item() = 18.495882034301758, p.grad.std().item() = 933.7076416015625\n",
      "n = 'layers.0.bias', p.grad.mean().item() = 751.821533203125, p.grad.std().item() = 1027.4312744140625\n",
      "n = 'layers.2.weight', p.grad.mean().item() = 6690.36279296875, p.grad.std().item() = 41525.609375\n",
      "n = 'layers.2.bias', p.grad.mean().item() = 0.00033855438232421875, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -6.95353519404307e-05, p.std().item() = 0.9747929573059082\n",
      "n = 'label_factors.weight', p.mean().item() = 0.00023523015261162072, p.std().item() = 0.9738810062408447\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0009023030870594084, p.std().item() = 0.058918170630931854\n",
      "n = 'layers.0.bias', p.mean().item() = -0.0748019739985466, p.std().item() = 0.050234586000442505\n",
      "n = 'layers.2.weight', p.mean().item() = -0.009363013319671154, p.std().item() = 0.03641073778271675\n",
      "n = 'layers.2.bias', p.mean().item() = 0.047248806804418564, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 26\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -6.95353519404307e-05, p.std().item() = 0.9747929573059082\n",
      "n = 'label_factors.weight', p.mean().item() = 0.00023523015261162072, p.std().item() = 0.9738810062408447\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0009023030870594084, p.std().item() = 0.058918170630931854\n",
      "n = 'layers.0.bias', p.mean().item() = -0.0748019739985466, p.std().item() = 0.050234586000442505\n",
      "n = 'layers.2.weight', p.mean().item() = -0.009363013319671154, p.std().item() = 0.03641073778271675\n",
      "n = 'layers.2.bias', p.mean().item() = 0.047248806804418564, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -0.3626441955566406, preds.std().item() = 0.6041624546051025\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.7017, device='cuda:0'), loss.std() = tensor(0.0242, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 9.027274761042037e-11, lambda_i.std().item() = 0.2284136414527893\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = -0.00034673724439926445, p.grad.std().item() = 10.347517013549805\n",
      "n = 'label_factors.weight', p.grad.mean().item() = -0.004400488920509815, p.grad.std().item() = 0.31422269344329834\n",
      "n = 'layers.0.weight', p.grad.mean().item() = -7.105541229248047, p.grad.std().item() = 887.264892578125\n",
      "n = 'layers.0.bias', p.grad.mean().item() = 754.4243774414062, p.grad.std().item() = 949.7815551757812\n",
      "n = 'layers.2.weight', p.grad.mean().item() = -5238.56982421875, p.grad.std().item() = 44093.34375\n",
      "n = 'layers.2.bias', p.grad.mean().item() = 0.00016021728515625, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -6.72092501190491e-05, p.std().item() = 0.9738330841064453\n",
      "n = 'label_factors.weight', p.mean().item() = 0.00025084614753723145, p.std().item() = 0.9729426503181458\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0009568384848535061, p.std().item() = 0.059508971869945526\n",
      "n = 'layers.0.bias', p.mean().item() = -0.07598714530467987, p.std().item() = 0.05060951039195061\n",
      "n = 'layers.2.weight', p.mean().item() = -0.008892232552170753, p.std().item() = 0.03671935573220253\n",
      "n = 'layers.2.bias', p.mean().item() = 0.0473206490278244, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 27\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -6.72092501190491e-05, p.std().item() = 0.9738330841064453\n",
      "n = 'label_factors.weight', p.mean().item() = 0.00025084614753723145, p.std().item() = 0.9729426503181458\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0009568384848535061, p.std().item() = 0.059508971869945526\n",
      "n = 'layers.0.bias', p.mean().item() = -0.07598714530467987, p.std().item() = 0.05060951039195061\n",
      "n = 'layers.2.weight', p.mean().item() = -0.008892232552170753, p.std().item() = 0.03671935573220253\n",
      "n = 'layers.2.bias', p.mean().item() = 0.0473206490278244, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -0.3545205891132355, preds.std().item() = 0.6566570997238159\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.6990, device='cuda:0'), loss.std() = tensor(0.0229, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 8.943785989590225e-11, lambda_i.std().item() = 0.3046964406967163\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = 0.0004087870183866471, p.grad.std().item() = 9.551858901977539\n",
      "n = 'label_factors.weight', p.grad.mean().item() = 0.0011071583721786737, p.grad.std().item() = 0.2737961709499359\n",
      "n = 'layers.0.weight', p.grad.mean().item() = -18.966970443725586, p.grad.std().item() = 907.970458984375\n",
      "n = 'layers.0.bias', p.grad.mean().item() = -217.54556274414062, p.grad.std().item() = 538.1668090820312\n",
      "n = 'layers.2.weight', p.grad.mean().item() = -4301.3134765625, p.grad.std().item() = 37247.99609375\n",
      "n = 'layers.2.bias', p.grad.mean().item() = -0.0001468658447265625, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -6.425623723771423e-05, p.std().item() = 0.9728744626045227\n",
      "n = 'label_factors.weight', p.mean().item() = 0.00025912400451488793, p.std().item() = 0.9720150232315063\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0010083052329719067, p.std().item() = 0.0601273775100708\n",
      "n = 'layers.0.bias', p.mean().item() = -0.07687433809041977, p.std().item() = 0.051019106060266495\n",
      "n = 'layers.2.weight', p.mean().item() = -0.008493210189044476, p.std().item() = 0.03746674582362175\n",
      "n = 'layers.2.bias', p.mean().item() = 0.047690775245428085, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 28\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -6.425623723771423e-05, p.std().item() = 0.9728744626045227\n",
      "n = 'label_factors.weight', p.mean().item() = 0.00025912400451488793, p.std().item() = 0.9720150232315063\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0010083052329719067, p.std().item() = 0.0601273775100708\n",
      "n = 'layers.0.bias', p.mean().item() = -0.07687433809041977, p.std().item() = 0.051019106060266495\n",
      "n = 'layers.2.weight', p.mean().item() = -0.008493210189044476, p.std().item() = 0.03746674582362175\n",
      "n = 'layers.2.bias', p.mean().item() = 0.047690775245428085, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -0.41497376561164856, preds.std().item() = 0.6590840220451355\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.6994, device='cuda:0'), loss.std() = tensor(0.0230, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 6.971351967921535e-11, lambda_i.std().item() = 0.21960295736789703\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = -0.0005325316451489925, p.grad.std().item() = 9.960606575012207\n",
      "n = 'label_factors.weight', p.grad.mean().item() = -0.0077864034101367, p.grad.std().item() = 0.3204253017902374\n",
      "n = 'layers.0.weight', p.grad.mean().item() = -18.54047203063965, p.grad.std().item() = 915.1238403320312\n",
      "n = 'layers.0.bias', p.grad.mean().item() = 489.6598205566406, p.grad.std().item() = 817.4848022460938\n",
      "n = 'layers.2.weight', p.grad.mean().item() = 4565.15576171875, p.grad.std().item() = 25357.56640625\n",
      "n = 'layers.2.bias', p.grad.mean().item() = -0.00021457672119140625, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -6.107668014010414e-05, p.std().item() = 0.9719170331954956\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0003094192361459136, p.std().item() = 0.9710880517959595\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0010279995622113347, p.std().item() = 0.06069847196340561\n",
      "n = 'layers.0.bias', p.mean().item() = -0.07816002517938614, p.std().item() = 0.05144021660089493\n",
      "n = 'layers.2.weight', p.mean().item() = -0.008168683387339115, p.std().item() = 0.037830423563718796\n",
      "n = 'layers.2.bias', p.mean().item() = 0.04847880080342293, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 29\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -6.107668014010414e-05, p.std().item() = 0.9719170331954956\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0003094192361459136, p.std().item() = 0.9710880517959595\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0010279995622113347, p.std().item() = 0.06069847196340561\n",
      "n = 'layers.0.bias', p.mean().item() = -0.07816002517938614, p.std().item() = 0.05144021660089493\n",
      "n = 'layers.2.weight', p.mean().item() = -0.008168683387339115, p.std().item() = 0.037830423563718796\n",
      "n = 'layers.2.bias', p.mean().item() = 0.04847880080342293, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -0.3563353717327118, preds.std().item() = 0.668209433555603\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.7130, device='cuda:0'), loss.std() = tensor(0.0200, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 7.461851275758491e-11, lambda_i.std().item() = 0.22393181920051575\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = -0.0006500882445834577, p.grad.std().item() = 11.410815238952637\n",
      "n = 'label_factors.weight', p.grad.mean().item() = -0.013406905345618725, p.grad.std().item() = 0.33818891644477844\n",
      "n = 'layers.0.weight', p.grad.mean().item() = 5.345865726470947, p.grad.std().item() = 940.920654296875\n",
      "n = 'layers.0.bias', p.grad.mean().item() = 901.743896484375, p.grad.std().item() = 1200.7720947265625\n",
      "n = 'layers.2.weight', p.grad.mean().item() = -4555.46826171875, p.grad.std().item() = 41160.84765625\n",
      "n = 'layers.2.bias', p.grad.mean().item() = -0.000598907470703125, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -5.770434654550627e-05, p.std().item() = 0.9709605574607849\n",
      "n = 'label_factors.weight', p.mean().item() = 0.00042027237941510975, p.std().item() = 0.9701593518257141\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0010463473154231906, p.std().item() = 0.06120927631855011\n",
      "n = 'layers.0.bias', p.mean().item() = -0.08005402982234955, p.std().item() = 0.05172063037753105\n",
      "n = 'layers.2.weight', p.mean().item() = -0.007456160616129637, p.std().item() = 0.03745713829994202\n",
      "n = 'layers.2.bias', p.mean().item() = 0.050425563007593155, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 30\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -5.770434654550627e-05, p.std().item() = 0.9709605574607849\n",
      "n = 'label_factors.weight', p.mean().item() = 0.00042027237941510975, p.std().item() = 0.9701593518257141\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0010463473154231906, p.std().item() = 0.06120927631855011\n",
      "n = 'layers.0.bias', p.mean().item() = -0.08005402982234955, p.std().item() = 0.05172063037753105\n",
      "n = 'layers.2.weight', p.mean().item() = -0.007456160616129637, p.std().item() = 0.03745713829994202\n",
      "n = 'layers.2.bias', p.mean().item() = 0.050425563007593155, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -0.35590094327926636, preds.std().item() = 0.6491313576698303\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.7001, device='cuda:0'), loss.std() = tensor(0.0182, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 8.176728044650972e-11, lambda_i.std().item() = 0.25124573707580566\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = -0.000435581459896639, p.grad.std().item() = 10.365165710449219\n",
      "n = 'label_factors.weight', p.grad.mean().item() = -0.000664934457745403, p.grad.std().item() = 0.3239499032497406\n",
      "n = 'layers.0.weight', p.grad.mean().item() = -32.45053482055664, p.grad.std().item() = 907.1541137695312\n",
      "n = 'layers.0.bias', p.grad.mean().item() = 9.510246276855469, p.grad.std().item() = 449.65673828125\n",
      "n = 'layers.2.weight', p.grad.mean().item() = -3979.10400390625, p.grad.std().item() = 29126.974609375\n",
      "n = 'layers.2.bias', p.grad.mean().item() = 8.392333984375e-05, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -5.44068607268855e-05, p.std().item() = 0.9700055718421936\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0005246132495813072, p.std().item() = 0.9692208170890808\n",
      "n = 'layers.0.weight', p.mean().item() = -0.001036784378811717, p.std().item() = 0.06169399619102478\n",
      "n = 'layers.0.bias', p.mean().item() = -0.08178993314504623, p.std().item() = 0.052077796310186386\n",
      "n = 'layers.2.weight', p.mean().item() = -0.006788718048483133, p.std().item() = 0.03741860017180443\n",
      "n = 'layers.2.bias', p.mean().item() = 0.05201740935444832, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 31\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -5.44068607268855e-05, p.std().item() = 0.9700055718421936\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0005246132495813072, p.std().item() = 0.9692208170890808\n",
      "n = 'layers.0.weight', p.mean().item() = -0.001036784378811717, p.std().item() = 0.06169399619102478\n",
      "n = 'layers.0.bias', p.mean().item() = -0.08178993314504623, p.std().item() = 0.052077796310186386\n",
      "n = 'layers.2.weight', p.mean().item() = -0.006788718048483133, p.std().item() = 0.03741860017180443\n",
      "n = 'layers.2.bias', p.mean().item() = 0.05201740935444832, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -0.3457132875919342, preds.std().item() = 0.6718911528587341\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.7017, device='cuda:0'), loss.std() = tensor(0.0196, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 1.0227432789555735e-10, lambda_i.std().item() = 0.2530584931373596\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = 0.00027104030596092343, p.grad.std().item() = 10.861027717590332\n",
      "n = 'label_factors.weight', p.grad.mean().item() = -0.0023902347311377525, p.grad.std().item() = 0.326768159866333\n",
      "n = 'layers.0.weight', p.grad.mean().item() = -4.396446704864502, p.grad.std().item() = 951.9741821289062\n",
      "n = 'layers.0.bias', p.grad.mean().item() = 31.970138549804688, p.grad.std().item() = 385.665283203125\n",
      "n = 'layers.2.weight', p.grad.mean().item() = -10727.7373046875, p.grad.std().item() = 28102.767578125\n",
      "n = 'layers.2.bias', p.grad.mean().item() = -5.7220458984375e-05, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -5.1315513701410964e-05, p.std().item() = 0.9690515398979187\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0006272460450418293, p.std().item() = 0.9682899713516235\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0010269035119563341, p.std().item() = 0.062186844646930695\n",
      "n = 'layers.0.bias', p.mean().item() = -0.08340487629175186, p.std().item() = 0.05246853455901146\n",
      "n = 'layers.2.weight', p.mean().item() = -0.005932847037911415, p.std().item() = 0.03764168545603752\n",
      "n = 'layers.2.bias', p.mean().item() = 0.053587328642606735, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 32\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -5.1315513701410964e-05, p.std().item() = 0.9690515398979187\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0006272460450418293, p.std().item() = 0.9682899713516235\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0010269035119563341, p.std().item() = 0.062186844646930695\n",
      "n = 'layers.0.bias', p.mean().item() = -0.08340487629175186, p.std().item() = 0.05246853455901146\n",
      "n = 'layers.2.weight', p.mean().item() = -0.005932847037911415, p.std().item() = 0.03764168545603752\n",
      "n = 'layers.2.bias', p.mean().item() = 0.053587328642606735, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -0.2919483482837677, preds.std().item() = 0.6492997407913208\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.7033, device='cuda:0'), loss.std() = tensor(0.0170, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 9.272524414960515e-11, lambda_i.std().item() = 0.23763763904571533\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = 0.00023348983086179942, p.grad.std().item() = 11.414762496948242\n",
      "n = 'label_factors.weight', p.grad.mean().item() = -0.007769303862005472, p.grad.std().item() = 0.3542114794254303\n",
      "n = 'layers.0.weight', p.grad.mean().item() = 16.066009521484375, p.grad.std().item() = 954.3826904296875\n",
      "n = 'layers.0.bias', p.grad.mean().item() = 778.9165649414062, p.grad.std().item() = 1244.322021484375\n",
      "n = 'layers.2.weight', p.grad.mean().item() = 5972.21728515625, p.grad.std().item() = 41017.78125\n",
      "n = 'layers.2.bias', p.grad.mean().item() = -0.0004749298095703125, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -4.8634232371114194e-05, p.std().item() = 0.9680982828140259\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0007664235308766365, p.std().item() = 0.9673562049865723\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0010235083755105734, p.std().item() = 0.06263093650341034\n",
      "n = 'layers.0.bias', p.mean().item() = -0.08540008217096329, p.std().item() = 0.05263226106762886\n",
      "n = 'layers.2.weight', p.mean().item() = -0.005114892031997442, p.std().item() = 0.03724212571978569\n",
      "n = 'layers.2.bias', p.mean().item() = 0.05600636079907417, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 33\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -4.8634232371114194e-05, p.std().item() = 0.9680982828140259\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0007664235308766365, p.std().item() = 0.9673562049865723\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0010235083755105734, p.std().item() = 0.06263093650341034\n",
      "n = 'layers.0.bias', p.mean().item() = -0.08540008217096329, p.std().item() = 0.05263226106762886\n",
      "n = 'layers.2.weight', p.mean().item() = -0.005114892031997442, p.std().item() = 0.03724212571978569\n",
      "n = 'layers.2.bias', p.mean().item() = 0.05600636079907417, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -0.24617236852645874, preds.std().item() = 0.6064730882644653\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.6932, device='cuda:0'), loss.std() = tensor(0.0203, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 1.1965052559848033e-10, lambda_i.std().item() = 0.2921978831291199\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = 0.0007973610190674663, p.grad.std().item() = 11.37447738647461\n",
      "n = 'label_factors.weight', p.grad.mean().item() = 0.0015573137206956744, p.grad.std().item() = 0.32877784967422485\n",
      "n = 'layers.0.weight', p.grad.mean().item() = -3.8402647972106934, p.grad.std().item() = 1057.69580078125\n",
      "n = 'layers.0.bias', p.grad.mean().item() = -287.0850524902344, p.grad.std().item() = 817.408203125\n",
      "n = 'layers.2.weight', p.grad.mean().item() = 3109.4912109375, p.grad.std().item() = 35546.37109375\n",
      "n = 'layers.2.bias', p.grad.mean().item() = 0.00026702880859375, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -4.66559395135846e-05, p.std().item() = 0.967146098613739\n",
      "n = 'label_factors.weight', p.mean().item() = 0.000875583034940064, p.std().item() = 0.9664195775985718\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0010098381899297237, p.std().item() = 0.06308238208293915\n",
      "n = 'layers.0.bias', p.mean().item() = -0.08696062117815018, p.std().item() = 0.05280420556664467\n",
      "n = 'layers.2.weight', p.mean().item() = -0.004643806256353855, p.std().item() = 0.03737003728747368\n",
      "n = 'layers.2.bias', p.mean().item() = 0.057617079466581345, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 34\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -4.66559395135846e-05, p.std().item() = 0.967146098613739\n",
      "n = 'label_factors.weight', p.mean().item() = 0.000875583034940064, p.std().item() = 0.9664195775985718\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0010098381899297237, p.std().item() = 0.06308238208293915\n",
      "n = 'layers.0.bias', p.mean().item() = -0.08696062117815018, p.std().item() = 0.05280420556664467\n",
      "n = 'layers.2.weight', p.mean().item() = -0.004643806256353855, p.std().item() = 0.03737003728747368\n",
      "n = 'layers.2.bias', p.mean().item() = 0.057617079466581345, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -0.1760755330324173, preds.std().item() = 0.5868532657623291\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.6919, device='cuda:0'), loss.std() = tensor(0.0211, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 6.820027875775736e-11, lambda_i.std().item() = 0.21389509737491608\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = 0.0024481266736984253, p.grad.std().item() = 10.09195613861084\n",
      "n = 'label_factors.weight', p.grad.mean().item() = -0.009931216947734356, p.grad.std().item() = 0.3375096917152405\n",
      "n = 'layers.0.weight', p.grad.mean().item() = -0.8532443046569824, p.grad.std().item() = 888.63671875\n",
      "n = 'layers.0.bias', p.grad.mean().item() = 607.7888793945312, p.grad.std().item() = 1355.3624267578125\n",
      "n = 'layers.2.weight', p.grad.mean().item() = 13249.859375, p.grad.std().item() = 44057.609375\n",
      "n = 'layers.2.bias', p.grad.mean().item() = -0.00020599365234375, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -4.542741226032376e-05, p.std().item() = 0.9661946296691895\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0010304099414497614, p.std().item() = 0.9655033349990845\n",
      "n = 'layers.0.weight', p.mean().item() = -0.000995623180642724, p.std().item() = 0.06353197246789932\n",
      "n = 'layers.0.bias', p.mean().item() = -0.08874174952507019, p.std().item() = 0.053002383559942245\n",
      "n = 'layers.2.weight', p.mean().item() = -0.0043660420924425125, p.std().item() = 0.0370715893805027\n",
      "n = 'layers.2.bias', p.mean().item() = 0.059525735676288605, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 35\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -4.542741226032376e-05, p.std().item() = 0.9661946296691895\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0010304099414497614, p.std().item() = 0.9655033349990845\n",
      "n = 'layers.0.weight', p.mean().item() = -0.000995623180642724, p.std().item() = 0.06353197246789932\n",
      "n = 'layers.0.bias', p.mean().item() = -0.08874174952507019, p.std().item() = 0.053002383559942245\n",
      "n = 'layers.2.weight', p.mean().item() = -0.0043660420924425125, p.std().item() = 0.0370715893805027\n",
      "n = 'layers.2.bias', p.mean().item() = 0.059525735676288605, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -0.2230525016784668, preds.std().item() = 0.6088410019874573\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.7017, device='cuda:0'), loss.std() = tensor(0.0213, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 1.1177123115935217e-10, lambda_i.std().item() = 0.2997035086154938\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = 0.001655520056374371, p.grad.std().item() = 11.725717544555664\n",
      "n = 'label_factors.weight', p.grad.mean().item() = 0.004458975978195667, p.grad.std().item() = 0.38722413778305054\n",
      "n = 'layers.0.weight', p.grad.mean().item() = -3.8748350143432617, p.grad.std().item() = 1029.355224609375\n",
      "n = 'layers.0.bias', p.grad.mean().item() = -501.9792175292969, p.grad.std().item() = 939.34912109375\n",
      "n = 'layers.2.weight', p.grad.mean().item() = -5198.36328125, p.grad.std().item() = 38626.96484375\n",
      "n = 'layers.2.bias', p.grad.mean().item() = 0.000270843505859375, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -4.5265795051818714e-05, p.std().item() = 0.9652442932128906\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0011392120504751801, p.std().item() = 0.9645870327949524\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0010064842645078897, p.std().item() = 0.06400086730718613\n",
      "n = 'layers.0.bias', p.mean().item() = -0.08996238559484482, p.std().item() = 0.05342729017138481\n",
      "n = 'layers.2.weight', p.mean().item() = -0.0041267829947173595, p.std().item() = 0.037538543343544006\n",
      "n = 'layers.2.bias', p.mean().item() = 0.060653410851955414, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 36\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -4.5265795051818714e-05, p.std().item() = 0.9652442932128906\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0011392120504751801, p.std().item() = 0.9645870327949524\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0010064842645078897, p.std().item() = 0.06400086730718613\n",
      "n = 'layers.0.bias', p.mean().item() = -0.08996238559484482, p.std().item() = 0.05342729017138481\n",
      "n = 'layers.2.weight', p.mean().item() = -0.0041267829947173595, p.std().item() = 0.037538543343544006\n",
      "n = 'layers.2.bias', p.mean().item() = 0.060653410851955414, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -0.25030431151390076, preds.std().item() = 0.6182973384857178\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.6908, device='cuda:0'), loss.std() = tensor(0.0178, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 1.0410065864885354e-10, lambda_i.std().item() = 0.2401471585035324\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = 0.0006278402870520949, p.grad.std().item() = 10.415088653564453\n",
      "n = 'label_factors.weight', p.grad.mean().item() = 0.0031096595339477062, p.grad.std().item() = 0.34593814611434937\n",
      "n = 'layers.0.weight', p.grad.mean().item() = 3.48330020904541, p.grad.std().item() = 942.6389770507812\n",
      "n = 'layers.0.bias', p.grad.mean().item() = -160.21961975097656, p.grad.std().item() = 629.2247924804688\n",
      "n = 'layers.2.weight', p.grad.mean().item() = 14389.08203125, p.grad.std().item() = 27751.671875\n",
      "n = 'layers.2.bias', p.grad.mean().item() = 6.866455078125e-05, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -4.6246823330875486e-05, p.std().item() = 0.9642952680587769\n",
      "n = 'label_factors.weight', p.mean().item() = 0.001216383883729577, p.std().item() = 0.9636850357055664\n",
      "n = 'layers.0.weight', p.mean().item() = -0.001016397261992097, p.std().item() = 0.0644838884472847\n",
      "n = 'layers.0.bias', p.mean().item() = -0.09097427129745483, p.std().item() = 0.05400577932596207\n",
      "n = 'layers.2.weight', p.mean().item() = -0.004371323622763157, p.std().item() = 0.038250550627708435\n",
      "n = 'layers.2.bias', p.mean().item() = 0.061522264033555984, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 37\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -4.6246823330875486e-05, p.std().item() = 0.9642952680587769\n",
      "n = 'label_factors.weight', p.mean().item() = 0.001216383883729577, p.std().item() = 0.9636850357055664\n",
      "n = 'layers.0.weight', p.mean().item() = -0.001016397261992097, p.std().item() = 0.0644838884472847\n",
      "n = 'layers.0.bias', p.mean().item() = -0.09097427129745483, p.std().item() = 0.05400577932596207\n",
      "n = 'layers.2.weight', p.mean().item() = -0.004371323622763157, p.std().item() = 0.038250550627708435\n",
      "n = 'layers.2.bias', p.mean().item() = 0.061522264033555984, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -0.19618773460388184, preds.std().item() = 0.6593761444091797\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.6927, device='cuda:0'), loss.std() = tensor(0.0177, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 1.095796370309543e-10, lambda_i.std().item() = 0.2551385760307312\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = 0.0006830081692896783, p.grad.std().item() = 11.424084663391113\n",
      "n = 'label_factors.weight', p.grad.mean().item() = 0.0035990984179079533, p.grad.std().item() = 0.3997037410736084\n",
      "n = 'layers.0.weight', p.grad.mean().item() = -1.935042381286621, p.grad.std().item() = 997.8712768554688\n",
      "n = 'layers.0.bias', p.grad.mean().item() = -296.0547180175781, p.grad.std().item() = 650.4102783203125\n",
      "n = 'layers.2.weight', p.grad.mean().item() = 10617.7568359375, p.grad.std().item() = 24151.3671875\n",
      "n = 'layers.2.bias', p.grad.mean().item() = -0.00035190582275390625, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -4.861339402850717e-05, p.std().item() = 0.9633472561836243\n",
      "n = 'label_factors.weight', p.mean().item() = 0.001264382852241397, p.std().item() = 0.9627877473831177\n",
      "n = 'layers.0.weight', p.mean().item() = -0.001038421643897891, p.std().item() = 0.06498481333255768\n",
      "n = 'layers.0.bias', p.mean().item() = -0.09167661517858505, p.std().item() = 0.054677776992321014\n",
      "n = 'layers.2.weight', p.mean().item() = -0.005059090908616781, p.std().item() = 0.0393298976123333\n",
      "n = 'layers.2.bias', p.mean().item() = 0.06308930367231369, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 38\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -4.861339402850717e-05, p.std().item() = 0.9633472561836243\n",
      "n = 'label_factors.weight', p.mean().item() = 0.001264382852241397, p.std().item() = 0.9627877473831177\n",
      "n = 'layers.0.weight', p.mean().item() = -0.001038421643897891, p.std().item() = 0.06498481333255768\n",
      "n = 'layers.0.bias', p.mean().item() = -0.09167661517858505, p.std().item() = 0.054677776992321014\n",
      "n = 'layers.2.weight', p.mean().item() = -0.005059090908616781, p.std().item() = 0.0393298976123333\n",
      "n = 'layers.2.bias', p.mean().item() = 0.06308930367231369, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -0.2955600321292877, preds.std().item() = 0.7013367414474487\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.7020, device='cuda:0'), loss.std() = tensor(0.0177, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 2.75514524389342e-11, lambda_i.std().item() = 0.2136775702238083\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = 0.0018994183046743274, p.grad.std().item() = 13.558540344238281\n",
      "n = 'label_factors.weight', p.grad.mean().item() = -0.015811553224921227, p.grad.std().item() = 0.4107781946659088\n",
      "n = 'layers.0.weight', p.grad.mean().item() = 4.065276622772217, p.grad.std().item() = 1152.69140625\n",
      "n = 'layers.0.bias', p.grad.mean().item() = 699.3985595703125, p.grad.std().item() = 1327.582763671875\n",
      "n = 'layers.2.weight', p.grad.mean().item() = 8608.626953125, p.grad.std().item() = 48471.55078125\n",
      "n = 'layers.2.bias', p.grad.mean().item() = -0.0001773834228515625, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -5.223457992542535e-05, p.std().item() = 0.9624001383781433\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0013844132190570235, p.std().item() = 0.9619036316871643\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0010601207613945007, p.std().item() = 0.06547669321298599\n",
      "n = 'layers.0.bias', p.mean().item() = -0.0927819013595581, p.std().item() = 0.05529047176241875\n",
      "n = 'layers.2.weight', p.mean().item() = -0.005735606886446476, p.std().item() = 0.03972921893000603\n",
      "n = 'layers.2.bias', p.mean().item() = 0.06491010636091232, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 39\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -5.223457992542535e-05, p.std().item() = 0.9624001383781433\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0013844132190570235, p.std().item() = 0.9619036316871643\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0010601207613945007, p.std().item() = 0.06547669321298599\n",
      "n = 'layers.0.bias', p.mean().item() = -0.0927819013595581, p.std().item() = 0.05529047176241875\n",
      "n = 'layers.2.weight', p.mean().item() = -0.005735606886446476, p.std().item() = 0.03972921893000603\n",
      "n = 'layers.2.bias', p.mean().item() = 0.06491010636091232, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -0.33267509937286377, preds.std().item() = 0.7067320942878723\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.6837, device='cuda:0'), loss.std() = tensor(0.0239, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 9.058583744225857e-11, lambda_i.std().item() = 0.22459259629249573\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = 0.00120804482139647, p.grad.std().item() = 9.801721572875977\n",
      "n = 'label_factors.weight', p.grad.mean().item() = -0.0060500591062009335, p.grad.std().item() = 0.4748227596282959\n",
      "n = 'layers.0.weight', p.grad.mean().item() = 13.489574432373047, p.grad.std().item() = 758.83349609375\n",
      "n = 'layers.0.bias', p.grad.mean().item() = 260.1484375, p.grad.std().item() = 671.9388427734375\n",
      "n = 'layers.2.weight', p.grad.mean().item() = 6634.3798828125, p.grad.std().item() = 23393.576171875\n",
      "n = 'layers.2.bias', p.grad.mean().item() = 2.86102294921875e-05, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -5.744384179706685e-05, p.std().item() = 0.9614542722702026\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0015177516033872962, p.std().item() = 0.9610190391540527\n",
      "n = 'layers.0.weight', p.mean().item() = -0.001078098895959556, p.std().item() = 0.06592752784490585\n",
      "n = 'layers.0.bias', p.mean().item() = -0.09400627017021179, p.std().item() = 0.05597937852144241\n",
      "n = 'layers.2.weight', p.mean().item() = -0.006429868750274181, p.std().item() = 0.039794690907001495\n",
      "n = 'layers.2.bias', p.mean().item() = 0.06649763137102127, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 40\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -5.744384179706685e-05, p.std().item() = 0.9614542722702026\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0015177516033872962, p.std().item() = 0.9610190391540527\n",
      "n = 'layers.0.weight', p.mean().item() = -0.001078098895959556, p.std().item() = 0.06592752784490585\n",
      "n = 'layers.0.bias', p.mean().item() = -0.09400627017021179, p.std().item() = 0.05597937852144241\n",
      "n = 'layers.2.weight', p.mean().item() = -0.006429868750274181, p.std().item() = 0.039794690907001495\n",
      "n = 'layers.2.bias', p.mean().item() = 0.06649763137102127, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -0.3809363543987274, preds.std().item() = 0.7714603543281555\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.6932, device='cuda:0'), loss.std() = tensor(0.0291, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 8.171509996435233e-11, lambda_i.std().item() = 0.26611897349357605\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = 0.002148823579773307, p.grad.std().item() = 11.324371337890625\n",
      "n = 'label_factors.weight', p.grad.mean().item() = 0.00820092111825943, p.grad.std().item() = 0.5373907685279846\n",
      "n = 'layers.0.weight', p.grad.mean().item() = -5.535007476806641, p.grad.std().item() = 945.7329711914062\n",
      "n = 'layers.0.bias', p.grad.mean().item() = -632.7279052734375, p.grad.std().item() = 1041.3812255859375\n",
      "n = 'layers.2.weight', p.grad.mean().item() = -4052.201171875, p.grad.std().item() = 53221.5390625\n",
      "n = 'layers.2.bias', p.grad.mean().item() = -0.000431060791015625, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -6.461620796471834e-05, p.std().item() = 0.9605098962783813\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0015896976692602038, p.std().item() = 0.9601402878761292\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0010938026243820786, p.std().item() = 0.0663984939455986\n",
      "n = 'layers.0.bias', p.mean().item() = -0.09464982897043228, p.std().item() = 0.05689694732427597\n",
      "n = 'layers.2.weight', p.mean().item() = -0.007245731074362993, p.std().item() = 0.04082070663571358\n",
      "n = 'layers.2.bias', p.mean().item() = 0.06890470534563065, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 41\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -6.461620796471834e-05, p.std().item() = 0.9605098962783813\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0015896976692602038, p.std().item() = 0.9601402878761292\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0010938026243820786, p.std().item() = 0.0663984939455986\n",
      "n = 'layers.0.bias', p.mean().item() = -0.09464982897043228, p.std().item() = 0.05689694732427597\n",
      "n = 'layers.2.weight', p.mean().item() = -0.007245731074362993, p.std().item() = 0.04082070663571358\n",
      "n = 'layers.2.bias', p.mean().item() = 0.06890470534563065, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -0.4237727224826813, preds.std().item() = 0.8230016231536865\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.7046, device='cuda:0'), loss.std() = tensor(0.0275, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 1.2387717240880392e-10, lambda_i.std().item() = 0.2731694281101227\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = -0.00025003525661304593, p.grad.std().item() = 11.739304542541504\n",
      "n = 'label_factors.weight', p.grad.mean().item() = 0.0061935847625136375, p.grad.std().item() = 0.5104656219482422\n",
      "n = 'layers.0.weight', p.grad.mean().item() = -4.651611804962158, p.grad.std().item() = 1044.1171875\n",
      "n = 'layers.0.bias', p.grad.mean().item() = -275.24114990234375, p.grad.std().item() = 751.3527221679688\n",
      "n = 'layers.2.weight', p.grad.mean().item() = -20023.79296875, p.grad.std().item() = 49628.48828125\n",
      "n = 'layers.2.bias', p.grad.mean().item() = -0.000335693359375, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -7.340475713135675e-05, p.std().item() = 0.9595667719841003\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0016304445452988148, p.std().item() = 0.9592388272285461\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0010911752469837666, p.std().item() = 0.06681577116250992\n",
      "n = 'layers.0.bias', p.mean().item() = -0.09494686126708984, p.std().item() = 0.057705108076334\n",
      "n = 'layers.2.weight', p.mean().item() = -0.00758319441229105, p.std().item() = 0.04241596534848213\n",
      "n = 'layers.2.bias', p.mean().item() = 0.07184476405382156, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 42\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -7.340475713135675e-05, p.std().item() = 0.9595667719841003\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0016304445452988148, p.std().item() = 0.9592388272285461\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0010911752469837666, p.std().item() = 0.06681577116250992\n",
      "n = 'layers.0.bias', p.mean().item() = -0.09494686126708984, p.std().item() = 0.057705108076334\n",
      "n = 'layers.2.weight', p.mean().item() = -0.00758319441229105, p.std().item() = 0.04241596534848213\n",
      "n = 'layers.2.bias', p.mean().item() = 0.07184476405382156, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -0.5003061890602112, preds.std().item() = 0.8920149207115173\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.7035, device='cuda:0'), loss.std() = tensor(0.0277, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 1.6937881652800968e-10, lambda_i.std().item() = 0.3306315541267395\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = 0.004389385227113962, p.grad.std().item() = 14.10799789428711\n",
      "n = 'label_factors.weight', p.grad.mean().item() = 0.013812430202960968, p.grad.std().item() = 0.6660010814666748\n",
      "n = 'layers.0.weight', p.grad.mean().item() = -11.389653205871582, p.grad.std().item() = 1151.21337890625\n",
      "n = 'layers.0.bias', p.grad.mean().item() = -1262.73388671875, p.grad.std().item() = 1959.70458984375\n",
      "n = 'layers.2.weight', p.grad.mean().item() = -8299.4375, p.grad.std().item() = 77627.0390625\n",
      "n = 'layers.2.bias', p.grad.mean().item() = 0.0005016326904296875, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -8.411648741457611e-05, p.std().item() = 0.958625316619873\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0015978541923686862, p.std().item() = 0.9583499431610107\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0010832053376361728, p.std().item() = 0.06730224192142487\n",
      "n = 'layers.0.bias', p.mean().item() = -0.09442003816366196, p.std().item() = 0.05897820368409157\n",
      "n = 'layers.2.weight', p.mean().item() = -0.00804506428539753, p.std().item() = 0.045059800148010254\n",
      "n = 'layers.2.bias', p.mean().item() = 0.07331597805023193, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 43\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -8.411648741457611e-05, p.std().item() = 0.958625316619873\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0015978541923686862, p.std().item() = 0.9583499431610107\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0010832053376361728, p.std().item() = 0.06730224192142487\n",
      "n = 'layers.0.bias', p.mean().item() = -0.09442003816366196, p.std().item() = 0.05897820368409157\n",
      "n = 'layers.2.weight', p.mean().item() = -0.00804506428539753, p.std().item() = 0.045059800148010254\n",
      "n = 'layers.2.bias', p.mean().item() = 0.07331597805023193, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -0.5375407338142395, preds.std().item() = 1.0357609987258911\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.7274, device='cuda:0'), loss.std() = tensor(0.0441, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 8.432413795000926e-11, lambda_i.std().item() = 0.23965834081172943\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = -0.0024479504209011793, p.grad.std().item() = 16.840452194213867\n",
      "n = 'label_factors.weight', p.grad.mean().item() = -0.009184013120830059, p.grad.std().item() = 0.592907190322876\n",
      "n = 'layers.0.weight', p.grad.mean().item() = -20.177852630615234, p.grad.std().item() = 1164.4722900390625\n",
      "n = 'layers.0.bias', p.grad.mean().item() = 554.4218139648438, p.grad.std().item() = 1046.786865234375\n",
      "n = 'layers.2.weight', p.grad.mean().item() = -3620.192626953125, p.grad.std().item() = 35506.359375\n",
      "n = 'layers.2.bias', p.grad.mean().item() = -0.00031280517578125, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -9.580262849340215e-05, p.std().item() = 0.9576844573020935\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0016127554699778557, p.std().item() = 0.9574612379074097\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0010758255375549197, p.std().item() = 0.0678032711148262\n",
      "n = 'layers.0.bias', p.mean().item() = -0.09424809366464615, p.std().item() = 0.059954844415187836\n",
      "n = 'layers.2.weight', p.mean().item() = -0.008168622851371765, p.std().item() = 0.04700426012277603\n",
      "n = 'layers.2.bias', p.mean().item() = 0.07535884529352188, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 44\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -9.580262849340215e-05, p.std().item() = 0.9576844573020935\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0016127554699778557, p.std().item() = 0.9574612379074097\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0010758255375549197, p.std().item() = 0.0678032711148262\n",
      "n = 'layers.0.bias', p.mean().item() = -0.09424809366464615, p.std().item() = 0.059954844415187836\n",
      "n = 'layers.2.weight', p.mean().item() = -0.008168622851371765, p.std().item() = 0.04700426012277603\n",
      "n = 'layers.2.bias', p.mean().item() = 0.07535884529352188, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -0.5396573543548584, preds.std().item() = 1.0400551557540894\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.7111, device='cuda:0'), loss.std() = tensor(0.0398, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 1.0697059904529738e-10, lambda_i.std().item() = 0.2804749310016632\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = 0.002588712377473712, p.grad.std().item() = 15.748218536376953\n",
      "n = 'label_factors.weight', p.grad.mean().item() = 0.020207783207297325, p.grad.std().item() = 0.8147677779197693\n",
      "n = 'layers.0.weight', p.grad.mean().item() = -2.5866448879241943, p.grad.std().item() = 1163.821044921875\n",
      "n = 'layers.0.bias', p.grad.mean().item() = -1369.641845703125, p.grad.std().item() = 1889.052490234375\n",
      "n = 'layers.2.weight', p.grad.mean().item() = -5475.111328125, p.grad.std().item() = 71619.734375\n",
      "n = 'layers.2.bias', p.grad.mean().item() = 0.000110626220703125, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.00010898237087531015, p.std().item() = 0.956744909286499\n",
      "n = 'label_factors.weight', p.mean().item() = 0.00154350814409554, p.std().item() = 0.9566011428833008\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0010700620478019118, p.std().item() = 0.06841978430747986\n",
      "n = 'layers.0.bias', p.mean().item() = -0.09317135810852051, p.std().item() = 0.06127028912305832\n",
      "n = 'layers.2.weight', p.mean().item() = -0.008475322276353836, p.std().item() = 0.04997619614005089\n",
      "n = 'layers.2.bias', p.mean().item() = 0.07695292681455612, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 45\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.00010898237087531015, p.std().item() = 0.956744909286499\n",
      "n = 'label_factors.weight', p.mean().item() = 0.00154350814409554, p.std().item() = 0.9566011428833008\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0010700620478019118, p.std().item() = 0.06841978430747986\n",
      "n = 'layers.0.bias', p.mean().item() = -0.09317135810852051, p.std().item() = 0.06127028912305832\n",
      "n = 'layers.2.weight', p.mean().item() = -0.008475322276353836, p.std().item() = 0.04997619614005089\n",
      "n = 'layers.2.bias', p.mean().item() = 0.07695292681455612, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -0.6981582641601562, preds.std().item() = 1.2197113037109375\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.7180, device='cuda:0'), loss.std() = tensor(0.0501, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 2.8021078513074116e-11, lambda_i.std().item() = 0.2392473816871643\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = -0.0026167011819779873, p.grad.std().item() = 20.164085388183594\n",
      "n = 'label_factors.weight', p.grad.mean().item() = -0.017251677811145782, p.grad.std().item() = 0.8455893993377686\n",
      "n = 'layers.0.weight', p.grad.mean().item() = -35.519813537597656, p.grad.std().item() = 1291.709716796875\n",
      "n = 'layers.0.bias', p.grad.mean().item() = 862.207275390625, p.grad.std().item() = 1173.91748046875\n",
      "n = 'layers.2.weight', p.grad.mean().item() = -12568.86328125, p.grad.std().item() = 44501.12890625\n",
      "n = 'layers.2.bias', p.grad.mean().item() = -0.0006809234619140625, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.00012285579578019679, p.std().item() = 0.9558061957359314\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0015527926152572036, p.std().item() = 0.9557167887687683\n",
      "n = 'layers.0.weight', p.mean().item() = -0.001028265804052353, p.std().item() = 0.06892739981412888\n",
      "n = 'layers.0.bias', p.mean().item() = -0.09301471710205078, p.std().item() = 0.062277089804410934\n",
      "n = 'layers.2.weight', p.mean().item() = -0.00819273479282856, p.std().item() = 0.05168639495968819\n",
      "n = 'layers.2.bias', p.mean().item() = 0.07989852875471115, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 46\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.00012285579578019679, p.std().item() = 0.9558061957359314\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0015527926152572036, p.std().item() = 0.9557167887687683\n",
      "n = 'layers.0.weight', p.mean().item() = -0.001028265804052353, p.std().item() = 0.06892739981412888\n",
      "n = 'layers.0.bias', p.mean().item() = -0.09301471710205078, p.std().item() = 0.062277089804410934\n",
      "n = 'layers.2.weight', p.mean().item() = -0.00819273479282856, p.std().item() = 0.05168639495968819\n",
      "n = 'layers.2.bias', p.mean().item() = 0.07989852875471115, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -0.7519160509109497, preds.std().item() = 1.3065855503082275\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.7224, device='cuda:0'), loss.std() = tensor(0.0393, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 5.536380728865531e-11, lambda_i.std().item() = 0.24856723845005035\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = 0.0015123040648177266, p.grad.std().item() = 20.7320613861084\n",
      "n = 'label_factors.weight', p.grad.mean().item() = 0.003438411047682166, p.grad.std().item() = 0.8872891068458557\n",
      "n = 'layers.0.weight', p.grad.mean().item() = 17.711393356323242, p.grad.std().item() = 1379.3851318359375\n",
      "n = 'layers.0.bias', p.grad.mean().item() = -284.72515869140625, p.grad.std().item() = 914.8158569335938\n",
      "n = 'layers.2.weight', p.grad.mean().item() = 594.3937377929688, p.grad.std().item() = 38158.87109375\n",
      "n = 'layers.2.bias', p.grad.mean().item() = -0.0004892349243164062, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.00013715933891944587, p.std().item() = 0.9548686146736145\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0015436316607519984, p.std().item() = 0.9548705816268921\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0010358329163864255, p.std().item() = 0.06952077895402908\n",
      "n = 'layers.0.bias', p.mean().item() = -0.0927376076579094, p.std().item() = 0.063250333070755\n",
      "n = 'layers.2.weight', p.mean().item() = -0.007993187755346298, p.std().item() = 0.053479038178920746\n",
      "n = 'layers.2.bias', p.mean().item() = 0.08363698422908783, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 47\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.00013715933891944587, p.std().item() = 0.9548686146736145\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0015436316607519984, p.std().item() = 0.9548705816268921\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0010358329163864255, p.std().item() = 0.06952077895402908\n",
      "n = 'layers.0.bias', p.mean().item() = -0.0927376076579094, p.std().item() = 0.063250333070755\n",
      "n = 'layers.2.weight', p.mean().item() = -0.007993187755346298, p.std().item() = 0.053479038178920746\n",
      "n = 'layers.2.bias', p.mean().item() = 0.08363698422908783, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -0.7229291796684265, preds.std().item() = 1.3643066883087158\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.7294, device='cuda:0'), loss.std() = tensor(0.0495, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 5.0928442019149145e-11, lambda_i.std().item() = 0.22805964946746826\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = -0.002234972547739744, p.grad.std().item() = 21.118024826049805\n",
      "n = 'label_factors.weight', p.grad.mean().item() = -0.012465321458876133, p.grad.std().item() = 0.8001601696014404\n",
      "n = 'layers.0.weight', p.grad.mean().item() = 0.5968597531318665, p.grad.std().item() = 1414.3941650390625\n",
      "n = 'layers.0.bias', p.grad.mean().item() = 793.9141845703125, p.grad.std().item() = 1337.001708984375\n",
      "n = 'layers.2.weight', p.grad.mean().item() = -14534.4521484375, p.grad.std().item() = 34107.4296875\n",
      "n = 'layers.2.bias', p.grad.mean().item() = -0.000335693359375, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.00015171041013672948, p.std().item() = 0.9539320468902588\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0015824754955247045, p.std().item() = 0.9540095329284668\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0010638191597536206, p.std().item() = 0.07003654539585114\n",
      "n = 'layers.0.bias', p.mean().item() = -0.09319084882736206, p.std().item() = 0.06406695395708084\n",
      "n = 'layers.2.weight', p.mean().item() = -0.0073448964394629, p.std().item() = 0.054582081735134125\n",
      "n = 'layers.2.bias', p.mean().item() = 0.0877702459692955, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 48\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.00015171041013672948, p.std().item() = 0.9539320468902588\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0015824754955247045, p.std().item() = 0.9540095329284668\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0010638191597536206, p.std().item() = 0.07003654539585114\n",
      "n = 'layers.0.bias', p.mean().item() = -0.09319084882736206, p.std().item() = 0.06406695395708084\n",
      "n = 'layers.2.weight', p.mean().item() = -0.0073448964394629, p.std().item() = 0.054582081735134125\n",
      "n = 'layers.2.bias', p.mean().item() = 0.0877702459692955, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -0.7425307631492615, preds.std().item() = 1.3542897701263428\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.7402, device='cuda:0'), loss.std() = tensor(0.0591, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 2.1185397186540555e-11, lambda_i.std().item() = 0.225770503282547\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = -0.003507925197482109, p.grad.std().item() = 31.461938858032227\n",
      "n = 'label_factors.weight', p.grad.mean().item() = -0.03058614209294319, p.grad.std().item() = 1.0045719146728516\n",
      "n = 'layers.0.weight', p.grad.mean().item() = 46.587425231933594, p.grad.std().item() = 1916.6243896484375\n",
      "n = 'layers.0.bias', p.grad.mean().item() = 1914.7081298828125, p.grad.std().item() = 2242.54052734375\n",
      "n = 'layers.2.weight', p.grad.mean().item() = -27293.013671875, p.grad.std().item() = 85266.796875\n",
      "n = 'layers.2.bias', p.grad.mean().item() = -0.00022125244140625, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.00016614445485174656, p.std().item() = 0.9529958963394165\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0017245952039957047, p.std().item() = 0.9531158804893494\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0011157677508890629, p.std().item() = 0.07036484777927399\n",
      "n = 'layers.0.bias', p.mean().item() = -0.09508929401636124, p.std().item() = 0.06449198722839355\n",
      "n = 'layers.2.weight', p.mean().item() = -0.005896169226616621, p.std().item() = 0.05386681854724884\n",
      "n = 'layers.2.bias', p.mean().item() = 0.09201955795288086, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 49\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.00016614445485174656, p.std().item() = 0.9529958963394165\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0017245952039957047, p.std().item() = 0.9531158804893494\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0011157677508890629, p.std().item() = 0.07036484777927399\n",
      "n = 'layers.0.bias', p.mean().item() = -0.09508929401636124, p.std().item() = 0.06449198722839355\n",
      "n = 'layers.2.weight', p.mean().item() = -0.005896169226616621, p.std().item() = 0.05386681854724884\n",
      "n = 'layers.2.bias', p.mean().item() = 0.09201955795288086, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -0.6382560133934021, preds.std().item() = 1.4030556678771973\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.7268, device='cuda:0'), loss.std() = tensor(0.0581, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 1.0347448592407105e-10, lambda_i.std().item() = 0.23235085606575012\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = 0.0009160470217466354, p.grad.std().item() = 19.655723571777344\n",
      "n = 'label_factors.weight', p.grad.mean().item() = 0.010453256778419018, p.grad.std().item() = 0.8754149675369263\n",
      "n = 'layers.0.weight', p.grad.mean().item() = -29.40019989013672, p.grad.std().item() = 1281.3299560546875\n",
      "n = 'layers.0.bias', p.grad.mean().item() = -522.2678833007812, p.grad.std().item() = 1090.039794921875\n",
      "n = 'layers.2.weight', p.grad.mean().item() = -6564.19873046875, p.grad.std().item() = 36372.3515625\n",
      "n = 'layers.2.bias', p.grad.mean().item() = 0.000110626220703125, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.00018092678510583937, p.std().item() = 0.9520607590675354\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0018116544233635068, p.std().item() = 0.9522441029548645\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0011732425773516297, p.std().item() = 0.0707830935716629\n",
      "n = 'layers.0.bias', p.mean().item() = -0.09647321701049805, p.std().item() = 0.06509006023406982\n",
      "n = 'layers.2.weight', p.mean().item() = -0.00451521435752511, p.std().item() = 0.05377345532178879\n",
      "n = 'layers.2.bias', p.mean().item() = 0.09561731666326523, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 50\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.00018092678510583937, p.std().item() = 0.9520607590675354\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0018116544233635068, p.std().item() = 0.9522441029548645\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0011732425773516297, p.std().item() = 0.0707830935716629\n",
      "n = 'layers.0.bias', p.mean().item() = -0.09647321701049805, p.std().item() = 0.06509006023406982\n",
      "n = 'layers.2.weight', p.mean().item() = -0.00451521435752511, p.std().item() = 0.05377345532178879\n",
      "n = 'layers.2.bias', p.mean().item() = 0.09561731666326523, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -0.5639384388923645, preds.std().item() = 1.348757266998291\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.7146, device='cuda:0'), loss.std() = tensor(0.0442, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 7.962787373916314e-11, lambda_i.std().item() = 0.21959978342056274\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = 0.00152697809971869, p.grad.std().item() = 17.974321365356445\n",
      "n = 'label_factors.weight', p.grad.mean().item() = -0.010899134911596775, p.grad.std().item() = 0.7890310883522034\n",
      "n = 'layers.0.weight', p.grad.mean().item() = -1.9715675115585327, p.grad.std().item() = 1145.5511474609375\n",
      "n = 'layers.0.bias', p.grad.mean().item() = 345.6839294433594, p.grad.std().item() = 875.6834106445312\n",
      "n = 'layers.2.weight', p.grad.mean().item() = 16212.2568359375, p.grad.std().item() = 31301.59375\n",
      "n = 'layers.2.bias', p.grad.mean().item() = -0.0006351470947265625, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0001958070497494191, p.std().item() = 0.9511265158653259\n",
      "n = 'label_factors.weight', p.mean().item() = 0.001907765632495284, p.std().item() = 0.9513866901397705\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0012290835147723556, p.std().item() = 0.07124940305948257\n",
      "n = 'layers.0.bias', p.mean().item() = -0.09796767681837082, p.std().item() = 0.06577149033546448\n",
      "n = 'layers.2.weight', p.mean().item() = -0.003623292315751314, p.std().item() = 0.05360526219010353\n",
      "n = 'layers.2.bias', p.mean().item() = 0.10024456679821014, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 51\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0001958070497494191, p.std().item() = 0.9511265158653259\n",
      "n = 'label_factors.weight', p.mean().item() = 0.001907765632495284, p.std().item() = 0.9513866901397705\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0012290835147723556, p.std().item() = 0.07124940305948257\n",
      "n = 'layers.0.bias', p.mean().item() = -0.09796767681837082, p.std().item() = 0.06577149033546448\n",
      "n = 'layers.2.weight', p.mean().item() = -0.003623292315751314, p.std().item() = 0.05360526219010353\n",
      "n = 'layers.2.bias', p.mean().item() = 0.10024456679821014, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -0.5565907955169678, preds.std().item() = 1.275183916091919\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.7192, device='cuda:0'), loss.std() = tensor(0.0380, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 1.1333665256296754e-10, lambda_i.std().item() = 0.24743562936782837\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = 0.0012669161660596728, p.grad.std().item() = 24.29429054260254\n",
      "n = 'label_factors.weight', p.grad.mean().item() = 0.022604621946811676, p.grad.std().item() = 0.8743523955345154\n",
      "n = 'layers.0.weight', p.grad.mean().item() = 2.101330041885376, p.grad.std().item() = 1434.7344970703125\n",
      "n = 'layers.0.bias', p.grad.mean().item() = -811.9478759765625, p.grad.std().item() = 1254.52685546875\n",
      "n = 'layers.2.weight', p.grad.mean().item() = 5514.19482421875, p.grad.std().item() = 43584.4140625\n",
      "n = 'layers.2.bias', p.grad.mean().item() = 0.000171661376953125, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.00021157425362616777, p.std().item() = 0.9501937627792358\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0019312020158395171, p.std().item() = 0.9505337476730347\n",
      "n = 'layers.0.weight', p.mean().item() = -0.001285064616240561, p.std().item() = 0.07175866514444351\n",
      "n = 'layers.0.bias', p.mean().item() = -0.09881618618965149, p.std().item() = 0.06672009080648422\n",
      "n = 'layers.2.weight', p.mean().item() = -0.0030860677361488342, p.std().item() = 0.05428330972790718\n",
      "n = 'layers.2.bias', p.mean().item() = 0.10403972119092941, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 52\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.00021157425362616777, p.std().item() = 0.9501937627792358\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0019312020158395171, p.std().item() = 0.9505337476730347\n",
      "n = 'layers.0.weight', p.mean().item() = -0.001285064616240561, p.std().item() = 0.07175866514444351\n",
      "n = 'layers.0.bias', p.mean().item() = -0.09881618618965149, p.std().item() = 0.06672009080648422\n",
      "n = 'layers.2.weight', p.mean().item() = -0.0030860677361488342, p.std().item() = 0.05428330972790718\n",
      "n = 'layers.2.bias', p.mean().item() = 0.10403972119092941, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -0.6903099417686462, preds.std().item() = 1.4437967538833618\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.7247, device='cuda:0'), loss.std() = tensor(0.0457, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 7.983659566779266e-11, lambda_i.std().item() = 0.204436257481575\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = 0.004283569753170013, p.grad.std().item() = 23.18157386779785\n",
      "n = 'label_factors.weight', p.grad.mean().item() = -0.030204441398382187, p.grad.std().item() = 0.9286621809005737\n",
      "n = 'layers.0.weight', p.grad.mean().item() = -28.960695266723633, p.grad.std().item() = 1466.79248046875\n",
      "n = 'layers.0.bias', p.grad.mean().item() = 1123.338623046875, p.grad.std().item() = 1667.1888427734375\n",
      "n = 'layers.2.weight', p.grad.mean().item() = 12890.982421875, p.grad.std().item() = 63138.7421875\n",
      "n = 'layers.2.bias', p.grad.mean().item() = 0.000247955322265625, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.00022783550957683474, p.std().item() = 0.9492619633674622\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0020185792818665504, p.std().item() = 0.9497054815292358\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0013270521303638816, p.std().item() = 0.07226870954036713\n",
      "n = 'layers.0.bias', p.mean().item() = -0.10043498873710632, p.std().item() = 0.06768793612718582\n",
      "n = 'layers.2.weight', p.mean().item() = -0.0026926985010504723, p.std().item() = 0.05411364883184433\n",
      "n = 'layers.2.bias', p.mean().item() = 0.10689040273427963, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 53\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.00022783550957683474, p.std().item() = 0.9492619633674622\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0020185792818665504, p.std().item() = 0.9497054815292358\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0013270521303638816, p.std().item() = 0.07226870954036713\n",
      "n = 'layers.0.bias', p.mean().item() = -0.10043498873710632, p.std().item() = 0.06768793612718582\n",
      "n = 'layers.2.weight', p.mean().item() = -0.0026926985010504723, p.std().item() = 0.05411364883184433\n",
      "n = 'layers.2.bias', p.mean().item() = 0.10689040273427963, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -0.5282233953475952, preds.std().item() = 1.3884437084197998\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.7123, device='cuda:0'), loss.std() = tensor(0.0410, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 8.716799504426831e-11, lambda_i.std().item() = 0.2251356691122055\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = 0.002497382229194045, p.grad.std().item() = 18.735084533691406\n",
      "n = 'label_factors.weight', p.grad.mean().item() = 0.016676628962159157, p.grad.std().item() = 1.0505766868591309\n",
      "n = 'layers.0.weight', p.grad.mean().item() = -4.977602005004883, p.grad.std().item() = 1124.2816162109375\n",
      "n = 'layers.0.bias', p.grad.mean().item() = -644.2503051757812, p.grad.std().item() = 1158.982666015625\n",
      "n = 'layers.2.weight', p.grad.mean().item() = 10730.1923828125, p.grad.std().item() = 30624.3359375\n",
      "n = 'layers.2.bias', p.grad.mean().item() = 0.00031948089599609375, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0002445829741191119, p.std().item() = 0.9483312964439392\n",
      "n = 'label_factors.weight', p.mean().item() = 0.002047843998298049, p.std().item() = 0.9489202499389648\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0013591706519946456, p.std().item() = 0.07293228805065155\n",
      "n = 'layers.0.bias', p.mean().item() = -0.10150200873613358, p.std().item() = 0.06886269152164459\n",
      "n = 'layers.2.weight', p.mean().item() = -0.0026991248596459627, p.std().item() = 0.05459194630384445\n",
      "n = 'layers.2.bias', p.mean().item() = 0.10870777070522308, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 54\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0002445829741191119, p.std().item() = 0.9483312964439392\n",
      "n = 'label_factors.weight', p.mean().item() = 0.002047843998298049, p.std().item() = 0.9489202499389648\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0013591706519946456, p.std().item() = 0.07293228805065155\n",
      "n = 'layers.0.bias', p.mean().item() = -0.10150200873613358, p.std().item() = 0.06886269152164459\n",
      "n = 'layers.2.weight', p.mean().item() = -0.0026991248596459627, p.std().item() = 0.05459194630384445\n",
      "n = 'layers.2.bias', p.mean().item() = 0.10870777070522308, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -0.6459894180297852, preds.std().item() = 1.4427869319915771\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.7556, device='cuda:0'), loss.std() = tensor(0.0667, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 6.950479775058582e-11, lambda_i.std().item() = 0.24857567250728607\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = 0.0023517911322414875, p.grad.std().item() = 31.5372371673584\n",
      "n = 'label_factors.weight', p.grad.mean().item() = -0.03760749101638794, p.grad.std().item() = 1.0728179216384888\n",
      "n = 'layers.0.weight', p.grad.mean().item() = 14.685855865478516, p.grad.std().item() = 1916.9205322265625\n",
      "n = 'layers.0.bias', p.grad.mean().item() = 1117.5472412109375, p.grad.std().item() = 1568.37158203125\n",
      "n = 'layers.2.weight', p.grad.mean().item() = -24757.267578125, p.grad.std().item() = 57735.91015625\n",
      "n = 'layers.2.bias', p.grad.mean().item() = -0.00041961669921875, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0002618126745801419, p.std().item() = 0.9474014043807983\n",
      "n = 'label_factors.weight', p.mean().item() = 0.002162051619961858, p.std().item() = 0.9481161832809448\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0013843945926055312, p.std().item() = 0.07346218079328537\n",
      "n = 'layers.0.bias', p.mean().item() = -0.10312224924564362, p.std().item() = 0.06964399665594101\n",
      "n = 'layers.2.weight', p.mean().item() = -0.0020389019045978785, p.std().item() = 0.054094359278678894\n",
      "n = 'layers.2.bias', p.mean().item() = 0.11131134629249573, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 55\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0002618126745801419, p.std().item() = 0.9474014043807983\n",
      "n = 'label_factors.weight', p.mean().item() = 0.002162051619961858, p.std().item() = 0.9481161832809448\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0013843945926055312, p.std().item() = 0.07346218079328537\n",
      "n = 'layers.0.bias', p.mean().item() = -0.10312224924564362, p.std().item() = 0.06964399665594101\n",
      "n = 'layers.2.weight', p.mean().item() = -0.0020389019045978785, p.std().item() = 0.054094359278678894\n",
      "n = 'layers.2.bias', p.mean().item() = 0.11131134629249573, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -0.588803768157959, preds.std().item() = 1.4179623126983643\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.7246, device='cuda:0'), loss.std() = tensor(0.0554, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 9.194253691724441e-11, lambda_i.std().item() = 0.22473366558551788\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = 0.001935875858180225, p.grad.std().item() = 22.251222610473633\n",
      "n = 'label_factors.weight', p.grad.mean().item() = -0.009075511246919632, p.grad.std().item() = 1.0147029161453247\n",
      "n = 'layers.0.weight', p.grad.mean().item() = 4.3589324951171875, p.grad.std().item() = 1318.42822265625\n",
      "n = 'layers.0.bias', p.grad.mean().item() = 195.03358459472656, p.grad.std().item() = 636.7884521484375\n",
      "n = 'layers.2.weight', p.grad.mean().item() = -10277.2958984375, p.grad.std().item() = 28740.9921875\n",
      "n = 'layers.2.bias', p.grad.mean().item() = -0.0002593994140625, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0002795589971356094, p.std().item() = 0.9464725852012634\n",
      "n = 'label_factors.weight', p.mean().item() = 0.002271226141601801, p.std().item() = 0.947316586971283\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0013987807324156165, p.std().item() = 0.07396936416625977\n",
      "n = 'layers.0.bias', p.mean().item() = -0.10470897704362869, p.std().item() = 0.07042334228754044\n",
      "n = 'layers.2.weight', p.mean().item() = -0.0011910643661394715, p.std().item() = 0.0534060001373291\n",
      "n = 'layers.2.bias', p.mean().item() = 0.11426820605993271, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 56\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0002795589971356094, p.std().item() = 0.9464725852012634\n",
      "n = 'label_factors.weight', p.mean().item() = 0.002271226141601801, p.std().item() = 0.947316586971283\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0013987807324156165, p.std().item() = 0.07396936416625977\n",
      "n = 'layers.0.bias', p.mean().item() = -0.10470897704362869, p.std().item() = 0.07042334228754044\n",
      "n = 'layers.2.weight', p.mean().item() = -0.0011910643661394715, p.std().item() = 0.0534060001373291\n",
      "n = 'layers.2.bias', p.mean().item() = 0.11426820605993271, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -0.6479774713516235, preds.std().item() = 1.3284287452697754\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.7097, device='cuda:0'), loss.std() = tensor(0.0420, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 1.2105941249451746e-10, lambda_i.std().item() = 0.29026174545288086\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = -4.76132299809251e-06, p.grad.std().item() = 28.494976043701172\n",
      "n = 'label_factors.weight', p.grad.mean().item() = 0.06629464775323868, p.grad.std().item() = 1.2640067338943481\n",
      "n = 'layers.0.weight', p.grad.mean().item() = -5.805830955505371, p.grad.std().item() = 1811.4266357421875\n",
      "n = 'layers.0.bias', p.grad.mean().item() = -1732.0555419921875, p.grad.std().item() = 2438.982177734375\n",
      "n = 'layers.2.weight', p.grad.mean().item() = -1621.153076171875, p.grad.std().item() = 83959.5625\n",
      "n = 'layers.2.bias', p.grad.mean().item() = -9.5367431640625e-05, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.00029896385967731476, p.std().item() = 0.9455452561378479\n",
      "n = 'label_factors.weight', p.mean().item() = 0.002229690784588456, p.std().item() = 0.9465570449829102\n",
      "n = 'layers.0.weight', p.mean().item() = -0.001418139785528183, p.std().item() = 0.07461123168468475\n",
      "n = 'layers.0.bias', p.mean().item() = -0.10510442405939102, p.std().item() = 0.07150304317474365\n",
      "n = 'layers.2.weight', p.mean().item() = -0.0006617061444558203, p.std().item() = 0.05405263230204582\n",
      "n = 'layers.2.bias', p.mean().item() = 0.11717113852500916, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 57\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.00029896385967731476, p.std().item() = 0.9455452561378479\n",
      "n = 'label_factors.weight', p.mean().item() = 0.002229690784588456, p.std().item() = 0.9465570449829102\n",
      "n = 'layers.0.weight', p.mean().item() = -0.001418139785528183, p.std().item() = 0.07461123168468475\n",
      "n = 'layers.0.bias', p.mean().item() = -0.10510442405939102, p.std().item() = 0.07150304317474365\n",
      "n = 'layers.2.weight', p.mean().item() = -0.0006617061444558203, p.std().item() = 0.05405263230204582\n",
      "n = 'layers.2.bias', p.mean().item() = 0.11717113852500916, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -0.6437767744064331, preds.std().item() = 1.4321166276931763\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.7465, device='cuda:0'), loss.std() = tensor(0.0691, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 6.222557885626756e-11, lambda_i.std().item() = 0.23199985921382904\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = 3.300301978015341e-05, p.grad.std().item() = 25.465452194213867\n",
      "n = 'label_factors.weight', p.grad.mean().item() = -0.0036856969818472862, p.grad.std().item() = 0.9686576724052429\n",
      "n = 'layers.0.weight', p.grad.mean().item() = -11.566617012023926, p.grad.std().item() = 1413.855224609375\n",
      "n = 'layers.0.bias', p.grad.mean().item() = 120.67489624023438, p.grad.std().item() = 839.9769897460938\n",
      "n = 'layers.2.weight', p.grad.mean().item() = 4097.74609375, p.grad.std().item() = 31711.615234375\n",
      "n = 'layers.2.bias', p.grad.mean().item() = 0.000324249267578125, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0003196116304025054, p.std().item() = 0.9446192383766174\n",
      "n = 'label_factors.weight', p.mean().item() = 0.002197247464209795, p.std().item() = 0.9457982778549194\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0014276758302003145, p.std().item() = 0.0752490758895874\n",
      "n = 'layers.0.bias', p.mean().item() = -0.10569857060909271, p.std().item() = 0.07275477051734924\n",
      "n = 'layers.2.weight', p.mean().item() = -0.00034316888195462525, p.std().item() = 0.05469100549817085\n",
      "n = 'layers.2.bias', p.mean().item() = 0.11900589615106583, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 58\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0003196116304025054, p.std().item() = 0.9446192383766174\n",
      "n = 'label_factors.weight', p.mean().item() = 0.002197247464209795, p.std().item() = 0.9457982778549194\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0014276758302003145, p.std().item() = 0.0752490758895874\n",
      "n = 'layers.0.bias', p.mean().item() = -0.10569857060909271, p.std().item() = 0.07275477051734924\n",
      "n = 'layers.2.weight', p.mean().item() = -0.00034316888195462525, p.std().item() = 0.05469100549817085\n",
      "n = 'layers.2.bias', p.mean().item() = 0.11900589615106583, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -0.5744296312332153, preds.std().item() = 1.4117542505264282\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.7230, device='cuda:0'), loss.std() = tensor(0.0618, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 8.881168717111976e-11, lambda_i.std().item() = 0.22066707909107208\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = 0.0021771632600575686, p.grad.std().item() = 21.447378158569336\n",
      "n = 'label_factors.weight', p.grad.mean().item() = 0.0032173392828553915, p.grad.std().item() = 1.0052067041397095\n",
      "n = 'layers.0.weight', p.grad.mean().item() = -8.97857666015625, p.grad.std().item() = 1243.473876953125\n",
      "n = 'layers.0.bias', p.grad.mean().item() = 17.150177001953125, p.grad.std().item() = 768.0466918945312\n",
      "n = 'layers.2.weight', p.grad.mean().item() = 8866.884765625, p.grad.std().item() = 25149.17578125\n",
      "n = 'layers.2.bias', p.grad.mean().item() = -0.0005893707275390625, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.00034131944994442165, p.std().item() = 0.9436942934989929\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0021585060749202967, p.std().item() = 0.9450450539588928\n",
      "n = 'layers.0.weight', p.mean().item() = -0.001428311108611524, p.std().item() = 0.07590316236019135\n",
      "n = 'layers.0.bias', p.mean().item() = -0.1062767282128334, p.std().item() = 0.07410592585802078\n",
      "n = 'layers.2.weight', p.mean().item() = -0.00020671247330028564, p.std().item() = 0.055342141538858414\n",
      "n = 'layers.2.bias', p.mean().item() = 0.12201409786939621, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 59\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.00034131944994442165, p.std().item() = 0.9436942934989929\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0021585060749202967, p.std().item() = 0.9450450539588928\n",
      "n = 'layers.0.weight', p.mean().item() = -0.001428311108611524, p.std().item() = 0.07590316236019135\n",
      "n = 'layers.0.bias', p.mean().item() = -0.1062767282128334, p.std().item() = 0.07410592585802078\n",
      "n = 'layers.2.weight', p.mean().item() = -0.00020671247330028564, p.std().item() = 0.055342141538858414\n",
      "n = 'layers.2.bias', p.mean().item() = 0.12201409786939621, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -0.6252335906028748, preds.std().item() = 1.5149062871932983\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.7245, device='cuda:0'), loss.std() = tensor(0.0564, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 9.152508612109145e-11, lambda_i.std().item() = 0.21326710283756256\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = 0.0014395861653611064, p.grad.std().item() = 28.69525718688965\n",
      "n = 'label_factors.weight', p.grad.mean().item() = -0.00548422709107399, p.grad.std().item() = 0.9971580505371094\n",
      "n = 'layers.0.weight', p.grad.mean().item() = 17.19231414794922, p.grad.std().item() = 1719.7449951171875\n",
      "n = 'layers.0.bias', p.grad.mean().item() = 235.17864990234375, p.grad.std().item() = 1002.093505859375\n",
      "n = 'layers.2.weight', p.grad.mean().item() = 4465.12158203125, p.grad.std().item() = 31783.990234375\n",
      "n = 'layers.2.bias', p.grad.mean().item() = 3.4332275390625e-05, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0003644033567979932, p.std().item() = 0.9427705407142639\n",
      "n = 'label_factors.weight', p.mean().item() = 0.002134190406650305, p.std().item() = 0.944280207157135\n",
      "n = 'layers.0.weight', p.mean().item() = -0.001437298720702529, p.std().item() = 0.0765622928738594\n",
      "n = 'layers.0.bias', p.mean().item() = -0.10692258924245834, p.std().item() = 0.07532833516597748\n",
      "n = 'layers.2.weight', p.mean().item() = -0.00015345215797424316, p.std().item() = 0.055980946868658066\n",
      "n = 'layers.2.bias', p.mean().item() = 0.12465473264455795, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 60\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0003644033567979932, p.std().item() = 0.9427705407142639\n",
      "n = 'label_factors.weight', p.mean().item() = 0.002134190406650305, p.std().item() = 0.944280207157135\n",
      "n = 'layers.0.weight', p.mean().item() = -0.001437298720702529, p.std().item() = 0.0765622928738594\n",
      "n = 'layers.0.bias', p.mean().item() = -0.10692258924245834, p.std().item() = 0.07532833516597748\n",
      "n = 'layers.2.weight', p.mean().item() = -0.00015345215797424316, p.std().item() = 0.055980946868658066\n",
      "n = 'layers.2.bias', p.mean().item() = 0.12465473264455795, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -0.6637246012687683, preds.std().item() = 1.5518678426742554\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.7510, device='cuda:0'), loss.std() = tensor(0.0758, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 7.493160258942311e-11, lambda_i.std().item() = 0.2093668282032013\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = 0.001749466871842742, p.grad.std().item() = 28.496599197387695\n",
      "n = 'label_factors.weight', p.grad.mean().item() = -0.023640573024749756, p.grad.std().item() = 1.013625979423523\n",
      "n = 'layers.0.weight', p.grad.mean().item() = 7.257013320922852, p.grad.std().item() = 1597.4879150390625\n",
      "n = 'layers.0.bias', p.grad.mean().item() = 649.9688720703125, p.grad.std().item() = 1154.5198974609375\n",
      "n = 'layers.2.weight', p.grad.mean().item() = 19104.1015625, p.grad.std().item() = 30883.25390625\n",
      "n = 'layers.2.bias', p.grad.mean().item() = 0.00087738037109375, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.00038828220567665994, p.std().item() = 0.9418475031852722\n",
      "n = 'label_factors.weight', p.mean().item() = 0.002148982137441635, p.std().item() = 0.9435119032859802\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0014409113209694624, p.std().item() = 0.0772438645362854\n",
      "n = 'layers.0.bias', p.mean().item() = -0.10801276564598083, p.std().item() = 0.07659482955932617\n",
      "n = 'layers.2.weight', p.mean().item() = -0.0005819915095344186, p.std().item() = 0.05629660189151764\n",
      "n = 'layers.2.bias', p.mean().item() = 0.12490933388471603, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 61\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.00038828220567665994, p.std().item() = 0.9418475031852722\n",
      "n = 'label_factors.weight', p.mean().item() = 0.002148982137441635, p.std().item() = 0.9435119032859802\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0014409113209694624, p.std().item() = 0.0772438645362854\n",
      "n = 'layers.0.bias', p.mean().item() = -0.10801276564598083, p.std().item() = 0.07659482955932617\n",
      "n = 'layers.2.weight', p.mean().item() = -0.0005819915095344186, p.std().item() = 0.05629660189151764\n",
      "n = 'layers.2.bias', p.mean().item() = 0.12490933388471603, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -0.7707844376564026, preds.std().item() = 1.6026185750961304\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.7265, device='cuda:0'), loss.std() = tensor(0.0610, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 7.842771571064944e-11, lambda_i.std().item() = 0.22850514948368073\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = 0.00010680838022381067, p.grad.std().item() = 24.36548614501953\n",
      "n = 'label_factors.weight', p.grad.mean().item() = 0.037244029343128204, p.grad.std().item() = 1.1469480991363525\n",
      "n = 'layers.0.weight', p.grad.mean().item() = -15.3952054977417, p.grad.std().item() = 1360.3341064453125\n",
      "n = 'layers.0.bias', p.grad.mean().item() = -944.8362426757812, p.grad.std().item() = 1353.941162109375\n",
      "n = 'layers.2.weight', p.grad.mean().item() = 10401.814453125, p.grad.std().item() = 50600.67578125\n",
      "n = 'layers.2.bias', p.grad.mean().item() = -0.0008487701416015625, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.00041358586167916656, p.std().item() = 0.9409260749816895\n",
      "n = 'label_factors.weight', p.mean().item() = 0.002097632270306349, p.std().item() = 0.9427318572998047\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0014344570226967335, p.std().item() = 0.07790683209896088\n",
      "n = 'layers.0.bias', p.mean().item() = -0.10848177969455719, p.std().item() = 0.07812932878732681\n",
      "n = 'layers.2.weight', p.mean().item() = -0.0013668545288965106, p.std().item() = 0.05736900493502617\n",
      "n = 'layers.2.bias', p.mean().item() = 0.12700803577899933, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 62\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.00041358586167916656, p.std().item() = 0.9409260749816895\n",
      "n = 'label_factors.weight', p.mean().item() = 0.002097632270306349, p.std().item() = 0.9427318572998047\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0014344570226967335, p.std().item() = 0.07790683209896088\n",
      "n = 'layers.0.bias', p.mean().item() = -0.10848177969455719, p.std().item() = 0.07812932878732681\n",
      "n = 'layers.2.weight', p.mean().item() = -0.0013668545288965106, p.std().item() = 0.05736900493502617\n",
      "n = 'layers.2.bias', p.mean().item() = 0.12700803577899933, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -0.788769006729126, preds.std().item() = 1.6054779291152954\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.7302, device='cuda:0'), loss.std() = tensor(0.0683, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 8.18194609286671e-11, lambda_i.std().item() = 0.224408358335495\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = 0.0006892665405757725, p.grad.std().item() = 26.629579544067383\n",
      "n = 'label_factors.weight', p.grad.mean().item() = -0.024474846199154854, p.grad.std().item() = 1.2582238912582397\n",
      "n = 'layers.0.weight', p.grad.mean().item() = 11.755592346191406, p.grad.std().item() = 1458.7733154296875\n",
      "n = 'layers.0.bias', p.grad.mean().item() = 611.2501220703125, p.grad.std().item() = 1244.01416015625\n",
      "n = 'layers.2.weight', p.grad.mean().item() = 5632.072265625, p.grad.std().item() = 42369.03125\n",
      "n = 'layers.2.bias', p.grad.mean().item() = 0.0001811981201171875, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0004395563737489283, p.std().item() = 0.9400056600570679\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0020983205176889896, p.std().item() = 0.9419362545013428\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0014064296847209334, p.std().item() = 0.07846145331859589\n",
      "n = 'layers.0.bias', p.mean().item() = -0.10941159725189209, p.std().item() = 0.07966303080320358\n",
      "n = 'layers.2.weight', p.mean().item() = -0.002138787182047963, p.std().item() = 0.05776688829064369\n",
      "n = 'layers.2.bias', p.mean().item() = 0.12849201261997223, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 63\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0004395563737489283, p.std().item() = 0.9400056600570679\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0020983205176889896, p.std().item() = 0.9419362545013428\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0014064296847209334, p.std().item() = 0.07846145331859589\n",
      "n = 'layers.0.bias', p.mean().item() = -0.10941159725189209, p.std().item() = 0.07966303080320358\n",
      "n = 'layers.2.weight', p.mean().item() = -0.002138787182047963, p.std().item() = 0.05776688829064369\n",
      "n = 'layers.2.bias', p.mean().item() = 0.12849201261997223, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -0.756932258605957, preds.std().item() = 1.7797585725784302\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.7309, device='cuda:0'), loss.std() = tensor(0.0767, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 8.573302484604639e-11, lambda_i.std().item() = 0.22277705371379852\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = 0.002786799566820264, p.grad.std().item() = 25.75371742248535\n",
      "n = 'label_factors.weight', p.grad.mean().item() = -0.00042235286673530936, p.grad.std().item() = 1.1984666585922241\n",
      "n = 'layers.0.weight', p.grad.mean().item() = 2.213824987411499, p.grad.std().item() = 1385.64208984375\n",
      "n = 'layers.0.bias', p.grad.mean().item() = 27.721710205078125, p.grad.std().item() = 837.9625854492188\n",
      "n = 'layers.2.weight', p.grad.mean().item() = 1924.1407470703125, p.grad.std().item() = 25295.41015625\n",
      "n = 'layers.2.bias', p.grad.mean().item() = 0.000186920166015625, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.00046601510257460177, p.std().item() = 0.9390863180160522\n",
      "n = 'label_factors.weight', p.mean().item() = 0.002095166128128767, p.std().item() = 0.9411537647247314\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0013763029128313065, p.std().item() = 0.07900159060955048\n",
      "n = 'layers.0.bias', p.mean().item() = -0.11032190918922424, p.std().item() = 0.08126045763492584\n",
      "n = 'layers.2.weight', p.mean().item() = -0.0028460759203881025, p.std().item() = 0.057985953986644745\n",
      "n = 'layers.2.bias', p.mean().item() = 0.12940165400505066, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 64\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.00046601510257460177, p.std().item() = 0.9390863180160522\n",
      "n = 'label_factors.weight', p.mean().item() = 0.002095166128128767, p.std().item() = 0.9411537647247314\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0013763029128313065, p.std().item() = 0.07900159060955048\n",
      "n = 'layers.0.bias', p.mean().item() = -0.11032190918922424, p.std().item() = 0.08126045763492584\n",
      "n = 'layers.2.weight', p.mean().item() = -0.0028460759203881025, p.std().item() = 0.057985953986644745\n",
      "n = 'layers.2.bias', p.mean().item() = 0.12940165400505066, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -0.8090407252311707, preds.std().item() = 1.6892842054367065\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.7377, device='cuda:0'), loss.std() = tensor(0.0580, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 7.007878999321093e-11, lambda_i.std().item() = 0.20824335515499115\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = 0.005680401809513569, p.grad.std().item() = 25.533193588256836\n",
      "n = 'label_factors.weight', p.grad.mean().item() = 0.00619525695219636, p.grad.std().item() = 1.2606796026229858\n",
      "n = 'layers.0.weight', p.grad.mean().item() = 1.8377209901809692, p.grad.std().item() = 1377.161376953125\n",
      "n = 'layers.0.bias', p.grad.mean().item() = 17.993560791015625, p.grad.std().item() = 1232.1129150390625\n",
      "n = 'layers.2.weight', p.grad.mean().item() = 1755.8365478515625, p.grad.std().item() = 39853.546875\n",
      "n = 'layers.2.bias', p.grad.mean().item() = 6.198883056640625e-05, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0004931120201945305, p.std().item() = 0.9381681084632874\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0020710828248411417, p.std().item() = 0.9404134750366211\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0013875506119802594, p.std().item() = 0.07968258857727051\n",
      "n = 'layers.0.bias', p.mean().item() = -0.11132153123617172, p.std().item() = 0.08317077904939651\n",
      "n = 'layers.2.weight', p.mean().item() = -0.003599775955080986, p.std().item() = 0.05846110358834267\n",
      "n = 'layers.2.bias', p.mean().item() = 0.1300739347934723, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 65\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0004931120201945305, p.std().item() = 0.9381681084632874\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0020710828248411417, p.std().item() = 0.9404134750366211\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0013875506119802594, p.std().item() = 0.07968258857727051\n",
      "n = 'layers.0.bias', p.mean().item() = -0.11132153123617172, p.std().item() = 0.08317077904939651\n",
      "n = 'layers.2.weight', p.mean().item() = -0.003599775955080986, p.std().item() = 0.05846110358834267\n",
      "n = 'layers.2.bias', p.mean().item() = 0.1300739347934723, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -0.9412024617195129, preds.std().item() = 1.8011783361434937\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.7462, device='cuda:0'), loss.std() = tensor(0.0712, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 7.858425715712158e-11, lambda_i.std().item() = 0.2067740261554718\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = 0.0012611454585567117, p.grad.std().item() = 28.24112892150879\n",
      "n = 'label_factors.weight', p.grad.mean().item() = -0.014393975026905537, p.grad.std().item() = 1.2011668682098389\n",
      "n = 'layers.0.weight', p.grad.mean().item() = 19.706260681152344, p.grad.std().item() = 1677.873779296875\n",
      "n = 'layers.0.bias', p.grad.mean().item() = 483.07708740234375, p.grad.std().item() = 920.0528564453125\n",
      "n = 'layers.2.weight', p.grad.mean().item() = -7664.70458984375, p.grad.std().item() = 49009.74609375\n",
      "n = 'layers.2.bias', p.grad.mean().item() = 2.6702880859375e-05, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0005203172913752496, p.std().item() = 0.9372508525848389\n",
      "n = 'label_factors.weight', p.mean().item() = 0.002071239287033677, p.std().item() = 0.9396896362304688\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0014164531603455544, p.std().item() = 0.08041393756866455\n",
      "n = 'layers.0.bias', p.mean().item() = -0.11269383877515793, p.std().item() = 0.08515197038650513\n",
      "n = 'layers.2.weight', p.mean().item() = -0.004044720437377691, p.std().item() = 0.0586501881480217\n",
      "n = 'layers.2.bias', p.mean().item() = 0.1306108683347702, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 66\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0005203172913752496, p.std().item() = 0.9372508525848389\n",
      "n = 'label_factors.weight', p.mean().item() = 0.002071239287033677, p.std().item() = 0.9396896362304688\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0014164531603455544, p.std().item() = 0.08041393756866455\n",
      "n = 'layers.0.bias', p.mean().item() = -0.11269383877515793, p.std().item() = 0.08515197038650513\n",
      "n = 'layers.2.weight', p.mean().item() = -0.004044720437377691, p.std().item() = 0.0586501881480217\n",
      "n = 'layers.2.bias', p.mean().item() = 0.1306108683347702, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -0.9893608689308167, preds.std().item() = 1.758284091949463\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.7419, device='cuda:0'), loss.std() = tensor(0.0678, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 8.972485254776785e-11, lambda_i.std().item() = 0.20869478583335876\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = 0.00045900564873591065, p.grad.std().item() = 30.4207820892334\n",
      "n = 'label_factors.weight', p.grad.mean().item() = 0.011389541439712048, p.grad.std().item() = 1.1832469701766968\n",
      "n = 'layers.0.weight', p.grad.mean().item() = -13.92588996887207, p.grad.std().item() = 1568.6539306640625\n",
      "n = 'layers.0.bias', p.grad.mean().item() = -311.5047302246094, p.grad.std().item() = 1030.6016845703125\n",
      "n = 'layers.2.weight', p.grad.mean().item() = -428.169677734375, p.grad.std().item() = 32204.423828125\n",
      "n = 'layers.2.bias', p.grad.mean().item() = -0.00037384033203125, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0005484545254148543, p.std().item() = 0.9363349080085754\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0020553735084831715, p.std().item() = 0.9389652013778687\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0014366720570251346, p.std().item() = 0.08112841099500656\n",
      "n = 'layers.0.bias', p.mean().item() = -0.11383403837680817, p.std().item() = 0.08716010302305222\n",
      "n = 'layers.2.weight', p.mean().item() = -0.004491342697292566, p.std().item() = 0.059001341462135315\n",
      "n = 'layers.2.bias', p.mean().item() = 0.1319485455751419, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 67\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0005484545254148543, p.std().item() = 0.9363349080085754\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0020553735084831715, p.std().item() = 0.9389652013778687\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0014366720570251346, p.std().item() = 0.08112841099500656\n",
      "n = 'layers.0.bias', p.mean().item() = -0.11383403837680817, p.std().item() = 0.08716010302305222\n",
      "n = 'layers.2.weight', p.mean().item() = -0.004491342697292566, p.std().item() = 0.059001341462135315\n",
      "n = 'layers.2.bias', p.mean().item() = 0.1319485455751419, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -1.198064923286438, preds.std().item() = 1.9235233068466187\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.7659, device='cuda:0'), loss.std() = tensor(0.0755, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 5.395492733151208e-11, lambda_i.std().item() = 0.2107360064983368\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = -0.0019861399196088314, p.grad.std().item() = 29.283966064453125\n",
      "n = 'label_factors.weight', p.grad.mean().item() = -0.024747716262936592, p.grad.std().item() = 1.232332468032837\n",
      "n = 'layers.0.weight', p.grad.mean().item() = 16.698339462280273, p.grad.std().item() = 1513.86767578125\n",
      "n = 'layers.0.bias', p.grad.mean().item() = 605.9649658203125, p.grad.std().item() = 1021.107666015625\n",
      "n = 'layers.2.weight', p.grad.mean().item() = -4483.69873046875, p.grad.std().item() = 33708.5546875\n",
      "n = 'layers.2.bias', p.grad.mean().item() = -2.574920654296875e-05, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0005770930438302457, p.std().item() = 0.9354204535484314\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0020861378870904446, p.std().item() = 0.9382141828536987\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0014346400275826454, p.std().item() = 0.08175776153802872\n",
      "n = 'layers.0.bias', p.mean().item() = -0.1153370663523674, p.std().item() = 0.08900324255228043\n",
      "n = 'layers.2.weight', p.mean().item() = -0.004793798550963402, p.std().item() = 0.058908749371767044\n",
      "n = 'layers.2.bias', p.mean().item() = 0.1332114040851593, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 68\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0005770930438302457, p.std().item() = 0.9354204535484314\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0020861378870904446, p.std().item() = 0.9382141828536987\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0014346400275826454, p.std().item() = 0.08175776153802872\n",
      "n = 'layers.0.bias', p.mean().item() = -0.1153370663523674, p.std().item() = 0.08900324255228043\n",
      "n = 'layers.2.weight', p.mean().item() = -0.004793798550963402, p.std().item() = 0.058908749371767044\n",
      "n = 'layers.2.bias', p.mean().item() = 0.1332114040851593, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -1.1817456483840942, preds.std().item() = 1.9311935901641846\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.7366, device='cuda:0'), loss.std() = tensor(0.0730, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 9.580391341357242e-11, lambda_i.std().item() = 0.20922201871871948\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = -0.0010980821680277586, p.grad.std().item() = 29.539386749267578\n",
      "n = 'label_factors.weight', p.grad.mean().item() = 0.022929655387997627, p.grad.std().item() = 1.422630786895752\n",
      "n = 'layers.0.weight', p.grad.mean().item() = -1.1455968618392944, p.grad.std().item() = 1498.7791748046875\n",
      "n = 'layers.0.bias', p.grad.mean().item() = -429.3090515136719, p.grad.std().item() = 1097.659912109375\n",
      "n = 'layers.2.weight', p.grad.mean().item() = -2387.189453125, p.grad.std().item() = 43952.96875\n",
      "n = 'layers.2.bias', p.grad.mean().item() = -0.00042247772216796875, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0006062695174477994, p.std().item() = 0.9345070719718933\n",
      "n = 'label_factors.weight', p.mean().item() = 0.002075811382383108, p.std().item() = 0.9374839067459106\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0014620694564655423, p.std().item() = 0.08247397094964981\n",
      "n = 'layers.0.bias', p.mean().item() = -0.11655662208795547, p.std().item() = 0.09103868901729584\n",
      "n = 'layers.2.weight', p.mean().item() = -0.005128311458975077, p.std().item() = 0.059183038771152496\n",
      "n = 'layers.2.bias', p.mean().item() = 0.1353219747543335, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 69\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0006062695174477994, p.std().item() = 0.9345070719718933\n",
      "n = 'label_factors.weight', p.mean().item() = 0.002075811382383108, p.std().item() = 0.9374839067459106\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0014620694564655423, p.std().item() = 0.08247397094964981\n",
      "n = 'layers.0.bias', p.mean().item() = -0.11655662208795547, p.std().item() = 0.09103868901729584\n",
      "n = 'layers.2.weight', p.mean().item() = -0.005128311458975077, p.std().item() = 0.059183038771152496\n",
      "n = 'layers.2.bias', p.mean().item() = 0.1353219747543335, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -1.3123875856399536, preds.std().item() = 1.9870566129684448\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.7553, device='cuda:0'), loss.std() = tensor(0.0861, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 9.2568702703133e-11, lambda_i.std().item() = 0.21206898987293243\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = 0.0011405979748815298, p.grad.std().item() = 25.603511810302734\n",
      "n = 'label_factors.weight', p.grad.mean().item() = -0.0257429126650095, p.grad.std().item() = 1.271328330039978\n",
      "n = 'layers.0.weight', p.grad.mean().item() = -15.642457008361816, p.grad.std().item() = 1399.580322265625\n",
      "n = 'layers.0.bias', p.grad.mean().item() = 611.5844116210938, p.grad.std().item() = 1202.4488525390625\n",
      "n = 'layers.2.weight', p.grad.mean().item() = 4352.31005859375, p.grad.std().item() = 35599.25\n",
      "n = 'layers.2.bias', p.grad.mean().item() = 0.00026416778564453125, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0006353999488055706, p.std().item() = 0.9335945844650269\n",
      "n = 'label_factors.weight', p.mean().item() = 0.002104556653648615, p.std().item() = 0.936748743057251\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0014896984212100506, p.std().item() = 0.08315970003604889\n",
      "n = 'layers.0.bias', p.mean().item() = -0.11810966581106186, p.std().item() = 0.09296005219221115\n",
      "n = 'layers.2.weight', p.mean().item() = -0.0054680597968399525, p.std().item() = 0.059067148715257645\n",
      "n = 'layers.2.bias', p.mean().item() = 0.13659584522247314, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 70\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0006353999488055706, p.std().item() = 0.9335945844650269\n",
      "n = 'label_factors.weight', p.mean().item() = 0.002104556653648615, p.std().item() = 0.936748743057251\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0014896984212100506, p.std().item() = 0.08315970003604889\n",
      "n = 'layers.0.bias', p.mean().item() = -0.11810966581106186, p.std().item() = 0.09296005219221115\n",
      "n = 'layers.2.weight', p.mean().item() = -0.0054680597968399525, p.std().item() = 0.059067148715257645\n",
      "n = 'layers.2.bias', p.mean().item() = 0.13659584522247314, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -1.221932291984558, preds.std().item() = 2.1151089668273926\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.7507, device='cuda:0'), loss.std() = tensor(0.0824, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 7.086149722557167e-11, lambda_i.std().item() = 0.19975733757019043\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = -0.0012994257267564535, p.grad.std().item() = 27.028667449951172\n",
      "n = 'label_factors.weight', p.grad.mean().item() = -0.061674199998378754, p.grad.std().item() = 1.6333155632019043\n",
      "n = 'layers.0.weight', p.grad.mean().item() = -32.9069938659668, p.grad.std().item() = 1377.270263671875\n",
      "n = 'layers.0.bias', p.grad.mean().item() = 1441.153076171875, p.grad.std().item() = 1700.3577880859375\n",
      "n = 'layers.2.weight', p.grad.mean().item() = 1335.406005859375, p.grad.std().item() = 68273.265625\n",
      "n = 'layers.2.bias', p.grad.mean().item() = -0.000278472900390625, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0006643451051786542, p.std().item() = 0.9326825737953186\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0022194692865014076, p.std().item() = 0.9360089898109436\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0015010974602773786, p.std().item() = 0.08380693942308426\n",
      "n = 'layers.0.bias', p.mean().item() = -0.12051185965538025, p.std().item() = 0.09449004381895065\n",
      "n = 'layers.2.weight', p.mean().item() = -0.005570616573095322, p.std().item() = 0.05789659172296524\n",
      "n = 'layers.2.bias', p.mean().item() = 0.13839316368103027, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 71\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0006643451051786542, p.std().item() = 0.9326825737953186\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0022194692865014076, p.std().item() = 0.9360089898109436\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0015010974602773786, p.std().item() = 0.08380693942308426\n",
      "n = 'layers.0.bias', p.mean().item() = -0.12051185965538025, p.std().item() = 0.09449004381895065\n",
      "n = 'layers.2.weight', p.mean().item() = -0.005570616573095322, p.std().item() = 0.05789659172296524\n",
      "n = 'layers.2.bias', p.mean().item() = 0.13839316368103027, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -1.230927586555481, preds.std().item() = 2.058295726776123\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.7484, device='cuda:0'), loss.std() = tensor(0.0767, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 5.49463599619493e-11, lambda_i.std().item() = 0.21181835234165192\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = -0.0019074279116466641, p.grad.std().item() = 27.029739379882812\n",
      "n = 'label_factors.weight', p.grad.mean().item() = -0.021801413968205452, p.grad.std().item() = 1.344342827796936\n",
      "n = 'layers.0.weight', p.grad.mean().item() = 13.903925895690918, p.grad.std().item() = 1336.928466796875\n",
      "n = 'layers.0.bias', p.grad.mean().item() = 543.6961669921875, p.grad.std().item() = 1004.4195556640625\n",
      "n = 'layers.2.weight', p.grad.mean().item() = 9456.294921875, p.grad.std().item() = 33544.09765625\n",
      "n = 'layers.2.bias', p.grad.mean().item() = 0.0002589225769042969, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0006934009143151343, p.std().item() = 0.9317716956138611\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0023545545991510153, p.std().item() = 0.9352335333824158\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0014889856101945043, p.std().item() = 0.0843527540564537\n",
      "n = 'layers.0.bias', p.mean().item() = -0.12301373481750488, p.std().item() = 0.09581612795591354\n",
      "n = 'layers.2.weight', p.mean().item() = -0.005867773666977882, p.std().item() = 0.0564902164041996\n",
      "n = 'layers.2.bias', p.mean().item() = 0.1393890529870987, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 72\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0006934009143151343, p.std().item() = 0.9317716956138611\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0023545545991510153, p.std().item() = 0.9352335333824158\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0014889856101945043, p.std().item() = 0.0843527540564537\n",
      "n = 'layers.0.bias', p.mean().item() = -0.12301373481750488, p.std().item() = 0.09581612795591354\n",
      "n = 'layers.2.weight', p.mean().item() = -0.005867773666977882, p.std().item() = 0.0564902164041996\n",
      "n = 'layers.2.bias', p.mean().item() = 0.1393890529870987, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -1.3548763990402222, preds.std().item() = 1.9547505378723145\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.7429, device='cuda:0'), loss.std() = tensor(0.0866, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 7.915824246085279e-11, lambda_i.std().item() = 0.21037770807743073\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = 0.003035617060959339, p.grad.std().item() = 30.069461822509766\n",
      "n = 'label_factors.weight', p.grad.mean().item() = 0.007347023580223322, p.grad.std().item() = 1.3651920557022095\n",
      "n = 'layers.0.weight', p.grad.mean().item() = 30.536123275756836, p.grad.std().item() = 1541.208740234375\n",
      "n = 'layers.0.bias', p.grad.mean().item() = -208.69927978515625, p.grad.std().item() = 968.6878662109375\n",
      "n = 'layers.2.weight', p.grad.mean().item() = 15566.6171875, p.grad.std().item() = 29432.96484375\n",
      "n = 'layers.2.bias', p.grad.mean().item() = 0.00016498565673828125, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0007226094021461904, p.std().item() = 0.9308621883392334\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0024568464141339064, p.std().item() = 0.9344925880432129\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0015286970883607864, p.std().item() = 0.08502357453107834\n",
      "n = 'layers.0.bias', p.mean().item() = -0.12509970366954803, p.std().item() = 0.09714917838573456\n",
      "n = 'layers.2.weight', p.mean().item() = -0.006519424729049206, p.std().item() = 0.05539606139063835\n",
      "n = 'layers.2.bias', p.mean().item() = 0.13988280296325684, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 73\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0007226094021461904, p.std().item() = 0.9308621883392334\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0024568464141339064, p.std().item() = 0.9344925880432129\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0015286970883607864, p.std().item() = 0.08502357453107834\n",
      "n = 'layers.0.bias', p.mean().item() = -0.12509970366954803, p.std().item() = 0.09714917838573456\n",
      "n = 'layers.2.weight', p.mean().item() = -0.006519424729049206, p.std().item() = 0.05539606139063835\n",
      "n = 'layers.2.bias', p.mean().item() = 0.13988280296325684, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -1.162660837173462, preds.std().item() = 1.8588660955429077\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.7609, device='cuda:0'), loss.std() = tensor(0.0927, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 9.596045486004456e-11, lambda_i.std().item() = 0.2136366367340088\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = 0.002317051636055112, p.grad.std().item() = 24.491670608520508\n",
      "n = 'label_factors.weight', p.grad.mean().item() = 0.010854863561689854, p.grad.std().item() = 1.2356334924697876\n",
      "n = 'layers.0.weight', p.grad.mean().item() = -6.3804402351379395, p.grad.std().item() = 1218.647705078125\n",
      "n = 'layers.0.bias', p.grad.mean().item() = -253.16416931152344, p.grad.std().item() = 955.3192138671875\n",
      "n = 'layers.2.weight', p.grad.mean().item() = 7706.59912109375, p.grad.std().item() = 28194.572265625\n",
      "n = 'layers.2.bias', p.grad.mean().item() = -0.00038433074951171875, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0007519443170167506, p.std().item() = 0.9299540519714355\n",
      "n = 'label_factors.weight', p.mean().item() = 0.002534026512876153, p.std().item() = 0.9337284564971924\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0015690596774220467, p.std().item() = 0.08559881150722504\n",
      "n = 'layers.0.bias', p.mean().item() = -0.12681947648525238, p.std().item() = 0.09846089035272598\n",
      "n = 'layers.2.weight', p.mean().item() = -0.007341892924159765, p.std().item() = 0.054750360548496246\n",
      "n = 'layers.2.bias', p.mean().item() = 0.14123617112636566, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 74\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0007519443170167506, p.std().item() = 0.9299540519714355\n",
      "n = 'label_factors.weight', p.mean().item() = 0.002534026512876153, p.std().item() = 0.9337284564971924\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0015690596774220467, p.std().item() = 0.08559881150722504\n",
      "n = 'layers.0.bias', p.mean().item() = -0.12681947648525238, p.std().item() = 0.09846089035272598\n",
      "n = 'layers.2.weight', p.mean().item() = -0.007341892924159765, p.std().item() = 0.054750360548496246\n",
      "n = 'layers.2.bias', p.mean().item() = 0.14123617112636566, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -1.501112937927246, preds.std().item() = 1.9290276765823364\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.7441, device='cuda:0'), loss.std() = tensor(0.0806, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 6.908735389332676e-11, lambda_i.std().item() = 0.2559370696544647\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = -0.0017835694598034024, p.grad.std().item() = 40.04988479614258\n",
      "n = 'label_factors.weight', p.grad.mean().item() = 0.0537237673997879, p.grad.std().item() = 1.458107590675354\n",
      "n = 'layers.0.weight', p.grad.mean().item() = -1.9950401782989502, p.grad.std().item() = 1872.9986572265625\n",
      "n = 'layers.0.bias', p.grad.mean().item() = -1340.5057373046875, p.grad.std().item() = 1738.71484375\n",
      "n = 'layers.2.weight', p.grad.mean().item() = -5595.2177734375, p.grad.std().item() = 66814.8046875\n",
      "n = 'layers.2.bias', p.grad.mean().item() = -0.0001220703125, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0007822656189091504, p.std().item() = 0.9290477633476257\n",
      "n = 'label_factors.weight', p.mean().item() = 0.002546630334109068, p.std().item() = 0.9329623579978943\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0016070589190348983, p.std().item() = 0.08618468046188354\n",
      "n = 'layers.0.bias', p.mean().item() = -0.12747099995613098, p.std().item() = 0.0999264270067215\n",
      "n = 'layers.2.weight', p.mean().item() = -0.008119682781398296, p.std().item() = 0.05524098128080368\n",
      "n = 'layers.2.bias', p.mean().item() = 0.14274580776691437, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 75\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0007822656189091504, p.std().item() = 0.9290477633476257\n",
      "n = 'label_factors.weight', p.mean().item() = 0.002546630334109068, p.std().item() = 0.9329623579978943\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0016070589190348983, p.std().item() = 0.08618468046188354\n",
      "n = 'layers.0.bias', p.mean().item() = -0.12747099995613098, p.std().item() = 0.0999264270067215\n",
      "n = 'layers.2.weight', p.mean().item() = -0.008119682781398296, p.std().item() = 0.05524098128080368\n",
      "n = 'layers.2.bias', p.mean().item() = 0.14274580776691437, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -1.572866439819336, preds.std().item() = 2.0645132064819336\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.7519, device='cuda:0'), loss.std() = tensor(0.0934, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 8.80811604209164e-11, lambda_i.std().item() = 0.21409226953983307\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = 0.00010283286246703938, p.grad.std().item() = 28.562156677246094\n",
      "n = 'label_factors.weight', p.grad.mean().item() = 0.06383731216192245, p.grad.std().item() = 1.5584224462509155\n",
      "n = 'layers.0.weight', p.grad.mean().item() = 26.32097816467285, p.grad.std().item() = 1467.8406982421875\n",
      "n = 'layers.0.bias', p.grad.mean().item() = -1357.2139892578125, p.grad.std().item() = 1658.302978515625\n",
      "n = 'layers.2.weight', p.grad.mean().item() = -1968.25146484375, p.grad.std().item() = 70729.4765625\n",
      "n = 'layers.2.bias', p.grad.mean().item() = -0.00023651123046875, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0008130273781716824, p.std().item() = 0.928143322467804\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0024836347438395023, p.std().item() = 0.9321836233139038\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0016425466164946556, p.std().item() = 0.0867631807923317\n",
      "n = 'layers.0.bias', p.mean().item() = -0.1272016167640686, p.std().item() = 0.10167285054922104\n",
      "n = 'layers.2.weight', p.mean().item() = -0.008954313583672047, p.std().item() = 0.056758083403110504\n",
      "n = 'layers.2.bias', p.mean().item() = 0.14467394351959229, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 76\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0008130273781716824, p.std().item() = 0.928143322467804\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0024836347438395023, p.std().item() = 0.9321836233139038\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0016425466164946556, p.std().item() = 0.0867631807923317\n",
      "n = 'layers.0.bias', p.mean().item() = -0.1272016167640686, p.std().item() = 0.10167285054922104\n",
      "n = 'layers.2.weight', p.mean().item() = -0.008954313583672047, p.std().item() = 0.056758083403110504\n",
      "n = 'layers.2.bias', p.mean().item() = 0.14467394351959229, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -1.647771954536438, preds.std().item() = 2.1263222694396973\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.7464, device='cuda:0'), loss.std() = tensor(0.0861, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 1.0561389957031153e-10, lambda_i.std().item() = 0.20377950370311737\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = -0.0007455070735886693, p.grad.std().item() = 30.262645721435547\n",
      "n = 'label_factors.weight', p.grad.mean().item() = 0.01780751906335354, p.grad.std().item() = 1.3723300695419312\n",
      "n = 'layers.0.weight', p.grad.mean().item() = 14.469393730163574, p.grad.std().item() = 1398.00634765625\n",
      "n = 'layers.0.bias', p.grad.mean().item() = -321.03839111328125, p.grad.std().item() = 977.91357421875\n",
      "n = 'layers.2.weight', p.grad.mean().item() = -1671.6068115234375, p.grad.std().item() = 34250.8359375\n",
      "n = 'layers.2.bias', p.grad.mean().item() = -0.0005655288696289062, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0008439241209998727, p.std().item() = 0.9272401928901672\n",
      "n = 'label_factors.weight', p.mean().item() = 0.002407316816970706, p.std().item() = 0.9314161539077759\n",
      "n = 'layers.0.weight', p.mean().item() = -0.001708792638964951, p.std().item() = 0.08736135065555573\n",
      "n = 'layers.0.bias', p.mean().item() = -0.12685254216194153, p.std().item() = 0.10342361032962799\n",
      "n = 'layers.2.weight', p.mean().item() = -0.00969959981739521, p.std().item() = 0.05848871171474457\n",
      "n = 'layers.2.bias', p.mean().item() = 0.1477418690919876, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 77\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0008439241209998727, p.std().item() = 0.9272401928901672\n",
      "n = 'label_factors.weight', p.mean().item() = 0.002407316816970706, p.std().item() = 0.9314161539077759\n",
      "n = 'layers.0.weight', p.mean().item() = -0.001708792638964951, p.std().item() = 0.08736135065555573\n",
      "n = 'layers.0.bias', p.mean().item() = -0.12685254216194153, p.std().item() = 0.10342361032962799\n",
      "n = 'layers.2.weight', p.mean().item() = -0.00969959981739521, p.std().item() = 0.05848871171474457\n",
      "n = 'layers.2.bias', p.mean().item() = 0.1477418690919876, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -1.6852471828460693, preds.std().item() = 2.3761720657348633\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.7357, device='cuda:0'), loss.std() = tensor(0.0946, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 9.990010207960864e-11, lambda_i.std().item() = 0.18116158246994019\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = 0.00158320099581033, p.grad.std().item() = 28.366989135742188\n",
      "n = 'label_factors.weight', p.grad.mean().item() = -0.03421757370233536, p.grad.std().item() = 1.620823621749878\n",
      "n = 'layers.0.weight', p.grad.mean().item() = -11.923681259155273, p.grad.std().item() = 1265.161376953125\n",
      "n = 'layers.0.bias', p.grad.mean().item() = 708.949462890625, p.grad.std().item() = 1014.159912109375\n",
      "n = 'layers.2.weight', p.grad.mean().item() = 8255.1279296875, p.grad.std().item() = 39263.328125\n",
      "n = 'layers.2.bias', p.grad.mean().item() = 0.00013065338134765625, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0008741211495362222, p.std().item() = 0.9263375997543335\n",
      "n = 'label_factors.weight', p.mean().item() = 0.002373842755332589, p.std().item() = 0.9306632876396179\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0017620290163904428, p.std().item() = 0.08798635005950928\n",
      "n = 'layers.0.bias', p.mean().item() = -0.12701396644115448, p.std().item() = 0.1049402579665184\n",
      "n = 'layers.2.weight', p.mean().item() = -0.010444876737892628, p.std().item() = 0.059547193348407745\n",
      "n = 'layers.2.bias', p.mean().item() = 0.15019090473651886, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 78\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0008741211495362222, p.std().item() = 0.9263375997543335\n",
      "n = 'label_factors.weight', p.mean().item() = 0.002373842755332589, p.std().item() = 0.9306632876396179\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0017620290163904428, p.std().item() = 0.08798635005950928\n",
      "n = 'layers.0.bias', p.mean().item() = -0.12701396644115448, p.std().item() = 0.1049402579665184\n",
      "n = 'layers.2.weight', p.mean().item() = -0.010444876737892628, p.std().item() = 0.059547193348407745\n",
      "n = 'layers.2.bias', p.mean().item() = 0.15019090473651886, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -1.8593666553497314, preds.std().item() = 2.427224636077881\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.7433, device='cuda:0'), loss.std() = tensor(0.1015, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 6.44954437079015e-11, lambda_i.std().item() = 0.20221960544586182\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = 0.0033197328448295593, p.grad.std().item() = 32.27437210083008\n",
      "n = 'label_factors.weight', p.grad.mean().item() = 0.008289181627333164, p.grad.std().item() = 1.4280136823654175\n",
      "n = 'layers.0.weight', p.grad.mean().item() = -18.929096221923828, p.grad.std().item() = 1418.2689208984375\n",
      "n = 'layers.0.bias', p.grad.mean().item() = -245.93373107910156, p.grad.std().item() = 1052.6397705078125\n",
      "n = 'layers.2.weight', p.grad.mean().item() = 19578.0, p.grad.std().item() = 36457.21875\n",
      "n = 'layers.2.bias', p.grad.mean().item() = -0.00022220611572265625, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0009039972210302949, p.std().item() = 0.92543625831604\n",
      "n = 'label_factors.weight', p.mean().item() = 0.002325571607798338, p.std().item() = 0.9299123883247375\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0017970000626519322, p.std().item() = 0.08858814835548401\n",
      "n = 'layers.0.bias', p.mean().item() = -0.12697410583496094, p.std().item() = 0.10648062080144882\n",
      "n = 'layers.2.weight', p.mean().item() = -0.01152802910655737, p.std().item() = 0.06056542322039604\n",
      "n = 'layers.2.bias', p.mean().item() = 0.15293650329113007, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 79\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0009039972210302949, p.std().item() = 0.92543625831604\n",
      "n = 'label_factors.weight', p.mean().item() = 0.002325571607798338, p.std().item() = 0.9299123883247375\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0017970000626519322, p.std().item() = 0.08858814835548401\n",
      "n = 'layers.0.bias', p.mean().item() = -0.12697410583496094, p.std().item() = 0.10648062080144882\n",
      "n = 'layers.2.weight', p.mean().item() = -0.01152802910655737, p.std().item() = 0.06056542322039604\n",
      "n = 'layers.2.bias', p.mean().item() = 0.15293650329113007, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -2.0587122440338135, preds.std().item() = 2.4523391723632812\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.8126, device='cuda:0'), loss.std() = tensor(0.1258, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 1.0963181751311168e-10, lambda_i.std().item() = 0.21664130687713623\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = 0.0029031261801719666, p.grad.std().item() = 32.868309020996094\n",
      "n = 'label_factors.weight', p.grad.mean().item() = 0.0009720505913719535, p.grad.std().item() = 1.5891379117965698\n",
      "n = 'layers.0.weight', p.grad.mean().item() = 1.4709495306015015, p.grad.std().item() = 1595.6964111328125\n",
      "n = 'layers.0.bias', p.grad.mean().item() = -150.67169189453125, p.grad.std().item() = 1266.462646484375\n",
      "n = 'layers.2.weight', p.grad.mean().item() = 14637.341796875, p.grad.std().item() = 37651.36328125\n",
      "n = 'layers.2.bias', p.grad.mean().item() = 0.00044345855712890625, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0009337522787973285, p.std().item() = 0.9245362281799316\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0022785004694014788, p.std().item() = 0.9291406869888306\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0017853379249572754, p.std().item() = 0.08907430619001389\n",
      "n = 'layers.0.bias', p.mean().item() = -0.1268751472234726, p.std().item() = 0.1082223430275917\n",
      "n = 'layers.2.weight', p.mean().item() = -0.012779442593455315, p.std().item() = 0.061265818774700165\n",
      "n = 'layers.2.bias', p.mean().item() = 0.1543004959821701, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 80\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0009337522787973285, p.std().item() = 0.9245362281799316\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0022785004694014788, p.std().item() = 0.9291406869888306\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0017853379249572754, p.std().item() = 0.08907430619001389\n",
      "n = 'layers.0.bias', p.mean().item() = -0.1268751472234726, p.std().item() = 0.1082223430275917\n",
      "n = 'layers.2.weight', p.mean().item() = -0.012779442593455315, p.std().item() = 0.061265818774700165\n",
      "n = 'layers.2.bias', p.mean().item() = 0.1543004959821701, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -2.149160385131836, preds.std().item() = 2.664134979248047\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.7972, device='cuda:0'), loss.std() = tensor(0.1315, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 6.199076668655934e-11, lambda_i.std().item() = 0.18578468263149261\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = 0.0007408579695038497, p.grad.std().item() = 36.16393280029297\n",
      "n = 'label_factors.weight', p.grad.mean().item() = -0.038477301597595215, p.grad.std().item() = 1.608323097229004\n",
      "n = 'layers.0.weight', p.grad.mean().item() = -56.26282501220703, p.grad.std().item() = 1651.111083984375\n",
      "n = 'layers.0.bias', p.grad.mean().item() = 906.09716796875, p.grad.std().item() = 1165.4229736328125\n",
      "n = 'layers.2.weight', p.grad.mean().item() = 11513.576171875, p.grad.std().item() = 61897.1796875\n",
      "n = 'layers.2.bias', p.grad.mean().item() = -0.0001678466796875, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0009624152444303036, p.std().item() = 0.923636257648468\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0022769642528146505, p.std().item() = 0.9283819198608398\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0017446221318095922, p.std().item() = 0.08955541998147964\n",
      "n = 'layers.0.bias', p.mean().item() = -0.12745486199855804, p.std().item() = 0.10968642681837082\n",
      "n = 'layers.2.weight', p.mean().item() = -0.013969178311526775, p.std().item() = 0.06094415485858917\n",
      "n = 'layers.2.bias', p.mean().item() = 0.15593452751636505, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 81\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0009624152444303036, p.std().item() = 0.923636257648468\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0022769642528146505, p.std().item() = 0.9283819198608398\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0017446221318095922, p.std().item() = 0.08955541998147964\n",
      "n = 'layers.0.bias', p.mean().item() = -0.12745486199855804, p.std().item() = 0.10968642681837082\n",
      "n = 'layers.2.weight', p.mean().item() = -0.013969178311526775, p.std().item() = 0.06094415485858917\n",
      "n = 'layers.2.bias', p.mean().item() = 0.15593452751636505, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -2.1403379440307617, preds.std().item() = 2.5879693031311035\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.7670, device='cuda:0'), loss.std() = tensor(0.0989, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 7.40445274538537e-11, lambda_i.std().item() = 0.1929784119129181\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = 0.000728493498172611, p.grad.std().item() = 27.32876205444336\n",
      "n = 'label_factors.weight', p.grad.mean().item() = -0.013732168823480606, p.grad.std().item() = 1.5089167356491089\n",
      "n = 'layers.0.weight', p.grad.mean().item() = -7.7192559242248535, p.grad.std().item() = 1170.1917724609375\n",
      "n = 'layers.0.bias', p.grad.mean().item() = 253.59803771972656, p.grad.std().item() = 715.21533203125\n",
      "n = 'layers.2.weight', p.grad.mean().item() = 7362.3974609375, p.grad.std().item() = 27687.14453125\n",
      "n = 'layers.2.bias', p.grad.mean().item() = -0.0002593994140625, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.000990316504612565, p.std().item() = 0.9227370023727417\n",
      "n = 'label_factors.weight', p.mean().item() = 0.002293001627549529, p.std().item() = 0.9276053309440613\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0016848428640514612, p.std().item() = 0.08995621651411057\n",
      "n = 'layers.0.bias', p.mean().item() = -0.12814486026763916, p.std().item() = 0.11100679636001587\n",
      "n = 'layers.2.weight', p.mean().item() = -0.01515056099742651, p.std().item() = 0.060454584658145905\n",
      "n = 'layers.2.bias', p.mean().item() = 0.15803784132003784, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 82\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.000990316504612565, p.std().item() = 0.9227370023727417\n",
      "n = 'label_factors.weight', p.mean().item() = 0.002293001627549529, p.std().item() = 0.9276053309440613\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0016848428640514612, p.std().item() = 0.08995621651411057\n",
      "n = 'layers.0.bias', p.mean().item() = -0.12814486026763916, p.std().item() = 0.11100679636001587\n",
      "n = 'layers.2.weight', p.mean().item() = -0.01515056099742651, p.std().item() = 0.060454584658145905\n",
      "n = 'layers.2.bias', p.mean().item() = 0.15803784132003784, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -2.333171844482422, preds.std().item() = 2.617741584777832\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.7992, device='cuda:0'), loss.std() = tensor(0.1242, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 7.94191448716397e-11, lambda_i.std().item() = 0.18247008323669434\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = 5.6567507272120565e-05, p.grad.std().item() = 34.092315673828125\n",
      "n = 'label_factors.weight', p.grad.mean().item() = -0.054382842034101486, p.grad.std().item() = 1.6117579936981201\n",
      "n = 'layers.0.weight', p.grad.mean().item() = -25.765758514404297, p.grad.std().item() = 1615.645263671875\n",
      "n = 'layers.0.bias', p.grad.mean().item() = 1082.996826171875, p.grad.std().item() = 1528.7589111328125\n",
      "n = 'layers.2.weight', p.grad.mean().item() = 9363.564453125, p.grad.std().item() = 57977.89453125\n",
      "n = 'layers.2.bias', p.grad.mean().item() = 0.00010013580322265625, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0010172248585149646, p.std().item() = 0.9218380451202393\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0023687565699219704, p.std().item() = 0.9268202185630798\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0016196668148040771, p.std().item() = 0.09032769501209259\n",
      "n = 'layers.0.bias', p.mean().item() = -0.12949980795383453, p.std().item() = 0.11208900064229965\n",
      "n = 'layers.2.weight', p.mean().item() = -0.016256701201200485, p.std().item() = 0.059229087084531784\n",
      "n = 'layers.2.bias', p.mean().item() = 0.15967993438243866, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 83\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0010172248585149646, p.std().item() = 0.9218380451202393\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0023687565699219704, p.std().item() = 0.9268202185630798\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0016196668148040771, p.std().item() = 0.09032769501209259\n",
      "n = 'layers.0.bias', p.mean().item() = -0.12949980795383453, p.std().item() = 0.11208900064229965\n",
      "n = 'layers.2.weight', p.mean().item() = -0.016256701201200485, p.std().item() = 0.059229087084531784\n",
      "n = 'layers.2.bias', p.mean().item() = 0.15967993438243866, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -2.549429178237915, preds.std().item() = 2.6285531520843506\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.7517, device='cuda:0'), loss.std() = tensor(0.1085, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 1.1004926830926465e-10, lambda_i.std().item() = 0.20102010667324066\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = 0.0029024919494986534, p.grad.std().item() = 30.448740005493164\n",
      "n = 'label_factors.weight', p.grad.mean().item() = 0.025567498058080673, p.grad.std().item() = 1.5252041816711426\n",
      "n = 'layers.0.weight', p.grad.mean().item() = -9.167072296142578, p.grad.std().item() = 1299.9774169921875\n",
      "n = 'layers.0.bias', p.grad.mean().item() = -557.0421142578125, p.grad.std().item() = 1207.244873046875\n",
      "n = 'layers.2.weight', p.grad.mean().item() = 5652.4833984375, p.grad.std().item() = 29197.34375\n",
      "n = 'layers.2.bias', p.grad.mean().item() = 6.771087646484375e-05, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0010436723241582513, p.std().item() = 0.9209404587745667\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0023983363062143326, p.std().item() = 0.9260590076446533\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0015512044774368405, p.std().item() = 0.09075399488210678\n",
      "n = 'layers.0.bias', p.mean().item() = -0.13036032021045685, p.std().item() = 0.11331421136856079\n",
      "n = 'layers.2.weight', p.mean().item() = -0.017413649708032608, p.std().item() = 0.05852371081709862\n",
      "n = 'layers.2.bias', p.mean().item() = 0.16098366677761078, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 84\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0010436723241582513, p.std().item() = 0.9209404587745667\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0023983363062143326, p.std().item() = 0.9260590076446533\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0015512044774368405, p.std().item() = 0.09075399488210678\n",
      "n = 'layers.0.bias', p.mean().item() = -0.13036032021045685, p.std().item() = 0.11331421136856079\n",
      "n = 'layers.2.weight', p.mean().item() = -0.017413649708032608, p.std().item() = 0.05852371081709862\n",
      "n = 'layers.2.bias', p.mean().item() = 0.16098366677761078, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -2.422860860824585, preds.std().item() = 2.7494091987609863\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.7950, device='cuda:0'), loss.std() = tensor(0.1326, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 9.157726660324883e-11, lambda_i.std().item() = 0.20140227675437927\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = -0.002853339770808816, p.grad.std().item() = 31.635108947753906\n",
      "n = 'label_factors.weight', p.grad.mean().item() = -0.036947816610336304, p.grad.std().item() = 1.5679110288619995\n",
      "n = 'layers.0.weight', p.grad.mean().item() = -52.65200424194336, p.grad.std().item() = 1389.9085693359375\n",
      "n = 'layers.0.bias', p.grad.mean().item() = 822.380126953125, p.grad.std().item() = 1056.6060791015625\n",
      "n = 'layers.2.weight', p.grad.mean().item() = 1899.8115234375, p.grad.std().item() = 51770.09765625\n",
      "n = 'layers.2.bias', p.grad.mean().item() = -4.38690185546875e-05, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0010695031378418207, p.std().item() = 0.9200437068939209\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0024836529046297073, p.std().item() = 0.9252537488937378\n",
      "n = 'layers.0.weight', p.mean().item() = -0.001411117729730904, p.std().item() = 0.09103727340698242\n",
      "n = 'layers.0.bias', p.mean().item() = -0.13169780373573303, p.std().item() = 0.11414831876754761\n",
      "n = 'layers.2.weight', p.mean().item() = -0.018364518880844116, p.std().item() = 0.05710799619555473\n",
      "n = 'layers.2.bias', p.mean().item() = 0.1622621864080429, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 85\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0010695031378418207, p.std().item() = 0.9200437068939209\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0024836529046297073, p.std().item() = 0.9252537488937378\n",
      "n = 'layers.0.weight', p.mean().item() = -0.001411117729730904, p.std().item() = 0.09103727340698242\n",
      "n = 'layers.0.bias', p.mean().item() = -0.13169780373573303, p.std().item() = 0.11414831876754761\n",
      "n = 'layers.2.weight', p.mean().item() = -0.018364518880844116, p.std().item() = 0.05710799619555473\n",
      "n = 'layers.2.bias', p.mean().item() = 0.1622621864080429, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -2.4420650005340576, preds.std().item() = 2.6597578525543213\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.7482, device='cuda:0'), loss.std() = tensor(0.1038, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 1.1208431405229646e-10, lambda_i.std().item() = 0.2008340209722519\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = -0.0033236430026590824, p.grad.std().item() = 26.95248031616211\n",
      "n = 'label_factors.weight', p.grad.mean().item() = 0.012179763987660408, p.grad.std().item() = 1.3392682075500488\n",
      "n = 'layers.0.weight', p.grad.mean().item() = -8.523544311523438, p.grad.std().item() = 1112.3682861328125\n",
      "n = 'layers.0.bias', p.grad.mean().item() = -107.56431579589844, p.grad.std().item() = 789.7516479492188\n",
      "n = 'layers.2.weight', p.grad.mean().item() = -15156.669921875, p.grad.std().item() = 26746.29296875\n",
      "n = 'layers.2.bias', p.grad.mean().item() = -0.000213623046875, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0010948858689516783, p.std().item() = 0.9191479682922363\n",
      "n = 'label_factors.weight', p.mean().item() = 0.002558707259595394, p.std().item() = 0.9244312644004822\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0012599921319633722, p.std().item() = 0.09130929410457611\n",
      "n = 'layers.0.bias', p.mean().item() = -0.1328754723072052, p.std().item() = 0.11482248455286026\n",
      "n = 'layers.2.weight', p.mean().item() = -0.01891399174928665, p.std().item() = 0.055865880101919174\n",
      "n = 'layers.2.bias', p.mean().item() = 0.16394802927970886, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 86\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0010948858689516783, p.std().item() = 0.9191479682922363\n",
      "n = 'label_factors.weight', p.mean().item() = 0.002558707259595394, p.std().item() = 0.9244312644004822\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0012599921319633722, p.std().item() = 0.09130929410457611\n",
      "n = 'layers.0.bias', p.mean().item() = -0.1328754723072052, p.std().item() = 0.11482248455286026\n",
      "n = 'layers.2.weight', p.mean().item() = -0.01891399174928665, p.std().item() = 0.055865880101919174\n",
      "n = 'layers.2.bias', p.mean().item() = 0.16394802927970886, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -2.506927967071533, preds.std().item() = 2.5936450958251953\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.7611, device='cuda:0'), loss.std() = tensor(0.1042, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 7.007878999321093e-11, lambda_i.std().item() = 0.19657425582408905\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = -0.002902305917814374, p.grad.std().item() = 29.496545791625977\n",
      "n = 'label_factors.weight', p.grad.mean().item() = -0.006241499911993742, p.grad.std().item() = 1.299261450767517\n",
      "n = 'layers.0.weight', p.grad.mean().item() = -14.529136657714844, p.grad.std().item() = 1314.1768798828125\n",
      "n = 'layers.0.bias', p.grad.mean().item() = 253.9252166748047, p.grad.std().item() = 967.6683349609375\n",
      "n = 'layers.2.weight', p.grad.mean().item() = -7329.72998046875, p.grad.std().item() = 28375.80859375\n",
      "n = 'layers.2.bias', p.grad.mean().item() = 0.000141143798828125, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0011198279680684209, p.std().item() = 0.9182529449462891\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0026408773846924305, p.std().item() = 0.9236106872558594\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0011169081553816795, p.std().item() = 0.0915997326374054\n",
      "n = 'layers.0.bias', p.mean().item() = -0.13409839570522308, p.std().item() = 0.11534886807203293\n",
      "n = 'layers.2.weight', p.mean().item() = -0.01924508437514305, p.std().item() = 0.054545994848012924\n",
      "n = 'layers.2.bias', p.mean().item() = 0.16509680449962616, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 87\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0011198279680684209, p.std().item() = 0.9182529449462891\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0026408773846924305, p.std().item() = 0.9236106872558594\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0011169081553816795, p.std().item() = 0.0915997326374054\n",
      "n = 'layers.0.bias', p.mean().item() = -0.13409839570522308, p.std().item() = 0.11534886807203293\n",
      "n = 'layers.2.weight', p.mean().item() = -0.01924508437514305, p.std().item() = 0.054545994848012924\n",
      "n = 'layers.2.bias', p.mean().item() = 0.16509680449962616, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -2.5698204040527344, preds.std().item() = 2.5635807514190674\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.7633, device='cuda:0'), loss.std() = tensor(0.0859, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 8.698536335671747e-11, lambda_i.std().item() = 0.2008604258298874\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = 0.0035622920840978622, p.grad.std().item() = 27.95442771911621\n",
      "n = 'label_factors.weight', p.grad.mean().item() = 0.052195869386196136, p.grad.std().item() = 1.5588772296905518\n",
      "n = 'layers.0.weight', p.grad.mean().item() = -28.9077205657959, p.grad.std().item() = 1301.2109375\n",
      "n = 'layers.0.bias', p.grad.mean().item() = -1462.8704833984375, p.grad.std().item() = 1916.8056640625\n",
      "n = 'layers.2.weight', p.grad.mean().item() = -4887.2958984375, p.grad.std().item() = 83447.734375\n",
      "n = 'layers.2.bias', p.grad.mean().item() = -0.000133514404296875, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0011446505086496472, p.std().item() = 0.9173594117164612\n",
      "n = 'label_factors.weight', p.mean().item() = 0.002639614976942539, p.std().item() = 0.922844648361206\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0009821243584156036, p.std().item() = 0.09208648651838303\n",
      "n = 'layers.0.bias', p.mean().item() = -0.13423693180084229, p.std().item() = 0.11646723002195358\n",
      "n = 'layers.2.weight', p.mean().item() = -0.019697096198797226, p.std().item() = 0.05469420552253723\n",
      "n = 'layers.2.bias', p.mean().item() = 0.16646744310855865, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 88\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0011446505086496472, p.std().item() = 0.9173594117164612\n",
      "n = 'label_factors.weight', p.mean().item() = 0.002639614976942539, p.std().item() = 0.922844648361206\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0009821243584156036, p.std().item() = 0.09208648651838303\n",
      "n = 'layers.0.bias', p.mean().item() = -0.13423693180084229, p.std().item() = 0.11646723002195358\n",
      "n = 'layers.2.weight', p.mean().item() = -0.019697096198797226, p.std().item() = 0.05469420552253723\n",
      "n = 'layers.2.bias', p.mean().item() = 0.16646744310855865, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -2.460087537765503, preds.std().item() = 2.5415258407592773\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.7642, device='cuda:0'), loss.std() = tensor(0.1116, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 6.371272953664686e-11, lambda_i.std().item() = 0.20477217435836792\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = 0.0011082340497523546, p.grad.std().item() = 27.610509872436523\n",
      "n = 'label_factors.weight', p.grad.mean().item() = 0.022459726780653, p.grad.std().item() = 1.280814290046692\n",
      "n = 'layers.0.weight', p.grad.mean().item() = 15.977059364318848, p.grad.std().item() = 1178.95947265625\n",
      "n = 'layers.0.bias', p.grad.mean().item() = -451.3932800292969, p.grad.std().item() = 896.2709350585938\n",
      "n = 'layers.2.weight', p.grad.mean().item() = 7227.8466796875, p.grad.std().item() = 32899.0078125\n",
      "n = 'layers.2.bias', p.grad.mean().item() = 0.0005083084106445312, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0011691524414345622, p.std().item() = 0.916467010974884\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0026106711011379957, p.std().item() = 0.9220656752586365\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0008575211395509541, p.std().item() = 0.09254509955644608\n",
      "n = 'layers.0.bias', p.mean().item() = -0.13410553336143494, p.std().item() = 0.11775995790958405\n",
      "n = 'layers.2.weight', p.mean().item() = -0.020320767536759377, p.std().item() = 0.055265288800001144\n",
      "n = 'layers.2.bias', p.mean().item() = 0.16636690497398376, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 89\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0011691524414345622, p.std().item() = 0.916467010974884\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0026106711011379957, p.std().item() = 0.9220656752586365\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0008575211395509541, p.std().item() = 0.09254509955644608\n",
      "n = 'layers.0.bias', p.mean().item() = -0.13410553336143494, p.std().item() = 0.11775995790958405\n",
      "n = 'layers.2.weight', p.mean().item() = -0.020320767536759377, p.std().item() = 0.055265288800001144\n",
      "n = 'layers.2.bias', p.mean().item() = 0.16636690497398376, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -2.559077024459839, preds.std().item() = 2.5690183639526367\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.7696, device='cuda:0'), loss.std() = tensor(0.1102, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 9.569955244925765e-11, lambda_i.std().item() = 0.19339510798454285\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = 0.0009680982329882681, p.grad.std().item() = 27.788524627685547\n",
      "n = 'label_factors.weight', p.grad.mean().item() = -0.03580959513783455, p.grad.std().item() = 1.408108115196228\n",
      "n = 'layers.0.weight', p.grad.mean().item() = -33.18265151977539, p.grad.std().item() = 1305.409423828125\n",
      "n = 'layers.0.bias', p.grad.mean().item() = 1061.9986572265625, p.grad.std().item() = 1337.383544921875\n",
      "n = 'layers.2.weight', p.grad.mean().item() = -177.62953186035156, p.grad.std().item() = 59371.15625\n",
      "n = 'layers.2.bias', p.grad.mean().item() = -0.00025844573974609375, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0011929655447602272, p.std().item() = 0.9155753254890442\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0026192371733486652, p.std().item() = 0.9212767481803894\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0007288835477083921, p.std().item() = 0.09292604774236679\n",
      "n = 'layers.0.bias', p.mean().item() = -0.13473080098628998, p.std().item() = 0.11867082118988037\n",
      "n = 'layers.2.weight', p.mean().item() = -0.020678598433732986, p.std().item() = 0.05491962656378746\n",
      "n = 'layers.2.bias', p.mean().item() = 0.1669275462627411, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 90\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0011929655447602272, p.std().item() = 0.9155753254890442\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0026192371733486652, p.std().item() = 0.9212767481803894\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0007288835477083921, p.std().item() = 0.09292604774236679\n",
      "n = 'layers.0.bias', p.mean().item() = -0.13473080098628998, p.std().item() = 0.11867082118988037\n",
      "n = 'layers.2.weight', p.mean().item() = -0.020678598433732986, p.std().item() = 0.05491962656378746\n",
      "n = 'layers.2.bias', p.mean().item() = 0.1669275462627411, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -2.3874199390411377, preds.std().item() = 2.519299030303955\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.7702, device='cuda:0'), loss.std() = tensor(0.0904, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 1.1385846432343527e-10, lambda_i.std().item() = 0.21670353412628174\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = 0.0003812300565186888, p.grad.std().item() = 23.247644424438477\n",
      "n = 'label_factors.weight', p.grad.mean().item() = 0.057214658707380295, p.grad.std().item() = 1.6873327493667603\n",
      "n = 'layers.0.weight', p.grad.mean().item() = 18.58108139038086, p.grad.std().item() = 1196.0855712890625\n",
      "n = 'layers.0.bias', p.grad.mean().item() = -1661.19775390625, p.grad.std().item() = 2251.69873046875\n",
      "n = 'layers.2.weight', p.grad.mean().item() = -694.0404663085938, p.grad.std().item() = 76587.2109375\n",
      "n = 'layers.2.bias', p.grad.mean().item() = 0.000324249267578125, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0012164595536887646, p.std().item() = 0.9146846532821655\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0025423860643059015, p.std().item() = 0.920562744140625\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0006754203350283206, p.std().item() = 0.09350568801164627\n",
      "n = 'layers.0.bias', p.mean().item() = -0.13433626294136047, p.std().item() = 0.1202673688530922\n",
      "n = 'layers.2.weight', p.mean().item() = -0.021167797967791557, p.std().item() = 0.055710356682538986\n",
      "n = 'layers.2.bias', p.mean().item() = 0.16657719016075134, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 91\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0012164595536887646, p.std().item() = 0.9146846532821655\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0025423860643059015, p.std().item() = 0.920562744140625\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0006754203350283206, p.std().item() = 0.09350568801164627\n",
      "n = 'layers.0.bias', p.mean().item() = -0.13433626294136047, p.std().item() = 0.1202673688530922\n",
      "n = 'layers.2.weight', p.mean().item() = -0.021167797967791557, p.std().item() = 0.055710356682538986\n",
      "n = 'layers.2.bias', p.mean().item() = 0.16657719016075134, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -2.638998031616211, preds.std().item() = 2.631531000137329\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.7625, device='cuda:0'), loss.std() = tensor(0.1147, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 1.1474553252011077e-10, lambda_i.std().item() = 0.21280211210250854\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = -0.002792449900880456, p.grad.std().item() = 26.146333694458008\n",
      "n = 'label_factors.weight', p.grad.mean().item() = 0.04445120692253113, p.grad.std().item() = 1.5578981637954712\n",
      "n = 'layers.0.weight', p.grad.mean().item() = 32.17649841308594, p.grad.std().item() = 1236.73583984375\n",
      "n = 'layers.0.bias', p.grad.mean().item() = -1380.5809326171875, p.grad.std().item() = 1900.7315673828125\n",
      "n = 'layers.2.weight', p.grad.mean().item() = -1949.3824462890625, p.grad.std().item() = 69054.328125\n",
      "n = 'layers.2.bias', p.grad.mean().item() = -0.000537872314453125, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.001239502802491188, p.std().item() = 0.9137954711914062\n",
      "n = 'label_factors.weight', p.mean().item() = 0.002416682429611683, p.std().item() = 0.9198790788650513\n",
      "n = 'layers.0.weight', p.mean().item() = -0.000672859838232398, p.std().item() = 0.0941721573472023\n",
      "n = 'layers.0.bias', p.mean().item() = -0.13312742114067078, p.std().item() = 0.12223915010690689\n",
      "n = 'layers.2.weight', p.mean().item() = -0.02175876498222351, p.std().item() = 0.057390641421079636\n",
      "n = 'layers.2.bias', p.mean().item() = 0.16762730479240417, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 92\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.001239502802491188, p.std().item() = 0.9137954711914062\n",
      "n = 'label_factors.weight', p.mean().item() = 0.002416682429611683, p.std().item() = 0.9198790788650513\n",
      "n = 'layers.0.weight', p.mean().item() = -0.000672859838232398, p.std().item() = 0.0941721573472023\n",
      "n = 'layers.0.bias', p.mean().item() = -0.13312742114067078, p.std().item() = 0.12223915010690689\n",
      "n = 'layers.2.weight', p.mean().item() = -0.02175876498222351, p.std().item() = 0.057390641421079636\n",
      "n = 'layers.2.bias', p.mean().item() = 0.16762730479240417, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -2.9280998706817627, preds.std().item() = 2.9699549674987793\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.7908, device='cuda:0'), loss.std() = tensor(0.1290, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 7.117458705740987e-11, lambda_i.std().item() = 0.19938445091247559\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = 0.00036972135421819985, p.grad.std().item() = 26.055465698242188\n",
      "n = 'label_factors.weight', p.grad.mean().item() = 0.008821533061563969, p.grad.std().item() = 1.465623140335083\n",
      "n = 'layers.0.weight', p.grad.mean().item() = -22.4599552154541, p.grad.std().item() = 1098.71826171875\n",
      "n = 'layers.0.bias', p.grad.mean().item() = -428.787109375, p.grad.std().item() = 1049.567138671875\n",
      "n = 'layers.2.weight', p.grad.mean().item() = -7016.85986328125, p.grad.std().item() = 34113.07421875\n",
      "n = 'layers.2.bias', p.grad.mean().item() = -9.5367431640625e-05, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0012617508182302117, p.std().item() = 0.912907063961029\n",
      "n = 'label_factors.weight', p.mean().item() = 0.002287772484123707, p.std().item() = 0.9192012548446655\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0006526220240630209, p.std().item() = 0.09485531598329544\n",
      "n = 'layers.0.bias', p.mean().item() = -0.1318276822566986, p.std().item() = 0.12428269535303116\n",
      "n = 'layers.2.weight', p.mean().item() = -0.022189965471625328, p.std().item() = 0.05932850390672684\n",
      "n = 'layers.2.bias', p.mean().item() = 0.16880963742733002, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 93\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0012617508182302117, p.std().item() = 0.912907063961029\n",
      "n = 'label_factors.weight', p.mean().item() = 0.002287772484123707, p.std().item() = 0.9192012548446655\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0006526220240630209, p.std().item() = 0.09485531598329544\n",
      "n = 'layers.0.bias', p.mean().item() = -0.1318276822566986, p.std().item() = 0.12428269535303116\n",
      "n = 'layers.2.weight', p.mean().item() = -0.022189965471625328, p.std().item() = 0.05932850390672684\n",
      "n = 'layers.2.bias', p.mean().item() = 0.16880963742733002, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -3.206573247909546, preds.std().item() = 3.1989526748657227\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.8047, device='cuda:0'), loss.std() = tensor(0.1452, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 9.267306366744776e-11, lambda_i.std().item() = 0.1901826709508896\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = 0.002064214553683996, p.grad.std().item() = 35.64094924926758\n",
      "n = 'label_factors.weight', p.grad.mean().item() = -0.049116842448711395, p.grad.std().item() = 1.8433011770248413\n",
      "n = 'layers.0.weight', p.grad.mean().item() = 12.676209449768066, p.grad.std().item() = 1555.162109375\n",
      "n = 'layers.0.bias', p.grad.mean().item() = 1494.1612548828125, p.grad.std().item() = 1771.2305908203125\n",
      "n = 'layers.2.weight', p.grad.mean().item() = -22855.138671875, p.grad.std().item() = 81824.9609375\n",
      "n = 'layers.2.bias', p.grad.mean().item() = 0.0001068115234375, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0012830591294914484, p.std().item() = 0.9120192527770996\n",
      "n = 'label_factors.weight', p.mean().item() = 0.002236422384157777, p.std().item() = 0.9184499382972717\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0006287622381933033, p.std().item() = 0.09532076120376587\n",
      "n = 'layers.0.bias', p.mean().item() = -0.1316484957933426, p.std().item() = 0.1256340593099594\n",
      "n = 'layers.2.weight', p.mean().item() = -0.021884843707084656, p.std().item() = 0.059767160564661026\n",
      "n = 'layers.2.bias', p.mean().item() = 0.16958653926849365, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 94\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0012830591294914484, p.std().item() = 0.9120192527770996\n",
      "n = 'label_factors.weight', p.mean().item() = 0.002236422384157777, p.std().item() = 0.9184499382972717\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0006287622381933033, p.std().item() = 0.09532076120376587\n",
      "n = 'layers.0.bias', p.mean().item() = -0.1316484957933426, p.std().item() = 0.1256340593099594\n",
      "n = 'layers.2.weight', p.mean().item() = -0.021884843707084656, p.std().item() = 0.059767160564661026\n",
      "n = 'layers.2.bias', p.mean().item() = 0.16958653926849365, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -3.2006070613861084, preds.std().item() = 3.1498613357543945\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.8378, device='cuda:0'), loss.std() = tensor(0.1435, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 8.275871654639388e-11, lambda_i.std().item() = 0.18090908229351044\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = 0.0019734613597393036, p.grad.std().item() = 29.85749053955078\n",
      "n = 'label_factors.weight', p.grad.mean().item() = -0.018479643389582634, p.grad.std().item() = 1.4353622198104858\n",
      "n = 'layers.0.weight', p.grad.mean().item() = 20.804508209228516, p.grad.std().item() = 1243.8885498046875\n",
      "n = 'layers.0.bias', p.grad.mean().item() = 519.234619140625, p.grad.std().item() = 1031.6195068359375\n",
      "n = 'layers.2.weight', p.grad.mean().item() = 519.9415283203125, p.grad.std().item() = 30154.607421875\n",
      "n = 'layers.2.bias', p.grad.mean().item() = -0.00011730194091796875, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0013035095762461424, p.std().item() = 0.9111320376396179\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0022112736478447914, p.std().item() = 0.9176831245422363\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0006138342432677746, p.std().item() = 0.09574030339717865\n",
      "n = 'layers.0.bias', p.mean().item() = -0.13188813626766205, p.std().item() = 0.12673398852348328\n",
      "n = 'layers.2.weight', p.mean().item() = -0.021515363827347755, p.std().item() = 0.05995362997055054\n",
      "n = 'layers.2.bias', p.mean().item() = 0.17058199644088745, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 95\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0013035095762461424, p.std().item() = 0.9111320376396179\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0022112736478447914, p.std().item() = 0.9176831245422363\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0006138342432677746, p.std().item() = 0.09574030339717865\n",
      "n = 'layers.0.bias', p.mean().item() = -0.13188813626766205, p.std().item() = 0.12673398852348328\n",
      "n = 'layers.2.weight', p.mean().item() = -0.021515363827347755, p.std().item() = 0.05995362997055054\n",
      "n = 'layers.2.bias', p.mean().item() = 0.17058199644088745, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -3.3425750732421875, preds.std().item() = 3.245729923248291\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.8061, device='cuda:0'), loss.std() = tensor(0.1378, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 6.293002230428613e-11, lambda_i.std().item() = 0.17597660422325134\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = 0.0034068082459270954, p.grad.std().item() = 40.90977096557617\n",
      "n = 'label_factors.weight', p.grad.mean().item() = -0.05423052981495857, p.grad.std().item() = 1.6754933595657349\n",
      "n = 'layers.0.weight', p.grad.mean().item() = 48.681575775146484, p.grad.std().item() = 1583.2684326171875\n",
      "n = 'layers.0.bias', p.grad.mean().item() = 1386.3116455078125, p.grad.std().item() = 1711.5439453125\n",
      "n = 'layers.2.weight', p.grad.mean().item() = 3945.29931640625, p.grad.std().item() = 80959.703125\n",
      "n = 'layers.2.bias', p.grad.mean().item() = 0.0001735687255859375, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.001322853029705584, p.std().item() = 0.9102451205253601\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0022428371012210846, p.std().item() = 0.9169191122055054\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0006286126445047557, p.std().item() = 0.09612887352705002\n",
      "n = 'layers.0.bias', p.mean().item() = -0.13299722969532013, p.std().item() = 0.12733493745326996\n",
      "n = 'layers.2.weight', p.mean().item() = -0.021026529371738434, p.std().item() = 0.05913790687918663\n",
      "n = 'layers.2.bias', p.mean().item() = 0.17100822925567627, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 96\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.001322853029705584, p.std().item() = 0.9102451205253601\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0022428371012210846, p.std().item() = 0.9169191122055054\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0006286126445047557, p.std().item() = 0.09612887352705002\n",
      "n = 'layers.0.bias', p.mean().item() = -0.13299722969532013, p.std().item() = 0.12733493745326996\n",
      "n = 'layers.2.weight', p.mean().item() = -0.021026529371738434, p.std().item() = 0.05913790687918663\n",
      "n = 'layers.2.bias', p.mean().item() = 0.17100822925567627, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -3.20409893989563, preds.std().item() = 3.0948290824890137\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.7988, device='cuda:0'), loss.std() = tensor(0.1434, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 9.011620616394822e-11, lambda_i.std().item() = 0.18334601819515228\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = 0.0014371090801432729, p.grad.std().item() = 32.495635986328125\n",
      "n = 'label_factors.weight', p.grad.mean().item() = 0.005737103521823883, p.grad.std().item() = 1.5755631923675537\n",
      "n = 'layers.0.weight', p.grad.mean().item() = 38.24758529663086, p.grad.std().item() = 1332.5155029296875\n",
      "n = 'layers.0.bias', p.grad.mean().item() = 149.81520080566406, p.grad.std().item() = 871.89453125\n",
      "n = 'layers.2.weight', p.grad.mean().item() = -5200.90478515625, p.grad.std().item() = 26771.751953125\n",
      "n = 'layers.2.bias', p.grad.mean().item() = -0.00014400482177734375, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0013414445566013455, p.std().item() = 0.9093587398529053\n",
      "n = 'label_factors.weight', p.mean().item() = 0.002254762686789036, p.std().item() = 0.9161702990531921\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0006838903063908219, p.std().item() = 0.09657370299100876\n",
      "n = 'layers.0.bias', p.mean().item() = -0.13416439294815063, p.std().item() = 0.1280120462179184\n",
      "n = 'layers.2.weight', p.mean().item() = -0.020435823127627373, p.std().item() = 0.058272190392017365\n",
      "n = 'layers.2.bias', p.mean().item() = 0.1717606782913208, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 97\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0013414445566013455, p.std().item() = 0.9093587398529053\n",
      "n = 'label_factors.weight', p.mean().item() = 0.002254762686789036, p.std().item() = 0.9161702990531921\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0006838903063908219, p.std().item() = 0.09657370299100876\n",
      "n = 'layers.0.bias', p.mean().item() = -0.13416439294815063, p.std().item() = 0.1280120462179184\n",
      "n = 'layers.2.weight', p.mean().item() = -0.020435823127627373, p.std().item() = 0.058272190392017365\n",
      "n = 'layers.2.bias', p.mean().item() = 0.1717606782913208, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -3.2807722091674805, preds.std().item() = 3.204521656036377\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.7768, device='cuda:0'), loss.std() = tensor(0.1187, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 6.663485735414199e-11, lambda_i.std().item() = 0.16813214123249054\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = 0.0015672834124416113, p.grad.std().item() = 22.565526962280273\n",
      "n = 'label_factors.weight', p.grad.mean().item() = -0.006014315877109766, p.grad.std().item() = 1.6408631801605225\n",
      "n = 'layers.0.weight', p.grad.mean().item() = 20.797863006591797, p.grad.std().item() = 1047.7205810546875\n",
      "n = 'layers.0.bias', p.grad.mean().item() = 105.1113052368164, p.grad.std().item() = 719.9381103515625\n",
      "n = 'layers.2.weight', p.grad.mean().item() = 10554.984375, p.grad.std().item() = 23114.291015625\n",
      "n = 'layers.2.bias', p.grad.mean().item() = -0.0004253387451171875, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0013594108168035746, p.std().item() = 0.9084734320640564\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0022674892097711563, p.std().item() = 0.9154281616210938\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0007576760253868997, p.std().item() = 0.09704867750406265\n",
      "n = 'layers.0.bias', p.mean().item() = -0.13527347147464752, p.std().item() = 0.12866728007793427\n",
      "n = 'layers.2.weight', p.mean().item() = -0.02008921653032303, p.std().item() = 0.057473015040159225\n",
      "n = 'layers.2.bias', p.mean().item() = 0.17354516685009003, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 98\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0013594108168035746, p.std().item() = 0.9084734320640564\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0022674892097711563, p.std().item() = 0.9154281616210938\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0007576760253868997, p.std().item() = 0.09704867750406265\n",
      "n = 'layers.0.bias', p.mean().item() = -0.13527347147464752, p.std().item() = 0.12866728007793427\n",
      "n = 'layers.2.weight', p.mean().item() = -0.02008921653032303, p.std().item() = 0.057473015040159225\n",
      "n = 'layers.2.bias', p.mean().item() = 0.17354516685009003, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -3.2650721073150635, preds.std().item() = 3.0173583030700684\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.7929, device='cuda:0'), loss.std() = tensor(0.1227, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 6.345182712585995e-11, lambda_i.std().item() = 0.2056456357240677\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = -0.0012685800902545452, p.grad.std().item() = 45.240577697753906\n",
      "n = 'label_factors.weight', p.grad.mean().item() = 0.01975439116358757, p.grad.std().item() = 1.443247675895691\n",
      "n = 'layers.0.weight', p.grad.mean().item() = 10.88570785522461, p.grad.std().item() = 1985.9307861328125\n",
      "n = 'layers.0.bias', p.grad.mean().item() = -578.3075561523438, p.grad.std().item() = 1049.8173828125\n",
      "n = 'layers.2.weight', p.grad.mean().item() = -36937.43359375, p.grad.std().item() = 43627.71484375\n",
      "n = 'layers.2.bias', p.grad.mean().item() = -0.000469207763671875, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0013776249252259731, p.std().item() = 0.9075899124145508\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0022631234023720026, p.std().item() = 0.9146654605865479\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0007949206046760082, p.std().item() = 0.09745702892541885\n",
      "n = 'layers.0.bias', p.mean().item() = -0.13590751588344574, p.std().item() = 0.12946882843971252\n",
      "n = 'layers.2.weight', p.mean().item() = -0.019048750400543213, p.std().item() = 0.056787408888339996\n",
      "n = 'layers.2.bias', p.mean().item() = 0.17636136710643768, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 99\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0013776249252259731, p.std().item() = 0.9075899124145508\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0022631234023720026, p.std().item() = 0.9146654605865479\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0007949206046760082, p.std().item() = 0.09745702892541885\n",
      "n = 'layers.0.bias', p.mean().item() = -0.13590751588344574, p.std().item() = 0.12946882843971252\n",
      "n = 'layers.2.weight', p.mean().item() = -0.019048750400543213, p.std().item() = 0.056787408888339996\n",
      "n = 'layers.2.bias', p.mean().item() = 0.17636136710643768, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -3.1143457889556885, preds.std().item() = 2.957336664199829\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.7912, device='cuda:0'), loss.std() = tensor(0.1257, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 1.0639661374156617e-10, lambda_i.std().item() = 0.1958763599395752\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = -0.0006521210889331996, p.grad.std().item() = 27.836082458496094\n",
      "n = 'label_factors.weight', p.grad.mean().item() = 0.05611535534262657, p.grad.std().item() = 1.7054851055145264\n",
      "n = 'layers.0.weight', p.grad.mean().item() = 2.1711254119873047, p.grad.std().item() = 1168.6187744140625\n",
      "n = 'layers.0.bias', p.grad.mean().item() = -1334.89794921875, p.grad.std().item() = 1635.4456787109375\n",
      "n = 'layers.2.weight', p.grad.mean().item() = -4871.86572265625, p.grad.std().item() = 69040.578125\n",
      "n = 'layers.2.bias', p.grad.mean().item() = 0.000194549560546875, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0013962418306618929, p.std().item() = 0.9067082405090332\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0021978404838591814, p.std().item() = 0.9139164090156555\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0008185544866137207, p.std().item() = 0.09788632392883301\n",
      "n = 'layers.0.bias', p.mean().item() = -0.13567675650119781, p.std().item() = 0.13065418601036072\n",
      "n = 'layers.2.weight', p.mean().item() = -0.018174506723880768, p.std().item() = 0.057201236486434937\n",
      "n = 'layers.2.bias', p.mean().item() = 0.178374782204628, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 100\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0013962418306618929, p.std().item() = 0.9067082405090332\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0021978404838591814, p.std().item() = 0.9139164090156555\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0008185544866137207, p.std().item() = 0.09788632392883301\n",
      "n = 'layers.0.bias', p.mean().item() = -0.13567675650119781, p.std().item() = 0.13065418601036072\n",
      "n = 'layers.2.weight', p.mean().item() = -0.018174506723880768, p.std().item() = 0.057201236486434937\n",
      "n = 'layers.2.bias', p.mean().item() = 0.178374782204628, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -3.1369011402130127, preds.std().item() = 3.152305841445923\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.7665, device='cuda:0'), loss.std() = tensor(0.1076, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 9.577782317249373e-11, lambda_i.std().item() = 0.1923971176147461\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = 0.0027701144572347403, p.grad.std().item() = 34.20102310180664\n",
      "n = 'label_factors.weight', p.grad.mean().item() = 0.007029685657471418, p.grad.std().item() = 1.4598779678344727\n",
      "n = 'layers.0.weight', p.grad.mean().item() = 0.9561564326286316, p.grad.std().item() = 1448.896240234375\n",
      "n = 'layers.0.bias', p.grad.mean().item() = -174.70773315429688, p.grad.std().item() = 811.3628540039062\n",
      "n = 'layers.2.weight', p.grad.mean().item() = -5578.4140625, p.grad.std().item() = 32655.927734375\n",
      "n = 'layers.2.bias', p.grad.mean().item() = -0.00027751922607421875, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0014150121714919806, p.std().item() = 0.9058273434638977\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0021272439043968916, p.std().item() = 0.9131777286529541\n",
      "n = 'layers.0.weight', p.mean().item() = -0.000864985806401819, p.std().item() = 0.09833867847919464\n",
      "n = 'layers.0.bias', p.mean().item() = -0.1353377252817154, p.std().item() = 0.1318577080965042\n",
      "n = 'layers.2.weight', p.mean().item() = -0.017297908663749695, p.std().item() = 0.05781867727637291\n",
      "n = 'layers.2.bias', p.mean().item() = 0.18091221153736115, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 101\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0014150121714919806, p.std().item() = 0.9058273434638977\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0021272439043968916, p.std().item() = 0.9131777286529541\n",
      "n = 'layers.0.weight', p.mean().item() = -0.000864985806401819, p.std().item() = 0.09833867847919464\n",
      "n = 'layers.0.bias', p.mean().item() = -0.1353377252817154, p.std().item() = 0.1318577080965042\n",
      "n = 'layers.2.weight', p.mean().item() = -0.017297908663749695, p.std().item() = 0.05781867727637291\n",
      "n = 'layers.2.bias', p.mean().item() = 0.18091221153736115, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -3.2514591217041016, preds.std().item() = 3.1411919593811035\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.7881, device='cuda:0'), loss.std() = tensor(0.1336, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 8.3332708789019e-11, lambda_i.std().item() = 0.19691510498523712\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = -2.6678597350837663e-05, p.grad.std().item() = 31.040510177612305\n",
      "n = 'label_factors.weight', p.grad.mean().item() = 0.012452855706214905, p.grad.std().item() = 1.4754208326339722\n",
      "n = 'layers.0.weight', p.grad.mean().item() = 27.494298934936523, p.grad.std().item() = 1305.1636962890625\n",
      "n = 'layers.0.bias', p.grad.mean().item() = -153.6822052001953, p.grad.std().item() = 658.3218994140625\n",
      "n = 'layers.2.weight', p.grad.mean().item() = -15956.810546875, p.grad.std().item() = 33167.77734375\n",
      "n = 'layers.2.bias', p.grad.mean().item() = 0.00032711029052734375, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0014339917106553912, p.std().item() = 0.9049473404884338\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0020503040868788958, p.std().item() = 0.9124370813369751\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0009049800573848188, p.std().item() = 0.09875909984111786\n",
      "n = 'layers.0.bias', p.mean().item() = -0.13491849601268768, p.std().item() = 0.13299217820167542\n",
      "n = 'layers.2.weight', p.mean().item() = -0.01620413549244404, p.std().item() = 0.05817849189043045\n",
      "n = 'layers.2.bias', p.mean().item() = 0.1823074072599411, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 102\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0014339917106553912, p.std().item() = 0.9049473404884338\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0020503040868788958, p.std().item() = 0.9124370813369751\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0009049800573848188, p.std().item() = 0.09875909984111786\n",
      "n = 'layers.0.bias', p.mean().item() = -0.13491849601268768, p.std().item() = 0.13299217820167542\n",
      "n = 'layers.2.weight', p.mean().item() = -0.01620413549244404, p.std().item() = 0.05817849189043045\n",
      "n = 'layers.2.bias', p.mean().item() = 0.1823074072599411, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -3.2830941677093506, preds.std().item() = 3.3028512001037598\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.7753, device='cuda:0'), loss.std() = tensor(0.1234, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 4.839767475672829e-11, lambda_i.std().item() = 0.16034702956676483\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = 6.490318628493696e-05, p.grad.std().item() = 23.35230255126953\n",
      "n = 'label_factors.weight', p.grad.mean().item() = -0.01404672209173441, p.grad.std().item() = 1.6618716716766357\n",
      "n = 'layers.0.weight', p.grad.mean().item() = 8.461518287658691, p.grad.std().item() = 956.73046875\n",
      "n = 'layers.0.bias', p.grad.mean().item() = 453.755859375, p.grad.std().item() = 651.3284301757812\n",
      "n = 'layers.2.weight', p.grad.mean().item() = 20135.123046875, p.grad.std().item() = 33163.39453125\n",
      "n = 'layers.2.bias', p.grad.mean().item() = 4.3392181396484375e-05, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.001452530617825687, p.std().item() = 0.9040675163269043\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0019913031719624996, p.std().item() = 0.911708414554596\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0009817328536882997, p.std().item() = 0.09924931824207306\n",
      "n = 'layers.0.bias', p.mean().item() = -0.13487085700035095, p.std().item() = 0.13396355509757996\n",
      "n = 'layers.2.weight', p.mean().item() = -0.015566622838377953, p.std().item() = 0.05832478776574135\n",
      "n = 'layers.2.bias', p.mean().item() = 0.1834377497434616, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 103\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.001452530617825687, p.std().item() = 0.9040675163269043\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0019913031719624996, p.std().item() = 0.911708414554596\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0009817328536882997, p.std().item() = 0.09924931824207306\n",
      "n = 'layers.0.bias', p.mean().item() = -0.13487085700035095, p.std().item() = 0.13396355509757996\n",
      "n = 'layers.2.weight', p.mean().item() = -0.015566622838377953, p.std().item() = 0.05832478776574135\n",
      "n = 'layers.2.bias', p.mean().item() = 0.1834377497434616, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -3.220489025115967, preds.std().item() = 3.195150375366211\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.8042, device='cuda:0'), loss.std() = tensor(0.1355, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 7.514032451805264e-11, lambda_i.std().item() = 0.19381076097488403\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = -0.002990064211189747, p.grad.std().item() = 38.21466827392578\n",
      "n = 'label_factors.weight', p.grad.mean().item() = -0.04268481582403183, p.grad.std().item() = 1.7855422496795654\n",
      "n = 'layers.0.weight', p.grad.mean().item() = 2.0533299446105957, p.grad.std().item() = 1546.822998046875\n",
      "n = 'layers.0.bias', p.grad.mean().item() = 1206.801025390625, p.grad.std().item() = 1552.8402099609375\n",
      "n = 'layers.2.weight', p.grad.mean().item() = -19336.810546875, p.grad.std().item() = 66501.0390625\n",
      "n = 'layers.2.bias', p.grad.mean().item() = -0.00037860870361328125, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0014712376287207007, p.std().item() = 0.9031884670257568\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0019941998180001974, p.std().item() = 0.9109038710594177\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0010075096506625414, p.std().item() = 0.09950777143239975\n",
      "n = 'layers.0.bias', p.mean().item() = -0.1355915516614914, p.std().item() = 0.13434156775474548\n",
      "n = 'layers.2.weight', p.mean().item() = -0.014469699934124947, p.std().item() = 0.0575215145945549\n",
      "n = 'layers.2.bias', p.mean().item() = 0.18545086681842804, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 104\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0014712376287207007, p.std().item() = 0.9031884670257568\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0019941998180001974, p.std().item() = 0.9109038710594177\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0010075096506625414, p.std().item() = 0.09950777143239975\n",
      "n = 'layers.0.bias', p.mean().item() = -0.1355915516614914, p.std().item() = 0.13434156775474548\n",
      "n = 'layers.2.weight', p.mean().item() = -0.014469699934124947, p.std().item() = 0.0575215145945549\n",
      "n = 'layers.2.bias', p.mean().item() = 0.18545086681842804, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -3.196944236755371, preds.std().item() = 3.1096901893615723\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.7812, device='cuda:0'), loss.std() = tensor(0.1171, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 7.76450015393948e-11, lambda_i.std().item() = 0.18045294284820557\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = 3.9269551052711904e-05, p.grad.std().item() = 29.408634185791016\n",
      "n = 'label_factors.weight', p.grad.mean().item() = -0.016777371987700462, p.grad.std().item() = 1.4400417804718018\n",
      "n = 'layers.0.weight', p.grad.mean().item() = 36.04988479614258, p.grad.std().item() = 1278.0504150390625\n",
      "n = 'layers.0.bias', p.grad.mean().item() = 374.8757629394531, p.grad.std().item() = 953.9278564453125\n",
      "n = 'layers.2.weight', p.grad.mean().item() = 4374.11669921875, p.grad.std().item() = 26060.841796875\n",
      "n = 'layers.2.bias', p.grad.mean().item() = -0.00018978118896484375, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0014900934183970094, p.std().item() = 0.9023105502128601\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0020165699534118176, p.std().item() = 0.9100782871246338\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0010513780871406198, p.std().item() = 0.09975900501012802\n",
      "n = 'layers.0.bias', p.mean().item() = -0.13642217218875885, p.std().item() = 0.1345938742160797\n",
      "n = 'layers.2.weight', p.mean().item() = -0.013560679741203785, p.std().item() = 0.056749727576971054\n",
      "n = 'layers.2.bias', p.mean().item() = 0.18776516616344452, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 105\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0014900934183970094, p.std().item() = 0.9023105502128601\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0020165699534118176, p.std().item() = 0.9100782871246338\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0010513780871406198, p.std().item() = 0.09975900501012802\n",
      "n = 'layers.0.bias', p.mean().item() = -0.13642217218875885, p.std().item() = 0.1345938742160797\n",
      "n = 'layers.2.weight', p.mean().item() = -0.013560679741203785, p.std().item() = 0.056749727576971054\n",
      "n = 'layers.2.bias', p.mean().item() = 0.18776516616344452, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -2.9809935092926025, preds.std().item() = 2.9869871139526367\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.7852, device='cuda:0'), loss.std() = tensor(0.1230, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 1.0290050062033984e-10, lambda_i.std().item() = 0.19030092656612396\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = -0.001096544787287712, p.grad.std().item() = 29.35496711730957\n",
      "n = 'label_factors.weight', p.grad.mean().item() = 0.024315224960446358, p.grad.std().item() = 1.560589075088501\n",
      "n = 'layers.0.weight', p.grad.mean().item() = 18.401960372924805, p.grad.std().item() = 1297.119140625\n",
      "n = 'layers.0.bias', p.grad.mean().item() = -624.5577392578125, p.grad.std().item() = 1025.0574951171875\n",
      "n = 'layers.2.weight', p.grad.mean().item() = -8507.576171875, p.grad.std().item() = 40536.171875\n",
      "n = 'layers.2.bias', p.grad.mean().item() = -0.000179290771484375, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0015094209229573607, p.std().item() = 0.9014336466789246\n",
      "n = 'label_factors.weight', p.mean().item() = 0.002021918771788478, p.std().item() = 0.9092504978179932\n",
      "n = 'layers.0.weight', p.mean().item() = -0.001095856772735715, p.std().item() = 0.10002228617668152\n",
      "n = 'layers.0.bias', p.mean().item() = -0.13678056001663208, p.std().item() = 0.13479341566562653\n",
      "n = 'layers.2.weight', p.mean().item() = -0.012628214433789253, p.std().item() = 0.05650210753083229\n",
      "n = 'layers.2.bias', p.mean().item() = 0.19032630324363708, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 106\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0015094209229573607, p.std().item() = 0.9014336466789246\n",
      "n = 'label_factors.weight', p.mean().item() = 0.002021918771788478, p.std().item() = 0.9092504978179932\n",
      "n = 'layers.0.weight', p.mean().item() = -0.001095856772735715, p.std().item() = 0.10002228617668152\n",
      "n = 'layers.0.bias', p.mean().item() = -0.13678056001663208, p.std().item() = 0.13479341566562653\n",
      "n = 'layers.2.weight', p.mean().item() = -0.012628214433789253, p.std().item() = 0.05650210753083229\n",
      "n = 'layers.2.bias', p.mean().item() = 0.19032630324363708, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -2.902092456817627, preds.std().item() = 2.8356971740722656\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.7420, device='cuda:0'), loss.std() = tensor(0.0905, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 9.139463491569799e-11, lambda_i.std().item() = 0.185678631067276\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = 0.003113793907687068, p.grad.std().item() = 23.490148544311523\n",
      "n = 'label_factors.weight', p.grad.mean().item() = 0.04631275311112404, p.grad.std().item() = 1.643416166305542\n",
      "n = 'layers.0.weight', p.grad.mean().item() = 13.331720352172852, p.grad.std().item() = 1078.6746826171875\n",
      "n = 'layers.0.bias', p.grad.mean().item() = -1268.2803955078125, p.grad.std().item() = 1437.323974609375\n",
      "n = 'layers.2.weight', p.grad.mean().item() = 7154.6650390625, p.grad.std().item() = 72685.1171875\n",
      "n = 'layers.2.bias', p.grad.mean().item() = 0.0001430511474609375, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0015295346966013312, p.std().item() = 0.9005582928657532\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0019863408524543047, p.std().item() = 0.9084384441375732\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0011668333318084478, p.std().item() = 0.10034926980733871\n",
      "n = 'layers.0.bias', p.mean().item() = -0.13618817925453186, p.std().item() = 0.1352505087852478\n",
      "n = 'layers.2.weight', p.mean().item() = -0.012080156244337559, p.std().item() = 0.057428788393735886\n",
      "n = 'layers.2.bias', p.mean().item() = 0.19223493337631226, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 107\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0015295346966013312, p.std().item() = 0.9005582928657532\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0019863408524543047, p.std().item() = 0.9084384441375732\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0011668333318084478, p.std().item() = 0.10034926980733871\n",
      "n = 'layers.0.bias', p.mean().item() = -0.13618817925453186, p.std().item() = 0.1352505087852478\n",
      "n = 'layers.2.weight', p.mean().item() = -0.012080156244337559, p.std().item() = 0.057428788393735886\n",
      "n = 'layers.2.bias', p.mean().item() = 0.19223493337631226, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -2.7706170082092285, preds.std().item() = 2.9162631034851074\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.7513, device='cuda:0'), loss.std() = tensor(0.0986, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 9.07945593708881e-11, lambda_i.std().item() = 0.18192653357982635\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = -0.0022287399042397738, p.grad.std().item() = 26.099864959716797\n",
      "n = 'label_factors.weight', p.grad.mean().item() = -0.009321002289652824, p.grad.std().item() = 1.4820060729980469\n",
      "n = 'layers.0.weight', p.grad.mean().item() = -28.796831130981445, p.grad.std().item() = 1066.0166015625\n",
      "n = 'layers.0.bias', p.grad.mean().item() = 251.5722198486328, p.grad.std().item() = 670.135498046875\n",
      "n = 'layers.2.weight', p.grad.mean().item() = 4414.86669921875, p.grad.std().item() = 20940.33984375\n",
      "n = 'layers.2.bias', p.grad.mean().item() = 0.0001964569091796875, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0015497407875955105, p.std().item() = 0.89968341588974\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0019671889021992683, p.std().item() = 0.9076261520385742\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0012255182955414057, p.std().item() = 0.10072124749422073\n",
      "n = 'layers.0.bias', p.mean().item() = -0.13575702905654907, p.std().item() = 0.1355404406785965\n",
      "n = 'layers.2.weight', p.mean().item() = -0.011607794091105461, p.std().item() = 0.05830729007720947\n",
      "n = 'layers.2.bias', p.mean().item() = 0.19340232014656067, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 108\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0015497407875955105, p.std().item() = 0.89968341588974\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0019671889021992683, p.std().item() = 0.9076261520385742\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0012255182955414057, p.std().item() = 0.10072124749422073\n",
      "n = 'layers.0.bias', p.mean().item() = -0.13575702905654907, p.std().item() = 0.1355404406785965\n",
      "n = 'layers.2.weight', p.mean().item() = -0.011607794091105461, p.std().item() = 0.05830729007720947\n",
      "n = 'layers.2.bias', p.mean().item() = 0.19340232014656067, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -3.075732469558716, preds.std().item() = 2.996744155883789\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.7967, device='cuda:0'), loss.std() = tensor(0.1191, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 1.0561389957031153e-10, lambda_i.std().item() = 0.19313646852970123\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = 0.005889348220080137, p.grad.std().item() = 32.562870025634766\n",
      "n = 'label_factors.weight', p.grad.mean().item() = 0.01504959911108017, p.grad.std().item() = 1.5122202634811401\n",
      "n = 'layers.0.weight', p.grad.mean().item() = -8.610806465148926, p.grad.std().item() = 1401.2698974609375\n",
      "n = 'layers.0.bias', p.grad.mean().item() = -595.9221801757812, p.grad.std().item() = 1350.2135009765625\n",
      "n = 'layers.2.weight', p.grad.mean().item() = 11471.39453125, p.grad.std().item() = 34920.78125\n",
      "n = 'layers.2.bias', p.grad.mean().item() = -0.0002651214599609375, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0015704486286267638, p.std().item() = 0.8988094329833984\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0019299015402793884, p.std().item() = 0.9068382978439331\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0012799707474187016, p.std().item() = 0.10111149400472641\n",
      "n = 'layers.0.bias', p.mean().item() = -0.13503813743591309, p.std().item() = 0.13602764904499054\n",
      "n = 'layers.2.weight', p.mean().item() = -0.011395897716283798, p.std().item() = 0.05940771847963333\n",
      "n = 'layers.2.bias', p.mean().item() = 0.1951664835214615, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 109\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0015704486286267638, p.std().item() = 0.8988094329833984\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0019299015402793884, p.std().item() = 0.9068382978439331\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0012799707474187016, p.std().item() = 0.10111149400472641\n",
      "n = 'layers.0.bias', p.mean().item() = -0.13503813743591309, p.std().item() = 0.13602764904499054\n",
      "n = 'layers.2.weight', p.mean().item() = -0.011395897716283798, p.std().item() = 0.05940771847963333\n",
      "n = 'layers.2.bias', p.mean().item() = 0.1951664835214615, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -3.065370798110962, preds.std().item() = 3.077261209487915\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.7541, device='cuda:0'), loss.std() = tensor(0.1083, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 7.22703841216088e-11, lambda_i.std().item() = 0.19343002140522003\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = -0.0012680807849392295, p.grad.std().item() = 39.59743118286133\n",
      "n = 'label_factors.weight', p.grad.mean().item() = -0.032627545297145844, p.grad.std().item() = 1.6250125169754028\n",
      "n = 'layers.0.weight', p.grad.mean().item() = -9.880276679992676, p.grad.std().item() = 1725.9447021484375\n",
      "n = 'layers.0.bias', p.grad.mean().item() = 892.3458251953125, p.grad.std().item() = 1234.520751953125\n",
      "n = 'layers.2.weight', p.grad.mean().item() = -21171.58203125, p.grad.std().item() = 71678.5234375\n",
      "n = 'layers.2.bias', p.grad.mean().item() = 0.00040435791015625, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.001591406064108014, p.std().item() = 0.8979359865188599\n",
      "n = 'label_factors.weight', p.mean().item() = 0.001926483935676515, p.std().item() = 0.9060320258140564\n",
      "n = 'layers.0.weight', p.mean().item() = -0.001297998009249568, p.std().item() = 0.1013823002576828\n",
      "n = 'layers.0.bias', p.mean().item() = -0.13506874442100525, p.std().item() = 0.13629212975502014\n",
      "n = 'layers.2.weight', p.mean().item() = -0.010687680914998055, p.std().item() = 0.05932758003473282\n",
      "n = 'layers.2.bias', p.mean().item() = 0.1956183761358261, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 110\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.001591406064108014, p.std().item() = 0.8979359865188599\n",
      "n = 'label_factors.weight', p.mean().item() = 0.001926483935676515, p.std().item() = 0.9060320258140564\n",
      "n = 'layers.0.weight', p.mean().item() = -0.001297998009249568, p.std().item() = 0.1013823002576828\n",
      "n = 'layers.0.bias', p.mean().item() = -0.13506874442100525, p.std().item() = 0.13629212975502014\n",
      "n = 'layers.2.weight', p.mean().item() = -0.010687680914998055, p.std().item() = 0.05932758003473282\n",
      "n = 'layers.2.bias', p.mean().item() = 0.1956183761358261, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -3.1413731575012207, preds.std().item() = 3.1692910194396973\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.8023, device='cuda:0'), loss.std() = tensor(0.1406, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 8.703754383887485e-11, lambda_i.std().item() = 0.185069739818573\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = 0.002058903221040964, p.grad.std().item() = 31.718000411987305\n",
      "n = 'label_factors.weight', p.grad.mean().item() = 0.024387015029788017, p.grad.std().item() = 1.583886742591858\n",
      "n = 'layers.0.weight', p.grad.mean().item() = -3.4189209938049316, p.grad.std().item() = 1303.8052978515625\n",
      "n = 'layers.0.bias', p.grad.mean().item() = -645.57568359375, p.grad.std().item() = 1031.9935302734375\n",
      "n = 'layers.2.weight', p.grad.mean().item() = 2481.562744140625, p.grad.std().item() = 50038.83203125\n",
      "n = 'layers.2.bias', p.grad.mean().item() = 0.00017452239990234375, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0016127279959619045, p.std().item() = 0.8970634937286377\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0018946720520034432, p.std().item() = 0.9052440524101257\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0013246557209640741, p.std().item() = 0.10169624537229538\n",
      "n = 'layers.0.bias', p.mean().item() = -0.13469287753105164, p.std().item() = 0.1367456316947937\n",
      "n = 'layers.2.weight', p.mean().item() = -0.010070975869894028, p.std().item() = 0.06000670790672302\n",
      "n = 'layers.2.bias', p.mean().item() = 0.19552625715732574, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 111\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0016127279959619045, p.std().item() = 0.8970634937286377\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0018946720520034432, p.std().item() = 0.9052440524101257\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0013246557209640741, p.std().item() = 0.10169624537229538\n",
      "n = 'layers.0.bias', p.mean().item() = -0.13469287753105164, p.std().item() = 0.1367456316947937\n",
      "n = 'layers.2.weight', p.mean().item() = -0.010070975869894028, p.std().item() = 0.06000670790672302\n",
      "n = 'layers.2.bias', p.mean().item() = 0.19552625715732574, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -3.0808420181274414, preds.std().item() = 3.1498219966888428\n",
      "loss.shape = torch.Size([32, 2231]), loss.mean() = tensor(0.7990, device='cuda:0'), loss.std() = tensor(0.1389, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 6.115587203314732e-11, lambda_i.std().item() = 0.17264722287654877\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = -0.0023720914032310247, p.grad.std().item() = 30.2102108001709\n",
      "n = 'label_factors.weight', p.grad.mean().item() = -0.041139572858810425, p.grad.std().item() = 1.6788650751113892\n",
      "n = 'layers.0.weight', p.grad.mean().item() = -16.1324520111084, p.grad.std().item() = 1266.2095947265625\n",
      "n = 'layers.0.bias', p.grad.mean().item() = 974.5369873046875, p.grad.std().item() = 1193.9256591796875\n",
      "n = 'layers.2.weight', p.grad.mean().item() = -351.7588195800781, p.grad.std().item() = 54943.20703125\n",
      "n = 'layers.2.bias', p.grad.mean().item() = -0.00016117095947265625, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0016335630789399147, p.std().item() = 0.8961915373802185\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0019070738926529884, p.std().item() = 0.9044297337532043\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0013296434190124273, p.std().item() = 0.10193848609924316\n",
      "n = 'layers.0.bias', p.mean().item() = -0.13507026433944702, p.std().item() = 0.1369725912809372\n",
      "n = 'layers.2.weight', p.mean().item() = -0.009424838237464428, p.std().item() = 0.059943828731775284\n",
      "n = 'layers.2.bias', p.mean().item() = 0.19587215781211853, p.std().item() = nan\n",
      "-------\n",
      "mini-batch: 112\n",
      "The params before update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0016335630789399147, p.std().item() = 0.8961915373802185\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0019070738926529884, p.std().item() = 0.9044297337532043\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0013296434190124273, p.std().item() = 0.10193848609924316\n",
      "n = 'layers.0.bias', p.mean().item() = -0.13507026433944702, p.std().item() = 0.1369725912809372\n",
      "n = 'layers.2.weight', p.mean().item() = -0.009424838237464428, p.std().item() = 0.059943828731775284\n",
      "n = 'layers.2.bias', p.mean().item() = 0.19587215781211853, p.std().item() = nan\n",
      "preds are good! No nan/inf, but preds.mean().item() = -3.947190761566162, preds.std().item() = 2.671699285507202\n",
      "loss.shape = torch.Size([4, 2231]), loss.mean() = tensor(0.6747, device='cuda:0'), loss.std() = tensor(0.0897, device='cuda:0')\n",
      "lambda_i are good! No nan/inf, but lambda_i.mean().item() = 6.478765857131918e-10, lambda_i.std().item() = 0.24233655631542206\n",
      "The gradients\n",
      "n = 'token_factors.weight', p.grad.mean().item() = 0.0007790217641741037, p.grad.std().item() = 32.56541442871094\n",
      "n = 'label_factors.weight', p.grad.mean().item() = 0.01978415809571743, p.grad.std().item() = 1.0051461458206177\n",
      "n = 'layers.0.weight', p.grad.mean().item() = 22.390331268310547, p.grad.std().item() = 1443.1566162109375\n",
      "n = 'layers.0.bias', p.grad.mean().item() = -600.4844360351562, p.grad.std().item() = 1094.5975341796875\n",
      "n = 'layers.2.weight', p.grad.mean().item() = 5181.07373046875, p.grad.std().item() = 31936.130859375\n",
      "n = 'layers.2.bias', p.grad.mean().item() = -0.000732421875, p.grad.std().item() = nan\n",
      "The params after update\n",
      "n = 'token_factors.weight', p.mean().item() = -0.0016522742807865143, p.std().item() = 0.8953196406364441\n",
      "n = 'label_factors.weight', p.mean().item() = 0.0019030466210097075, p.std().item() = 0.9035980701446533\n",
      "n = 'layers.0.weight', p.mean().item() = -0.0013063851511105895, p.std().item() = 0.10213199257850647\n",
      "n = 'layers.0.bias', p.mean().item() = -0.13495877385139465, p.std().item() = 0.13731786608695984\n",
      "n = 'layers.2.weight', p.mean().item() = -0.00900451559573412, p.std().item() = 0.06021321564912796\n",
      "n = 'layers.2.bias', p.mean().item() = 0.19813936948776245, p.std().item() = nan\n",
      "-------\n"
     ]
    }
   ],
   "source": [
    "for i,xb in enumerate(progress_bar(dls.train)):\n",
    "    print(f\"mini-batch: {i}\")\n",
    "    \n",
    "    print('The params before update')\n",
    "    for n,p in model.named_parameters():\n",
    "        print(f\"{n = }, {p.mean().item() = }, {p.std().item() = }\")\n",
    "        \n",
    "        if p.sum().isnan():\n",
    "            print(\"there is a nan in {n}\")\n",
    "            import pdb; pdb.set_trace()\n",
    "            print('---')\n",
    "            \n",
    "    preds = model(xb)\n",
    "    \n",
    "    if ( ~(torch.isnan(preds).logical_not().all()) or ~(torch.isinf(preds).logical_not().all()) ):\n",
    "        print(f\"DAMN! preds have nan/inf: {i}\")\n",
    "        import pdb; pdb.set_trace()\n",
    "        print(\"broken\")\n",
    "    else: print(f\"preds are good! No nan/inf, but {preds.mean().item() = }, {preds.std().item() = }\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        loss = loss_func(preds, xb)\n",
    "        if ( ~(torch.isnan(loss).logical_not().all()) or ~(torch.isinf(loss).logical_not().all()) ):\n",
    "            print(f\"DAMN! loss is nan/inf: {i}\")\n",
    "            import pdb; pdb.set_trace()\n",
    "            print(\"broken\")\n",
    "        else: print(f\"{loss.shape = }, {loss.mean() = }, {loss.std() = }\")\n",
    "    \n",
    "    preds, lambda_i = grad_func(preds, xb)\n",
    "    preds, lambda_i = preds.squeeze(-1), lambda_i.squeeze(-1)\n",
    "    with torch.no_grad():\n",
    "        norm_lambda_i = nn.BatchNorm1d(2231, device=xb.device)(lambda_i)\n",
    "    preds.backward(norm_lambda_i)\n",
    "    \n",
    "    if ( ~(torch.isnan(lambda_i).logical_not().all()) or ~(torch.isinf(lambda_i).logical_not().all()) ):\n",
    "        print(f\"DAMN! lambda_i have nan/inf: {i}\")\n",
    "        import pdb; pdb.set_trace()\n",
    "        print(\"broken\")\n",
    "    else: print(f\"lambda_i are good! No nan/inf, but {lambda_i.mean().item() = }, {lambda_i.std().item() = }\")\n",
    "    \n",
    "    print('The gradients')\n",
    "    for n,p in model.named_parameters():\n",
    "        print(f\"{n = }, {p.grad.mean().item() = }, {p.grad.std().item() = }\")\n",
    "        \n",
    "        if p.grad.sum().isnan():\n",
    "            print(f\"DAMN! There is a nan/inf in {n} grad\")\n",
    "            import pdb; pdb.set_trace()\n",
    "            print('broken')\n",
    "            \n",
    "    opt.step()\n",
    "    opt.zero_grad()\n",
    "    \n",
    "    print('The params after update')\n",
    "    for n,p in model.named_parameters():\n",
    "        print(f\"{n = }, {p.mean().item() = }, {p.std().item() = }\")\n",
    "        \n",
    "        if p.sum().isnan():\n",
    "            print(f\"DAMN! There is a nan/inf in {n}\")\n",
    "            import pdb; pdb.set_trace()\n",
    "            print('broken')\n",
    "        \n",
    "    print(\"-------\")\n",
    "    \n",
    "    # if not torch.logical_not(torch.isnan(preds.flatten())).all():\n",
    "    #     print(f\"DAMN: {i}\")\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a857d6b-c2bf-43f9-a0f9-d113e3b9e34b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_ndcg (candi. 32)</th>\n",
       "      <th>val ndcg@6 (candi. 32)</th>\n",
       "      <th>val_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.8255</td>\n",
       "      <td>0.3121</td>\n",
       "      <td>0.1117</td>\n",
       "      <td>0.5525</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learner.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61820dfb-21a6-4873-8b10-d406824f5260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 2231, 64]), torch.Size([4, 2231, 64]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_i.shape, preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a662f5-eb5a-4ea4-a327-256e89780438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 2231, 1])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_i_sum = lambda_i.sum(-1).unsqueeze(-1)\n",
    "lambda_i_sum.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c777164c-e85b-447d-8169-ff8c6d874e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_i_norm = lambda_i/lambda_i_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33b52a5-fc41-4fd1-8bb7-4b52f7ea3d82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.1677e-01,  1.0218e-01,  8.9841e-02,  7.9228e-02,  6.9976e-02,  6.1817e-02,  5.4553e-02,  4.8031e-02,  4.9332e-09,  4.5990e-09,  4.2766e-09,  3.9651e-09,  3.6640e-09,  3.3728e-09,\n",
       "         3.0908e-09,  2.8176e-09,  3.7630e-03,  4.4693e-03,  5.1448e-03,  5.7916e-03,  6.4118e-03,  7.0072e-03,  7.5795e-03,  8.1301e-03,  9.6947e-01,  5.1925e-01,  3.5953e-01,  2.7497e-01,\n",
       "         2.2151e-01,  1.8413e-01,  1.5622e-01,  1.3443e-01,  3.9025e-02,  3.3656e-02,  2.8742e-02,  2.4221e-02,  2.0044e-02,  1.6168e-02,  1.2558e-02,  9.1857e-03,  2.5526e-09,  2.2956e-09,\n",
       "         2.0461e-09,  1.8037e-09,  1.5680e-09,  1.3389e-09,  1.1159e-09,  8.9876e-10, -3.8376e-08, -3.9198e-08, -3.9999e-08, -4.0780e-08, -4.1543e-08, -4.2288e-08, -4.3015e-08, -4.3726e-08,\n",
       "        -4.3490e-01, -4.4265e-01, -4.4995e-01, -4.5685e-01, -4.6338e-01, -4.6957e-01, -4.7545e-01, -4.8105e-01], device='cuda:0')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_i[0, 56]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22a6597-1b0c-4caa-8f08-c202495323cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 9.7954e+05,  8.5716e+05,  7.5364e+05,  6.6461e+05,  5.8700e+05,  5.1856e+05,  4.5763e+05,  4.0291e+05,  4.1383e-02,  3.8579e-02,  3.5874e-02,  3.3262e-02,  3.0736e-02,  2.8293e-02,\n",
       "         2.5927e-02,  2.3635e-02,  3.1567e+04,  3.7491e+04,  4.3157e+04,  4.8583e+04,  5.3786e+04,  5.8781e+04,  6.3581e+04,  6.8201e+04,  8.1325e+06,  4.3558e+06,  3.0160e+06,  2.3066e+06,\n",
       "         1.8582e+06,  1.5446e+06,  1.3105e+06,  1.1276e+06,  3.2737e+05,  2.8233e+05,  2.4110e+05,  2.0318e+05,  1.6814e+05,  1.3562e+05,  1.0534e+05,  7.7055e+04,  2.1413e-02,  1.9257e-02,\n",
       "         1.7164e-02,  1.5130e-02,  1.3154e-02,  1.1231e-02,  9.3607e-03,  7.5393e-03, -3.2192e-01, -3.2881e-01, -3.3554e-01, -3.4209e-01, -3.4849e-01, -3.5474e-01, -3.6084e-01, -3.6680e-01,\n",
       "        -3.6482e+06, -3.7132e+06, -3.7745e+06, -3.8323e+06, -3.8871e+06, -3.9390e+06, -3.9884e+06, -4.0353e+06], device='cuda:0')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_i_norm[0, 56]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c822e433-fa07-4b73-82f4-e46d6b1f6457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds.shape, preds.sum().isnan(), preds.min().isinf(), preds.max().isinf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e24c879-cd0e-48fb-91ad-b68946a972c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name = 'token_factors.weight', p.shape = torch.Size([57353, 200])\n",
      "nan:  False\n",
      "inf:  False\n",
      "-inf:  False\n",
      "p.mean().item() = -0.030002744868397713, p.std().item() = 4.039689540863037\n",
      "p.grad.mean().item() = 0.0, p.grad.std().item() = 0.0\n",
      "---\n",
      "name = 'token_bias.weight', p.shape = torch.Size([57353, 1])\n",
      "nan:  False\n",
      "inf:  False\n",
      "-inf:  False\n",
      "p.mean().item() = -1.710799217224121, p.std().item() = 5.6113080978393555\n",
      "p.grad.mean().item() = 0.0, p.grad.std().item() = 0.0\n",
      "---\n",
      "name = 'label_factors.weight', p.shape = torch.Size([8923, 200])\n",
      "nan:  False\n",
      "inf:  False\n",
      "-inf:  False\n",
      "p.mean().item() = 0.015834469348192215, p.std().item() = 1.6407610177993774\n",
      "p.grad.mean().item() = 0.0, p.grad.std().item() = 0.0\n",
      "---\n",
      "name = 'label_bias.weight', p.shape = torch.Size([8923, 1])\n",
      "nan:  False\n",
      "inf:  False\n",
      "-inf:  False\n",
      "p.mean().item() = 16.79933738708496, p.std().item() = 10.657652854919434\n",
      "p.grad.mean().item() = 0.0, p.grad.std().item() = 0.0\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "model = learner.model\n",
    "for name,p in model.named_parameters():\n",
    "    print(f\"{name = }, {p.shape = }\")\n",
    "    print(\"nan: \", p.sum().isnan().item())\n",
    "    print(\"inf: \", p.max().isinf().item())\n",
    "    print(\"-inf: \", p.min().isinf().item())\n",
    "    print(f\"{p.mean().item() = }, {p.std().item() = }\")\n",
    "    if p.grad is not None: print(f\"{p.grad.mean().item() = }, {p.grad.std().item() = }\") \n",
    "    else: print('grad is None')\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7d8973-3a06-458c-974d-6d337cfb33ae",
   "metadata": {},
   "source": [
    "Save the model: <mark> Also need to save optimizer's state now </mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ce2036-0432-47d4-a58f-219612829330",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_sd = opt.state_dict()\n",
    "torch.save(opt_sd, 'ranknet_opt_sd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921909a4-0edc-4c82-a083-09e834859711",
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = rank_model.state_dict()\n",
    "sum([sys.getsizeof(v.storage()) for v in sd.values()])/1024**2\n",
    "torch.save(sd, 'ranknet_sd')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41efa9b2-7a1b-4eab-a84c-31569bb3c220",
   "metadata": {},
   "source": [
    "Load it back and validate: <mark>(fix this later, as we are now using the fastai's optimizer)</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed5dcc3-3849-49a3-ae84-c88c1820f4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranknet_sd = torch.load('ranknet_sd')\n",
    "rank_model.load_state_dict(ranknet_sd)\n",
    "opt_sd = torch.load('ranknet_opt_sd')\n",
    "opt.load_state_dict(opt_sd)\n",
    "# [torch.equal(p1, p2) for p1, p2 in zip(opt.params, rank_model.state_dict().values())]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29f66a6-4451-46e8-8bff-e80254dfb537",
   "metadata": {},
   "source": [
    "#### Toy Implementation of RankNet and LambdaRank:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0be44b3-aae4-4105-880c-86dd938f1d9e",
   "metadata": {},
   "source": [
    "Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2d3ba1-b40a-42a6-bfc4-c3098029b269",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 50\n",
    "n_docs = 20\n",
    "n_rel = 5\n",
    "n_irr = n_docs - n_rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3004c2ed-6afc-4f76-b662-07cc320240e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| doc_features.shape: torch.Size([20, 50])\n"
     ]
    }
   ],
   "source": [
    "doc_features = torch.randn(n_docs, input_dim).to('cuda:0')\n",
    "ic(doc_features.shape);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea27b80-ec96-4341-ba21-7d43fd21782a",
   "metadata": {},
   "source": [
    "Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99359a5-98c5-4d29-b36a-0d0fba6c0fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(\n",
    "    nn.Linear(input_dim, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f53977a-7497-48f7-a795-34bc2e834acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to('cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5840c73-3006-4798-97d6-427346c679ee",
   "metadata": {},
   "source": [
    "Document scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99ed89a-53d5-401a-a58e-33cbe50a6e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_scores = model(doc_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2442a4-4950-4fb0-8d42-5e1639cac9c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 1])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_scores.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2bd87f-e126-44a6-88f9-361b67680937",
   "metadata": {},
   "source": [
    "Document Ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09849c10-47e9-42dc-9f37-9109ec8d6aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_scores, sorted_idxs = doc_scores.sort(dim=0, descending=True)\n",
    "doc_ranks = torch.zeros(n_docs).to('cuda:0')\n",
    "doc_ranks[sorted_idxs] = 1 + torch.arange(n_docs).view((n_docs, 1)).to('cuda:0').float()\n",
    "doc_ranks = doc_ranks.view((n_docs, 1))\n",
    "# Alternatively,\n",
    "doc_ranks2 = 1 + doc_scores.argsort(descending=True, dim=0).argsort(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9fa056-9786-4b97-9b93-4411275ea6f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scores</th>\n",
       "      <th>ranks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.263623</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.251701</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.231704</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.195766</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.256772</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.238881</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.224846</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.284661</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.215124</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.310080</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.251531</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.256881</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.120806</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.288171</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.249740</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.321594</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.206383</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.221547</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.277746</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.257285</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      scores  ranks\n",
       "0  -0.263623   15.0\n",
       "1  -0.251701   11.0\n",
       "2  -0.231704    7.0\n",
       "3  -0.195766    2.0\n",
       "4  -0.256772   12.0\n",
       "5  -0.238881    8.0\n",
       "6  -0.224846    6.0\n",
       "7  -0.284661   17.0\n",
       "8  -0.215124    4.0\n",
       "9  -0.310080   19.0\n",
       "10 -0.251531   10.0\n",
       "11 -0.256881   13.0\n",
       "12 -0.120806    1.0\n",
       "13 -0.288171   18.0\n",
       "14 -0.249740    9.0\n",
       "15 -0.321594   20.0\n",
       "16 -0.206383    3.0\n",
       "17 -0.221547    5.0\n",
       "18 -0.277746   16.0\n",
       "19 -0.257285   14.0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(torch.cat((doc_scores, doc_ranks), dim=-1), columns = ['scores', 'ranks',])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a558b0da-d3a2-4688-9ef4-be9aa1924edd",
   "metadata": {},
   "source": [
    "Compute Lambdas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83c4bbc-421c-42d2-a3b3-13a95ae6339d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def idcg(n_rel):\n",
    "    # Assuming binary relevance.\n",
    "    nums = np.ones(n_rel)\n",
    "    denoms = np.log2(np.arange(n_rel) + 1 + 1)\n",
    "    return (nums / denoms).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce85385-43a1-4ae5-877b-6a68ccb847b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| score_diffs.shape: torch.Size([5, 15])\n",
      "ic| dcg_diffs.shape: torch.Size([5, 15])\n"
     ]
    }
   ],
   "source": [
    "score_diffs = doc_scores[:n_rel] - doc_scores[n_rel:].view(n_irr)\n",
    "ic(score_diffs.shape);\n",
    "\n",
    "exped = score_diffs.exp()\n",
    "\n",
    "N = 1 / idcg(n_rel)\n",
    "\n",
    "dcg_diffs = 1 / ( 1 + doc_ranks[:n_rel] ).log2() - 1 / ( 1 + doc_ranks[n_rel:] ).log2().view(n_irr)\n",
    "ic(dcg_diffs.shape);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad5bac2-31bc-4444-a439-a2bd821909d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>j: less relevant</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i: more relevant</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.024741</td>\n",
       "      <td>-0.038777</td>\n",
       "      <td>0.021039</td>\n",
       "      <td>-0.048499</td>\n",
       "      <td>0.046457</td>\n",
       "      <td>-0.012092</td>\n",
       "      <td>-0.006741</td>\n",
       "      <td>-0.142816</td>\n",
       "      <td>0.024549</td>\n",
       "      <td>-0.013882</td>\n",
       "      <td>0.057971</td>\n",
       "      <td>-0.057239</td>\n",
       "      <td>-0.042075</td>\n",
       "      <td>0.014123</td>\n",
       "      <td>-0.006337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.012820</td>\n",
       "      <td>-0.026855</td>\n",
       "      <td>0.032960</td>\n",
       "      <td>-0.036577</td>\n",
       "      <td>0.058379</td>\n",
       "      <td>-0.000170</td>\n",
       "      <td>0.005180</td>\n",
       "      <td>-0.130895</td>\n",
       "      <td>0.036470</td>\n",
       "      <td>-0.001961</td>\n",
       "      <td>0.069893</td>\n",
       "      <td>-0.045318</td>\n",
       "      <td>-0.030154</td>\n",
       "      <td>0.026045</td>\n",
       "      <td>0.005584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.007177</td>\n",
       "      <td>-0.006858</td>\n",
       "      <td>0.052957</td>\n",
       "      <td>-0.016581</td>\n",
       "      <td>0.078375</td>\n",
       "      <td>0.019826</td>\n",
       "      <td>0.025177</td>\n",
       "      <td>-0.110898</td>\n",
       "      <td>0.056467</td>\n",
       "      <td>0.018036</td>\n",
       "      <td>0.089889</td>\n",
       "      <td>-0.025321</td>\n",
       "      <td>-0.010157</td>\n",
       "      <td>0.046042</td>\n",
       "      <td>0.025581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.043115</td>\n",
       "      <td>0.029079</td>\n",
       "      <td>0.088895</td>\n",
       "      <td>0.019357</td>\n",
       "      <td>0.114313</td>\n",
       "      <td>0.055764</td>\n",
       "      <td>0.061115</td>\n",
       "      <td>-0.074960</td>\n",
       "      <td>0.092405</td>\n",
       "      <td>0.053974</td>\n",
       "      <td>0.125827</td>\n",
       "      <td>0.010617</td>\n",
       "      <td>0.025781</td>\n",
       "      <td>0.081980</td>\n",
       "      <td>0.061519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.017891</td>\n",
       "      <td>-0.031926</td>\n",
       "      <td>0.027889</td>\n",
       "      <td>-0.041649</td>\n",
       "      <td>0.053308</td>\n",
       "      <td>-0.005241</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>-0.135966</td>\n",
       "      <td>0.031399</td>\n",
       "      <td>-0.007032</td>\n",
       "      <td>0.064822</td>\n",
       "      <td>-0.050389</td>\n",
       "      <td>-0.035225</td>\n",
       "      <td>0.020974</td>\n",
       "      <td>0.000513</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "j: less relevant        0         1         2         3         4         5   \\\n",
       "i: more relevant                                                               \n",
       "0                -0.024741 -0.038777  0.021039 -0.048499  0.046457 -0.012092   \n",
       "1                -0.012820 -0.026855  0.032960 -0.036577  0.058379 -0.000170   \n",
       "2                 0.007177 -0.006858  0.052957 -0.016581  0.078375  0.019826   \n",
       "3                 0.043115  0.029079  0.088895  0.019357  0.114313  0.055764   \n",
       "4                -0.017891 -0.031926  0.027889 -0.041649  0.053308 -0.005241   \n",
       "\n",
       "j: less relevant        6         7         8         9         10        11  \\\n",
       "i: more relevant                                                               \n",
       "0                -0.006741 -0.142816  0.024549 -0.013882  0.057971 -0.057239   \n",
       "1                 0.005180 -0.130895  0.036470 -0.001961  0.069893 -0.045318   \n",
       "2                 0.025177 -0.110898  0.056467  0.018036  0.089889 -0.025321   \n",
       "3                 0.061115 -0.074960  0.092405  0.053974  0.125827  0.010617   \n",
       "4                 0.000109 -0.135966  0.031399 -0.007032  0.064822 -0.050389   \n",
       "\n",
       "j: less relevant        12        13        14  \n",
       "i: more relevant                                \n",
       "0                -0.042075  0.014123 -0.006337  \n",
       "1                -0.030154  0.026045  0.005584  \n",
       "2                -0.010157  0.046042  0.025581  \n",
       "3                 0.025781  0.081980  0.061519  \n",
       "4                -0.035225  0.020974  0.000513  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(score_diffs)\n",
    "df.index.name = 'i: more relevant'\n",
    "df.columns.name = 'j: less relevant'\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfab474-002a-4019-9401-91bfff27b719",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| lamb_updates.shape: torch.Size([5, 15])\n"
     ]
    }
   ],
   "source": [
    "lamb_updates = 1 / (1 + exped) * N * dcg_diffs.abs()\n",
    "ic(lamb_updates.shape);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1248622d-110d-40ad-ae1b-8108b7a1b1c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>j: less relevant</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i: more relevant</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.011239</td>\n",
       "      <td>0.018360</td>\n",
       "      <td>0.001709</td>\n",
       "      <td>0.031382</td>\n",
       "      <td>0.003085</td>\n",
       "      <td>0.006665</td>\n",
       "      <td>0.002152</td>\n",
       "      <td>0.136252</td>\n",
       "      <td>0.002444</td>\n",
       "      <td>0.008714</td>\n",
       "      <td>0.003677</td>\n",
       "      <td>0.043608</td>\n",
       "      <td>0.023696</td>\n",
       "      <td>0.000901</td>\n",
       "      <td>0.001014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.006233</td>\n",
       "      <td>0.013278</td>\n",
       "      <td>0.006526</td>\n",
       "      <td>0.026202</td>\n",
       "      <td>0.007831</td>\n",
       "      <td>0.001717</td>\n",
       "      <td>0.002756</td>\n",
       "      <td>0.130268</td>\n",
       "      <td>0.007248</td>\n",
       "      <td>0.003749</td>\n",
       "      <td>0.008391</td>\n",
       "      <td>0.038336</td>\n",
       "      <td>0.018575</td>\n",
       "      <td>0.005740</td>\n",
       "      <td>0.003887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.003019</td>\n",
       "      <td>0.003892</td>\n",
       "      <td>0.015439</td>\n",
       "      <td>0.016644</td>\n",
       "      <td>0.016612</td>\n",
       "      <td>0.007433</td>\n",
       "      <td>0.011836</td>\n",
       "      <td>0.119316</td>\n",
       "      <td>0.016137</td>\n",
       "      <td>0.005429</td>\n",
       "      <td>0.017114</td>\n",
       "      <td>0.028621</td>\n",
       "      <td>0.009122</td>\n",
       "      <td>0.014693</td>\n",
       "      <td>0.012953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.052343</td>\n",
       "      <td>0.045910</td>\n",
       "      <td>0.063380</td>\n",
       "      <td>0.033630</td>\n",
       "      <td>0.063887</td>\n",
       "      <td>0.056357</td>\n",
       "      <td>0.060545</td>\n",
       "      <td>0.064932</td>\n",
       "      <td>0.063976</td>\n",
       "      <td>0.054435</td>\n",
       "      <td>0.064088</td>\n",
       "      <td>0.022085</td>\n",
       "      <td>0.040857</td>\n",
       "      <td>0.062822</td>\n",
       "      <td>0.061632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.007738</td>\n",
       "      <td>0.014811</td>\n",
       "      <td>0.005088</td>\n",
       "      <td>0.027774</td>\n",
       "      <td>0.006414</td>\n",
       "      <td>0.003201</td>\n",
       "      <td>0.001287</td>\n",
       "      <td>0.132153</td>\n",
       "      <td>0.005814</td>\n",
       "      <td>0.005240</td>\n",
       "      <td>0.006985</td>\n",
       "      <td>0.039944</td>\n",
       "      <td>0.020124</td>\n",
       "      <td>0.004294</td>\n",
       "      <td>0.002421</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "j: less relevant        0         1         2         3         4         5   \\\n",
       "i: more relevant                                                               \n",
       "0                 0.011239  0.018360  0.001709  0.031382  0.003085  0.006665   \n",
       "1                 0.006233  0.013278  0.006526  0.026202  0.007831  0.001717   \n",
       "2                 0.003019  0.003892  0.015439  0.016644  0.016612  0.007433   \n",
       "3                 0.052343  0.045910  0.063380  0.033630  0.063887  0.056357   \n",
       "4                 0.007738  0.014811  0.005088  0.027774  0.006414  0.003201   \n",
       "\n",
       "j: less relevant        6         7         8         9         10        11  \\\n",
       "i: more relevant                                                               \n",
       "0                 0.002152  0.136252  0.002444  0.008714  0.003677  0.043608   \n",
       "1                 0.002756  0.130268  0.007248  0.003749  0.008391  0.038336   \n",
       "2                 0.011836  0.119316  0.016137  0.005429  0.017114  0.028621   \n",
       "3                 0.060545  0.064932  0.063976  0.054435  0.064088  0.022085   \n",
       "4                 0.001287  0.132153  0.005814  0.005240  0.006985  0.039944   \n",
       "\n",
       "j: less relevant        12        13        14  \n",
       "i: more relevant                                \n",
       "0                 0.023696  0.000901  0.001014  \n",
       "1                 0.018575  0.005740  0.003887  \n",
       "2                 0.009122  0.014693  0.012953  \n",
       "3                 0.040857  0.062822  0.061632  \n",
       "4                 0.020124  0.004294  0.002421  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(lamb_updates)\n",
    "df.index.name = 'i: more relevant'\n",
    "df.columns.name = 'j: less relevant'\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff68b68d-d89c-46b2-8128-7abf426622bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambs = torch.zeros((n_docs, 1)).to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9faaa6a0-2088-435e-b2df-8397c60f94c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2765],\n",
       "        [0.2428],\n",
       "        [0.3217],\n",
       "        [0.1853],\n",
       "        [1.7076]], device='cuda:0', grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lamb_updates.sum(dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdaaf2e8-0a90-4d24-85b2-542829f30b3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1494],\n",
       "        [0.2626],\n",
       "        [0.1673],\n",
       "        [0.1906],\n",
       "        [0.1732],\n",
       "        [0.1788],\n",
       "        [0.1588],\n",
       "        [0.1950],\n",
       "        [0.1970],\n",
       "        [0.1991],\n",
       "        [0.1422],\n",
       "        [0.1870],\n",
       "        [0.1939],\n",
       "        [0.1836],\n",
       "        [0.1553]], device='cuda:0', grad_fn=<TBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lamb_updates.sum(dim=0, keepdim=True).t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed45ecec-5f5e-43cb-9c5e-5360d58b1770",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101699fc-986f-440a-965a-11551f1cb2ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d2a1022-5b8f-4e28-8413-d15e40ce2cc3",
   "metadata": {},
   "source": [
    "#### Analysis to find out what the L2R model is upto:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8eb5ec-bbe7-4c73-914c-48f7a486358f",
   "metadata": {},
   "source": [
    "**On the entire dataset:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd4e444-20c3-4cd1-97c6-ab7bde7b4ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_accuracy(dset):\n",
    "    dset = dset.unsqueeze(0)\n",
    "    dset_chnked = torch.split(dset, 100, dim=1)\n",
    "    acc = []\n",
    "    for chunk in  dset_chnked:\n",
    "        btch_acc = batch_lbs_accuracy(rank_model(chunk), chunk)\n",
    "        acc.append(btch_acc)\n",
    "    acc = torch.cat(acc, dim=-1)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0aecaec-8830-4af2-9b70-0c62099f301b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ndcg_at_k(dset, k=20):\n",
    "    dset = dset.unsqueeze(0)\n",
    "    dset_chnked = torch.split(dset, 100, dim=1)\n",
    "    ndcg_at_k_list = []\n",
    "    for chunk in  dset_chnked:\n",
    "        *_, ndcg_at_k = ndcg(rank_model(chunk), chunk, k=k)\n",
    "        ndcg_at_k_list.append(ndcg_at_k)\n",
    "    ndcg_at_k_all = torch.cat(ndcg_at_k_list, dim=-1)\n",
    "    return ndcg_at_k_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd0fa89-0b5d-44b5-ba25-351d9f15bc78",
   "metadata": {},
   "source": [
    "The `model` we are analysing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04416069-80b0-4933-8580-4dbc62d72064",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = rank_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e552673-7f1e-4b86-b28d-21412b380e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| trn.shape: torch.Size([8922, 57352, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([8922, 57352, 3])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn, _ = torch.load('trn_val_split.pkl')\n",
    "trn = trn.to(\"cuda:0\")\n",
    "# trn_data = dls.train.dataset.to(\"cuda:0\")\n",
    "ic(trn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda60ba7-1478-4d30-b72f-e3aff49e5548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.5 s, sys: 0 ns, total: 11.5 s\n",
      "Wall time: 12.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ndcg_at_20 = ndcg_at_k(trn, model, k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d1b5ff-0f4c-4f2f-9940-d6585209cb8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| ndcg_at_20.shape: torch.Size([1, 8922])\n",
      "    ndcg_at_20.min(): tensor(1.1836e-26, device='cuda:0')\n",
      "    ndcg_at_20.mean(): tensor(0.0003, device='cuda:0')\n",
      "    ndcg_at_20.max(): tensor(0.2974, device='cuda:0')\n",
      "    ndcg_at_20.median(): tensor(1.9523e-18, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "ic(ndcg_at_20.shape, ndcg_at_20.min(), ndcg_at_20.mean(), ndcg_at_20.max(), ndcg_at_20.median());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d685084-2ce7-4850-8a74-bed9efa563ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 47s, sys: 602 ms, total: 1min 48s\n",
      "Wall time: 13.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "acc = accuracy(trn, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ad2c1e-0225-4730-957b-30e01c669099",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| acc.shape: torch.Size([1, 8922])\n",
      "    acc.min(): tensor(0.9998, device='cuda:0')\n",
      "    acc.mean(): tensor(1.0000, device='cuda:0')\n",
      "    acc.max(): tensor(1., device='cuda:0')\n",
      "    acc.median(): tensor(1., device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "ic(acc.shape, acc.min(), acc.mean(), acc.max(), acc.median());"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcd63a6-e197-4253-89b9-7bb2aed7d67e",
   "metadata": {},
   "source": [
    "(Below is ndcg formulation with just relevance on the numerator instead of 2 to the power of relevance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bb6ead-4801-4a62-9768-020e5bd7e73d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| ndcg_at_20.shape: torch.Size([1, 8922])\n",
      "    ndcg_at_20.min(): tensor(0.0988, device='cuda:0')\n",
      "    ndcg_at_20.mean(): tensor(0.4004, device='cuda:0')\n",
      "    ndcg_at_20.max(): tensor(0.6808, device='cuda:0')\n",
      "    ndcg_at_20.median(): tensor(0.4071, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "ic(ndcg_at_20.shape, ndcg_at_20.min(), ndcg_at_20.mean(), ndcg_at_20.max(), ndcg_at_20.median());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee1c09e-3ba0-43b8-8d7e-abf0837d86d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndcg_at_20 = [None]\n",
    "import gc; gc.collect(); torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93dfaa5-af6e-4704-b4a7-9fb3e08e18d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ic(len(trn_data_chunked), trn_data.shape[1]/100, trn_data_chunked[-1].shape, trn_data.shape[1]%100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d2baa4-81e2-44ce-aa1e-13cadadd4586",
   "metadata": {},
   "source": [
    "On some selected labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b024e4b-1fdc-4ae0-9163-5f53d2bd5209",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| xb.shape: torch.Size([1, 200, 57352, 3])\n"
     ]
    }
   ],
   "source": [
    "xb = trn[7000:7200]\n",
    "xb = xb.unsqueeze(0)\n",
    "ic(xb.shape);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d073746-d059-4724-8fb3-1d6936cbe630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 699 ms, sys: 10.7 ms, total: 709 ms\n",
      "Wall time: 1.24 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "preds, preds_rank, ideal_rank, discnt_fac, ideal_discnt_fac, discntd_gain, ideal_discntd_gain, dcg, idcg, _ndcg, _ndcg_at_k = ndcg(model(xb), xb, k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bd782b-ce2f-4aa9-99e1-32d3affb5b5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.6429e-07, device='cuda:0')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_ndcg_at_k.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cd359e-19e8-44c6-b35f-8c934df48294",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca38c4f-cb4f-4c75-b82a-daff3ac07e27",
   "metadata": {},
   "source": [
    "**On a training batch:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913892a7-8288-4579-9d04-9954c73cf158",
   "metadata": {},
   "outputs": [],
   "source": [
    "xb_iter = iter(dls.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563e3566-974f-4845-88cf-5f2f38cc665a",
   "metadata": {},
   "outputs": [],
   "source": [
    "xb = next(xb_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f9f7c1-ba3e-4f69-b4ed-d182a4511f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| xb.shape: torch.Size([64, 2233, 64, 4])\n",
      "    xb.device: device(type='cuda', index=0)\n",
      "    preds.shape: torch.Size([64, 2233, 64, 1])\n",
      "    preds.device: device(type='cuda', index=0)\n"
     ]
    }
   ],
   "source": [
    "preds = rank_model(xb)\n",
    "ic(xb.shape, xb.device, preds.shape, preds.device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dfc2ef-7007-4d37-b6be-83d53521cec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.41 s, sys: 3.31 ms, total: 1.42 s\n",
      "Wall time: 3.33 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "preds, preds_rank, ideal_rank, discnt_fac, ideal_discnt_fac, discntd_gain, ideal_discntd_gain, dcg, idcg, _ndcg, _ndcg_at_k = ndcg(model(xb), xb, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f16a2cd-6b28-44ea-96b3-baec42507a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| _ndcg.shape: torch.Size([64, 2233])\n",
      "    _ndcg.mean(): tensor(0.7898, device='cuda:0')\n",
      "    _ndcg_at_k.shape: torch.Size([64, 2233])\n",
      "    _ndcg_at_k.mean(): tensor(0.2059, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "ic(_ndcg.shape, _ndcg.mean(), _ndcg_at_k.shape, _ndcg_at_k.mean());"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1f81bf-52e8-4d4d-bd1c-d4b4116db54b",
   "metadata": {},
   "source": [
    "Let's compute order accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0892049e-fab0-4451-9c8e-c8f50994ba7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| btch_acc.shape: torch.Size([64, 2233])\n",
      "    btch_acc.mean(): tensor(0.6112, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "btch_acc = accuracy(xb, rank_model)\n",
    "ic(btch_acc.shape, btch_acc.mean());"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ef7551-9e7c-4094-9b19-a496daaee801",
   "metadata": {},
   "source": [
    "**On a validation batch:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc553f9c-9b74-4ae3-a3a5-17023d53af06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| xb_val.shape: torch.Size([1, 8922, 32, 4])\n",
      "    xb_val.device: device(type='cuda', index=0)\n"
     ]
    }
   ],
   "source": [
    "xb_val = dls.valid.one_batch()\n",
    "ic(xb_val.shape, xb_val.device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7980f5-05bf-475c-809f-f0426fa91966",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, preds_rank, ideal_rank, discnt_fac, ideal_discnt_fac, discntd_gain, ideal_discntd_gain, dcg, idcg, _ndcg, _ndcg_at_k = ndcg(rank_model(xb_val), xb_val, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bf23fe-a51b-4237-8c3e-d1518998183d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| _ndcg.shape: torch.Size([1, 8922])\n",
      "    _ndcg.mean(): tensor(0.7097, device='cuda:0')\n",
      "    _ndcg_at_k.shape: torch.Size([1, 8922])\n",
      "    _ndcg_at_k.mean(): tensor(0.1040, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "ic(_ndcg.shape, _ndcg.mean(), _ndcg_at_k.shape, _ndcg_at_k.mean());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b699c9f9-b670-4380-be0c-9e7039487c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "xb = xb_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d896ed-2993-4e20-8307-399c38836029",
   "metadata": {},
   "source": [
    "Let's compute order accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3172059a-5b84-46d1-b77e-858e61f883c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| acc.shape: torch.Size([1, 8922])\n",
      "    acc.mean(): tensor(0.5952, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "acc = accuracy(xb, rank_model)\n",
    "ic(acc.shape, acc.mean());"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411ba8bf-7a5f-424e-8799-5144f88c55f8",
   "metadata": {},
   "source": [
    "**Let's pick a label and see the rankings produced by the model:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a4c70d-445f-468d-abe7-53ab2dd7a6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl = 7056"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369ac76e-2858-4132-bf1a-4e13443136c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 200, 57352, 3])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bccce07-6a7f-4854-b3df-bd61af691604",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7056.], device='cuda:0')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb[0, lbl%100, :, 1].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5e6712-df78-42cf-a339-a3824511fde9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| dcg[0, lbl%100]: tensor(49457.4609, device='cuda:0')\n",
      "    idcg[0, lbl%100]: tensor(51637.4922, device='cuda:0')\n",
      "    _ndcg[0, lbl%100]: tensor(0.9578, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "ic(dcg[0, lbl%100], idcg[0, lbl%100], _ndcg[0, lbl%100]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c949db1d-c217-4ad6-a8ee-c0c0b6393f9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.8407e-16, device='cuda:0')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_ndcg_at_k[0, lbl%100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed0df86-9be3-4a02-a7d0-517d81a246ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| len(_df): 57352\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tok</th>\n",
       "      <th>lbl</th>\n",
       "      <th>score</th>\n",
       "      <th>preds</th>\n",
       "      <th>model_rank</th>\n",
       "      <th>ideal_rank</th>\n",
       "      <th>model_discount factor</th>\n",
       "      <th>ideal_discount_factor</th>\n",
       "      <th>discounted gain</th>\n",
       "      <th>ideal discounted gain</th>\n",
       "      <th>random_rank</th>\n",
       "      <th>random_discount_factor</th>\n",
       "      <th>random_discnt_gain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>7056.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>455.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.836050</td>\n",
       "      <td>4.503600e+15</td>\n",
       "      <td>7.652698e+14</td>\n",
       "      <td>53131</td>\n",
       "      <td>15.697321</td>\n",
       "      <td>2.869024e+14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>7056.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>39530.0</td>\n",
       "      <td>1.584962</td>\n",
       "      <td>15.270733</td>\n",
       "      <td>9.463946e+00</td>\n",
       "      <td>5.726525e+01</td>\n",
       "      <td>19867</td>\n",
       "      <td>14.278232</td>\n",
       "      <td>1.050550e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>7056.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>39531.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>15.270770</td>\n",
       "      <td>7.500000e+00</td>\n",
       "      <td>5.726539e+01</td>\n",
       "      <td>38645</td>\n",
       "      <td>15.238069</td>\n",
       "      <td>9.843767e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>7056.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>39532.0</td>\n",
       "      <td>2.321928</td>\n",
       "      <td>15.270806</td>\n",
       "      <td>6.460148e+00</td>\n",
       "      <td>5.726552e+01</td>\n",
       "      <td>2377</td>\n",
       "      <td>11.216140</td>\n",
       "      <td>1.337358e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>7056.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>39533.0</td>\n",
       "      <td>2.584962</td>\n",
       "      <td>15.270843</td>\n",
       "      <td>5.802793e+00</td>\n",
       "      <td>5.726566e+01</td>\n",
       "      <td>24434</td>\n",
       "      <td>14.576720</td>\n",
       "      <td>1.029038e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tok     lbl  score  preds  model_rank  ideal_rank  model_discount factor  \\\n",
       "0  0.0  7056.0   52.0    0.0         0.0       455.0               1.000000   \n",
       "1  1.0  7056.0    4.0    0.0         1.0     39530.0               1.584962   \n",
       "2  2.0  7056.0    4.0    0.0         2.0     39531.0               2.000000   \n",
       "3  3.0  7056.0    4.0    0.0         3.0     39532.0               2.321928   \n",
       "4  4.0  7056.0    4.0    0.0         4.0     39533.0               2.584962   \n",
       "\n",
       "   ideal_discount_factor  discounted gain  ideal discounted gain  random_rank  \\\n",
       "0               8.836050     4.503600e+15           7.652698e+14        53131   \n",
       "1              15.270733     9.463946e+00           5.726525e+01        19867   \n",
       "2              15.270770     7.500000e+00           5.726539e+01        38645   \n",
       "3              15.270806     6.460148e+00           5.726552e+01         2377   \n",
       "4              15.270843     5.802793e+00           5.726566e+01        24434   \n",
       "\n",
       "   random_discount_factor  random_discnt_gain  \n",
       "0               15.697321        2.869024e+14  \n",
       "1               14.278232        1.050550e+00  \n",
       "2               15.238069        9.843767e-01  \n",
       "3               11.216140        1.337358e+00  \n",
       "4               14.576720        1.029038e+00  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eps = 1e-15\n",
    "_data = torch.concat(\n",
    "            (xb[0, lbl%100], preds[0, lbl%100].unsqueeze(-1), preds_rank[0, lbl%100].unsqueeze(-1), ideal_rank[0, lbl%100].unsqueeze(-1), discnt_fac[0, lbl%100].unsqueeze(-1), ideal_discnt_fac[0, lbl%100].unsqueeze(-1), discntd_gain[0, lbl%100].unsqueeze(-1), ideal_discntd_gain[0, lbl%100].unsqueeze(-1)\n",
    "        ), dim=-1)\n",
    "_df = pd.DataFrame(_data, \n",
    "             columns=['tok', 'lbl', 'score', 'preds', 'model_rank', 'ideal_rank', 'model_discount factor', 'ideal_discount_factor', 'discounted gain', 'ideal discounted gain'])#.sort_values(by='score', ascending=False)\n",
    "ic(len(_df));\n",
    "random_rank = torch.randperm(len(_df))\n",
    "_df['random_rank'] = random_rank\n",
    "random_discnt_fac = torch.log2(2 + random_rank)\n",
    "_df['random_discount_factor'] = random_discnt_fac\n",
    "\n",
    "_df['discounted gain'] = (np.power(2, _df['score']) - 1) / (_df['model_discount factor'] + eps)\n",
    "_df['ideal discounted gain'] = (np.power(2, _df['score']) - 1) / (_df['ideal discounted gain'] + eps)\n",
    "_df['random_discnt_gain'] = (np.power(2, _df['score']) - 1) / (_df['random_discount_factor'] + eps)\n",
    "\n",
    "_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b068a5-87a5-4ca4-b6a1-45be7d327b36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tok</th>\n",
       "      <th>lbl</th>\n",
       "      <th>score</th>\n",
       "      <th>preds</th>\n",
       "      <th>model_rank</th>\n",
       "      <th>ideal_rank</th>\n",
       "      <th>model_discount factor</th>\n",
       "      <th>ideal_discount_factor</th>\n",
       "      <th>discounted gain</th>\n",
       "      <th>ideal discounted gain</th>\n",
       "      <th>random_rank</th>\n",
       "      <th>random_discount_factor</th>\n",
       "      <th>random_discnt_gain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>7056.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>455.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.836050</td>\n",
       "      <td>4.503600e+15</td>\n",
       "      <td>7.652698e+14</td>\n",
       "      <td>53131</td>\n",
       "      <td>15.697321</td>\n",
       "      <td>2.869024e+14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39429</th>\n",
       "      <td>39429.0</td>\n",
       "      <td>7056.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39429.0</td>\n",
       "      <td>29941.0</td>\n",
       "      <td>15.267043</td>\n",
       "      <td>14.869931</td>\n",
       "      <td>8.318572e+00</td>\n",
       "      <td>2.697831e+02</td>\n",
       "      <td>17258</td>\n",
       "      <td>14.075145</td>\n",
       "      <td>9.022998e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38227</th>\n",
       "      <td>38227.0</td>\n",
       "      <td>7056.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38227.0</td>\n",
       "      <td>24383.0</td>\n",
       "      <td>15.222380</td>\n",
       "      <td>14.573707</td>\n",
       "      <td>6.720368e+01</td>\n",
       "      <td>1.490890e+03</td>\n",
       "      <td>45839</td>\n",
       "      <td>15.484351</td>\n",
       "      <td>6.606670e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38228</th>\n",
       "      <td>38228.0</td>\n",
       "      <td>7056.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38228.0</td>\n",
       "      <td>49027.0</td>\n",
       "      <td>15.222418</td>\n",
       "      <td>15.581347</td>\n",
       "      <td>1.970778e-01</td>\n",
       "      <td>2.337202e+01</td>\n",
       "      <td>21011</td>\n",
       "      <td>14.358994</td>\n",
       "      <td>2.089283e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38229</th>\n",
       "      <td>38229.0</td>\n",
       "      <td>7056.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38229.0</td>\n",
       "      <td>49028.0</td>\n",
       "      <td>15.222455</td>\n",
       "      <td>15.581377</td>\n",
       "      <td>1.970773e-01</td>\n",
       "      <td>2.337206e+01</td>\n",
       "      <td>47460</td>\n",
       "      <td>15.534485</td>\n",
       "      <td>1.931187e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38230</th>\n",
       "      <td>38230.0</td>\n",
       "      <td>7056.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38230.0</td>\n",
       "      <td>26157.0</td>\n",
       "      <td>15.222493</td>\n",
       "      <td>14.675020</td>\n",
       "      <td>3.356874e+01</td>\n",
       "      <td>8.332151e+02</td>\n",
       "      <td>26441</td>\n",
       "      <td>14.690598</td>\n",
       "      <td>3.478415e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38231</th>\n",
       "      <td>38231.0</td>\n",
       "      <td>7056.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38231.0</td>\n",
       "      <td>28058.0</td>\n",
       "      <td>15.222530</td>\n",
       "      <td>14.776227</td>\n",
       "      <td>1.675149e+01</td>\n",
       "      <td>4.709922e+02</td>\n",
       "      <td>50023</td>\n",
       "      <td>15.610362</td>\n",
       "      <td>1.633530e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38232</th>\n",
       "      <td>38232.0</td>\n",
       "      <td>7056.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38232.0</td>\n",
       "      <td>26158.0</td>\n",
       "      <td>15.222569</td>\n",
       "      <td>14.675075</td>\n",
       "      <td>3.356858e+01</td>\n",
       "      <td>8.332181e+02</td>\n",
       "      <td>25692</td>\n",
       "      <td>14.649144</td>\n",
       "      <td>3.488258e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38233</th>\n",
       "      <td>38233.0</td>\n",
       "      <td>7056.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38233.0</td>\n",
       "      <td>24384.0</td>\n",
       "      <td>15.222607</td>\n",
       "      <td>14.573766</td>\n",
       "      <td>6.720268e+01</td>\n",
       "      <td>1.490896e+03</td>\n",
       "      <td>42174</td>\n",
       "      <td>15.364135</td>\n",
       "      <td>6.658364e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38234</th>\n",
       "      <td>38234.0</td>\n",
       "      <td>7056.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38234.0</td>\n",
       "      <td>26159.0</td>\n",
       "      <td>15.222644</td>\n",
       "      <td>14.675130</td>\n",
       "      <td>3.356841e+01</td>\n",
       "      <td>8.332213e+02</td>\n",
       "      <td>23758</td>\n",
       "      <td>14.536247</td>\n",
       "      <td>3.515350e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           tok     lbl  score  preds  model_rank  ideal_rank  \\\n",
       "0          0.0  7056.0   52.0    0.0         0.0       455.0   \n",
       "39429  39429.0  7056.0    7.0    0.0     39429.0     29941.0   \n",
       "38227  38227.0  7056.0   10.0    0.0     38227.0     24383.0   \n",
       "38228  38228.0  7056.0    2.0    0.0     38228.0     49027.0   \n",
       "38229  38229.0  7056.0    2.0    0.0     38229.0     49028.0   \n",
       "38230  38230.0  7056.0    9.0    0.0     38230.0     26157.0   \n",
       "38231  38231.0  7056.0    8.0    0.0     38231.0     28058.0   \n",
       "38232  38232.0  7056.0    9.0    0.0     38232.0     26158.0   \n",
       "38233  38233.0  7056.0   10.0    0.0     38233.0     24384.0   \n",
       "38234  38234.0  7056.0    9.0    0.0     38234.0     26159.0   \n",
       "\n",
       "       model_discount factor  ideal_discount_factor  discounted gain  \\\n",
       "0                   1.000000               8.836050     4.503600e+15   \n",
       "39429              15.267043              14.869931     8.318572e+00   \n",
       "38227              15.222380              14.573707     6.720368e+01   \n",
       "38228              15.222418              15.581347     1.970778e-01   \n",
       "38229              15.222455              15.581377     1.970773e-01   \n",
       "38230              15.222493              14.675020     3.356874e+01   \n",
       "38231              15.222530              14.776227     1.675149e+01   \n",
       "38232              15.222569              14.675075     3.356858e+01   \n",
       "38233              15.222607              14.573766     6.720268e+01   \n",
       "38234              15.222644              14.675130     3.356841e+01   \n",
       "\n",
       "       ideal discounted gain  random_rank  random_discount_factor  \\\n",
       "0               7.652698e+14        53131               15.697321   \n",
       "39429           2.697831e+02        17258               14.075145   \n",
       "38227           1.490890e+03        45839               15.484351   \n",
       "38228           2.337202e+01        21011               14.358994   \n",
       "38229           2.337206e+01        47460               15.534485   \n",
       "38230           8.332151e+02        26441               14.690598   \n",
       "38231           4.709922e+02        50023               15.610362   \n",
       "38232           8.332181e+02        25692               14.649144   \n",
       "38233           1.490896e+03        42174               15.364135   \n",
       "38234           8.332213e+02        23758               14.536247   \n",
       "\n",
       "       random_discnt_gain  \n",
       "0            2.869024e+14  \n",
       "39429        9.022998e+00  \n",
       "38227        6.606670e+01  \n",
       "38228        2.089283e-01  \n",
       "38229        1.931187e-01  \n",
       "38230        3.478415e+01  \n",
       "38231        1.633530e+01  \n",
       "38232        3.488258e+01  \n",
       "38233        6.658364e+01  \n",
       "38234        3.515350e+01  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_df1 = _df.sort_values(by='preds', ascending=False).head(20)\n",
    "_df1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130ddf4f-688e-480c-8ab5-40920db7ba44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5481220478165346.0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lbl_dcg20 = ( (pow(2, _df1.score) - 1) / (np.log2(2 + np.arange(20)) + eps) ).sum()\n",
    "lbl_dcg20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527a1322-9db4-4255-9a78-beb5d9c75c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lbl_dcg20 = _df1['discounted gain'].sum()\n",
    "# lbl_dcg20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1c6073-f4a4-44c6-9ca2-8072f2e3cbf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tok</th>\n",
       "      <th>lbl</th>\n",
       "      <th>score</th>\n",
       "      <th>preds</th>\n",
       "      <th>model_rank</th>\n",
       "      <th>ideal_rank</th>\n",
       "      <th>model_discount factor</th>\n",
       "      <th>ideal_discount_factor</th>\n",
       "      <th>discounted gain</th>\n",
       "      <th>ideal discounted gain</th>\n",
       "      <th>random_rank</th>\n",
       "      <th>random_discount_factor</th>\n",
       "      <th>random_discnt_gain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>41633</th>\n",
       "      <td>41633.0</td>\n",
       "      <td>7056.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>2.175395</td>\n",
       "      <td>21063.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.362560</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.765215e+29</td>\n",
       "      <td>2.510199e+28</td>\n",
       "      <td>46470</td>\n",
       "      <td>15.504074</td>\n",
       "      <td>1.635248e+29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46021</th>\n",
       "      <td>46021.0</td>\n",
       "      <td>7056.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>-0.183124</td>\n",
       "      <td>25097.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>14.615342</td>\n",
       "      <td>2.807355</td>\n",
       "      <td>8.673424e+28</td>\n",
       "      <td>3.558745e+28</td>\n",
       "      <td>27876</td>\n",
       "      <td>14.766839</td>\n",
       "      <td>8.584442e+28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22263</th>\n",
       "      <td>22263.0</td>\n",
       "      <td>7056.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>-1.027545</td>\n",
       "      <td>26530.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>14.695446</td>\n",
       "      <td>2.584962</td>\n",
       "      <td>8.626146e+28</td>\n",
       "      <td>3.276829e+28</td>\n",
       "      <td>37411</td>\n",
       "      <td>15.191252</td>\n",
       "      <td>8.344610e+28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18628</th>\n",
       "      <td>18628.0</td>\n",
       "      <td>7056.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>32.953648</td>\n",
       "      <td>1527.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.578373</td>\n",
       "      <td>1.584962</td>\n",
       "      <td>1.198342e+29</td>\n",
       "      <td>2.009179e+28</td>\n",
       "      <td>6920</td>\n",
       "      <td>12.756973</td>\n",
       "      <td>9.936923e+28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18638</th>\n",
       "      <td>18638.0</td>\n",
       "      <td>7056.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.686037</td>\n",
       "      <td>21875.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>14.417128</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>8.792671e+28</td>\n",
       "      <td>2.535301e+28</td>\n",
       "      <td>52030</td>\n",
       "      <td>15.667111</td>\n",
       "      <td>8.091157e+28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20038</th>\n",
       "      <td>20038.0</td>\n",
       "      <td>7056.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>-13.829051</td>\n",
       "      <td>46904.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>15.517485</td>\n",
       "      <td>2.321928</td>\n",
       "      <td>8.169175e+28</td>\n",
       "      <td>2.943393e+28</td>\n",
       "      <td>41464</td>\n",
       "      <td>15.339642</td>\n",
       "      <td>8.263886e+28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17155</th>\n",
       "      <td>17155.0</td>\n",
       "      <td>7056.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>-12.660876</td>\n",
       "      <td>45510.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.473959</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.096077e+28</td>\n",
       "      <td>1.920683e+28</td>\n",
       "      <td>25385</td>\n",
       "      <td>14.631803</td>\n",
       "      <td>4.331833e+28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14456</th>\n",
       "      <td>14456.0</td>\n",
       "      <td>7056.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>-23.737192</td>\n",
       "      <td>54115.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>15.723794</td>\n",
       "      <td>3.169925</td>\n",
       "      <td>1.007749e+28</td>\n",
       "      <td>5.178296e+27</td>\n",
       "      <td>55496</td>\n",
       "      <td>15.760148</td>\n",
       "      <td>1.005424e+28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14135</th>\n",
       "      <td>14135.0</td>\n",
       "      <td>7056.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>4.254682</td>\n",
       "      <td>17779.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>14.118049</td>\n",
       "      <td>3.321928</td>\n",
       "      <td>5.611835e+27</td>\n",
       "      <td>2.741565e+27</td>\n",
       "      <td>53704</td>\n",
       "      <td>15.712795</td>\n",
       "      <td>5.042270e+27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16272</th>\n",
       "      <td>16272.0</td>\n",
       "      <td>7056.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>33.406235</td>\n",
       "      <td>1496.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.548821</td>\n",
       "      <td>3.459432</td>\n",
       "      <td>3.755309e+27</td>\n",
       "      <td>1.442550e+27</td>\n",
       "      <td>39661</td>\n",
       "      <td>15.275506</td>\n",
       "      <td>2.593307e+27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           tok     lbl  score      preds  model_rank  ideal_rank  \\\n",
       "41633  41633.0  7056.0  101.0   2.175395     21063.0         0.0   \n",
       "46021  46021.0  7056.0  100.0  -0.183124     25097.0         5.0   \n",
       "22263  22263.0  7056.0  100.0  -1.027545     26530.0         4.0   \n",
       "18628  18628.0  7056.0  100.0  32.953648      1527.0         1.0   \n",
       "18638  18638.0  7056.0  100.0   1.686037     21875.0         2.0   \n",
       "20038  20038.0  7056.0  100.0 -13.829051     46904.0         3.0   \n",
       "17155  17155.0  7056.0   99.0 -12.660876     45510.0         6.0   \n",
       "14456  14456.0  7056.0   97.0 -23.737192     54115.0         7.0   \n",
       "14135  14135.0  7056.0   96.0   4.254682     17779.0         8.0   \n",
       "16272  16272.0  7056.0   95.0  33.406235      1496.0         9.0   \n",
       "\n",
       "       model_discount factor  ideal_discount_factor  discounted gain  \\\n",
       "41633              14.362560               1.000000     1.765215e+29   \n",
       "46021              14.615342               2.807355     8.673424e+28   \n",
       "22263              14.695446               2.584962     8.626146e+28   \n",
       "18628              10.578373               1.584962     1.198342e+29   \n",
       "18638              14.417128               2.000000     8.792671e+28   \n",
       "20038              15.517485               2.321928     8.169175e+28   \n",
       "17155              15.473959               3.000000     4.096077e+28   \n",
       "14456              15.723794               3.169925     1.007749e+28   \n",
       "14135              14.118049               3.321928     5.611835e+27   \n",
       "16272              10.548821               3.459432     3.755309e+27   \n",
       "\n",
       "       ideal discounted gain  random_rank  random_discount_factor  \\\n",
       "41633           2.510199e+28        46470               15.504074   \n",
       "46021           3.558745e+28        27876               14.766839   \n",
       "22263           3.276829e+28        37411               15.191252   \n",
       "18628           2.009179e+28         6920               12.756973   \n",
       "18638           2.535301e+28        52030               15.667111   \n",
       "20038           2.943393e+28        41464               15.339642   \n",
       "17155           1.920683e+28        25385               14.631803   \n",
       "14456           5.178296e+27        55496               15.760148   \n",
       "14135           2.741565e+27        53704               15.712795   \n",
       "16272           1.442550e+27        39661               15.275506   \n",
       "\n",
       "       random_discnt_gain  \n",
       "41633        1.635248e+29  \n",
       "46021        8.584442e+28  \n",
       "22263        8.344610e+28  \n",
       "18628        9.936923e+28  \n",
       "18638        8.091157e+28  \n",
       "20038        8.263886e+28  \n",
       "17155        4.331833e+28  \n",
       "14456        1.005424e+28  \n",
       "14135        5.042270e+27  \n",
       "16272        2.593307e+27  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_df2 = _df.sort_values(by='score', ascending=False).head(20)\n",
    "_df2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f32a77-2c02-4eac-94eb-b329d997e6ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.762852934816773e+30"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lbl_idcg20 = ( (pow(2, _df2.score) - 1) / (np.log2(2 + np.arange(20)) + eps) ).sum()\n",
    "lbl_idcg20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef7db8a-05b8-475b-a498-bb20e19a7b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lbl_idcg20 = _df2['ideal discounted gain'].sum()\n",
    "# lbl_idcg20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce1bdfb-b27f-46ef-a52f-69e67c48a6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lbl_ndcg20 = lbl_dcg20/lbl_idcg20\n",
    "# lbl_ndcg20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6238ff12-fddd-4fed-8ff7-3410cbe19c15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.511296818022338e-16"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lbl_ndcg20 = lbl_dcg20 /lbl_idcg20\n",
    "lbl_ndcg20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7acfe35-90a9-469e-b1bf-c948a74c2679",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isclose(lbl_ndcg20, _ndcg_at_k[0, lbl%100].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f18dd1a-c852-461c-9cec-e2f45651c40e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tok</th>\n",
       "      <th>lbl</th>\n",
       "      <th>score</th>\n",
       "      <th>preds</th>\n",
       "      <th>model_rank</th>\n",
       "      <th>ideal_rank</th>\n",
       "      <th>model_discount factor</th>\n",
       "      <th>ideal_discount_factor</th>\n",
       "      <th>discounted gain</th>\n",
       "      <th>ideal discounted gain</th>\n",
       "      <th>random_rank</th>\n",
       "      <th>random_discount_factor</th>\n",
       "      <th>random_discnt_gain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6452</th>\n",
       "      <td>6452.0</td>\n",
       "      <td>7056.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-8.187328</td>\n",
       "      <td>38783.0</td>\n",
       "      <td>36091.0</td>\n",
       "      <td>15.243211</td>\n",
       "      <td>15.139431</td>\n",
       "      <td>2.033692e+00</td>\n",
       "      <td>9.386447e+01</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.100000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16794</th>\n",
       "      <td>16794.0</td>\n",
       "      <td>7056.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>-7.473664</td>\n",
       "      <td>37611.0</td>\n",
       "      <td>7553.0</td>\n",
       "      <td>15.198944</td>\n",
       "      <td>12.883216</td>\n",
       "      <td>2.759602e+05</td>\n",
       "      <td>2.456187e+06</td>\n",
       "      <td>1</td>\n",
       "      <td>1.584962</td>\n",
       "      <td>2.646310e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35009</th>\n",
       "      <td>35009.0</td>\n",
       "      <td>7056.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-11.835570</td>\n",
       "      <td>44385.0</td>\n",
       "      <td>27803.0</td>\n",
       "      <td>15.437850</td>\n",
       "      <td>14.763057</td>\n",
       "      <td>1.651784e+01</td>\n",
       "      <td>4.705724e+02</td>\n",
       "      <td>2</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.275000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57162</th>\n",
       "      <td>57162.0</td>\n",
       "      <td>7056.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.476361</td>\n",
       "      <td>19059.0</td>\n",
       "      <td>39396.0</td>\n",
       "      <td>14.218336</td>\n",
       "      <td>15.265835</td>\n",
       "      <td>2.180283e+00</td>\n",
       "      <td>9.464817e+01</td>\n",
       "      <td>3</td>\n",
       "      <td>2.321928</td>\n",
       "      <td>1.335097e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47032</th>\n",
       "      <td>47032.0</td>\n",
       "      <td>7056.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-19.453012</td>\n",
       "      <td>51849.0</td>\n",
       "      <td>35224.0</td>\n",
       "      <td>15.662085</td>\n",
       "      <td>15.104353</td>\n",
       "      <td>4.022453e+00</td>\n",
       "      <td>1.585957e+02</td>\n",
       "      <td>4</td>\n",
       "      <td>2.584963</td>\n",
       "      <td>2.437173e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39202</th>\n",
       "      <td>39202.0</td>\n",
       "      <td>7056.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.024749</td>\n",
       "      <td>19741.0</td>\n",
       "      <td>26507.0</td>\n",
       "      <td>14.269053</td>\n",
       "      <td>14.694195</td>\n",
       "      <td>3.581177e+01</td>\n",
       "      <td>8.343037e+02</td>\n",
       "      <td>5</td>\n",
       "      <td>2.807355</td>\n",
       "      <td>1.820219e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55155</th>\n",
       "      <td>55155.0</td>\n",
       "      <td>7056.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-7.111054</td>\n",
       "      <td>37005.0</td>\n",
       "      <td>56015.0</td>\n",
       "      <td>15.175510</td>\n",
       "      <td>15.773577</td>\n",
       "      <td>6.589564e-02</td>\n",
       "      <td>1.577358e+01</td>\n",
       "      <td>6</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.333333e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15844</th>\n",
       "      <td>15844.0</td>\n",
       "      <td>7056.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-7.391142</td>\n",
       "      <td>37482.0</td>\n",
       "      <td>43443.0</td>\n",
       "      <td>15.193987</td>\n",
       "      <td>15.406902</td>\n",
       "      <td>4.607086e-01</td>\n",
       "      <td>3.594944e+01</td>\n",
       "      <td>7</td>\n",
       "      <td>3.169925</td>\n",
       "      <td>2.208254e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35714</th>\n",
       "      <td>35714.0</td>\n",
       "      <td>7056.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-2.225584</td>\n",
       "      <td>28616.0</td>\n",
       "      <td>23093.0</td>\n",
       "      <td>14.804635</td>\n",
       "      <td>14.495293</td>\n",
       "      <td>6.909998e+01</td>\n",
       "      <td>1.482869e+03</td>\n",
       "      <td>8</td>\n",
       "      <td>3.321928</td>\n",
       "      <td>3.079537e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20591</th>\n",
       "      <td>20591.0</td>\n",
       "      <td>7056.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-16.564301</td>\n",
       "      <td>49692.0</td>\n",
       "      <td>44555.0</td>\n",
       "      <td>15.600784</td>\n",
       "      <td>15.443364</td>\n",
       "      <td>4.486954e-01</td>\n",
       "      <td>3.603452e+01</td>\n",
       "      <td>9</td>\n",
       "      <td>3.459432</td>\n",
       "      <td>2.023454e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54762</th>\n",
       "      <td>54762.0</td>\n",
       "      <td>7056.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-13.837959</td>\n",
       "      <td>46917.0</td>\n",
       "      <td>38602.0</td>\n",
       "      <td>15.517884</td>\n",
       "      <td>15.236463</td>\n",
       "      <td>1.997695e+00</td>\n",
       "      <td>9.446606e+01</td>\n",
       "      <td>10</td>\n",
       "      <td>3.584963</td>\n",
       "      <td>8.647231e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2342</th>\n",
       "      <td>2342.0</td>\n",
       "      <td>7056.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>3.376895</td>\n",
       "      <td>19198.0</td>\n",
       "      <td>2445.0</td>\n",
       "      <td>14.228819</td>\n",
       "      <td>11.256799</td>\n",
       "      <td>1.207400e+09</td>\n",
       "      <td>5.687951e+09</td>\n",
       "      <td>11</td>\n",
       "      <td>3.700440</td>\n",
       "      <td>4.642656e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53977</th>\n",
       "      <td>53977.0</td>\n",
       "      <td>7056.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-22.591963</td>\n",
       "      <td>53561.0</td>\n",
       "      <td>55106.0</td>\n",
       "      <td>15.708949</td>\n",
       "      <td>15.749974</td>\n",
       "      <td>6.365798e-02</td>\n",
       "      <td>1.574998e+01</td>\n",
       "      <td>12</td>\n",
       "      <td>3.807355</td>\n",
       "      <td>2.626495e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3246</th>\n",
       "      <td>3246.0</td>\n",
       "      <td>7056.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>-12.946091</td>\n",
       "      <td>45862.0</td>\n",
       "      <td>8963.0</td>\n",
       "      <td>15.485075</td>\n",
       "      <td>13.130088</td>\n",
       "      <td>6.771520e+04</td>\n",
       "      <td>6.883941e+05</td>\n",
       "      <td>13</td>\n",
       "      <td>3.906891</td>\n",
       "      <td>2.683912e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24586</th>\n",
       "      <td>24586.0</td>\n",
       "      <td>7056.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>-13.977197</td>\n",
       "      <td>47075.0</td>\n",
       "      <td>12858.0</td>\n",
       "      <td>15.522735</td>\n",
       "      <td>13.650603</td>\n",
       "      <td>8.443809e+03</td>\n",
       "      <td>1.052470e+05</td>\n",
       "      <td>14</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.276775e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11426</th>\n",
       "      <td>11426.0</td>\n",
       "      <td>7056.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>26.802952</td>\n",
       "      <td>2047.0</td>\n",
       "      <td>2719.0</td>\n",
       "      <td>11.000704</td>\n",
       "      <td>11.409922</td>\n",
       "      <td>7.808532e+08</td>\n",
       "      <td>2.970014e+09</td>\n",
       "      <td>15</td>\n",
       "      <td>4.087463</td>\n",
       "      <td>2.101532e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29003</th>\n",
       "      <td>29003.0</td>\n",
       "      <td>7056.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>-2.219074</td>\n",
       "      <td>28602.0</td>\n",
       "      <td>17870.0</td>\n",
       "      <td>14.803929</td>\n",
       "      <td>14.125414</td>\n",
       "      <td>5.532991e+02</td>\n",
       "      <td>8.900098e+03</td>\n",
       "      <td>16</td>\n",
       "      <td>4.169925</td>\n",
       "      <td>1.964304e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36063</th>\n",
       "      <td>36063.0</td>\n",
       "      <td>7056.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-7.784392</td>\n",
       "      <td>38155.0</td>\n",
       "      <td>27863.0</td>\n",
       "      <td>15.219660</td>\n",
       "      <td>14.766167</td>\n",
       "      <td>1.675464e+01</td>\n",
       "      <td>4.706716e+02</td>\n",
       "      <td>17</td>\n",
       "      <td>4.247928</td>\n",
       "      <td>6.002927e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7105</th>\n",
       "      <td>7105.0</td>\n",
       "      <td>7056.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>-3.259494</td>\n",
       "      <td>30447.0</td>\n",
       "      <td>12178.0</td>\n",
       "      <td>14.894107</td>\n",
       "      <td>13.572227</td>\n",
       "      <td>8.800192e+03</td>\n",
       "      <td>1.046427e+05</td>\n",
       "      <td>18</td>\n",
       "      <td>4.321928</td>\n",
       "      <td>3.032697e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55287</th>\n",
       "      <td>55287.0</td>\n",
       "      <td>7056.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.269970</td>\n",
       "      <td>13489.0</td>\n",
       "      <td>56114.0</td>\n",
       "      <td>13.719709</td>\n",
       "      <td>15.776125</td>\n",
       "      <td>7.288784e-02</td>\n",
       "      <td>1.577612e+01</td>\n",
       "      <td>19</td>\n",
       "      <td>4.392317</td>\n",
       "      <td>2.276703e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           tok     lbl  score      preds  model_rank  ideal_rank  \\\n",
       "6452    6452.0  7056.0    5.0  -8.187328     38783.0     36091.0   \n",
       "16794  16794.0  7056.0   22.0  -7.473664     37611.0      7553.0   \n",
       "35009  35009.0  7056.0    8.0 -11.835570     44385.0     27803.0   \n",
       "57162  57162.0  7056.0    5.0   3.476361     19059.0     39396.0   \n",
       "47032  47032.0  7056.0    6.0 -19.453012     51849.0     35224.0   \n",
       "39202  39202.0  7056.0    9.0   3.024749     19741.0     26507.0   \n",
       "55155  55155.0  7056.0    1.0  -7.111054     37005.0     56015.0   \n",
       "15844  15844.0  7056.0    3.0  -7.391142     37482.0     43443.0   \n",
       "35714  35714.0  7056.0   10.0  -2.225584     28616.0     23093.0   \n",
       "20591  20591.0  7056.0    3.0 -16.564301     49692.0     44555.0   \n",
       "54762  54762.0  7056.0    5.0 -13.837959     46917.0     38602.0   \n",
       "2342    2342.0  7056.0   34.0   3.376895     19198.0      2445.0   \n",
       "53977  53977.0  7056.0    1.0 -22.591963     53561.0     55106.0   \n",
       "3246    3246.0  7056.0   20.0 -12.946091     45862.0      8963.0   \n",
       "24586  24586.0  7056.0   17.0 -13.977197     47075.0     12858.0   \n",
       "11426  11426.0  7056.0   33.0  26.802952      2047.0      2719.0   \n",
       "29003  29003.0  7056.0   13.0  -2.219074     28602.0     17870.0   \n",
       "36063  36063.0  7056.0    8.0  -7.784392     38155.0     27863.0   \n",
       "7105    7105.0  7056.0   17.0  -3.259494     30447.0     12178.0   \n",
       "55287  55287.0  7056.0    1.0   7.269970     13489.0     56114.0   \n",
       "\n",
       "       model_discount factor  ideal_discount_factor  discounted gain  \\\n",
       "6452               15.243211              15.139431     2.033692e+00   \n",
       "16794              15.198944              12.883216     2.759602e+05   \n",
       "35009              15.437850              14.763057     1.651784e+01   \n",
       "57162              14.218336              15.265835     2.180283e+00   \n",
       "47032              15.662085              15.104353     4.022453e+00   \n",
       "39202              14.269053              14.694195     3.581177e+01   \n",
       "55155              15.175510              15.773577     6.589564e-02   \n",
       "15844              15.193987              15.406902     4.607086e-01   \n",
       "35714              14.804635              14.495293     6.909998e+01   \n",
       "20591              15.600784              15.443364     4.486954e-01   \n",
       "54762              15.517884              15.236463     1.997695e+00   \n",
       "2342               14.228819              11.256799     1.207400e+09   \n",
       "53977              15.708949              15.749974     6.365798e-02   \n",
       "3246               15.485075              13.130088     6.771520e+04   \n",
       "24586              15.522735              13.650603     8.443809e+03   \n",
       "11426              11.000704              11.409922     7.808532e+08   \n",
       "29003              14.803929              14.125414     5.532991e+02   \n",
       "36063              15.219660              14.766167     1.675464e+01   \n",
       "7105               14.894107              13.572227     8.800192e+03   \n",
       "55287              13.719709              15.776125     7.288784e-02   \n",
       "\n",
       "       ideal discounted gain  random_rank  random_discount_factor  \\\n",
       "6452            9.386447e+01            0                1.000000   \n",
       "16794           2.456187e+06            1                1.584962   \n",
       "35009           4.705724e+02            2                2.000000   \n",
       "57162           9.464817e+01            3                2.321928   \n",
       "47032           1.585957e+02            4                2.584963   \n",
       "39202           8.343037e+02            5                2.807355   \n",
       "55155           1.577358e+01            6                3.000000   \n",
       "15844           3.594944e+01            7                3.169925   \n",
       "35714           1.482869e+03            8                3.321928   \n",
       "20591           3.603452e+01            9                3.459432   \n",
       "54762           9.446606e+01           10                3.584963   \n",
       "2342            5.687951e+09           11                3.700440   \n",
       "53977           1.574998e+01           12                3.807355   \n",
       "3246            6.883941e+05           13                3.906891   \n",
       "24586           1.052470e+05           14                4.000000   \n",
       "11426           2.970014e+09           15                4.087463   \n",
       "29003           8.900098e+03           16                4.169925   \n",
       "36063           4.706716e+02           17                4.247928   \n",
       "7105            1.046427e+05           18                4.321928   \n",
       "55287           1.577612e+01           19                4.392317   \n",
       "\n",
       "       random_discnt_gain  \n",
       "6452         3.100000e+01  \n",
       "16794        2.646310e+06  \n",
       "35009        1.275000e+02  \n",
       "57162        1.335097e+01  \n",
       "47032        2.437173e+01  \n",
       "39202        1.820219e+02  \n",
       "55155        3.333333e-01  \n",
       "15844        2.208254e+00  \n",
       "35714        3.079537e+02  \n",
       "20591        2.023454e+00  \n",
       "54762        8.647231e+00  \n",
       "2342         4.642656e+09  \n",
       "53977        2.626495e-01  \n",
       "3246         2.683912e+05  \n",
       "24586        3.276775e+04  \n",
       "11426        2.101532e+09  \n",
       "29003        1.964304e+03  \n",
       "36063        6.002927e+01  \n",
       "7105         3.032697e+04  \n",
       "55287        2.276703e-01  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_df3 = _df.sort_values(by='random_rank', ascending=True).head(20)\n",
    "_df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd839029-e890-4c1a-a34b-c78a99bd2c32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6747168816.981719"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lbl_rnd_dcg20 = ( (pow(2, _df3.score) - 1) / (np.log2(2 + np.arange(20)) + eps) ).sum()\n",
    "lbl_rnd_dcg20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8d39bb-20fe-41e5-956b-2482e345bdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lbl_rnd_dcg20 = _df3['random_discnt_gain'].sum()\n",
    "# lbl_rnd_dcg20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768de66b-3144-41c4-af65-480700114a0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1708035747742436e-21"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lbl_rnd_ndcg20 = lbl_rnd_dcg20/lbl_idcg20\n",
    "lbl_rnd_ndcg20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904c36c8-b483-4275-8d41-9569427267a3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac9e8bf-b73d-4d75-9b88-b6b2e1818633",
   "metadata": {},
   "source": [
    "#### Looking at the token ranks for each label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5222fc16-2f7a-46ea-9fbd-1f806b69686a",
   "metadata": {},
   "source": [
    "##### Setting things up ... for analysis later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e41285b-39fc-49bd-b42e-9d9029870f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path.parent/'data'/'code_desc.pkl', 'rb') as f: lbs_desc = pickle.load(f)\n",
    "df_toks = pd.read_feather(collab_tok_path)\n",
    "df_lbs = pd.read_feather(collab_lbl_path)\n",
    "df_lbs['description'] = df_lbs['lbl_val'].map(lambda x: lbs_desc.get(x, \"Not Found\"))\n",
    "# df_collab = pd.read_feather(collab_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e45036-9f3e-4657-aa7f-c2ef764ec139",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_collab = pd.DataFrame()\n",
    "lst = [df_collab]\n",
    "del lst\n",
    "del df_collab\n",
    "import gc; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb0b078-60a6-4f29-a79e-2bb05d9b70e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_toks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4de2e05-16fd-4da9-9a41-e73341766b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_lbs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e218d7d6-f9b1-40d4-880c-0599be8503ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_toks_tiny = df_toks.iloc[:num]\n",
    "df_lbs_tiny = df_lbs.iloc[:num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfcd006-560c-40df-8efe-e9d47ea472a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lbs_list = random.sample(list(df_lbs.lbl_val), k=10)\n",
    "# df_lbs[df_lbs.lbl_val.isin(lbs_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fae98eb-08ee-49f2-bd5c-e3269fa0adf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_collab.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d468a91-2cb9-484e-93ef-1b9f4155b69e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| datetime.fromtimestamp(time.time()): datetime.datetime(2022, 7, 30, 16, 33, 23, 596222)\n",
      "ic| datetime.fromtimestamp(collab_path.stat().st_ctime): datetime.datetime(2022, 7, 28, 7, 31, 24, 743458)\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "ic(datetime.fromtimestamp(time.time()))\n",
    "ic(datetime.fromtimestamp(collab_path.stat().st_ctime));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53360df5-3a1b-4298-bbc2-bdc5f0362763",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls_collab = torch.load(dls_collab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d1cb11-8683-42dd-9744-aef8de761258",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls_collab.classes['token'].map_objs([9, 17]), dls_collab.classes['token'].map_ids([1,8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68542627-9a0e-4008-97ec-f91722950ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "collab_dict = load_learner(collab_path, cpu=False)\n",
    "if 'model' in collab_dict.keys(): collab_dict = collab_dict['model'] # in case the optimizer was saved as well\n",
    "test_eq(type(collab_dict), OrderedDict)\n",
    "test_eq(collab_dict.keys(), ['token_factors.weight', 'token_bias.weight', 'label_factors.weight', 'label_bias.weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246d809a-221b-452d-bd82-694258435460",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| key: 'token_factors.weight'\n",
      "    collab_dict[key].shape: torch.Size([57352, 400])\n",
      "ic| key: 'token_bias.weight'\n",
      "    collab_dict[key].shape: torch.Size([57352, 1])\n",
      "ic| key: 'label_factors.weight'\n",
      "    collab_dict[key].shape: torch.Size([8922, 400])\n",
      "ic| key: 'label_bias.weight'\n",
      "    collab_dict[key].shape: torch.Size([8922, 1])\n"
     ]
    }
   ],
   "source": [
    "for key in collab_dict.keys():\n",
    "    ic(key, collab_dict[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950270de-f881-42eb-b893-41b648fd8266",
   "metadata": {},
   "outputs": [],
   "source": [
    "toks_red = df_toks.token.to_numpy() #dls_collab.classes['token']\n",
    "lbs = df_lbs.lbl.to_numpy() #dls_collab.classes['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9fc54c-9a23-489f-8981-2153af627978",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_factors = 400\n",
    "# 0 is a nan added by collab dataloaders\n",
    "tok_wgts = collab_dict['u_weight.weight'].detach(); test_eq(tok_wgts.shape, (len(toks_red), n_factors))\n",
    "lbs_wgts = collab_dict['i_weight.weight'].detach(); test_eq(lbs_wgts.shape, (len(lbs), n_factors))\n",
    "tok_bias = collab_dict['u_bias.weight'].detach(); test_eq(tok_bias.shape, [len(toks_red), 1])\n",
    "lbs_bias = collab_dict['i_bias.weight'].detach(); test_eq(lbs_bias.shape, [len(lbs), 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1458c8bc-1533-4395-beed-28e7391fb9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_lbl_list = ['038.2', '038.19']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fee141-7e85-4729-8804-e9ecd439ab8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lbl</th>\n",
       "      <th>lbl_val</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>78</td>\n",
       "      <td>038.19</td>\n",
       "      <td>Other staphylococcal septicemia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>79</td>\n",
       "      <td>038.2</td>\n",
       "      <td>Pneumococcal septicemia [Streptococcus pneumoniae septicemia]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    lbl lbl_val                                                    description\n",
       "78   78  038.19                                Other staphylococcal septicemia\n",
       "79   79   038.2  Pneumococcal septicemia [Streptococcus pneumoniae septicemia]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lbs[df_lbs.lbl_val.isin(a_lbl_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab758b62-de46-463b-8799-fe3042e89a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| lbl_list: array([78, 79])\n"
     ]
    }
   ],
   "source": [
    "lbl_list = df_lbs.loc[df_lbs.lbl_val.isin(a_lbl_list)].lbl.to_numpy()\n",
    "# lbl_list = df_lbs[df_lbs.lbl_val.isin(a_lbl_list)].index\n",
    "ic(lbl_list);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604e18e3-7e4a-4b6a-9a10-3346935266a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| df_lbs.iloc[lbl_list]:     lbl lbl_val                                                    description\n",
      "                           78   78  038.19                                Other staphylococcal septicemia\n",
      "                           79   79   038.2  Pneumococcal septicemia [Streptococcus pneumoniae septicemia]\n"
     ]
    }
   ],
   "source": [
    "ic(df_lbs.iloc[lbl_list]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcffbd3-e9ee-48d5-bd57-3bb3aaa7bfbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| lbl_idxs: [79, 80]\n"
     ]
    }
   ],
   "source": [
    "lbl_idxs = lbs.map_objs(lbl_list)\n",
    "ic(lbl_idxs);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dc452c-c821-4724-b369-c5a16f093721",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| lbs_wgts.shape: torch.Size([8923, 400])\n"
     ]
    }
   ],
   "source": [
    "ic(lbs_wgts.shape);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6e7953-5b3b-4cce-bc96-bfb87480c648",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| sclt_lbl_wgts.shape: torch.Size([2, 400])\n"
     ]
    }
   ],
   "source": [
    "sclt_lbl_wgts = lbs_wgts[lbl_idxs]\n",
    "ic(sclt_lbl_wgts.shape);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ae0798-0811-46f0-88b1-655aff1f1e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| sclt_lbl_bias.shape: torch.Size([2, 1])\n"
     ]
    }
   ],
   "source": [
    "sclt_lbl_bias = lbs_bias[lbl_idxs]\n",
    "ic(sclt_lbl_bias.shape);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e29854-f7fa-4c0e-a507-4a66d8930d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| tok_wgts.shape: torch.Size([33648, 400])\n",
      "ic| tok_bias.shape: torch.Size([33648, 1])\n"
     ]
    }
   ],
   "source": [
    "ic(tok_wgts.shape)\n",
    "ic(tok_bias.shape);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44269029-9483-4790-9267-5865b36d675a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| lbl_all_toks.shape: torch.Size([33647, 2])\n"
     ]
    }
   ],
   "source": [
    "lbl_all_toks = (tok_wgts[1:] @ sclt_lbl_wgts.T) + tok_bias[1:] + sclt_lbl_bias.T \n",
    "lbl_all_toks = sigmoid_range(lbl_all_toks, low=0, high=1)\n",
    "ic(lbl_all_toks.shape);\n",
    "# ic('#######')\n",
    "# lbl_all_toks_full = (tok_wgts @ sclt_lbl_wgts.T) + tok_bias + sclt_lbl_bias.T \n",
    "# lbl_all_toks_full = sigmoid_range(lbl_all_toks_full, low=0, high=1)\n",
    "# ic(lbl_all_toks_full.shape);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7325f316-0cb9-41ae-9799-0feaa8810bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| sorted_tok_idxs: tensor([[17573, 17573],\n",
      "                             [33311, 27710],\n",
      "                             [31448, 33311],\n",
      "                             ...,\n",
      "                             [  383,  7374],\n",
      "                             [ 7731,  7731],\n",
      "                             [  313, 10875]], device='cuda:0')\n",
      "ic| sorted_tok_idxs.shape: torch.Size([33647, 2])\n"
     ]
    }
   ],
   "source": [
    "sorted_tok_idxs = torch.argsort(lbl_all_toks, dim=0, descending=True)\n",
    "ic(sorted_tok_idxs)\n",
    "ic(sorted_tok_idxs.shape);\n",
    "# ic('########')\n",
    "# sorted_tok_idxs_full = torch.argsort(lbl_all_toks_full, dim=0, descending=True)\n",
    "# ic(sorted_tok_idxs_full)\n",
    "# ic(sorted_tok_idxs_full.shape);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a52c7a-26ec-4372-9f8b-82ecd9b19385",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "toks_red_array = L(copy.deepcopy(toks_red))\n",
    "toks_red_array.remove('#na#')\n",
    "toks_red_array = tensor(toks_red_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911f89ea-428a-425b-a43e-1c22e16a6f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| toks_all: array(['xxunk', 'xxpad', 'xxbos', ..., 'pipelle', 'xxfake', 'xxfake'], dtype='<U29')\n"
     ]
    }
   ],
   "source": [
    "toks_all = df_toks.tok_val.to_numpy(dtype=np.str_)\n",
    "ic(toks_all);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b06ddc-a038-4066-aa74-bd2bf1d1c22b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| toks_array.shape: (33647, 2)\n"
     ]
    }
   ],
   "source": [
    "# %%timeit\n",
    "toks_array = toks_all[toks_red_array[sorted_tok_idxs]]\n",
    "ic(toks_array.shape);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e88a77-2012-4245-82a1-d659674c7ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "# toks_array_0 = toks_all[toks_red_array[sorted_tok_idxs[:,0]]]\n",
    "# toks_array_1 = toks_all[toks_red_array[sorted_tok_idxs[:,1]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3c1f13-97c1-4ce5-af04-d04720b830be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "# toks_0 = df_toks.iloc[toks_red_array[sorted_tok_idxs[:,0]]].tok_val.to_numpy(dtype=np.str_)\n",
    "# toks_1 = df_toks.iloc[toks_red_array[sorted_tok_idxs[:,1]]].tok_val.to_numpy(dtype=np.str_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe96be61-550e-44eb-8653-186028a2d7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "# toks_0_full = df_toks.iloc[(o for o in toks_red[sorted_tok_idxs_full[:,0]] if o!='#na#')].tok_val.to_numpy(dtype=np.str_)\n",
    "# toks_1_full = df_toks.iloc[(o for o in toks_red[sorted_tok_idxs_full[:,1]] if o!='#na#')].tok_val.to_numpy(dtype=np.str_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba612f61-ca22-4426-8805-4d3de9fb2536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.logical_not(tensor(toks_0 == toks_0_full)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfa8760-4d77-458e-ab25-cad412017673",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lbs_sorted_tokens(df_toks, df_lbs, collab_path, dls_collab_path=None, lbl_list=None):\n",
    "    # import pdb; pdb.set_trace()\n",
    "    collab_dict = load_learner(collab_path, cpu=False)\n",
    "    if 'model' in collab_dict.keys(): collab_dict = collab_dict['model'] # in case the optimizer was saved as well\n",
    "    test_eq(type(collab_dict), OrderedDict)\n",
    "    test_eq(collab_dict.keys(), ['token_factors.weight', 'token_bias.weight', 'label_factors.weight', 'label_bias.weight'])\n",
    "    \n",
    "    toks = df_toks.token.to_numpy() # dls_collab.classes['token'] # not needed\n",
    "    toks_arr = to_device(tensor(toks))\n",
    "    lbs = df_lbs.lbl.to_numpy() # dls_collab.classes['label'] # not needed\n",
    "    \n",
    "    n_factors = collab_dict['token_factors.weight'].shape[1]\n",
    "    tok_wgts = collab_dict['token_factors.weight'].detach(); test_eq(tok_wgts.shape, (len(toks), n_factors))\n",
    "    lbs_wgts = collab_dict['label_factors.weight'].detach(); test_eq(lbs_wgts.shape, (len(lbs), n_factors))\n",
    "    tok_bias = collab_dict['token_bias.weight'].detach(); test_eq(tok_bias.shape, [len(toks), 1])\n",
    "    lbs_bias = collab_dict['label_bias.weight'].detach(); test_eq(lbs_bias.shape, [len(lbs), 1])\n",
    "    \n",
    "    lbl_idxs = df_lbs.loc[df_lbs.lbl_val.isin(lbl_list)].lbl.to_numpy() if lbl_list is not None else df_lbs.lbl.to_numpy()\n",
    "    sclt_lbl_wgts = lbs_wgts[lbl_idxs]\n",
    "    sclt_lbl_bias = lbs_bias[lbl_idxs]\n",
    "    lbl_all_toks = (tok_wgts @ sclt_lbl_wgts.T) + tok_bias + sclt_lbl_bias.T # there is no nan\n",
    "    test_eq(lbl_all_toks.shape, (len(toks), len(lbs)))\n",
    "    # lbl_all_toks = sigmoid_range(lbl_all_toks, low=0, high=1) # not needed\n",
    "    sorted_tok_vals, sorted_tok_idxs = torch.sort(lbl_all_toks, dim=0, descending=True)\n",
    "    sorted_tok_vals, sorted_tok_idxs = sorted_tok_vals.cpu(), sorted_tok_idxs.cpu()\n",
    "    torch.cuda.empty_cache()\n",
    "    # sorted_toks = toks_arr[sorted_tok_idxs].cpu() # no longer needed\n",
    "    # test_eq(sorted_toks, sorted_tok_idxs.cpu())\n",
    "    # toks_names = df_toks.tok_val.to_numpy(dtype=np.str_)\n",
    "    # sorted_tok_names= toks_names[sorted_toks]\n",
    "    return sorted_tok_vals, sorted_tok_idxs #, sorted_tok_names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0e8a34-315c-4de4-8feb-8e23ada854bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "lbs_list = None\n",
    "lbs_idxs = df_lbs_tiny.loc[df_lbs_tiny.lbl_val.isin(lbs_list)].lbl.to_numpy() if lbs_list is not None else df_lbs_tiny.lbl.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8efcbc-d61c-43f8-b43a-70bf76beb99f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 985 ms, sys: 182 ms, total: 1.17 s\n",
      "Wall time: 2.38 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sorted_tok_vals, sorted_tok_idxs = lbs_sorted_tokens(df_toks_tiny, df_lbs_tiny, collab_path_tiny, lbl_list=lbs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1026e9d-b5e9-4e3a-945e-689e697ed442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 7.77 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "13.9 s ± 11.2 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "with open('sorted_toks.pt', 'wb') as f: pickle.dump(sorted_toks, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2ffac1-6bdb-45b3-941e-f8c88468df6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.58 s, sys: 15.3 s, total: 16.9 s\n",
      "Wall time: 17.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "torch.save((sorted_tok_vals, sorted_tok_idxs), 'sorted_toks.pt');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113f7d85-bf0e-49cb-8996-4b9aa7b33cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| df_cf.memory_usage().sum()/1024**3: 1.906476616859436\n"
     ]
    }
   ],
   "source": [
    "cols = np.arange(sorted_tok_idxs.shape[0]).astype(np.str)\n",
    "df_cf = pd.DataFrame(sorted_tok_idxs.T, index=range(sorted_tok_idxs.shape[1]), columns=cols, dtype=np.int32)\n",
    "df_cf['lbl'] = lbs_idxs\n",
    "cols = list(df_cf.columns)\n",
    "cols =  [cols[-1]] + cols[:-1]\n",
    "df_cf = df_cf[cols]\n",
    "df_cf = pd.merge(df_lbs, df_cf, on='lbl')\n",
    "ic(df_cf.memory_usage().sum()/1024**3);\n",
    "df_cf.to_feather('mut_info_cf.ft')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92f1367-ff17-4524-a6c4-2ce34e7d6f78",
   "metadata": {},
   "source": [
    "##### Loading things up.. for analysis now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b979cb65-fe2b-4e4a-aa21-09548f61d08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cf = pd.read_feather('mut_info_cf.ft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04e0574-25a7-4979-91b2-84ac9ad7dd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_tok_vals, sorted_tok_idxs = torch.load('sorted_toks.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f177b8-e4b1-4dfd-be3c-9b2a1e64fc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "toks_names = df_toks_tiny.tok_val.to_numpy(dtype=np.str_)\n",
    "lbl_names = df_lbs_tiny.lbl_val.to_numpy(dtype=np.str_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f410d782-187f-4498-90b4-7bd0eff5f269",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| tid: 3374, lid: 434\n"
     ]
    }
   ],
   "source": [
    "tid = np.where(toks_names == 'quetiapine')[0].item()\n",
    "lid = np.where(lbl_names == '157.3')[0].item()\n",
    "ic(tid, lid);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f60fbf-d029-47e7-b0ef-11633fe55604",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (df_collab.token == tid) & (df_collab.label == lid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438ab4d8-018d-4302-af6b-d12f4407b7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_collab.head(5)\n",
    "# df_collab.pivot(index='token', columns='label', values='rank') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fb550b-65be-40f3-a981-174c446e1978",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| collab_bootstrap.keys(): dict_keys(['toks', 'lbs', 'mut_info_lbl_entropy', 'mutual_info_jaccard'])\n"
     ]
    }
   ],
   "source": [
    "collab_bootstrap = torch.load(collab_bootst_path)\n",
    "ic(collab_bootstrap.keys());\n",
    "info = collab_bootstrap.get('mutual_info_jaccard', None)\n",
    "assert info is not None\n",
    "info = torch.tensor(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5970efd3-8a10-4f47-a0e1-c87eb2ffb963",
   "metadata": {},
   "source": [
    "Removing negs before boxcox:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d32813-7ca7-4ffc-bd07-db8478d05a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| torch.sum(info<0): tensor(111226814, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "ic(torch.sum(info<0));\n",
    "eps = torch.ones(1).new_empty(1).fill_(1e-20).item()\n",
    "info[info<0] = eps\n",
    "test_eq(torch.sum(info<0), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4acedcd-6a6d-4514-862c-9056441a4fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 19s, sys: 22.9 s, total: 3min 42s\n",
      "Wall time: 3min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "bcx_info, *_ = boxcox(info.cpu().flatten() + eps)\n",
    "bcx_info = bcx_info.reshape(info.shape[0], info.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02545605-8754-4911-8f4b-30c0bce2c928",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(bcx_info, 'bcx_info.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab7628c-5679-46bb-94f4-cf2b7a477ca6",
   "metadata": {},
   "source": [
    "Rank the bcx_info in the GPU, then move it to CPU and free up the GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92c20ab-e0a6-45f8-beac-595dc66bbfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "bcx_info = torch.load('bcx_info.pkl')\n",
    "test_eq(type(bcx_info), ndarray)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d07130-0a1b-4db6-9ca2-25518ddb9088",
   "metadata": {},
   "source": [
    "**Use the following cell only for the tiny dataset:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee2757e-6703-492b-bd08-5aa9cd09d5ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| bcx_info.shape: (100, 100)\n"
     ]
    }
   ],
   "source": [
    "bcx_info = bcx_info[:num, :num]\n",
    "ic(bcx_info.shape);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece75124-924a-45f8-8ca2-afd15ecef16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bcx_info = torch.as_tensor(bcx_info, device=default_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13729b76-e7e7-49ec-b7f1-9a2458f63466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.14 ms, sys: 423 µs, total: 2.56 ms\n",
      "Wall time: 33 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rnk_info = torch.argsort(bcx_info, dim=0, descending=True).argsort(dim=0)\n",
    "rnk_info = to_device(rnk_info, device=torch.device(\"cpu\"))\n",
    "rnk_info = rnk_info.numpy()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb9fa0f-cddf-43e2-aac3-b74dd6a79121",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| rnk_info.shape: (100, 100)\n"
     ]
    }
   ],
   "source": [
    "ic(rnk_info.shape); # these are the ranks of all the tokens for the corresponding labels\n",
    "# rnk_info_df = pd.DataFrame(rnk_info)\n",
    "# rnk_info_df.columns.name = 'labels'\n",
    "# rnk_info_df.index.name = 'tokens'\n",
    "# rnk_info_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b20a6f4-b13f-4975-a847-f87f684c08fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.84 ms, sys: 0 ns, total: 4.84 ms\n",
      "Wall time: 3.23 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sorted_toks_orig_ranks = np.empty_like(sorted_tok_idxs)\n",
    "for lbl in range(sorted_tok_idxs.shape[1]):\n",
    "    sorted_toks_orig_ranks[:, lbl] = rnk_info[:, lbl][sorted_tok_idxs[:, lbl]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4c6b03-6e46-4e29-ab91-0ae5b72389ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.46 ms, sys: 461 µs, total: 2.92 ms\n",
      "Wall time: 2.58 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "bcx_info_cpu = bcx_info.cpu().numpy()\n",
    "sorted_toks_bcx_info = np.empty_like(sorted_tok_idxs, dtype=np.float)\n",
    "for lbl in range(sorted_tok_idxs.shape[1]):\n",
    "    sorted_toks_bcx_info[:, lbl] = bcx_info_cpu[:, lbl][sorted_tok_idxs[:, lbl]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ba35e5-1401-4a37-b24e-ef89bd4dae80",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_toks = np.stack((sorted_tok_idxs, sorted_toks_bcx_info, sorted_toks_orig_ranks, sorted_tok_vals), axis=-1)\n",
    "test_eq(sorted_toks.shape, (*sorted_tok_idxs.shape, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d564c2c9-e8b2-47a7-9703-cbcf03cba3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = pd.MultiIndex.from_product([range(sorted_toks.shape[1]), ('idx', 'info', 'act_rank', 'pred')], names=['label', 'key2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c00b3b-bd83-414d-ba3d-92b7fc6a6968",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th colspan=\"4\" halign=\"left\">0</th>\n",
       "      <th colspan=\"4\" halign=\"left\">1</th>\n",
       "      <th colspan=\"4\" halign=\"left\">2</th>\n",
       "      <th colspan=\"4\" halign=\"left\">3</th>\n",
       "      <th colspan=\"4\" halign=\"left\">4</th>\n",
       "      <th colspan=\"4\" halign=\"left\">5</th>\n",
       "      <th colspan=\"4\" halign=\"left\">6</th>\n",
       "      <th colspan=\"4\" halign=\"left\">7</th>\n",
       "      <th colspan=\"4\" halign=\"left\">8</th>\n",
       "      <th colspan=\"4\" halign=\"left\">9</th>\n",
       "      <th colspan=\"4\" halign=\"left\">10</th>\n",
       "      <th colspan=\"4\" halign=\"left\">11</th>\n",
       "      <th colspan=\"4\" halign=\"left\">12</th>\n",
       "      <th colspan=\"4\" halign=\"left\">13</th>\n",
       "      <th colspan=\"4\" halign=\"left\">14</th>\n",
       "      <th colspan=\"4\" halign=\"left\">15</th>\n",
       "      <th colspan=\"4\" halign=\"left\">16</th>\n",
       "      <th colspan=\"4\" halign=\"left\">17</th>\n",
       "      <th colspan=\"4\" halign=\"left\">18</th>\n",
       "      <th colspan=\"4\" halign=\"left\">19</th>\n",
       "      <th colspan=\"4\" halign=\"left\">20</th>\n",
       "      <th colspan=\"4\" halign=\"left\">21</th>\n",
       "      <th colspan=\"4\" halign=\"left\">22</th>\n",
       "      <th colspan=\"4\" halign=\"left\">23</th>\n",
       "      <th colspan=\"4\" halign=\"left\">24</th>\n",
       "      <th colspan=\"4\" halign=\"left\">25</th>\n",
       "      <th colspan=\"4\" halign=\"left\">26</th>\n",
       "      <th colspan=\"4\" halign=\"left\">27</th>\n",
       "      <th colspan=\"4\" halign=\"left\">28</th>\n",
       "      <th colspan=\"4\" halign=\"left\">29</th>\n",
       "      <th colspan=\"4\" halign=\"left\">30</th>\n",
       "      <th colspan=\"4\" halign=\"left\">31</th>\n",
       "      <th colspan=\"4\" halign=\"left\">32</th>\n",
       "      <th colspan=\"4\" halign=\"left\">33</th>\n",
       "      <th colspan=\"4\" halign=\"left\">34</th>\n",
       "      <th colspan=\"4\" halign=\"left\">35</th>\n",
       "      <th colspan=\"4\" halign=\"left\">36</th>\n",
       "      <th colspan=\"4\" halign=\"left\">37</th>\n",
       "      <th colspan=\"4\" halign=\"left\">38</th>\n",
       "      <th colspan=\"4\" halign=\"left\">39</th>\n",
       "      <th colspan=\"4\" halign=\"left\">40</th>\n",
       "      <th colspan=\"4\" halign=\"left\">41</th>\n",
       "      <th colspan=\"4\" halign=\"left\">42</th>\n",
       "      <th colspan=\"4\" halign=\"left\">43</th>\n",
       "      <th colspan=\"4\" halign=\"left\">44</th>\n",
       "      <th colspan=\"4\" halign=\"left\">45</th>\n",
       "      <th colspan=\"4\" halign=\"left\">46</th>\n",
       "      <th colspan=\"4\" halign=\"left\">47</th>\n",
       "      <th colspan=\"4\" halign=\"left\">48</th>\n",
       "      <th colspan=\"4\" halign=\"left\">49</th>\n",
       "      <th colspan=\"4\" halign=\"left\">50</th>\n",
       "      <th colspan=\"4\" halign=\"left\">51</th>\n",
       "      <th colspan=\"4\" halign=\"left\">52</th>\n",
       "      <th colspan=\"4\" halign=\"left\">53</th>\n",
       "      <th colspan=\"4\" halign=\"left\">54</th>\n",
       "      <th colspan=\"4\" halign=\"left\">55</th>\n",
       "      <th colspan=\"4\" halign=\"left\">56</th>\n",
       "      <th colspan=\"4\" halign=\"left\">57</th>\n",
       "      <th colspan=\"4\" halign=\"left\">58</th>\n",
       "      <th colspan=\"4\" halign=\"left\">59</th>\n",
       "      <th colspan=\"4\" halign=\"left\">60</th>\n",
       "      <th colspan=\"4\" halign=\"left\">61</th>\n",
       "      <th colspan=\"4\" halign=\"left\">62</th>\n",
       "      <th colspan=\"4\" halign=\"left\">63</th>\n",
       "      <th colspan=\"4\" halign=\"left\">64</th>\n",
       "      <th colspan=\"4\" halign=\"left\">65</th>\n",
       "      <th colspan=\"4\" halign=\"left\">66</th>\n",
       "      <th colspan=\"4\" halign=\"left\">67</th>\n",
       "      <th colspan=\"4\" halign=\"left\">68</th>\n",
       "      <th colspan=\"4\" halign=\"left\">69</th>\n",
       "      <th colspan=\"4\" halign=\"left\">70</th>\n",
       "      <th colspan=\"4\" halign=\"left\">71</th>\n",
       "      <th colspan=\"4\" halign=\"left\">72</th>\n",
       "      <th colspan=\"4\" halign=\"left\">73</th>\n",
       "      <th colspan=\"4\" halign=\"left\">74</th>\n",
       "      <th colspan=\"4\" halign=\"left\">75</th>\n",
       "      <th colspan=\"4\" halign=\"left\">76</th>\n",
       "      <th colspan=\"4\" halign=\"left\">77</th>\n",
       "      <th colspan=\"4\" halign=\"left\">78</th>\n",
       "      <th colspan=\"4\" halign=\"left\">79</th>\n",
       "      <th colspan=\"4\" halign=\"left\">80</th>\n",
       "      <th colspan=\"4\" halign=\"left\">81</th>\n",
       "      <th colspan=\"4\" halign=\"left\">82</th>\n",
       "      <th colspan=\"4\" halign=\"left\">83</th>\n",
       "      <th colspan=\"4\" halign=\"left\">84</th>\n",
       "      <th colspan=\"4\" halign=\"left\">85</th>\n",
       "      <th colspan=\"4\" halign=\"left\">86</th>\n",
       "      <th colspan=\"4\" halign=\"left\">87</th>\n",
       "      <th colspan=\"4\" halign=\"left\">88</th>\n",
       "      <th colspan=\"4\" halign=\"left\">89</th>\n",
       "      <th colspan=\"4\" halign=\"left\">90</th>\n",
       "      <th colspan=\"4\" halign=\"left\">91</th>\n",
       "      <th colspan=\"4\" halign=\"left\">92</th>\n",
       "      <th colspan=\"4\" halign=\"left\">93</th>\n",
       "      <th colspan=\"4\" halign=\"left\">94</th>\n",
       "      <th colspan=\"4\" halign=\"left\">95</th>\n",
       "      <th colspan=\"4\" halign=\"left\">96</th>\n",
       "      <th colspan=\"4\" halign=\"left\">97</th>\n",
       "      <th colspan=\"4\" halign=\"left\">98</th>\n",
       "      <th colspan=\"4\" halign=\"left\">99</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>key2</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokens</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>99.0</td>\n",
       "      <td>-6.209785</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-6.206440</td>\n",
       "      <td>92.0</td>\n",
       "      <td>-6.378193</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-6.409137</td>\n",
       "      <td>76.0</td>\n",
       "      <td>-6.588891</td>\n",
       "      <td>32.0</td>\n",
       "      <td>-6.129146</td>\n",
       "      <td>44.0</td>\n",
       "      <td>-6.182239</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-6.207527</td>\n",
       "      <td>96.0</td>\n",
       "      <td>-6.221110</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-6.218801</td>\n",
       "      <td>43.0</td>\n",
       "      <td>-5.775692</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.789716</td>\n",
       "      <td>75.0</td>\n",
       "      <td>-6.275874</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-6.282557</td>\n",
       "      <td>32.0</td>\n",
       "      <td>-5.709177</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.726708</td>\n",
       "      <td>66.0</td>\n",
       "      <td>-6.060895</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-6.059132</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-5.983247</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.989470</td>\n",
       "      <td>37.0</td>\n",
       "      <td>-6.028867</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-6.041323</td>\n",
       "      <td>74.0</td>\n",
       "      <td>-6.117087</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-6.113903</td>\n",
       "      <td>46.0</td>\n",
       "      <td>-5.901704</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.909033</td>\n",
       "      <td>48.0</td>\n",
       "      <td>-5.891960</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.903122</td>\n",
       "      <td>22.0</td>\n",
       "      <td>-5.789740</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.805620</td>\n",
       "      <td>74.0</td>\n",
       "      <td>-3.940301</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.934020</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-5.983247</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.991196</td>\n",
       "      <td>82.0</td>\n",
       "      <td>-6.077859</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-6.072291</td>\n",
       "      <td>95.0</td>\n",
       "      <td>-6.196057</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-6.237668</td>\n",
       "      <td>43.0</td>\n",
       "      <td>-6.070198</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-6.083841</td>\n",
       "      <td>36.0</td>\n",
       "      <td>-6.006469</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-6.007827</td>\n",
       "      <td>75.0</td>\n",
       "      <td>-5.770156</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-5.778278</td>\n",
       "      <td>52.0</td>\n",
       "      <td>-5.245574</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-5.248561</td>\n",
       "      <td>86.0</td>\n",
       "      <td>-5.542492</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.536958</td>\n",
       "      <td>27.0</td>\n",
       "      <td>-5.703646</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.705223</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-6.717038</td>\n",
       "      <td>58.0</td>\n",
       "      <td>-5.601665</td>\n",
       "      <td>25.0</td>\n",
       "      <td>-5.507920</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.510865</td>\n",
       "      <td>84.0</td>\n",
       "      <td>-5.930282</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.927581</td>\n",
       "      <td>37.0</td>\n",
       "      <td>-6.028867</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-6.030078</td>\n",
       "      <td>56.0</td>\n",
       "      <td>-6.134938</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-6.143692</td>\n",
       "      <td>26.0</td>\n",
       "      <td>-5.849577</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.856101</td>\n",
       "      <td>88.0</td>\n",
       "      <td>-6.158427</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-6.153932</td>\n",
       "      <td>27.0</td>\n",
       "      <td>-5.374472</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.366094</td>\n",
       "      <td>96.0</td>\n",
       "      <td>-6.600779</td>\n",
       "      <td>13.0</td>\n",
       "      <td>-6.036506</td>\n",
       "      <td>54.0</td>\n",
       "      <td>-5.874065</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.871864</td>\n",
       "      <td>80.0</td>\n",
       "      <td>-6.313117</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-6.334147</td>\n",
       "      <td>39.0</td>\n",
       "      <td>-6.140011</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-6.140745</td>\n",
       "      <td>84.0</td>\n",
       "      <td>-5.766344</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.767742</td>\n",
       "      <td>26.0</td>\n",
       "      <td>-5.849577</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.855848</td>\n",
       "      <td>43.0</td>\n",
       "      <td>-6.070198</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-6.075658</td>\n",
       "      <td>84.0</td>\n",
       "      <td>-6.195378</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-6.216457</td>\n",
       "      <td>68.0</td>\n",
       "      <td>-6.107600</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-6.121827</td>\n",
       "      <td>87.0</td>\n",
       "      <td>-6.146663</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-6.165133</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-5.983247</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.989765</td>\n",
       "      <td>96.0</td>\n",
       "      <td>-6.549222</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-6.550990</td>\n",
       "      <td>68.0</td>\n",
       "      <td>-5.836143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.833795</td>\n",
       "      <td>46.0</td>\n",
       "      <td>-5.901704</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.920306</td>\n",
       "      <td>23.0</td>\n",
       "      <td>-5.586688</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.599098</td>\n",
       "      <td>56.0</td>\n",
       "      <td>-6.134938</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-6.145071</td>\n",
       "      <td>95.0</td>\n",
       "      <td>-6.196057</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-6.221061</td>\n",
       "      <td>86.0</td>\n",
       "      <td>-6.365438</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-6.397769</td>\n",
       "      <td>75.0</td>\n",
       "      <td>-6.111759</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-6.105064</td>\n",
       "      <td>56.0</td>\n",
       "      <td>-6.134938</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-6.150967</td>\n",
       "      <td>24.0</td>\n",
       "      <td>-6.476279</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-6.491059</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-5.983247</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.978225</td>\n",
       "      <td>26.0</td>\n",
       "      <td>-5.849577</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.874820</td>\n",
       "      <td>75.0</td>\n",
       "      <td>-6.470216</td>\n",
       "      <td>45.0</td>\n",
       "      <td>-5.782003</td>\n",
       "      <td>20.0</td>\n",
       "      <td>-5.077253</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.086127</td>\n",
       "      <td>67.0</td>\n",
       "      <td>-6.091972</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-6.144179</td>\n",
       "      <td>88.0</td>\n",
       "      <td>-6.158427</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-6.189448</td>\n",
       "      <td>65.0</td>\n",
       "      <td>-9.727593</td>\n",
       "      <td>90.0</td>\n",
       "      <td>-5.534416</td>\n",
       "      <td>26.0</td>\n",
       "      <td>-6.161404</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-6.165813</td>\n",
       "      <td>74.0</td>\n",
       "      <td>-5.728308</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.728362</td>\n",
       "      <td>58.0</td>\n",
       "      <td>-6.238058</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-6.234853</td>\n",
       "      <td>98.0</td>\n",
       "      <td>-5.779319</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.781091</td>\n",
       "      <td>37.0</td>\n",
       "      <td>-5.751393</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.752360</td>\n",
       "      <td>62.0</td>\n",
       "      <td>-6.228091</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-6.245189</td>\n",
       "      <td>60.0</td>\n",
       "      <td>-5.562530</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.576107</td>\n",
       "      <td>62.0</td>\n",
       "      <td>-6.228091</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-6.249218</td>\n",
       "      <td>17.0</td>\n",
       "      <td>-6.337839</td>\n",
       "      <td>35.0</td>\n",
       "      <td>-5.600626</td>\n",
       "      <td>24.0</td>\n",
       "      <td>-6.289662</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-6.283247</td>\n",
       "      <td>37.0</td>\n",
       "      <td>-6.358535</td>\n",
       "      <td>26.0</td>\n",
       "      <td>-5.970652</td>\n",
       "      <td>90.0</td>\n",
       "      <td>-6.354778</td>\n",
       "      <td>11.0</td>\n",
       "      <td>-5.834315</td>\n",
       "      <td>63.0</td>\n",
       "      <td>-6.286141</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-6.307220</td>\n",
       "      <td>86.0</td>\n",
       "      <td>-4.806090</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.806323</td>\n",
       "      <td>34.0</td>\n",
       "      <td>-5.789685</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.785295</td>\n",
       "      <td>72.0</td>\n",
       "      <td>-4.939904</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-4.936449</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.282387</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-5.004199</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-4.841265</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.838974</td>\n",
       "      <td>9.0</td>\n",
       "      <td>-5.101474</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.154175</td>\n",
       "      <td>30.0</td>\n",
       "      <td>-4.732475</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.740738</td>\n",
       "      <td>26.0</td>\n",
       "      <td>-5.446089</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-5.444146</td>\n",
       "      <td>34.0</td>\n",
       "      <td>-5.688428</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.677827</td>\n",
       "      <td>86.0</td>\n",
       "      <td>-4.690676</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.702187</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-4.719597</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.720963</td>\n",
       "      <td>18.0</td>\n",
       "      <td>-4.849282</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.856553</td>\n",
       "      <td>22.0</td>\n",
       "      <td>-4.846297</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.843096</td>\n",
       "      <td>18.0</td>\n",
       "      <td>-4.961876</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-4.961539</td>\n",
       "      <td>86.0</td>\n",
       "      <td>-3.930341</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.929331</td>\n",
       "      <td>82.0</td>\n",
       "      <td>-6.873151</td>\n",
       "      <td>63.0</td>\n",
       "      <td>-5.735173</td>\n",
       "      <td>89.0</td>\n",
       "      <td>-4.908507</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.912350</td>\n",
       "      <td>37.0</td>\n",
       "      <td>-6.534252</td>\n",
       "      <td>32.0</td>\n",
       "      <td>-6.265768</td>\n",
       "      <td>31.0</td>\n",
       "      <td>-5.719459</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.722820</td>\n",
       "      <td>20.0</td>\n",
       "      <td>-6.070054</td>\n",
       "      <td>15.0</td>\n",
       "      <td>-5.759802</td>\n",
       "      <td>89.0</td>\n",
       "      <td>-5.099403</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.100125</td>\n",
       "      <td>84.0</td>\n",
       "      <td>-5.623547</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.616222</td>\n",
       "      <td>68.0</td>\n",
       "      <td>-6.771860</td>\n",
       "      <td>69.0</td>\n",
       "      <td>-5.219338</td>\n",
       "      <td>61.0</td>\n",
       "      <td>-5.299616</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.303560</td>\n",
       "      <td>36.0</td>\n",
       "      <td>-7.353806</td>\n",
       "      <td>80.0</td>\n",
       "      <td>-5.779118</td>\n",
       "      <td>86.0</td>\n",
       "      <td>-4.959163</td>\n",
       "      <td>26.0</td>\n",
       "      <td>-4.489558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>97.0</td>\n",
       "      <td>-6.219368</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-6.242119</td>\n",
       "      <td>94.0</td>\n",
       "      <td>-6.454948</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-6.457011</td>\n",
       "      <td>46.0</td>\n",
       "      <td>-6.230028</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-6.223404</td>\n",
       "      <td>75.0</td>\n",
       "      <td>-6.275874</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-6.293755</td>\n",
       "      <td>43.0</td>\n",
       "      <td>-6.240831</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-6.240847</td>\n",
       "      <td>61.0</td>\n",
       "      <td>-5.910188</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-5.917663</td>\n",
       "      <td>35.0</td>\n",
       "      <td>-6.331505</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-6.344807</td>\n",
       "      <td>79.0</td>\n",
       "      <td>-6.130860</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-6.114508</td>\n",
       "      <td>80.0</td>\n",
       "      <td>-6.894968</td>\n",
       "      <td>72.0</td>\n",
       "      <td>-6.097312</td>\n",
       "      <td>54.0</td>\n",
       "      <td>-6.143131</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-6.140545</td>\n",
       "      <td>71.0</td>\n",
       "      <td>-6.044409</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-6.052176</td>\n",
       "      <td>98.0</td>\n",
       "      <td>-6.156014</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-6.159112</td>\n",
       "      <td>87.0</td>\n",
       "      <td>-6.146663</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-6.154080</td>\n",
       "      <td>56.0</td>\n",
       "      <td>-6.134938</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-6.136778</td>\n",
       "      <td>68.0</td>\n",
       "      <td>-6.107600</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-6.112395</td>\n",
       "      <td>83.0</td>\n",
       "      <td>-4.462110</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-4.458435</td>\n",
       "      <td>88.0</td>\n",
       "      <td>-6.158427</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-6.172926</td>\n",
       "      <td>36.0</td>\n",
       "      <td>-6.161673</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-6.162069</td>\n",
       "      <td>99.0</td>\n",
       "      <td>-6.455512</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-6.438261</td>\n",
       "      <td>97.0</td>\n",
       "      <td>-6.219368</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-6.203945</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-6.039419</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-6.032581</td>\n",
       "      <td>73.0</td>\n",
       "      <td>-5.807189</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-5.814392</td>\n",
       "      <td>28.0</td>\n",
       "      <td>-5.266717</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-5.268564</td>\n",
       "      <td>66.0</td>\n",
       "      <td>-5.580114</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-5.579801</td>\n",
       "      <td>70.0</td>\n",
       "      <td>-5.769703</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-5.777030</td>\n",
       "      <td>45.0</td>\n",
       "      <td>-5.765828</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.761786</td>\n",
       "      <td>31.0</td>\n",
       "      <td>-5.951787</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-5.954525</td>\n",
       "      <td>24.0</td>\n",
       "      <td>-6.232370</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-6.245910</td>\n",
       "      <td>71.0</td>\n",
       "      <td>-6.044409</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-6.056815</td>\n",
       "      <td>54.0</td>\n",
       "      <td>-6.143131</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-6.147087</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-5.983247</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-5.984535</td>\n",
       "      <td>76.0</td>\n",
       "      <td>-6.221133</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-6.235856</td>\n",
       "      <td>40.0</td>\n",
       "      <td>-5.911748</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-5.805452</td>\n",
       "      <td>28.0</td>\n",
       "      <td>-6.282807</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-6.310916</td>\n",
       "      <td>76.0</td>\n",
       "      <td>-6.588891</td>\n",
       "      <td>33.0</td>\n",
       "      <td>-6.050550</td>\n",
       "      <td>74.0</td>\n",
       "      <td>-6.370296</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-6.390576</td>\n",
       "      <td>88.0</td>\n",
       "      <td>-6.158427</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-6.163455</td>\n",
       "      <td>66.0</td>\n",
       "      <td>-6.402689</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-6.155170</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-5.983247</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-5.987694</td>\n",
       "      <td>48.0</td>\n",
       "      <td>-6.216141</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-6.221207</td>\n",
       "      <td>78.0</td>\n",
       "      <td>-6.438037</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-6.432988</td>\n",
       "      <td>88.0</td>\n",
       "      <td>-6.158427</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-6.163053</td>\n",
       "      <td>62.0</td>\n",
       "      <td>-6.228091</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-6.242920</td>\n",
       "      <td>62.0</td>\n",
       "      <td>-6.228091</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-6.238089</td>\n",
       "      <td>42.0</td>\n",
       "      <td>-6.548010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-6.554848</td>\n",
       "      <td>73.0</td>\n",
       "      <td>-5.952051</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-5.943097</td>\n",
       "      <td>79.0</td>\n",
       "      <td>-6.130860</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-6.132979</td>\n",
       "      <td>19.0</td>\n",
       "      <td>-6.079284</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-6.078047</td>\n",
       "      <td>54.0</td>\n",
       "      <td>-6.143131</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-6.156491</td>\n",
       "      <td>80.0</td>\n",
       "      <td>-6.313117</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-6.321089</td>\n",
       "      <td>36.0</td>\n",
       "      <td>-6.643223</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-6.443179</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-6.976879</td>\n",
       "      <td>71.0</td>\n",
       "      <td>-6.221260</td>\n",
       "      <td>84.0</td>\n",
       "      <td>-6.195378</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-6.209301</td>\n",
       "      <td>42.0</td>\n",
       "      <td>-6.548010</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-6.546859</td>\n",
       "      <td>62.0</td>\n",
       "      <td>-6.228091</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-6.227862</td>\n",
       "      <td>83.0</td>\n",
       "      <td>-6.300496</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-6.304437</td>\n",
       "      <td>35.0</td>\n",
       "      <td>-5.946779</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-5.866240</td>\n",
       "      <td>55.0</td>\n",
       "      <td>-5.116639</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-5.119152</td>\n",
       "      <td>87.0</td>\n",
       "      <td>-6.146663</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-6.144928</td>\n",
       "      <td>35.0</td>\n",
       "      <td>-6.331505</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-6.334547</td>\n",
       "      <td>69.0</td>\n",
       "      <td>-6.200252</td>\n",
       "      <td>23.0</td>\n",
       "      <td>-5.721776</td>\n",
       "      <td>85.0</td>\n",
       "      <td>-6.223698</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-6.219057</td>\n",
       "      <td>72.0</td>\n",
       "      <td>-5.730041</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-5.736298</td>\n",
       "      <td>29.0</td>\n",
       "      <td>-6.328879</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-6.331000</td>\n",
       "      <td>22.0</td>\n",
       "      <td>-5.814093</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-5.811885</td>\n",
       "      <td>88.0</td>\n",
       "      <td>-6.618454</td>\n",
       "      <td>39.0</td>\n",
       "      <td>-6.066234</td>\n",
       "      <td>90.0</td>\n",
       "      <td>-6.221301</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-6.250992</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-5.702554</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-5.704838</td>\n",
       "      <td>74.0</td>\n",
       "      <td>-6.370296</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-6.389161</td>\n",
       "      <td>64.0</td>\n",
       "      <td>-5.892754</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.894948</td>\n",
       "      <td>85.0</td>\n",
       "      <td>-6.296583</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-6.306055</td>\n",
       "      <td>57.0</td>\n",
       "      <td>-6.049853</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-6.049420</td>\n",
       "      <td>31.0</td>\n",
       "      <td>-6.583543</td>\n",
       "      <td>38.0</td>\n",
       "      <td>-6.090720</td>\n",
       "      <td>80.0</td>\n",
       "      <td>-6.313117</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-6.328473</td>\n",
       "      <td>70.0</td>\n",
       "      <td>-4.925375</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-4.932771</td>\n",
       "      <td>97.0</td>\n",
       "      <td>-5.810853</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-5.802298</td>\n",
       "      <td>48.0</td>\n",
       "      <td>-4.960993</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-4.964064</td>\n",
       "      <td>53.0</td>\n",
       "      <td>-5.115004</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.109031</td>\n",
       "      <td>17.0</td>\n",
       "      <td>-5.132164</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-5.136580</td>\n",
       "      <td>33.0</td>\n",
       "      <td>-5.350877</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-5.353909</td>\n",
       "      <td>40.0</td>\n",
       "      <td>-5.365047</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-5.359856</td>\n",
       "      <td>82.0</td>\n",
       "      <td>-5.758117</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-5.755056</td>\n",
       "      <td>45.0</td>\n",
       "      <td>-5.691516</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-5.698607</td>\n",
       "      <td>70.0</td>\n",
       "      <td>-4.875638</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-4.857039</td>\n",
       "      <td>14.0</td>\n",
       "      <td>-4.913362</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-4.917946</td>\n",
       "      <td>23.0</td>\n",
       "      <td>-5.686593</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-5.682671</td>\n",
       "      <td>70.0</td>\n",
       "      <td>-5.158384</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-4.854918</td>\n",
       "      <td>72.0</td>\n",
       "      <td>-5.289307</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-5.292850</td>\n",
       "      <td>82.0</td>\n",
       "      <td>-3.970979</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-3.969098</td>\n",
       "      <td>55.0</td>\n",
       "      <td>-6.512794</td>\n",
       "      <td>42.0</td>\n",
       "      <td>-6.092467</td>\n",
       "      <td>58.0</td>\n",
       "      <td>-5.908459</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-5.917121</td>\n",
       "      <td>84.0</td>\n",
       "      <td>-6.467428</td>\n",
       "      <td>20.0</td>\n",
       "      <td>-6.272301</td>\n",
       "      <td>72.0</td>\n",
       "      <td>-5.962330</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-5.962266</td>\n",
       "      <td>98.0</td>\n",
       "      <td>-5.797782</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-5.794234</td>\n",
       "      <td>28.0</td>\n",
       "      <td>-6.024241</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-6.027439</td>\n",
       "      <td>37.0</td>\n",
       "      <td>-5.727442</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-5.726045</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-6.245051</td>\n",
       "      <td>45.0</td>\n",
       "      <td>-5.355571</td>\n",
       "      <td>34.0</td>\n",
       "      <td>-5.383586</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-5.384628</td>\n",
       "      <td>48.0</td>\n",
       "      <td>-6.480022</td>\n",
       "      <td>44.0</td>\n",
       "      <td>-5.847100</td>\n",
       "      <td>54.0</td>\n",
       "      <td>-4.977380</td>\n",
       "      <td>27.0</td>\n",
       "      <td>-4.505052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>61.0</td>\n",
       "      <td>-6.240334</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-6.246603</td>\n",
       "      <td>24.0</td>\n",
       "      <td>-6.662710</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-6.541097</td>\n",
       "      <td>21.0</td>\n",
       "      <td>-6.480742</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-6.307346</td>\n",
       "      <td>35.0</td>\n",
       "      <td>-6.331505</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-6.365513</td>\n",
       "      <td>92.0</td>\n",
       "      <td>-6.364440</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-6.359645</td>\n",
       "      <td>73.0</td>\n",
       "      <td>-6.217120</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-6.219978</td>\n",
       "      <td>28.0</td>\n",
       "      <td>-6.780406</td>\n",
       "      <td>32.0</td>\n",
       "      <td>-6.414482</td>\n",
       "      <td>84.0</td>\n",
       "      <td>-6.195378</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-6.190402</td>\n",
       "      <td>35.0</td>\n",
       "      <td>-6.940001</td>\n",
       "      <td>76.0</td>\n",
       "      <td>-6.320796</td>\n",
       "      <td>77.0</td>\n",
       "      <td>-6.324052</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-6.328280</td>\n",
       "      <td>76.0</td>\n",
       "      <td>-6.221133</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-6.217917</td>\n",
       "      <td>63.0</td>\n",
       "      <td>-6.171896</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-6.180599</td>\n",
       "      <td>44.0</td>\n",
       "      <td>-6.182239</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-6.180636</td>\n",
       "      <td>64.0</td>\n",
       "      <td>-6.150773</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-6.165330</td>\n",
       "      <td>91.0</td>\n",
       "      <td>-6.249234</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-6.252928</td>\n",
       "      <td>86.0</td>\n",
       "      <td>-4.488357</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-4.481386</td>\n",
       "      <td>90.0</td>\n",
       "      <td>-6.221301</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-6.223852</td>\n",
       "      <td>45.0</td>\n",
       "      <td>-6.619645</td>\n",
       "      <td>44.0</td>\n",
       "      <td>-6.169834</td>\n",
       "      <td>97.0</td>\n",
       "      <td>-6.464425</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-6.464385</td>\n",
       "      <td>88.0</td>\n",
       "      <td>-6.618454</td>\n",
       "      <td>39.0</td>\n",
       "      <td>-6.215768</td>\n",
       "      <td>43.0</td>\n",
       "      <td>-6.372712</td>\n",
       "      <td>28.0</td>\n",
       "      <td>-6.037009</td>\n",
       "      <td>70.0</td>\n",
       "      <td>-5.815135</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-5.819547</td>\n",
       "      <td>19.0</td>\n",
       "      <td>-5.288770</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-5.295113</td>\n",
       "      <td>62.0</td>\n",
       "      <td>-5.639951</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-5.641968</td>\n",
       "      <td>68.0</td>\n",
       "      <td>-5.819183</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-5.819251</td>\n",
       "      <td>65.0</td>\n",
       "      <td>-6.212671</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-5.965829</td>\n",
       "      <td>35.0</td>\n",
       "      <td>-7.591816</td>\n",
       "      <td>78.0</td>\n",
       "      <td>-6.204265</td>\n",
       "      <td>38.0</td>\n",
       "      <td>-6.257469</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-6.251998</td>\n",
       "      <td>84.0</td>\n",
       "      <td>-6.195378</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-6.207242</td>\n",
       "      <td>77.0</td>\n",
       "      <td>-6.324052</td>\n",
       "      <td>15.0</td>\n",
       "      <td>-6.150760</td>\n",
       "      <td>62.0</td>\n",
       "      <td>-6.228091</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-6.227783</td>\n",
       "      <td>62.0</td>\n",
       "      <td>-6.228091</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-6.238778</td>\n",
       "      <td>64.0</td>\n",
       "      <td>-7.128593</td>\n",
       "      <td>72.0</td>\n",
       "      <td>-5.965558</td>\n",
       "      <td>74.0</td>\n",
       "      <td>-6.370296</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-6.383852</td>\n",
       "      <td>38.0</td>\n",
       "      <td>-6.257469</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-6.269179</td>\n",
       "      <td>24.0</td>\n",
       "      <td>-6.476279</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-6.495346</td>\n",
       "      <td>44.0</td>\n",
       "      <td>-6.182239</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-6.201119</td>\n",
       "      <td>67.0</td>\n",
       "      <td>-6.511463</td>\n",
       "      <td>30.0</td>\n",
       "      <td>-6.343021</td>\n",
       "      <td>62.0</td>\n",
       "      <td>-6.228091</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-6.231473</td>\n",
       "      <td>61.0</td>\n",
       "      <td>-6.240334</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-6.227340</td>\n",
       "      <td>24.0</td>\n",
       "      <td>-6.476279</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-6.471385</td>\n",
       "      <td>73.0</td>\n",
       "      <td>-6.217120</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-6.210379</td>\n",
       "      <td>80.0</td>\n",
       "      <td>-6.313117</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-6.312078</td>\n",
       "      <td>88.0</td>\n",
       "      <td>-6.833242</td>\n",
       "      <td>43.0</td>\n",
       "      <td>-6.443336</td>\n",
       "      <td>29.0</td>\n",
       "      <td>-6.586070</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-6.586352</td>\n",
       "      <td>26.0</td>\n",
       "      <td>-6.161404</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-6.155777</td>\n",
       "      <td>56.0</td>\n",
       "      <td>-6.134938</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-6.144638</td>\n",
       "      <td>26.0</td>\n",
       "      <td>-6.161404</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-6.160436</td>\n",
       "      <td>82.0</td>\n",
       "      <td>-6.253382</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-6.261949</td>\n",
       "      <td>53.0</td>\n",
       "      <td>-6.359660</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-6.384645</td>\n",
       "      <td>99.0</td>\n",
       "      <td>-6.455512</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-6.459650</td>\n",
       "      <td>92.0</td>\n",
       "      <td>-6.261224</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-6.256142</td>\n",
       "      <td>65.0</td>\n",
       "      <td>-6.329358</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-6.338180</td>\n",
       "      <td>96.0</td>\n",
       "      <td>-6.549222</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-6.557016</td>\n",
       "      <td>83.0</td>\n",
       "      <td>-6.300496</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-6.303432</td>\n",
       "      <td>42.0</td>\n",
       "      <td>-6.603251</td>\n",
       "      <td>9.0</td>\n",
       "      <td>-6.348537</td>\n",
       "      <td>72.0</td>\n",
       "      <td>-5.920697</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-5.922410</td>\n",
       "      <td>89.0</td>\n",
       "      <td>-5.262535</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-5.258935</td>\n",
       "      <td>84.0</td>\n",
       "      <td>-6.195378</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-6.192425</td>\n",
       "      <td>77.0</td>\n",
       "      <td>-6.324052</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-6.335538</td>\n",
       "      <td>32.0</td>\n",
       "      <td>-6.084221</td>\n",
       "      <td>13.0</td>\n",
       "      <td>-5.723261</td>\n",
       "      <td>58.0</td>\n",
       "      <td>-6.238058</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-6.241138</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.753846</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-5.751369</td>\n",
       "      <td>68.0</td>\n",
       "      <td>-6.637797</td>\n",
       "      <td>34.0</td>\n",
       "      <td>-6.346422</td>\n",
       "      <td>26.0</td>\n",
       "      <td>-5.894030</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-5.902766</td>\n",
       "      <td>43.0</td>\n",
       "      <td>-6.070198</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-6.067514</td>\n",
       "      <td>66.0</td>\n",
       "      <td>-6.317314</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-6.313829</td>\n",
       "      <td>79.0</td>\n",
       "      <td>-5.861041</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-5.867354</td>\n",
       "      <td>97.0</td>\n",
       "      <td>-6.464425</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-6.459091</td>\n",
       "      <td>54.0</td>\n",
       "      <td>-5.896406</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-5.898395</td>\n",
       "      <td>99.0</td>\n",
       "      <td>-6.307454</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-6.307156</td>\n",
       "      <td>52.0</td>\n",
       "      <td>-6.057899</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-6.054447</td>\n",
       "      <td>25.0</td>\n",
       "      <td>-6.660027</td>\n",
       "      <td>45.0</td>\n",
       "      <td>-6.236250</td>\n",
       "      <td>65.0</td>\n",
       "      <td>-6.329358</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-6.344307</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.046092</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-5.046387</td>\n",
       "      <td>25.0</td>\n",
       "      <td>-5.791017</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-5.806690</td>\n",
       "      <td>83.0</td>\n",
       "      <td>-4.978734</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-4.975702</td>\n",
       "      <td>66.0</td>\n",
       "      <td>-5.133687</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-5.133000</td>\n",
       "      <td>9.0</td>\n",
       "      <td>-5.485133</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-5.478284</td>\n",
       "      <td>86.0</td>\n",
       "      <td>-5.392830</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-5.398625</td>\n",
       "      <td>27.0</td>\n",
       "      <td>-5.364374</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-5.371370</td>\n",
       "      <td>34.0</td>\n",
       "      <td>-5.764222</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-5.763424</td>\n",
       "      <td>58.0</td>\n",
       "      <td>-5.975066</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-5.973187</td>\n",
       "      <td>22.0</td>\n",
       "      <td>-4.989712</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-4.936715</td>\n",
       "      <td>48.0</td>\n",
       "      <td>-5.255649</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-5.254946</td>\n",
       "      <td>46.0</td>\n",
       "      <td>-5.778876</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-5.777435</td>\n",
       "      <td>68.0</td>\n",
       "      <td>-6.042273</td>\n",
       "      <td>48.0</td>\n",
       "      <td>-4.873288</td>\n",
       "      <td>13.0</td>\n",
       "      <td>-5.386950</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-5.380569</td>\n",
       "      <td>90.0</td>\n",
       "      <td>-4.010327</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-4.009221</td>\n",
       "      <td>98.0</td>\n",
       "      <td>-6.098917</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-6.095601</td>\n",
       "      <td>64.0</td>\n",
       "      <td>-6.150773</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-6.141796</td>\n",
       "      <td>71.0</td>\n",
       "      <td>-6.530053</td>\n",
       "      <td>31.0</td>\n",
       "      <td>-6.275233</td>\n",
       "      <td>91.0</td>\n",
       "      <td>-6.039551</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-6.039298</td>\n",
       "      <td>46.0</td>\n",
       "      <td>-5.795397</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-5.797428</td>\n",
       "      <td>80.0</td>\n",
       "      <td>-6.056923</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-6.049633</td>\n",
       "      <td>28.0</td>\n",
       "      <td>-6.649372</td>\n",
       "      <td>48.0</td>\n",
       "      <td>-5.730793</td>\n",
       "      <td>98.0</td>\n",
       "      <td>-6.324862</td>\n",
       "      <td>52.0</td>\n",
       "      <td>-5.527510</td>\n",
       "      <td>31.0</td>\n",
       "      <td>-5.452182</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-5.452777</td>\n",
       "      <td>99.0</td>\n",
       "      <td>-5.945722</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.945576</td>\n",
       "      <td>35.0</td>\n",
       "      <td>-4.511155</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.519406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39.0</td>\n",
       "      <td>-6.625558</td>\n",
       "      <td>39.0</td>\n",
       "      <td>-6.251883</td>\n",
       "      <td>96.0</td>\n",
       "      <td>-6.549222</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-6.554122</td>\n",
       "      <td>42.0</td>\n",
       "      <td>-6.309279</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-6.312391</td>\n",
       "      <td>99.0</td>\n",
       "      <td>-6.455512</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-6.456026</td>\n",
       "      <td>72.0</td>\n",
       "      <td>-6.370844</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-6.363751</td>\n",
       "      <td>91.0</td>\n",
       "      <td>-6.249234</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-6.249772</td>\n",
       "      <td>96.0</td>\n",
       "      <td>-6.600779</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-6.455039</td>\n",
       "      <td>98.0</td>\n",
       "      <td>-6.311167</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-6.304888</td>\n",
       "      <td>29.0</td>\n",
       "      <td>-6.349417</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-6.354245</td>\n",
       "      <td>72.0</td>\n",
       "      <td>-6.368240</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-6.381281</td>\n",
       "      <td>90.0</td>\n",
       "      <td>-6.221301</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-6.231865</td>\n",
       "      <td>28.0</td>\n",
       "      <td>-6.172530</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-6.180791</td>\n",
       "      <td>73.0</td>\n",
       "      <td>-6.217120</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-6.216530</td>\n",
       "      <td>70.0</td>\n",
       "      <td>-6.202125</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-6.204170</td>\n",
       "      <td>96.0</td>\n",
       "      <td>-6.549222</td>\n",
       "      <td>11.0</td>\n",
       "      <td>-6.258662</td>\n",
       "      <td>48.0</td>\n",
       "      <td>-4.507672</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-4.510556</td>\n",
       "      <td>77.0</td>\n",
       "      <td>-6.324052</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-6.326668</td>\n",
       "      <td>71.0</td>\n",
       "      <td>-6.174577</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-6.183572</td>\n",
       "      <td>94.0</td>\n",
       "      <td>-6.454948</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-6.475404</td>\n",
       "      <td>39.0</td>\n",
       "      <td>-6.572208</td>\n",
       "      <td>28.0</td>\n",
       "      <td>-6.216390</td>\n",
       "      <td>92.0</td>\n",
       "      <td>-6.110777</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-6.113202</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.728776</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.827851</td>\n",
       "      <td>70.0</td>\n",
       "      <td>-5.302331</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-5.305170</td>\n",
       "      <td>76.0</td>\n",
       "      <td>-5.644109</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-5.644223</td>\n",
       "      <td>67.0</td>\n",
       "      <td>-5.826686</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-5.822942</td>\n",
       "      <td>57.0</td>\n",
       "      <td>-6.049853</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-6.048114</td>\n",
       "      <td>59.0</td>\n",
       "      <td>-6.990874</td>\n",
       "      <td>69.0</td>\n",
       "      <td>-6.221961</td>\n",
       "      <td>36.0</td>\n",
       "      <td>-6.259565</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-6.257571</td>\n",
       "      <td>90.0</td>\n",
       "      <td>-6.221301</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-6.227348</td>\n",
       "      <td>70.0</td>\n",
       "      <td>-6.202125</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-6.202123</td>\n",
       "      <td>78.0</td>\n",
       "      <td>-6.691357</td>\n",
       "      <td>16.0</td>\n",
       "      <td>-6.465548</td>\n",
       "      <td>81.0</td>\n",
       "      <td>-6.235842</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-6.249428</td>\n",
       "      <td>86.0</td>\n",
       "      <td>-6.009958</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-6.012503</td>\n",
       "      <td>21.0</td>\n",
       "      <td>-6.411710</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-6.413867</td>\n",
       "      <td>66.0</td>\n",
       "      <td>-6.539048</td>\n",
       "      <td>20.0</td>\n",
       "      <td>-6.273180</td>\n",
       "      <td>38.0</td>\n",
       "      <td>-6.500648</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-6.502325</td>\n",
       "      <td>91.0</td>\n",
       "      <td>-6.249234</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-6.250063</td>\n",
       "      <td>92.0</td>\n",
       "      <td>-6.364440</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-6.361938</td>\n",
       "      <td>99.0</td>\n",
       "      <td>-6.455512</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-6.463291</td>\n",
       "      <td>46.0</td>\n",
       "      <td>-6.230028</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-6.230957</td>\n",
       "      <td>85.0</td>\n",
       "      <td>-6.468452</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-6.483478</td>\n",
       "      <td>62.0</td>\n",
       "      <td>-6.228091</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-6.222253</td>\n",
       "      <td>97.0</td>\n",
       "      <td>-6.464425</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-6.451281</td>\n",
       "      <td>91.0</td>\n",
       "      <td>-6.799016</td>\n",
       "      <td>35.0</td>\n",
       "      <td>-6.497480</td>\n",
       "      <td>52.0</td>\n",
       "      <td>-6.627138</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-6.617065</td>\n",
       "      <td>93.0</td>\n",
       "      <td>-6.524523</td>\n",
       "      <td>16.0</td>\n",
       "      <td>-6.360283</td>\n",
       "      <td>88.0</td>\n",
       "      <td>-6.158427</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-6.168409</td>\n",
       "      <td>43.0</td>\n",
       "      <td>-6.740957</td>\n",
       "      <td>42.0</td>\n",
       "      <td>-6.160839</td>\n",
       "      <td>75.0</td>\n",
       "      <td>-6.275874</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-6.286424</td>\n",
       "      <td>97.0</td>\n",
       "      <td>-6.464425</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-6.451509</td>\n",
       "      <td>85.0</td>\n",
       "      <td>-6.468452</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-6.470255</td>\n",
       "      <td>46.0</td>\n",
       "      <td>-6.477499</td>\n",
       "      <td>30.0</td>\n",
       "      <td>-6.262456</td>\n",
       "      <td>24.0</td>\n",
       "      <td>-6.476279</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-6.472601</td>\n",
       "      <td>29.0</td>\n",
       "      <td>-6.566573</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-6.569449</td>\n",
       "      <td>86.0</td>\n",
       "      <td>-6.365438</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-6.368578</td>\n",
       "      <td>86.0</td>\n",
       "      <td>-6.365438</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-6.370860</td>\n",
       "      <td>53.0</td>\n",
       "      <td>-5.926846</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-5.925239</td>\n",
       "      <td>34.0</td>\n",
       "      <td>-5.688428</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-5.692982</td>\n",
       "      <td>82.0</td>\n",
       "      <td>-6.253382</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-6.262632</td>\n",
       "      <td>99.0</td>\n",
       "      <td>-6.455512</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-6.466951</td>\n",
       "      <td>98.0</td>\n",
       "      <td>-5.826158</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.831306</td>\n",
       "      <td>49.0</td>\n",
       "      <td>-6.258624</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-6.264011</td>\n",
       "      <td>64.0</td>\n",
       "      <td>-5.867534</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-5.870164</td>\n",
       "      <td>52.0</td>\n",
       "      <td>-6.392454</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-6.385984</td>\n",
       "      <td>99.0</td>\n",
       "      <td>-5.945722</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-5.951723</td>\n",
       "      <td>77.0</td>\n",
       "      <td>-6.068260</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-6.067588</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-6.336077</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-6.344388</td>\n",
       "      <td>64.0</td>\n",
       "      <td>-5.882263</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-5.878037</td>\n",
       "      <td>99.0</td>\n",
       "      <td>-6.455512</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-6.459464</td>\n",
       "      <td>56.0</td>\n",
       "      <td>-5.900869</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-5.900371</td>\n",
       "      <td>92.0</td>\n",
       "      <td>-6.364440</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-6.365460</td>\n",
       "      <td>49.0</td>\n",
       "      <td>-6.393664</td>\n",
       "      <td>29.0</td>\n",
       "      <td>-6.097986</td>\n",
       "      <td>21.0</td>\n",
       "      <td>-6.237360</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-6.239362</td>\n",
       "      <td>92.0</td>\n",
       "      <td>-6.378193</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-6.372351</td>\n",
       "      <td>18.0</td>\n",
       "      <td>-7.200351</td>\n",
       "      <td>88.0</td>\n",
       "      <td>-5.091616</td>\n",
       "      <td>99.0</td>\n",
       "      <td>-5.832678</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-5.833019</td>\n",
       "      <td>61.0</td>\n",
       "      <td>-4.990871</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-4.980806</td>\n",
       "      <td>35.0</td>\n",
       "      <td>-5.157475</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-5.156066</td>\n",
       "      <td>61.0</td>\n",
       "      <td>-5.504945</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-5.516371</td>\n",
       "      <td>60.0</td>\n",
       "      <td>-5.462380</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-5.452002</td>\n",
       "      <td>86.0</td>\n",
       "      <td>-5.521792</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-5.517632</td>\n",
       "      <td>43.0</td>\n",
       "      <td>-5.785132</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-5.782202</td>\n",
       "      <td>96.0</td>\n",
       "      <td>-6.113518</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-6.110330</td>\n",
       "      <td>83.0</td>\n",
       "      <td>-4.971684</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-4.973066</td>\n",
       "      <td>61.0</td>\n",
       "      <td>-5.355205</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-5.319679</td>\n",
       "      <td>61.0</td>\n",
       "      <td>-5.775953</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-5.778328</td>\n",
       "      <td>86.0</td>\n",
       "      <td>-5.071329</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-5.069152</td>\n",
       "      <td>86.0</td>\n",
       "      <td>-5.384874</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-5.382573</td>\n",
       "      <td>70.0</td>\n",
       "      <td>-4.102417</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-4.103675</td>\n",
       "      <td>83.0</td>\n",
       "      <td>-6.104436</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-6.100065</td>\n",
       "      <td>44.0</td>\n",
       "      <td>-6.182239</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-6.185272</td>\n",
       "      <td>34.0</td>\n",
       "      <td>-6.627779</td>\n",
       "      <td>42.0</td>\n",
       "      <td>-6.332142</td>\n",
       "      <td>39.0</td>\n",
       "      <td>-6.096475</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-6.095916</td>\n",
       "      <td>22.0</td>\n",
       "      <td>-5.833680</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-5.833448</td>\n",
       "      <td>85.0</td>\n",
       "      <td>-6.223698</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-6.217341</td>\n",
       "      <td>48.0</td>\n",
       "      <td>-5.782372</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-5.777117</td>\n",
       "      <td>61.0</td>\n",
       "      <td>-5.641105</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-5.642994</td>\n",
       "      <td>60.0</td>\n",
       "      <td>-5.492447</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-5.496153</td>\n",
       "      <td>97.0</td>\n",
       "      <td>-5.955873</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-5.959344</td>\n",
       "      <td>28.0</td>\n",
       "      <td>-4.586379</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-4.583322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>96.0</td>\n",
       "      <td>-6.310939</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-6.314555</td>\n",
       "      <td>29.0</td>\n",
       "      <td>-6.586070</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-6.578732</td>\n",
       "      <td>96.0</td>\n",
       "      <td>-6.310939</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-6.320697</td>\n",
       "      <td>97.0</td>\n",
       "      <td>-6.464425</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-6.485068</td>\n",
       "      <td>86.0</td>\n",
       "      <td>-6.372328</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-6.376309</td>\n",
       "      <td>28.0</td>\n",
       "      <td>-6.282807</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-6.273282</td>\n",
       "      <td>94.0</td>\n",
       "      <td>-6.454948</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-6.464305</td>\n",
       "      <td>35.0</td>\n",
       "      <td>-6.331505</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-6.338234</td>\n",
       "      <td>52.0</td>\n",
       "      <td>-6.392454</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-6.390116</td>\n",
       "      <td>53.0</td>\n",
       "      <td>-6.359660</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-6.381303</td>\n",
       "      <td>93.0</td>\n",
       "      <td>-6.344256</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-6.355586</td>\n",
       "      <td>81.0</td>\n",
       "      <td>-6.199615</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-6.201021</td>\n",
       "      <td>72.0</td>\n",
       "      <td>-6.736660</td>\n",
       "      <td>23.0</td>\n",
       "      <td>-6.224261</td>\n",
       "      <td>63.0</td>\n",
       "      <td>-6.286141</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-6.286999</td>\n",
       "      <td>75.0</td>\n",
       "      <td>-6.275874</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-6.281922</td>\n",
       "      <td>22.0</td>\n",
       "      <td>-4.536525</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-4.540329</td>\n",
       "      <td>81.0</td>\n",
       "      <td>-6.805135</td>\n",
       "      <td>37.0</td>\n",
       "      <td>-6.462296</td>\n",
       "      <td>94.0</td>\n",
       "      <td>-6.202522</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-6.205570</td>\n",
       "      <td>29.0</td>\n",
       "      <td>-6.586070</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-6.588339</td>\n",
       "      <td>99.0</td>\n",
       "      <td>-6.209785</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-6.228810</td>\n",
       "      <td>86.0</td>\n",
       "      <td>-6.119213</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-6.119181</td>\n",
       "      <td>64.0</td>\n",
       "      <td>-5.842688</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-5.849417</td>\n",
       "      <td>86.0</td>\n",
       "      <td>-5.273103</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-5.314286</td>\n",
       "      <td>70.0</td>\n",
       "      <td>-5.655513</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-5.658987</td>\n",
       "      <td>61.0</td>\n",
       "      <td>-5.901546</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-5.899521</td>\n",
       "      <td>52.0</td>\n",
       "      <td>-6.057899</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-6.062016</td>\n",
       "      <td>74.0</td>\n",
       "      <td>-6.267476</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-6.262732</td>\n",
       "      <td>64.0</td>\n",
       "      <td>-6.620191</td>\n",
       "      <td>33.0</td>\n",
       "      <td>-6.302452</td>\n",
       "      <td>76.0</td>\n",
       "      <td>-6.221133</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-6.230107</td>\n",
       "      <td>76.0</td>\n",
       "      <td>-6.221133</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-6.223449</td>\n",
       "      <td>99.0</td>\n",
       "      <td>-6.455512</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-6.466901</td>\n",
       "      <td>80.0</td>\n",
       "      <td>-6.313117</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-6.317309</td>\n",
       "      <td>66.0</td>\n",
       "      <td>-6.043469</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-6.052843</td>\n",
       "      <td>94.0</td>\n",
       "      <td>-6.454948</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-6.447219</td>\n",
       "      <td>42.0</td>\n",
       "      <td>-6.309279</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-6.309877</td>\n",
       "      <td>96.0</td>\n",
       "      <td>-6.549222</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-6.539858</td>\n",
       "      <td>82.0</td>\n",
       "      <td>-6.253382</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-6.259783</td>\n",
       "      <td>86.0</td>\n",
       "      <td>-6.372328</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-6.369914</td>\n",
       "      <td>97.0</td>\n",
       "      <td>-6.464425</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-6.467321</td>\n",
       "      <td>70.0</td>\n",
       "      <td>-6.675759</td>\n",
       "      <td>33.0</td>\n",
       "      <td>-6.270102</td>\n",
       "      <td>38.0</td>\n",
       "      <td>-6.500648</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-6.519234</td>\n",
       "      <td>81.0</td>\n",
       "      <td>-6.235842</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-6.230178</td>\n",
       "      <td>75.0</td>\n",
       "      <td>-6.784420</td>\n",
       "      <td>34.0</td>\n",
       "      <td>-6.454251</td>\n",
       "      <td>96.0</td>\n",
       "      <td>-6.549222</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-6.544451</td>\n",
       "      <td>24.0</td>\n",
       "      <td>-6.662710</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-6.617170</td>\n",
       "      <td>52.0</td>\n",
       "      <td>-6.392454</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-6.382545</td>\n",
       "      <td>62.0</td>\n",
       "      <td>-6.228091</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-6.226757</td>\n",
       "      <td>58.0</td>\n",
       "      <td>-6.238058</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-6.229351</td>\n",
       "      <td>38.0</td>\n",
       "      <td>-6.500648</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-6.489244</td>\n",
       "      <td>99.0</td>\n",
       "      <td>-6.455512</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-6.454201</td>\n",
       "      <td>97.0</td>\n",
       "      <td>-6.464425</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-6.471692</td>\n",
       "      <td>72.0</td>\n",
       "      <td>-6.268482</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-6.279225</td>\n",
       "      <td>94.0</td>\n",
       "      <td>-6.677761</td>\n",
       "      <td>15.0</td>\n",
       "      <td>-6.478706</td>\n",
       "      <td>52.0</td>\n",
       "      <td>-6.627138</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-6.619775</td>\n",
       "      <td>24.0</td>\n",
       "      <td>-6.476279</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-6.437866</td>\n",
       "      <td>24.0</td>\n",
       "      <td>-6.476279</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-6.476952</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.943341</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-5.945606</td>\n",
       "      <td>19.0</td>\n",
       "      <td>-6.399796</td>\n",
       "      <td>26.0</td>\n",
       "      <td>-5.937406</td>\n",
       "      <td>35.0</td>\n",
       "      <td>-6.331505</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-6.330876</td>\n",
       "      <td>57.0</td>\n",
       "      <td>-6.619365</td>\n",
       "      <td>9.0</td>\n",
       "      <td>-6.499627</td>\n",
       "      <td>42.0</td>\n",
       "      <td>-5.890154</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-5.886685</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-6.338272</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-6.339762</td>\n",
       "      <td>67.0</td>\n",
       "      <td>-5.895584</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-5.892632</td>\n",
       "      <td>57.0</td>\n",
       "      <td>-6.385644</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-6.397864</td>\n",
       "      <td>93.0</td>\n",
       "      <td>-6.284039</td>\n",
       "      <td>16.0</td>\n",
       "      <td>-5.981733</td>\n",
       "      <td>22.0</td>\n",
       "      <td>-6.086826</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-6.088886</td>\n",
       "      <td>24.0</td>\n",
       "      <td>-6.476279</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-6.453732</td>\n",
       "      <td>84.0</td>\n",
       "      <td>-5.930282</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-5.930590</td>\n",
       "      <td>84.0</td>\n",
       "      <td>-6.819258</td>\n",
       "      <td>40.0</td>\n",
       "      <td>-6.476225</td>\n",
       "      <td>71.0</td>\n",
       "      <td>-5.942923</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-5.946708</td>\n",
       "      <td>72.0</td>\n",
       "      <td>-6.370844</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-6.376408</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-6.108628</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-6.106754</td>\n",
       "      <td>76.0</td>\n",
       "      <td>-6.354255</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-6.253492</td>\n",
       "      <td>74.0</td>\n",
       "      <td>-6.370296</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-6.394812</td>\n",
       "      <td>55.0</td>\n",
       "      <td>-5.102791</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-5.103552</td>\n",
       "      <td>89.0</td>\n",
       "      <td>-5.834847</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-5.835123</td>\n",
       "      <td>30.0</td>\n",
       "      <td>-5.006681</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-5.013330</td>\n",
       "      <td>83.0</td>\n",
       "      <td>-5.156481</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-5.165165</td>\n",
       "      <td>66.0</td>\n",
       "      <td>-6.225242</td>\n",
       "      <td>51.0</td>\n",
       "      <td>-5.516737</td>\n",
       "      <td>13.0</td>\n",
       "      <td>-5.448400</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-5.452592</td>\n",
       "      <td>20.0</td>\n",
       "      <td>-5.512191</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-5.522265</td>\n",
       "      <td>9.0</td>\n",
       "      <td>-6.003216</td>\n",
       "      <td>16.0</td>\n",
       "      <td>-5.809843</td>\n",
       "      <td>77.0</td>\n",
       "      <td>-6.184637</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-6.187559</td>\n",
       "      <td>67.0</td>\n",
       "      <td>-5.016879</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-5.008533</td>\n",
       "      <td>40.0</td>\n",
       "      <td>-5.339369</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-5.344602</td>\n",
       "      <td>22.0</td>\n",
       "      <td>-5.818552</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-5.816669</td>\n",
       "      <td>91.0</td>\n",
       "      <td>-5.262899</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-5.264668</td>\n",
       "      <td>82.0</td>\n",
       "      <td>-5.794258</td>\n",
       "      <td>27.0</td>\n",
       "      <td>-5.400883</td>\n",
       "      <td>93.0</td>\n",
       "      <td>-4.195492</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-4.199290</td>\n",
       "      <td>80.0</td>\n",
       "      <td>-6.096283</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-6.102666</td>\n",
       "      <td>84.0</td>\n",
       "      <td>-6.195378</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-6.191125</td>\n",
       "      <td>86.0</td>\n",
       "      <td>-6.372328</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-6.369525</td>\n",
       "      <td>40.0</td>\n",
       "      <td>-6.343839</td>\n",
       "      <td>15.0</td>\n",
       "      <td>-6.110246</td>\n",
       "      <td>34.0</td>\n",
       "      <td>-5.863704</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-5.861121</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-6.222270</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-6.219057</td>\n",
       "      <td>69.0</td>\n",
       "      <td>-5.969609</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-5.813753</td>\n",
       "      <td>70.0</td>\n",
       "      <td>-7.245023</td>\n",
       "      <td>81.0</td>\n",
       "      <td>-5.683380</td>\n",
       "      <td>45.0</td>\n",
       "      <td>-5.510035</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-5.513776</td>\n",
       "      <td>52.0</td>\n",
       "      <td>-6.142846</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-6.135499</td>\n",
       "      <td>53.0</td>\n",
       "      <td>-4.603284</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-4.602691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>12.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>97.0</td>\n",
       "      <td>-9.736045</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>95.0</td>\n",
       "      <td>-9.736048</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>99.0</td>\n",
       "      <td>-9.732467</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>98.0</td>\n",
       "      <td>-9.739051</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>92.0</td>\n",
       "      <td>-9.741116</td>\n",
       "      <td>14.0</td>\n",
       "      <td>-9.727593</td>\n",
       "      <td>87.0</td>\n",
       "      <td>-9.738787</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>95.0</td>\n",
       "      <td>-9.739703</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>97.0</td>\n",
       "      <td>-9.739460</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>99.0</td>\n",
       "      <td>-9.734538</td>\n",
       "      <td>11.0</td>\n",
       "      <td>-9.727593</td>\n",
       "      <td>91.0</td>\n",
       "      <td>-9.735209</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>92.0</td>\n",
       "      <td>-9.738401</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>94.0</td>\n",
       "      <td>-9.732499</td>\n",
       "      <td>17.0</td>\n",
       "      <td>-9.727593</td>\n",
       "      <td>88.0</td>\n",
       "      <td>-9.733868</td>\n",
       "      <td>17.0</td>\n",
       "      <td>-9.727593</td>\n",
       "      <td>88.0</td>\n",
       "      <td>-9.737289</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>99.0</td>\n",
       "      <td>-9.742003</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>95.0</td>\n",
       "      <td>-9.728175</td>\n",
       "      <td>16.0</td>\n",
       "      <td>-9.727593</td>\n",
       "      <td>90.0</td>\n",
       "      <td>-9.736752</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>92.0</td>\n",
       "      <td>-9.744748</td>\n",
       "      <td>16.0</td>\n",
       "      <td>-9.727593</td>\n",
       "      <td>90.0</td>\n",
       "      <td>-9.742700</td>\n",
       "      <td>51.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>96.0</td>\n",
       "      <td>-9.730780</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>93.0</td>\n",
       "      <td>-9.736245</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>98.0</td>\n",
       "      <td>-9.730020</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>95.0</td>\n",
       "      <td>-9.733356</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>99.0</td>\n",
       "      <td>-9.734082</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>97.0</td>\n",
       "      <td>-9.721565</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>98.0</td>\n",
       "      <td>-9.734002</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>98.0</td>\n",
       "      <td>-9.733186</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>94.0</td>\n",
       "      <td>-9.739254</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>92.0</td>\n",
       "      <td>-9.744131</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>94.0</td>\n",
       "      <td>-9.738722</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>94.0</td>\n",
       "      <td>-9.738281</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>93.0</td>\n",
       "      <td>-9.737617</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>93.0</td>\n",
       "      <td>-9.737058</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>93.0</td>\n",
       "      <td>-9.734204</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>98.0</td>\n",
       "      <td>-9.733026</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>97.0</td>\n",
       "      <td>-9.731474</td>\n",
       "      <td>16.0</td>\n",
       "      <td>-9.727593</td>\n",
       "      <td>90.0</td>\n",
       "      <td>-9.727341</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>92.0</td>\n",
       "      <td>-9.733254</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>92.0</td>\n",
       "      <td>-9.736431</td>\n",
       "      <td>29.0</td>\n",
       "      <td>-9.727593</td>\n",
       "      <td>91.0</td>\n",
       "      <td>-9.721487</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>94.0</td>\n",
       "      <td>-9.737429</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>95.0</td>\n",
       "      <td>-9.746287</td>\n",
       "      <td>51.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>96.0</td>\n",
       "      <td>-9.742451</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>94.0</td>\n",
       "      <td>-9.734765</td>\n",
       "      <td>17.0</td>\n",
       "      <td>-9.727593</td>\n",
       "      <td>88.0</td>\n",
       "      <td>-9.737298</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>94.0</td>\n",
       "      <td>-9.731956</td>\n",
       "      <td>17.0</td>\n",
       "      <td>-9.727593</td>\n",
       "      <td>88.0</td>\n",
       "      <td>-9.728438</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>98.0</td>\n",
       "      <td>-9.729780</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>97.0</td>\n",
       "      <td>-9.737868</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>98.0</td>\n",
       "      <td>-9.737443</td>\n",
       "      <td>51.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>96.0</td>\n",
       "      <td>-9.737937</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>98.0</td>\n",
       "      <td>-9.736635</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>94.0</td>\n",
       "      <td>-9.733876</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>95.0</td>\n",
       "      <td>-9.735138</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>94.0</td>\n",
       "      <td>-9.733182</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>95.0</td>\n",
       "      <td>-9.737107</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>98.0</td>\n",
       "      <td>-9.725956</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-9.727593</td>\n",
       "      <td>89.0</td>\n",
       "      <td>-9.730526</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-9.727593</td>\n",
       "      <td>89.0</td>\n",
       "      <td>-9.734692</td>\n",
       "      <td>11.0</td>\n",
       "      <td>-9.727593</td>\n",
       "      <td>91.0</td>\n",
       "      <td>-9.738967</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>98.0</td>\n",
       "      <td>-9.734627</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>94.0</td>\n",
       "      <td>-9.734056</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>94.0</td>\n",
       "      <td>-9.730145</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>97.0</td>\n",
       "      <td>-9.726907</td>\n",
       "      <td>51.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>96.0</td>\n",
       "      <td>-9.727899</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>94.0</td>\n",
       "      <td>-9.729106</td>\n",
       "      <td>11.0</td>\n",
       "      <td>-9.727593</td>\n",
       "      <td>91.0</td>\n",
       "      <td>-9.726359</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>98.0</td>\n",
       "      <td>-9.779644</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>98.0</td>\n",
       "      <td>-9.734325</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>93.0</td>\n",
       "      <td>-9.735436</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>97.0</td>\n",
       "      <td>-9.734463</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>95.0</td>\n",
       "      <td>-9.733220</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>95.0</td>\n",
       "      <td>-9.734617</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>92.0</td>\n",
       "      <td>-9.736211</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>94.0</td>\n",
       "      <td>-9.731956</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>99.0</td>\n",
       "      <td>-9.735878</td>\n",
       "      <td>51.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>96.0</td>\n",
       "      <td>-9.733600</td>\n",
       "      <td>51.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>96.0</td>\n",
       "      <td>-9.730465</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>99.0</td>\n",
       "      <td>-9.728991</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>92.0</td>\n",
       "      <td>-9.739847</td>\n",
       "      <td>51.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>96.0</td>\n",
       "      <td>-9.728591</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>94.0</td>\n",
       "      <td>-9.731763</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-9.727593</td>\n",
       "      <td>89.0</td>\n",
       "      <td>-9.741066</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>93.0</td>\n",
       "      <td>-9.729050</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>92.0</td>\n",
       "      <td>-9.734412</td>\n",
       "      <td>90.0</td>\n",
       "      <td>-9.727593</td>\n",
       "      <td>91.0</td>\n",
       "      <td>-9.732044</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>94.0</td>\n",
       "      <td>-9.735693</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>97.0</td>\n",
       "      <td>-9.727961</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>98.0</td>\n",
       "      <td>-9.729873</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>97.0</td>\n",
       "      <td>-9.736639</td>\n",
       "      <td>14.0</td>\n",
       "      <td>-9.727593</td>\n",
       "      <td>87.0</td>\n",
       "      <td>-9.732518</td>\n",
       "      <td>11.0</td>\n",
       "      <td>-9.727593</td>\n",
       "      <td>86.0</td>\n",
       "      <td>-9.733492</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>99.0</td>\n",
       "      <td>-9.734880</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>98.0</td>\n",
       "      <td>-9.732196</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>93.0</td>\n",
       "      <td>-9.732396</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>95.0</td>\n",
       "      <td>-9.731427</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>94.0</td>\n",
       "      <td>-9.733614</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>99.0</td>\n",
       "      <td>-9.732234</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>99.0</td>\n",
       "      <td>-9.735483</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>99.0</td>\n",
       "      <td>-9.733828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>8.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>99.0</td>\n",
       "      <td>-9.739898</td>\n",
       "      <td>11.0</td>\n",
       "      <td>-9.727593</td>\n",
       "      <td>91.0</td>\n",
       "      <td>-9.736523</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>92.0</td>\n",
       "      <td>-9.736560</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>97.0</td>\n",
       "      <td>-9.739160</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>93.0</td>\n",
       "      <td>-9.744051</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>93.0</td>\n",
       "      <td>-9.739154</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>98.0</td>\n",
       "      <td>-9.741989</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>94.0</td>\n",
       "      <td>-9.747756</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>93.0</td>\n",
       "      <td>-9.737733</td>\n",
       "      <td>14.0</td>\n",
       "      <td>-9.727593</td>\n",
       "      <td>87.0</td>\n",
       "      <td>-9.737255</td>\n",
       "      <td>14.0</td>\n",
       "      <td>-9.727593</td>\n",
       "      <td>87.0</td>\n",
       "      <td>-9.739338</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>93.0</td>\n",
       "      <td>-9.733826</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>99.0</td>\n",
       "      <td>-9.737035</td>\n",
       "      <td>51.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>96.0</td>\n",
       "      <td>-9.737843</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>94.0</td>\n",
       "      <td>-9.742458</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>93.0</td>\n",
       "      <td>-9.730686</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>97.0</td>\n",
       "      <td>-9.740916</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>93.0</td>\n",
       "      <td>-9.745674</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>98.0</td>\n",
       "      <td>-9.744297</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>99.0</td>\n",
       "      <td>-9.736571</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>95.0</td>\n",
       "      <td>-9.737804</td>\n",
       "      <td>51.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>96.0</td>\n",
       "      <td>-9.737329</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>94.0</td>\n",
       "      <td>-9.735886</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>98.0</td>\n",
       "      <td>-9.738252</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>95.0</td>\n",
       "      <td>-9.731416</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>92.0</td>\n",
       "      <td>-9.737104</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>97.0</td>\n",
       "      <td>-9.740986</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>93.0</td>\n",
       "      <td>-9.739285</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>95.0</td>\n",
       "      <td>-9.744469</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>97.0</td>\n",
       "      <td>-9.739348</td>\n",
       "      <td>14.0</td>\n",
       "      <td>-9.727593</td>\n",
       "      <td>87.0</td>\n",
       "      <td>-9.738815</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>94.0</td>\n",
       "      <td>-9.737807</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>97.0</td>\n",
       "      <td>-9.739939</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>98.0</td>\n",
       "      <td>-9.735894</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>93.0</td>\n",
       "      <td>-9.733351</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-9.727593</td>\n",
       "      <td>89.0</td>\n",
       "      <td>-9.732134</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>92.0</td>\n",
       "      <td>-9.734338</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>93.0</td>\n",
       "      <td>-9.734562</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>98.0</td>\n",
       "      <td>-9.737681</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>93.0</td>\n",
       "      <td>-9.728613</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>98.0</td>\n",
       "      <td>-9.738865</td>\n",
       "      <td>14.0</td>\n",
       "      <td>-9.727593</td>\n",
       "      <td>87.0</td>\n",
       "      <td>-9.747201</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>92.0</td>\n",
       "      <td>-9.745400</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>99.0</td>\n",
       "      <td>-9.735175</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>98.0</td>\n",
       "      <td>-9.738612</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>93.0</td>\n",
       "      <td>-9.735113</td>\n",
       "      <td>51.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>96.0</td>\n",
       "      <td>-9.731973</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>97.0</td>\n",
       "      <td>-9.736673</td>\n",
       "      <td>51.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>96.0</td>\n",
       "      <td>-9.738751</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>95.0</td>\n",
       "      <td>-9.743547</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>97.0</td>\n",
       "      <td>-9.739881</td>\n",
       "      <td>51.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>96.0</td>\n",
       "      <td>-9.738748</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>98.0</td>\n",
       "      <td>-9.735381</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>98.0</td>\n",
       "      <td>-9.735368</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>93.0</td>\n",
       "      <td>-9.736837</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>94.0</td>\n",
       "      <td>-9.737754</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>93.0</td>\n",
       "      <td>-9.729997</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>93.0</td>\n",
       "      <td>-9.734992</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>94.0</td>\n",
       "      <td>-9.738513</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>99.0</td>\n",
       "      <td>-9.745404</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>95.0</td>\n",
       "      <td>-9.734641</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>99.0</td>\n",
       "      <td>-9.734358</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>97.0</td>\n",
       "      <td>-9.735061</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>98.0</td>\n",
       "      <td>-9.741680</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>95.0</td>\n",
       "      <td>-9.728675</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>99.0</td>\n",
       "      <td>-9.735067</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>98.0</td>\n",
       "      <td>-9.736040</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>99.0</td>\n",
       "      <td>-9.797387</td>\n",
       "      <td>17.0</td>\n",
       "      <td>-9.727593</td>\n",
       "      <td>88.0</td>\n",
       "      <td>-9.735950</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>98.0</td>\n",
       "      <td>-9.736580</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>99.0</td>\n",
       "      <td>-9.734673</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>99.0</td>\n",
       "      <td>-9.735376</td>\n",
       "      <td>51.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>96.0</td>\n",
       "      <td>-9.738548</td>\n",
       "      <td>11.0</td>\n",
       "      <td>-9.727593</td>\n",
       "      <td>91.0</td>\n",
       "      <td>-9.737751</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>92.0</td>\n",
       "      <td>-9.738721</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>97.0</td>\n",
       "      <td>-9.737626</td>\n",
       "      <td>77.0</td>\n",
       "      <td>-9.727593</td>\n",
       "      <td>91.0</td>\n",
       "      <td>-9.735757</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>97.0</td>\n",
       "      <td>-9.731701</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>98.0</td>\n",
       "      <td>-9.735601</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>93.0</td>\n",
       "      <td>-9.740124</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>92.0</td>\n",
       "      <td>-9.729023</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>98.0</td>\n",
       "      <td>-9.733335</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>92.0</td>\n",
       "      <td>-9.741921</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>94.0</td>\n",
       "      <td>-9.729384</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>97.0</td>\n",
       "      <td>-9.736106</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>98.0</td>\n",
       "      <td>-9.733565</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>92.0</td>\n",
       "      <td>-9.744117</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>93.0</td>\n",
       "      <td>-9.729630</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>94.0</td>\n",
       "      <td>-9.730498</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>99.0</td>\n",
       "      <td>-9.736886</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-9.727593</td>\n",
       "      <td>89.0</td>\n",
       "      <td>-9.737035</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>92.0</td>\n",
       "      <td>-9.734220</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>95.0</td>\n",
       "      <td>-9.736528</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>92.0</td>\n",
       "      <td>-9.734276</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>94.0</td>\n",
       "      <td>-9.736037</td>\n",
       "      <td>51.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>96.0</td>\n",
       "      <td>-9.731923</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>98.0</td>\n",
       "      <td>-9.734844</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>93.0</td>\n",
       "      <td>-9.733034</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>93.0</td>\n",
       "      <td>-9.736398</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>93.0</td>\n",
       "      <td>-9.735018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>7.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>94.0</td>\n",
       "      <td>-9.741494</td>\n",
       "      <td>51.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>96.0</td>\n",
       "      <td>-9.737504</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>93.0</td>\n",
       "      <td>-9.741411</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-9.727593</td>\n",
       "      <td>89.0</td>\n",
       "      <td>-9.744978</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>97.0</td>\n",
       "      <td>-9.747046</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>99.0</td>\n",
       "      <td>-9.740463</td>\n",
       "      <td>11.0</td>\n",
       "      <td>-9.727593</td>\n",
       "      <td>91.0</td>\n",
       "      <td>-9.744608</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>95.0</td>\n",
       "      <td>-9.750010</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>92.0</td>\n",
       "      <td>-9.742346</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>92.0</td>\n",
       "      <td>-9.748471</td>\n",
       "      <td>17.0</td>\n",
       "      <td>-9.727593</td>\n",
       "      <td>88.0</td>\n",
       "      <td>-9.740447</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>98.0</td>\n",
       "      <td>-9.734141</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>97.0</td>\n",
       "      <td>-9.746912</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>92.0</td>\n",
       "      <td>-9.737889</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>95.0</td>\n",
       "      <td>-9.757664</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>94.0</td>\n",
       "      <td>-9.736979</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>99.0</td>\n",
       "      <td>-9.745246</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>94.0</td>\n",
       "      <td>-9.784444</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>94.0</td>\n",
       "      <td>-9.745339</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>95.0</td>\n",
       "      <td>-9.736653</td>\n",
       "      <td>51.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>96.0</td>\n",
       "      <td>-9.753265</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>93.0</td>\n",
       "      <td>-9.739262</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>97.0</td>\n",
       "      <td>-9.741858</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>92.0</td>\n",
       "      <td>-9.743327</td>\n",
       "      <td>48.0</td>\n",
       "      <td>-9.727593</td>\n",
       "      <td>91.0</td>\n",
       "      <td>-9.732510</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>94.0</td>\n",
       "      <td>-9.737425</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-9.727593</td>\n",
       "      <td>89.0</td>\n",
       "      <td>-9.741468</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>92.0</td>\n",
       "      <td>-9.744000</td>\n",
       "      <td>51.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>96.0</td>\n",
       "      <td>-9.747035</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>99.0</td>\n",
       "      <td>-9.742479</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>92.0</td>\n",
       "      <td>-9.740854</td>\n",
       "      <td>16.0</td>\n",
       "      <td>-9.727593</td>\n",
       "      <td>90.0</td>\n",
       "      <td>-9.739771</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>92.0</td>\n",
       "      <td>-9.868471</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-9.727593</td>\n",
       "      <td>89.0</td>\n",
       "      <td>-9.737282</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>97.0</td>\n",
       "      <td>-9.739572</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>94.0</td>\n",
       "      <td>-9.736856</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>99.0</td>\n",
       "      <td>-9.740038</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>94.0</td>\n",
       "      <td>-9.737960</td>\n",
       "      <td>51.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>96.0</td>\n",
       "      <td>-9.739258</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>94.0</td>\n",
       "      <td>-9.743909</td>\n",
       "      <td>11.0</td>\n",
       "      <td>-9.727593</td>\n",
       "      <td>91.0</td>\n",
       "      <td>-9.740138</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>93.0</td>\n",
       "      <td>-9.751760</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>99.0</td>\n",
       "      <td>-9.745702</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>93.0</td>\n",
       "      <td>-9.741208</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>94.0</td>\n",
       "      <td>-9.739818</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>98.0</td>\n",
       "      <td>-9.739193</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-9.727593</td>\n",
       "      <td>89.0</td>\n",
       "      <td>-9.733766</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>95.0</td>\n",
       "      <td>-9.740248</td>\n",
       "      <td>16.0</td>\n",
       "      <td>-9.727593</td>\n",
       "      <td>90.0</td>\n",
       "      <td>-9.751773</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>92.0</td>\n",
       "      <td>-9.744169</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>99.0</td>\n",
       "      <td>-9.741224</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>94.0</td>\n",
       "      <td>-9.749399</td>\n",
       "      <td>14.0</td>\n",
       "      <td>-9.727593</td>\n",
       "      <td>87.0</td>\n",
       "      <td>-9.747395</td>\n",
       "      <td>51.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>96.0</td>\n",
       "      <td>-9.737254</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>99.0</td>\n",
       "      <td>-9.739243</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-9.727593</td>\n",
       "      <td>89.0</td>\n",
       "      <td>-9.744504</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>95.0</td>\n",
       "      <td>-9.731354</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>97.0</td>\n",
       "      <td>-9.736590</td>\n",
       "      <td>17.0</td>\n",
       "      <td>-9.727593</td>\n",
       "      <td>88.0</td>\n",
       "      <td>-9.739035</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>98.0</td>\n",
       "      <td>-9.745927</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>97.0</td>\n",
       "      <td>-9.740032</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>92.0</td>\n",
       "      <td>-9.735862</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>99.0</td>\n",
       "      <td>-9.739370</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>95.0</td>\n",
       "      <td>-9.742630</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>93.0</td>\n",
       "      <td>-9.739616</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>93.0</td>\n",
       "      <td>-9.735845</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>99.0</td>\n",
       "      <td>-9.749786</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>94.0</td>\n",
       "      <td>-9.811434</td>\n",
       "      <td>11.0</td>\n",
       "      <td>-9.727593</td>\n",
       "      <td>91.0</td>\n",
       "      <td>-9.742347</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>99.0</td>\n",
       "      <td>-9.737024</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>94.0</td>\n",
       "      <td>-9.737544</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>98.0</td>\n",
       "      <td>-9.737366</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>94.0</td>\n",
       "      <td>-9.740950</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>97.0</td>\n",
       "      <td>-9.738492</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>95.0</td>\n",
       "      <td>-9.739339</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>98.0</td>\n",
       "      <td>-9.741815</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>95.0</td>\n",
       "      <td>-9.736565</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>92.0</td>\n",
       "      <td>-9.738268</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>95.0</td>\n",
       "      <td>-9.735865</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>99.0</td>\n",
       "      <td>-9.747544</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>97.0</td>\n",
       "      <td>-9.734732</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>97.0</td>\n",
       "      <td>-9.734577</td>\n",
       "      <td>51.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>96.0</td>\n",
       "      <td>-9.742312</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>95.0</td>\n",
       "      <td>-9.735067</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>98.0</td>\n",
       "      <td>-9.737699</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>92.0</td>\n",
       "      <td>-9.733966</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>98.0</td>\n",
       "      <td>-9.750275</td>\n",
       "      <td>51.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>96.0</td>\n",
       "      <td>-9.735549</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>97.0</td>\n",
       "      <td>-9.731700</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>92.0</td>\n",
       "      <td>-9.737006</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>99.0</td>\n",
       "      <td>-9.740822</td>\n",
       "      <td>51.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>96.0</td>\n",
       "      <td>-9.738712</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>97.0</td>\n",
       "      <td>-9.740213</td>\n",
       "      <td>51.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>96.0</td>\n",
       "      <td>-9.735433</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>92.0</td>\n",
       "      <td>-9.742655</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>97.0</td>\n",
       "      <td>-9.732054</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>99.0</td>\n",
       "      <td>-9.737084</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>95.0</td>\n",
       "      <td>-9.734362</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>92.0</td>\n",
       "      <td>-9.736816</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>98.0</td>\n",
       "      <td>-9.737608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>93.0</td>\n",
       "      <td>-9.744768</td>\n",
       "      <td>17.0</td>\n",
       "      <td>-9.727593</td>\n",
       "      <td>88.0</td>\n",
       "      <td>-9.739672</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>98.0</td>\n",
       "      <td>-9.746467</td>\n",
       "      <td>51.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>96.0</td>\n",
       "      <td>-9.749709</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>99.0</td>\n",
       "      <td>-9.798129</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>92.0</td>\n",
       "      <td>-9.745497</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>93.0</td>\n",
       "      <td>-9.746089</td>\n",
       "      <td>51.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>96.0</td>\n",
       "      <td>-9.771820</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>97.0</td>\n",
       "      <td>-9.747684</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>98.0</td>\n",
       "      <td>-9.749693</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-9.727593</td>\n",
       "      <td>89.0</td>\n",
       "      <td>-9.745355</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>99.0</td>\n",
       "      <td>-9.734818</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>98.0</td>\n",
       "      <td>-9.751837</td>\n",
       "      <td>16.0</td>\n",
       "      <td>-9.727593</td>\n",
       "      <td>90.0</td>\n",
       "      <td>-9.745402</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>92.0</td>\n",
       "      <td>-9.773057</td>\n",
       "      <td>51.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>96.0</td>\n",
       "      <td>-9.737529</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>93.0</td>\n",
       "      <td>-9.747725</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>95.0</td>\n",
       "      <td>-9.812429</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>97.0</td>\n",
       "      <td>-9.746693</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>98.0</td>\n",
       "      <td>-9.740093</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>92.0</td>\n",
       "      <td>-9.780827</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>99.0</td>\n",
       "      <td>-9.743031</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>99.0</td>\n",
       "      <td>-9.742970</td>\n",
       "      <td>51.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>96.0</td>\n",
       "      <td>-9.746374</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>99.0</td>\n",
       "      <td>-9.734055</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>95.0</td>\n",
       "      <td>-9.740233</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>95.0</td>\n",
       "      <td>-9.741498</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>97.0</td>\n",
       "      <td>-9.748805</td>\n",
       "      <td>16.0</td>\n",
       "      <td>-9.727593</td>\n",
       "      <td>90.0</td>\n",
       "      <td>-9.760663</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>98.0</td>\n",
       "      <td>-9.743685</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>97.0</td>\n",
       "      <td>-9.745809</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>92.0</td>\n",
       "      <td>-9.740050</td>\n",
       "      <td>51.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>96.0</td>\n",
       "      <td>-9.887436</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>94.0</td>\n",
       "      <td>-9.747350</td>\n",
       "      <td>51.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>96.0</td>\n",
       "      <td>-9.743623</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>93.0</td>\n",
       "      <td>-9.737259</td>\n",
       "      <td>11.0</td>\n",
       "      <td>-9.727593</td>\n",
       "      <td>91.0</td>\n",
       "      <td>-9.742881</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>99.0</td>\n",
       "      <td>-9.738618</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>99.0</td>\n",
       "      <td>-9.742168</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>98.0</td>\n",
       "      <td>-9.746951</td>\n",
       "      <td>16.0</td>\n",
       "      <td>-9.727593</td>\n",
       "      <td>90.0</td>\n",
       "      <td>-9.746779</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>99.0</td>\n",
       "      <td>-9.773637</td>\n",
       "      <td>16.0</td>\n",
       "      <td>-9.727593</td>\n",
       "      <td>90.0</td>\n",
       "      <td>-9.764245</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>98.0</td>\n",
       "      <td>-9.743615</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>93.0</td>\n",
       "      <td>-9.742810</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>99.0</td>\n",
       "      <td>-9.739234</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>95.0</td>\n",
       "      <td>-9.735850</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>92.0</td>\n",
       "      <td>-9.752468</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>94.0</td>\n",
       "      <td>-9.760437</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>97.0</td>\n",
       "      <td>-9.746710</td>\n",
       "      <td>14.0</td>\n",
       "      <td>-9.727593</td>\n",
       "      <td>87.0</td>\n",
       "      <td>-9.741903</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>92.0</td>\n",
       "      <td>-9.754594</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>92.0</td>\n",
       "      <td>-9.754959</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>92.0</td>\n",
       "      <td>-9.739686</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-9.727593</td>\n",
       "      <td>89.0</td>\n",
       "      <td>-9.763390</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>97.0</td>\n",
       "      <td>-9.748107</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>92.0</td>\n",
       "      <td>-9.742269</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>94.0</td>\n",
       "      <td>-9.736703</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>93.0</td>\n",
       "      <td>-9.740134</td>\n",
       "      <td>16.0</td>\n",
       "      <td>-9.727593</td>\n",
       "      <td>90.0</td>\n",
       "      <td>-9.756600</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>92.0</td>\n",
       "      <td>-9.740938</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>97.0</td>\n",
       "      <td>-9.738839</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>95.0</td>\n",
       "      <td>-9.742396</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>99.0</td>\n",
       "      <td>-9.748480</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>98.0</td>\n",
       "      <td>-9.741485</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>95.0</td>\n",
       "      <td>-9.742193</td>\n",
       "      <td>51.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>96.0</td>\n",
       "      <td>-9.749865</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>92.0</td>\n",
       "      <td>-9.859339</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>94.0</td>\n",
       "      <td>-9.746536</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>92.0</td>\n",
       "      <td>-9.737098</td>\n",
       "      <td>51.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>96.0</td>\n",
       "      <td>-9.745873</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>93.0</td>\n",
       "      <td>-9.873932</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>99.0</td>\n",
       "      <td>-9.790779</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>95.0</td>\n",
       "      <td>-9.741970</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>93.0</td>\n",
       "      <td>-9.740574</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>93.0</td>\n",
       "      <td>-9.745522</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>97.0</td>\n",
       "      <td>-9.738027</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>95.0</td>\n",
       "      <td>-9.739939</td>\n",
       "      <td>51.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>96.0</td>\n",
       "      <td>-9.740923</td>\n",
       "      <td>51.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>96.0</td>\n",
       "      <td>-9.747748</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>98.0</td>\n",
       "      <td>-9.740969</td>\n",
       "      <td>51.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>96.0</td>\n",
       "      <td>-9.735214</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>93.0</td>\n",
       "      <td>-9.814561</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>97.0</td>\n",
       "      <td>-9.738807</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>95.0</td>\n",
       "      <td>-9.737867</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>95.0</td>\n",
       "      <td>-9.735077</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>97.0</td>\n",
       "      <td>-9.933986</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>95.0</td>\n",
       "      <td>-9.740797</td>\n",
       "      <td>51.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>96.0</td>\n",
       "      <td>-9.734659</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>98.0</td>\n",
       "      <td>-9.737267</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>95.0</td>\n",
       "      <td>-9.749562</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>95.0</td>\n",
       "      <td>-9.739071</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>98.0</td>\n",
       "      <td>-9.843504</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>93.0</td>\n",
       "      <td>-9.735964</td>\n",
       "      <td>51.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>96.0</td>\n",
       "      <td>-9.743469</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>92.0</td>\n",
       "      <td>-9.738361</td>\n",
       "      <td>88.0</td>\n",
       "      <td>-9.727593</td>\n",
       "      <td>89.0</td>\n",
       "      <td>-9.737380</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>98.0</td>\n",
       "      <td>-9.734757</td>\n",
       "      <td>51.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>96.0</td>\n",
       "      <td>-9.739737</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>92.0</td>\n",
       "      <td>-9.742069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>4.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>95.0</td>\n",
       "      <td>-9.746672</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>98.0</td>\n",
       "      <td>-9.741522</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>95.0</td>\n",
       "      <td>-9.817535</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>99.0</td>\n",
       "      <td>-9.753163</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>98.0</td>\n",
       "      <td>-9.852698</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>97.0</td>\n",
       "      <td>-9.745788</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>94.0</td>\n",
       "      <td>-9.755100</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>93.0</td>\n",
       "      <td>-9.788496</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>95.0</td>\n",
       "      <td>-9.751492</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>93.0</td>\n",
       "      <td>-9.753802</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>97.0</td>\n",
       "      <td>-9.748753</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>97.0</td>\n",
       "      <td>-9.735373</td>\n",
       "      <td>51.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>96.0</td>\n",
       "      <td>-9.755458</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>94.0</td>\n",
       "      <td>-9.749751</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>97.0</td>\n",
       "      <td>-9.798615</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>92.0</td>\n",
       "      <td>-9.740290</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>98.0</td>\n",
       "      <td>-9.752284</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-9.727593</td>\n",
       "      <td>89.0</td>\n",
       "      <td>-9.815358</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>99.0</td>\n",
       "      <td>-9.761417</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>97.0</td>\n",
       "      <td>-9.745934</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>94.0</td>\n",
       "      <td>-9.789155</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>97.0</td>\n",
       "      <td>-9.748559</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>98.0</td>\n",
       "      <td>-9.745940</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>93.0</td>\n",
       "      <td>-9.910607</td>\n",
       "      <td>51.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>96.0</td>\n",
       "      <td>-9.736728</td>\n",
       "      <td>51.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>96.0</td>\n",
       "      <td>-9.787331</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>93.0</td>\n",
       "      <td>-9.806970</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>95.0</td>\n",
       "      <td>-9.749821</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-9.727593</td>\n",
       "      <td>89.0</td>\n",
       "      <td>-9.823350</td>\n",
       "      <td>51.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>96.0</td>\n",
       "      <td>-9.749327</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-9.727593</td>\n",
       "      <td>89.0</td>\n",
       "      <td>-9.848978</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>97.0</td>\n",
       "      <td>-9.741735</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>94.0</td>\n",
       "      <td>-9.975471</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>95.0</td>\n",
       "      <td>-9.749283</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>94.0</td>\n",
       "      <td>-9.744203</td>\n",
       "      <td>17.0</td>\n",
       "      <td>-9.727593</td>\n",
       "      <td>88.0</td>\n",
       "      <td>-9.754106</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>97.0</td>\n",
       "      <td>-9.747803</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>98.0</td>\n",
       "      <td>-9.743312</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-9.727593</td>\n",
       "      <td>89.0</td>\n",
       "      <td>-9.801399</td>\n",
       "      <td>51.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>96.0</td>\n",
       "      <td>-9.748071</td>\n",
       "      <td>14.0</td>\n",
       "      <td>-9.727593</td>\n",
       "      <td>87.0</td>\n",
       "      <td>-9.752300</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>97.0</td>\n",
       "      <td>-9.780558</td>\n",
       "      <td>17.0</td>\n",
       "      <td>-9.727593</td>\n",
       "      <td>88.0</td>\n",
       "      <td>-9.801290</td>\n",
       "      <td>16.0</td>\n",
       "      <td>-9.727593</td>\n",
       "      <td>90.0</td>\n",
       "      <td>-9.748940</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>92.0</td>\n",
       "      <td>-9.750406</td>\n",
       "      <td>51.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>96.0</td>\n",
       "      <td>-9.739717</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>98.0</td>\n",
       "      <td>-9.741558</td>\n",
       "      <td>51.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>96.0</td>\n",
       "      <td>-9.755829</td>\n",
       "      <td>14.0</td>\n",
       "      <td>-9.727593</td>\n",
       "      <td>87.0</td>\n",
       "      <td>-9.845098</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>99.0</td>\n",
       "      <td>-9.754340</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>94.0</td>\n",
       "      <td>-9.743132</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>93.0</td>\n",
       "      <td>-9.756827</td>\n",
       "      <td>16.0</td>\n",
       "      <td>-9.727593</td>\n",
       "      <td>90.0</td>\n",
       "      <td>-9.766459</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>94.0</td>\n",
       "      <td>-9.740934</td>\n",
       "      <td>11.0</td>\n",
       "      <td>-9.727593</td>\n",
       "      <td>91.0</td>\n",
       "      <td>-9.784596</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>99.0</td>\n",
       "      <td>-9.748240</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>94.0</td>\n",
       "      <td>-9.749172</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>99.0</td>\n",
       "      <td>-9.740966</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>92.0</td>\n",
       "      <td>-9.741769</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>97.0</td>\n",
       "      <td>-9.762114</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>93.0</td>\n",
       "      <td>-9.741757</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>95.0</td>\n",
       "      <td>-9.827401</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>98.0</td>\n",
       "      <td>-9.745969</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>93.0</td>\n",
       "      <td>-9.766183</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>97.0</td>\n",
       "      <td>-9.756805</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>98.0</td>\n",
       "      <td>-9.751494</td>\n",
       "      <td>17.0</td>\n",
       "      <td>-9.727593</td>\n",
       "      <td>88.0</td>\n",
       "      <td>-9.752332</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>93.0</td>\n",
       "      <td>-9.920288</td>\n",
       "      <td>51.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>96.0</td>\n",
       "      <td>-9.747378</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>94.0</td>\n",
       "      <td>-9.743192</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>95.0</td>\n",
       "      <td>-9.746218</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>92.0</td>\n",
       "      <td>-9.891241</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>92.0</td>\n",
       "      <td>-9.817722</td>\n",
       "      <td>17.0</td>\n",
       "      <td>-9.727593</td>\n",
       "      <td>88.0</td>\n",
       "      <td>-9.746170</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>97.0</td>\n",
       "      <td>-9.742110</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>92.0</td>\n",
       "      <td>-9.748466</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>98.0</td>\n",
       "      <td>-9.742846</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>93.0</td>\n",
       "      <td>-9.741092</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>93.0</td>\n",
       "      <td>-9.742655</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>94.0</td>\n",
       "      <td>-9.751253</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>94.0</td>\n",
       "      <td>-9.742553</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>95.0</td>\n",
       "      <td>-9.742047</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>95.0</td>\n",
       "      <td>-9.819942</td>\n",
       "      <td>51.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>96.0</td>\n",
       "      <td>-9.739064</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>94.0</td>\n",
       "      <td>-9.742714</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>94.0</td>\n",
       "      <td>-9.735802</td>\n",
       "      <td>51.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>96.0</td>\n",
       "      <td>-10.009768</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>92.0</td>\n",
       "      <td>-9.742281</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>99.0</td>\n",
       "      <td>-9.735478</td>\n",
       "      <td>51.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>96.0</td>\n",
       "      <td>-9.848498</td>\n",
       "      <td>51.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>96.0</td>\n",
       "      <td>-9.766806</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>97.0</td>\n",
       "      <td>-9.750229</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>94.0</td>\n",
       "      <td>-9.934364</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>95.0</td>\n",
       "      <td>-9.742120</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>97.0</td>\n",
       "      <td>-9.749361</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>94.0</td>\n",
       "      <td>-9.744190</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>97.0</td>\n",
       "      <td>-9.742350</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>97.0</td>\n",
       "      <td>-9.745341</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>98.0</td>\n",
       "      <td>-9.739832</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-9.734209</td>\n",
       "      <td>94.0</td>\n",
       "      <td>-9.742805</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 400 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "label     0                                  1                                \\\n",
       "key2     idx      info act_rank      pred   idx      info act_rank      pred   \n",
       "tokens                                                                         \n",
       "0       99.0 -6.209785      0.0 -6.206440  92.0 -6.378193      1.0 -6.409137   \n",
       "1       97.0 -6.219368      1.0 -6.242119  94.0 -6.454948      2.0 -6.457011   \n",
       "2       61.0 -6.240334      2.0 -6.246603  24.0 -6.662710     10.0 -6.541097   \n",
       "3       39.0 -6.625558     39.0 -6.251883  96.0 -6.549222      3.0 -6.554122   \n",
       "4       96.0 -6.310939      3.0 -6.314555  29.0 -6.586070      4.0 -6.578732   \n",
       "...      ...       ...      ...       ...   ...       ...      ...       ...   \n",
       "95      12.0 -9.734209     97.0 -9.736045   4.0 -9.734209     95.0 -9.736048   \n",
       "96       8.0 -9.734209     99.0 -9.739898  11.0 -9.727593     91.0 -9.736523   \n",
       "97       7.0 -9.734209     94.0 -9.741494  51.0 -9.734209     96.0 -9.737504   \n",
       "98       1.0 -9.734209     93.0 -9.744768  17.0 -9.727593     88.0 -9.739672   \n",
       "99       4.0 -9.734209     95.0 -9.746672   3.0 -9.734209     98.0 -9.741522   \n",
       "\n",
       "label     2                                  3                                \\\n",
       "key2     idx      info act_rank      pred   idx      info act_rank      pred   \n",
       "tokens                                                                         \n",
       "0       76.0 -6.588891     32.0 -6.129146  44.0 -6.182239      0.0 -6.207527   \n",
       "1       46.0 -6.230028      0.0 -6.223404  75.0 -6.275874      1.0 -6.293755   \n",
       "2       21.0 -6.480742     10.0 -6.307346  35.0 -6.331505      3.0 -6.365513   \n",
       "3       42.0 -6.309279      1.0 -6.312391  99.0 -6.455512      4.0 -6.456026   \n",
       "4       96.0 -6.310939      2.0 -6.320697  97.0 -6.464425      5.0 -6.485068   \n",
       "...      ...       ...      ...       ...   ...       ...      ...       ...   \n",
       "95       8.0 -9.734209     99.0 -9.732467   3.0 -9.734209     98.0 -9.739051   \n",
       "96       2.0 -9.734209     92.0 -9.736560  12.0 -9.734209     97.0 -9.739160   \n",
       "97       1.0 -9.734209     93.0 -9.741411  50.0 -9.727593     89.0 -9.744978   \n",
       "98       3.0 -9.734209     98.0 -9.746467  51.0 -9.734209     96.0 -9.749709   \n",
       "99       4.0 -9.734209     95.0 -9.817535   8.0 -9.734209     99.0 -9.753163   \n",
       "\n",
       "label     4                                  5                                \\\n",
       "key2     idx      info act_rank      pred   idx      info act_rank      pred   \n",
       "tokens                                                                         \n",
       "0       96.0 -6.221110      0.0 -6.218801  43.0 -5.775692      0.0 -5.789716   \n",
       "1       43.0 -6.240831      1.0 -6.240847  61.0 -5.910188      1.0 -5.917663   \n",
       "2       92.0 -6.364440      3.0 -6.359645  73.0 -6.217120      4.0 -6.219978   \n",
       "3       72.0 -6.370844      5.0 -6.363751  91.0 -6.249234      5.0 -6.249772   \n",
       "4       86.0 -6.372328      6.0 -6.376309  28.0 -6.282807      7.0 -6.273282   \n",
       "...      ...       ...      ...       ...   ...       ...      ...       ...   \n",
       "95       2.0 -9.734209     92.0 -9.741116  14.0 -9.727593     87.0 -9.738787   \n",
       "96       1.0 -9.734209     93.0 -9.744051   1.0 -9.734209     93.0 -9.739154   \n",
       "97      12.0 -9.734209     97.0 -9.747046   8.0 -9.734209     99.0 -9.740463   \n",
       "98       8.0 -9.734209     99.0 -9.798129   2.0 -9.734209     92.0 -9.745497   \n",
       "99       3.0 -9.734209     98.0 -9.852698  12.0 -9.734209     97.0 -9.745788   \n",
       "\n",
       "label     6                                  7                                \\\n",
       "key2     idx      info act_rank      pred   idx      info act_rank      pred   \n",
       "tokens                                                                         \n",
       "0       75.0 -6.275874      2.0 -6.282557  32.0 -5.709177      0.0 -5.726708   \n",
       "1       35.0 -6.331505      3.0 -6.344807  79.0 -6.130860      1.0 -6.114508   \n",
       "2       28.0 -6.780406     32.0 -6.414482  84.0 -6.195378      2.0 -6.190402   \n",
       "3       96.0 -6.600779     10.0 -6.455039  98.0 -6.311167      3.0 -6.304888   \n",
       "4       94.0 -6.454948      4.0 -6.464305  35.0 -6.331505      4.0 -6.338234   \n",
       "...      ...       ...      ...       ...   ...       ...      ...       ...   \n",
       "95       4.0 -9.734209     95.0 -9.739703  12.0 -9.734209     97.0 -9.739460   \n",
       "96       3.0 -9.734209     98.0 -9.741989   7.0 -9.734209     94.0 -9.747756   \n",
       "97      11.0 -9.727593     91.0 -9.744608   4.0 -9.734209     95.0 -9.750010   \n",
       "98       1.0 -9.734209     93.0 -9.746089  51.0 -9.734209     96.0 -9.771820   \n",
       "99       7.0 -9.734209     94.0 -9.755100   1.0 -9.734209     93.0 -9.788496   \n",
       "\n",
       "label     8                                  9                                \\\n",
       "key2     idx      info act_rank      pred   idx      info act_rank      pred   \n",
       "tokens                                                                         \n",
       "0       66.0 -6.060895      1.0 -6.059132   6.0 -5.983247      0.0 -5.989470   \n",
       "1       80.0 -6.894968     72.0 -6.097312  54.0 -6.143131      1.0 -6.140545   \n",
       "2       35.0 -6.940001     76.0 -6.320796  77.0 -6.324052      2.0 -6.328280   \n",
       "3       29.0 -6.349417      2.0 -6.354245  72.0 -6.368240      4.0 -6.381281   \n",
       "4       52.0 -6.392454      5.0 -6.390116  53.0 -6.359660      3.0 -6.381303   \n",
       "...      ...       ...      ...       ...   ...       ...      ...       ...   \n",
       "95       8.0 -9.734209     99.0 -9.734538  11.0 -9.727593     91.0 -9.735209   \n",
       "96       1.0 -9.734209     93.0 -9.737733  14.0 -9.727593     87.0 -9.737255   \n",
       "97       2.0 -9.734209     92.0 -9.742346   2.0 -9.734209     92.0 -9.748471   \n",
       "98      12.0 -9.734209     97.0 -9.747684   3.0 -9.734209     98.0 -9.749693   \n",
       "99       4.0 -9.734209     95.0 -9.751492   1.0 -9.734209     93.0 -9.753802   \n",
       "\n",
       "label     10                                 11                               \\\n",
       "key2     idx      info act_rank      pred   idx      info act_rank      pred   \n",
       "tokens                                                                         \n",
       "0       37.0 -6.028867      0.0 -6.041323  74.0 -6.117087      0.0 -6.113903   \n",
       "1       71.0 -6.044409      1.0 -6.052176  98.0 -6.156014      3.0 -6.159112   \n",
       "2       76.0 -6.221133      2.0 -6.217917  63.0 -6.171896      4.0 -6.180599   \n",
       "3       90.0 -6.221301      3.0 -6.231865  28.0 -6.172530      5.0 -6.180791   \n",
       "4       93.0 -6.344256      5.0 -6.355586  81.0 -6.199615      7.0 -6.201021   \n",
       "...      ...       ...      ...       ...   ...       ...      ...       ...   \n",
       "95       2.0 -9.734209     92.0 -9.738401   7.0 -9.734209     94.0 -9.732499   \n",
       "96      14.0 -9.727593     87.0 -9.739338   1.0 -9.734209     93.0 -9.733826   \n",
       "97      17.0 -9.727593     88.0 -9.740447   3.0 -9.734209     98.0 -9.734141   \n",
       "98      50.0 -9.727593     89.0 -9.745355   8.0 -9.734209     99.0 -9.734818   \n",
       "99      12.0 -9.734209     97.0 -9.748753  12.0 -9.734209     97.0 -9.735373   \n",
       "\n",
       "label     12                                 13                               \\\n",
       "key2     idx      info act_rank      pred   idx      info act_rank      pred   \n",
       "tokens                                                                         \n",
       "0       46.0 -5.901704      0.0 -5.909033  48.0 -5.891960      0.0 -5.903122   \n",
       "1       87.0 -6.146663      1.0 -6.154080  56.0 -6.134938      1.0 -6.136778   \n",
       "2       44.0 -6.182239      2.0 -6.180636  64.0 -6.150773      3.0 -6.165330   \n",
       "3       73.0 -6.217120      3.0 -6.216530  70.0 -6.202125      4.0 -6.204170   \n",
       "4       72.0 -6.736660     23.0 -6.224261  63.0 -6.286141      5.0 -6.286999   \n",
       "...      ...       ...      ...       ...   ...       ...      ...       ...   \n",
       "95      17.0 -9.727593     88.0 -9.733868  17.0 -9.727593     88.0 -9.737289   \n",
       "96       8.0 -9.734209     99.0 -9.737035  51.0 -9.734209     96.0 -9.737843   \n",
       "97      12.0 -9.734209     97.0 -9.746912   2.0 -9.734209     92.0 -9.737889   \n",
       "98       3.0 -9.734209     98.0 -9.751837  16.0 -9.727593     90.0 -9.745402   \n",
       "99      51.0 -9.734209     96.0 -9.755458   7.0 -9.734209     94.0 -9.749751   \n",
       "\n",
       "label     14                                 15                               \\\n",
       "key2     idx      info act_rank      pred   idx      info act_rank      pred   \n",
       "tokens                                                                         \n",
       "0       22.0 -5.789740      0.0 -5.805620  74.0 -3.940301      0.0 -3.934020   \n",
       "1       68.0 -6.107600      1.0 -6.112395  83.0 -4.462110      2.0 -4.458435   \n",
       "2       91.0 -6.249234      2.0 -6.252928  86.0 -4.488357      3.0 -4.481386   \n",
       "3       96.0 -6.549222     11.0 -6.258662  48.0 -4.507672      4.0 -4.510556   \n",
       "4       75.0 -6.275874      3.0 -6.281922  22.0 -4.536525      5.0 -4.540329   \n",
       "...      ...       ...      ...       ...   ...       ...      ...       ...   \n",
       "95       8.0 -9.734209     99.0 -9.742003   4.0 -9.734209     95.0 -9.728175   \n",
       "96       7.0 -9.734209     94.0 -9.742458   1.0 -9.734209     93.0 -9.730686   \n",
       "97       4.0 -9.734209     95.0 -9.757664   7.0 -9.734209     94.0 -9.736979   \n",
       "98       2.0 -9.734209     92.0 -9.773057  51.0 -9.734209     96.0 -9.737529   \n",
       "99      12.0 -9.734209     97.0 -9.798615   2.0 -9.734209     92.0 -9.740290   \n",
       "\n",
       "label     16                                 17                               \\\n",
       "key2     idx      info act_rank      pred   idx      info act_rank      pred   \n",
       "tokens                                                                         \n",
       "0        6.0 -5.983247      0.0 -5.991196  82.0 -6.077859      0.0 -6.072291   \n",
       "1       88.0 -6.158427      1.0 -6.172926  36.0 -6.161673      1.0 -6.162069   \n",
       "2       90.0 -6.221301      2.0 -6.223852  45.0 -6.619645     44.0 -6.169834   \n",
       "3       77.0 -6.324052      4.0 -6.326668  71.0 -6.174577      2.0 -6.183572   \n",
       "4       81.0 -6.805135     37.0 -6.462296  94.0 -6.202522      3.0 -6.205570   \n",
       "...      ...       ...      ...       ...   ...       ...      ...       ...   \n",
       "95      16.0 -9.727593     90.0 -9.736752   2.0 -9.734209     92.0 -9.744748   \n",
       "96      12.0 -9.734209     97.0 -9.740916   1.0 -9.734209     93.0 -9.745674   \n",
       "97       8.0 -9.734209     99.0 -9.745246   7.0 -9.734209     94.0 -9.784444   \n",
       "98       1.0 -9.734209     93.0 -9.747725   4.0 -9.734209     95.0 -9.812429   \n",
       "99       3.0 -9.734209     98.0 -9.752284  50.0 -9.727593     89.0 -9.815358   \n",
       "\n",
       "label     18                                 19                               \\\n",
       "key2     idx      info act_rank      pred   idx      info act_rank      pred   \n",
       "tokens                                                                         \n",
       "0       95.0 -6.196057      0.0 -6.237668  43.0 -6.070198      1.0 -6.083841   \n",
       "1       99.0 -6.455512      2.0 -6.438261  97.0 -6.219368      3.0 -6.203945   \n",
       "2       97.0 -6.464425      3.0 -6.464385  88.0 -6.618454     39.0 -6.215768   \n",
       "3       94.0 -6.454948      1.0 -6.475404  39.0 -6.572208     28.0 -6.216390   \n",
       "4       29.0 -6.586070      5.0 -6.588339  99.0 -6.209785      2.0 -6.228810   \n",
       "...      ...       ...      ...       ...   ...       ...      ...       ...   \n",
       "95      16.0 -9.727593     90.0 -9.742700  51.0 -9.734209     96.0 -9.730780   \n",
       "96       3.0 -9.734209     98.0 -9.744297   8.0 -9.734209     99.0 -9.736571   \n",
       "97       7.0 -9.734209     94.0 -9.745339   4.0 -9.734209     95.0 -9.736653   \n",
       "98      12.0 -9.734209     97.0 -9.746693   3.0 -9.734209     98.0 -9.740093   \n",
       "99       8.0 -9.734209     99.0 -9.761417  12.0 -9.734209     97.0 -9.745934   \n",
       "\n",
       "label     20                                 21                               \\\n",
       "key2     idx      info act_rank      pred   idx      info act_rank      pred   \n",
       "tokens                                                                         \n",
       "0       36.0 -6.006469      0.0 -6.007827  75.0 -5.770156      2.0 -5.778278   \n",
       "1        5.0 -6.039419      1.0 -6.032581  73.0 -5.807189      3.0 -5.814392   \n",
       "2       43.0 -6.372712     28.0 -6.037009  70.0 -5.815135      4.0 -5.819547   \n",
       "3       92.0 -6.110777      3.0 -6.113202   0.0 -5.728776      0.0 -5.827851   \n",
       "4       86.0 -6.119213      5.0 -6.119181  64.0 -5.842688      5.0 -5.849417   \n",
       "...      ...       ...      ...       ...   ...       ...      ...       ...   \n",
       "95       1.0 -9.734209     93.0 -9.736245   3.0 -9.734209     98.0 -9.730020   \n",
       "96       4.0 -9.734209     95.0 -9.737804  51.0 -9.734209     96.0 -9.737329   \n",
       "97      51.0 -9.734209     96.0 -9.753265   1.0 -9.734209     93.0 -9.739262   \n",
       "98       2.0 -9.734209     92.0 -9.780827   8.0 -9.734209     99.0 -9.743031   \n",
       "99       7.0 -9.734209     94.0 -9.789155  12.0 -9.734209     97.0 -9.748559   \n",
       "\n",
       "label     22                                 23                               \\\n",
       "key2     idx      info act_rank      pred   idx      info act_rank      pred   \n",
       "tokens                                                                         \n",
       "0       52.0 -5.245574      1.0 -5.248561  86.0 -5.542492      0.0 -5.536958   \n",
       "1       28.0 -5.266717      2.0 -5.268564  66.0 -5.580114      1.0 -5.579801   \n",
       "2       19.0 -5.288770      4.0 -5.295113  62.0 -5.639951      2.0 -5.641968   \n",
       "3       70.0 -5.302331      5.0 -5.305170  76.0 -5.644109      3.0 -5.644223   \n",
       "4       86.0 -5.273103      3.0 -5.314286  70.0 -5.655513      4.0 -5.658987   \n",
       "...      ...       ...      ...       ...   ...       ...      ...       ...   \n",
       "95       4.0 -9.734209     95.0 -9.733356   8.0 -9.734209     99.0 -9.734082   \n",
       "96       7.0 -9.734209     94.0 -9.735886   3.0 -9.734209     98.0 -9.738252   \n",
       "97      12.0 -9.734209     97.0 -9.741858   2.0 -9.734209     92.0 -9.743327   \n",
       "98       8.0 -9.734209     99.0 -9.742970  51.0 -9.734209     96.0 -9.746374   \n",
       "99       3.0 -9.734209     98.0 -9.745940   1.0 -9.734209     93.0 -9.910607   \n",
       "\n",
       "label     24                                 25                               \\\n",
       "key2     idx      info act_rank      pred   idx      info act_rank      pred   \n",
       "tokens                                                                         \n",
       "0       27.0 -5.703646      0.0 -5.705223   5.0 -6.717038     58.0 -5.601665   \n",
       "1       70.0 -5.769703      1.0 -5.777030  45.0 -5.765828      0.0 -5.761786   \n",
       "2       68.0 -5.819183      2.0 -5.819251  65.0 -6.212671      8.0 -5.965829   \n",
       "3       67.0 -5.826686      3.0 -5.822942  57.0 -6.049853      1.0 -6.048114   \n",
       "4       61.0 -5.901546      6.0 -5.899521  52.0 -6.057899      2.0 -6.062016   \n",
       "...      ...       ...      ...       ...   ...       ...      ...       ...   \n",
       "95      12.0 -9.734209     97.0 -9.721565   3.0 -9.734209     98.0 -9.734002   \n",
       "96       4.0 -9.734209     95.0 -9.731416   2.0 -9.734209     92.0 -9.737104   \n",
       "97      48.0 -9.727593     91.0 -9.732510   7.0 -9.734209     94.0 -9.737425   \n",
       "98       8.0 -9.734209     99.0 -9.734055   4.0 -9.734209     95.0 -9.740233   \n",
       "99      51.0 -9.734209     96.0 -9.736728  51.0 -9.734209     96.0 -9.787331   \n",
       "\n",
       "label     26                                 27                               \\\n",
       "key2     idx      info act_rank      pred   idx      info act_rank      pred   \n",
       "tokens                                                                         \n",
       "0       25.0 -5.507920      0.0 -5.510865  84.0 -5.930282      0.0 -5.927581   \n",
       "1       31.0 -5.951787      1.0 -5.954525  24.0 -6.232370      1.0 -6.245910   \n",
       "2       35.0 -7.591816     78.0 -6.204265  38.0 -6.257469      2.0 -6.251998   \n",
       "3       59.0 -6.990874     69.0 -6.221961  36.0 -6.259565      3.0 -6.257571   \n",
       "4       74.0 -6.267476      3.0 -6.262732  64.0 -6.620191     33.0 -6.302452   \n",
       "...      ...       ...      ...       ...   ...       ...      ...       ...   \n",
       "95       3.0 -9.734209     98.0 -9.733186   7.0 -9.734209     94.0 -9.739254   \n",
       "96      12.0 -9.734209     97.0 -9.740986   1.0 -9.734209     93.0 -9.739285   \n",
       "97      50.0 -9.727593     89.0 -9.741468   2.0 -9.734209     92.0 -9.744000   \n",
       "98       4.0 -9.734209     95.0 -9.741498  12.0 -9.734209     97.0 -9.748805   \n",
       "99       1.0 -9.734209     93.0 -9.806970   4.0 -9.734209     95.0 -9.749821   \n",
       "\n",
       "label     28                                 29                               \\\n",
       "key2     idx      info act_rank      pred   idx      info act_rank      pred   \n",
       "tokens                                                                         \n",
       "0       37.0 -6.028867      0.0 -6.030078  56.0 -6.134938      2.0 -6.143692   \n",
       "1       71.0 -6.044409      1.0 -6.056815  54.0 -6.143131      3.0 -6.147087   \n",
       "2       84.0 -6.195378      2.0 -6.207242  77.0 -6.324052     15.0 -6.150760   \n",
       "3       90.0 -6.221301      4.0 -6.227348  70.0 -6.202125      5.0 -6.202123   \n",
       "4       76.0 -6.221133      3.0 -6.230107  76.0 -6.221133      7.0 -6.223449   \n",
       "...      ...       ...      ...       ...   ...       ...      ...       ...   \n",
       "95       2.0 -9.734209     92.0 -9.744131   7.0 -9.734209     94.0 -9.738722   \n",
       "96       4.0 -9.734209     95.0 -9.744469  12.0 -9.734209     97.0 -9.739348   \n",
       "97      51.0 -9.734209     96.0 -9.747035   8.0 -9.734209     99.0 -9.742479   \n",
       "98      16.0 -9.727593     90.0 -9.760663   3.0 -9.734209     98.0 -9.743685   \n",
       "99      50.0 -9.727593     89.0 -9.823350  51.0 -9.734209     96.0 -9.749327   \n",
       "\n",
       "label     30                                 31                               \\\n",
       "key2     idx      info act_rank      pred   idx      info act_rank      pred   \n",
       "tokens                                                                         \n",
       "0       26.0 -5.849577      0.0 -5.856101  88.0 -6.158427      0.0 -6.153932   \n",
       "1        6.0 -5.983247      1.0 -5.984535  76.0 -6.221133      1.0 -6.235856   \n",
       "2       62.0 -6.228091      2.0 -6.227783  62.0 -6.228091      3.0 -6.238778   \n",
       "3       78.0 -6.691357     16.0 -6.465548  81.0 -6.235842      4.0 -6.249428   \n",
       "4       99.0 -6.455512      3.0 -6.466901  80.0 -6.313117      5.0 -6.317309   \n",
       "...      ...       ...      ...       ...   ...       ...      ...       ...   \n",
       "95       7.0 -9.734209     94.0 -9.738281   1.0 -9.734209     93.0 -9.737617   \n",
       "96      14.0 -9.727593     87.0 -9.738815   7.0 -9.734209     94.0 -9.737807   \n",
       "97       2.0 -9.734209     92.0 -9.740854  16.0 -9.727593     90.0 -9.739771   \n",
       "98      12.0 -9.734209     97.0 -9.745809   2.0 -9.734209     92.0 -9.740050   \n",
       "99      50.0 -9.727593     89.0 -9.848978  12.0 -9.734209     97.0 -9.741735   \n",
       "\n",
       "label     32                                 33                               \\\n",
       "key2     idx      info act_rank      pred   idx      info act_rank      pred   \n",
       "tokens                                                                         \n",
       "0       27.0 -5.374472      0.0 -5.366094  96.0 -6.600779     13.0 -6.036506   \n",
       "1       40.0 -5.911748      2.0 -5.805452  28.0 -6.282807      0.0 -6.310916   \n",
       "2       64.0 -7.128593     72.0 -5.965558  74.0 -6.370296      1.0 -6.383852   \n",
       "3       86.0 -6.009958      3.0 -6.012503  21.0 -6.411710      2.0 -6.413867   \n",
       "4       66.0 -6.043469      4.0 -6.052843  94.0 -6.454948      3.0 -6.447219   \n",
       "...      ...       ...      ...       ...   ...       ...      ...       ...   \n",
       "95       1.0 -9.734209     93.0 -9.737058   1.0 -9.734209     93.0 -9.734204   \n",
       "96      12.0 -9.734209     97.0 -9.739939   3.0 -9.734209     98.0 -9.735894   \n",
       "97       2.0 -9.734209     92.0 -9.868471  50.0 -9.727593     89.0 -9.737282   \n",
       "98      51.0 -9.734209     96.0 -9.887436   7.0 -9.734209     94.0 -9.747350   \n",
       "99       7.0 -9.734209     94.0 -9.975471   4.0 -9.734209     95.0 -9.749283   \n",
       "\n",
       "label     34                                 35                               \\\n",
       "key2     idx      info act_rank      pred   idx      info act_rank      pred   \n",
       "tokens                                                                         \n",
       "0       54.0 -5.874065      0.0 -5.871864  80.0 -6.313117      0.0 -6.334147   \n",
       "1       76.0 -6.588891     33.0 -6.050550  74.0 -6.370296      1.0 -6.390576   \n",
       "2       38.0 -6.257469      1.0 -6.269179  24.0 -6.476279      2.0 -6.495346   \n",
       "3       66.0 -6.539048     20.0 -6.273180  38.0 -6.500648      3.0 -6.502325   \n",
       "4       42.0 -6.309279      2.0 -6.309877  96.0 -6.549222      5.0 -6.539858   \n",
       "...      ...       ...      ...       ...   ...       ...      ...       ...   \n",
       "95       3.0 -9.734209     98.0 -9.733026  12.0 -9.734209     97.0 -9.731474   \n",
       "96       1.0 -9.734209     93.0 -9.733351  50.0 -9.727593     89.0 -9.732134   \n",
       "97      12.0 -9.734209     97.0 -9.739572   7.0 -9.734209     94.0 -9.736856   \n",
       "98      51.0 -9.734209     96.0 -9.743623   1.0 -9.734209     93.0 -9.737259   \n",
       "99       7.0 -9.734209     94.0 -9.744203  17.0 -9.727593     88.0 -9.754106   \n",
       "\n",
       "label     36                                 37                               \\\n",
       "key2     idx      info act_rank      pred   idx      info act_rank      pred   \n",
       "tokens                                                                         \n",
       "0       39.0 -6.140011      0.0 -6.140745  84.0 -5.766344      0.0 -5.767742   \n",
       "1       88.0 -6.158427      1.0 -6.163455  66.0 -6.402689     10.0 -6.155170   \n",
       "2       44.0 -6.182239      2.0 -6.201119  67.0 -6.511463     30.0 -6.343021   \n",
       "3       91.0 -6.249234      3.0 -6.250063  92.0 -6.364440      2.0 -6.361938   \n",
       "4       82.0 -6.253382      4.0 -6.259783  86.0 -6.372328      5.0 -6.369914   \n",
       "...      ...       ...      ...       ...   ...       ...      ...       ...   \n",
       "95      16.0 -9.727593     90.0 -9.727341   2.0 -9.734209     92.0 -9.733254   \n",
       "96       2.0 -9.734209     92.0 -9.734338   1.0 -9.734209     93.0 -9.734562   \n",
       "97       8.0 -9.734209     99.0 -9.740038   7.0 -9.734209     94.0 -9.737960   \n",
       "98      11.0 -9.727593     91.0 -9.742881   8.0 -9.734209     99.0 -9.738618   \n",
       "99      12.0 -9.734209     97.0 -9.747803   3.0 -9.734209     98.0 -9.743312   \n",
       "\n",
       "label     38                                 39                               \\\n",
       "key2     idx      info act_rank      pred   idx      info act_rank      pred   \n",
       "tokens                                                                         \n",
       "0       26.0 -5.849577      0.0 -5.855848  43.0 -6.070198      0.0 -6.075658   \n",
       "1        6.0 -5.983247      1.0 -5.987694  48.0 -6.216141      1.0 -6.221207   \n",
       "2       62.0 -6.228091      2.0 -6.231473  61.0 -6.240334      3.0 -6.227340   \n",
       "3       99.0 -6.455512      3.0 -6.463291  46.0 -6.230028      2.0 -6.230957   \n",
       "4       97.0 -6.464425      4.0 -6.467321  70.0 -6.675759     33.0 -6.270102   \n",
       "...      ...       ...      ...       ...   ...       ...      ...       ...   \n",
       "95       2.0 -9.734209     92.0 -9.736431  29.0 -9.727593     91.0 -9.721487   \n",
       "96       3.0 -9.734209     98.0 -9.737681   1.0 -9.734209     93.0 -9.728613   \n",
       "97      51.0 -9.734209     96.0 -9.739258   7.0 -9.734209     94.0 -9.743909   \n",
       "98       8.0 -9.734209     99.0 -9.742168   3.0 -9.734209     98.0 -9.746951   \n",
       "99      50.0 -9.727593     89.0 -9.801399  51.0 -9.734209     96.0 -9.748071   \n",
       "\n",
       "label     40                                 41                               \\\n",
       "key2     idx      info act_rank      pred   idx      info act_rank      pred   \n",
       "tokens                                                                         \n",
       "0       84.0 -6.195378      0.0 -6.216457  68.0 -6.107600      1.0 -6.121827   \n",
       "1       78.0 -6.438037      1.0 -6.432988  88.0 -6.158427      2.0 -6.163053   \n",
       "2       24.0 -6.476279      3.0 -6.471385  73.0 -6.217120      3.0 -6.210379   \n",
       "3       85.0 -6.468452      2.0 -6.483478  62.0 -6.228091      4.0 -6.222253   \n",
       "4       38.0 -6.500648      4.0 -6.519234  81.0 -6.235842      5.0 -6.230178   \n",
       "...      ...       ...      ...       ...   ...       ...      ...       ...   \n",
       "95       7.0 -9.734209     94.0 -9.737429   4.0 -9.734209     95.0 -9.746287   \n",
       "96       3.0 -9.734209     98.0 -9.738865  14.0 -9.727593     87.0 -9.747201   \n",
       "97      11.0 -9.727593     91.0 -9.740138   1.0 -9.734209     93.0 -9.751760   \n",
       "98      16.0 -9.727593     90.0 -9.746779   8.0 -9.734209     99.0 -9.773637   \n",
       "99      14.0 -9.727593     87.0 -9.752300  12.0 -9.734209     97.0 -9.780558   \n",
       "\n",
       "label     42                                 43                               \\\n",
       "key2     idx      info act_rank      pred   idx      info act_rank      pred   \n",
       "tokens                                                                         \n",
       "0       87.0 -6.146663      1.0 -6.165133   6.0 -5.983247      0.0 -5.989765   \n",
       "1       62.0 -6.228091      2.0 -6.242920  62.0 -6.228091      1.0 -6.238089   \n",
       "2       80.0 -6.313117      3.0 -6.312078  88.0 -6.833242     43.0 -6.443336   \n",
       "3       97.0 -6.464425      6.0 -6.451281  91.0 -6.799016     35.0 -6.497480   \n",
       "4       75.0 -6.784420     34.0 -6.454251  96.0 -6.549222      3.0 -6.544451   \n",
       "...      ...       ...      ...       ...   ...       ...      ...       ...   \n",
       "95      51.0 -9.734209     96.0 -9.742451   7.0 -9.734209     94.0 -9.734765   \n",
       "96       2.0 -9.734209     92.0 -9.745400   8.0 -9.734209     99.0 -9.735175   \n",
       "97       8.0 -9.734209     99.0 -9.745702   1.0 -9.734209     93.0 -9.741208   \n",
       "98      16.0 -9.727593     90.0 -9.764245   3.0 -9.734209     98.0 -9.743615   \n",
       "99      17.0 -9.727593     88.0 -9.801290  16.0 -9.727593     90.0 -9.748940   \n",
       "\n",
       "label     44                                 45                               \\\n",
       "key2     idx      info act_rank      pred   idx      info act_rank      pred   \n",
       "tokens                                                                         \n",
       "0       96.0 -6.549222      1.0 -6.550990  68.0 -5.836143      0.0 -5.833795   \n",
       "1       42.0 -6.548010      0.0 -6.554848  73.0 -5.952051      1.0 -5.943097   \n",
       "2       29.0 -6.586070      2.0 -6.586352  26.0 -6.161404      2.0 -6.155777   \n",
       "3       52.0 -6.627138      4.0 -6.617065  93.0 -6.524523     16.0 -6.360283   \n",
       "4       24.0 -6.662710      7.0 -6.617170  52.0 -6.392454      4.0 -6.382545   \n",
       "...      ...       ...      ...       ...   ...       ...      ...       ...   \n",
       "95      17.0 -9.727593     88.0 -9.737298   7.0 -9.734209     94.0 -9.731956   \n",
       "96       3.0 -9.734209     98.0 -9.738612   1.0 -9.734209     93.0 -9.735113   \n",
       "97       7.0 -9.734209     94.0 -9.739818   3.0 -9.734209     98.0 -9.739193   \n",
       "98       1.0 -9.734209     93.0 -9.742810   8.0 -9.734209     99.0 -9.739234   \n",
       "99       2.0 -9.734209     92.0 -9.750406  51.0 -9.734209     96.0 -9.739717   \n",
       "\n",
       "label     46                                 47                               \\\n",
       "key2     idx      info act_rank      pred   idx      info act_rank      pred   \n",
       "tokens                                                                         \n",
       "0       46.0 -5.901704      0.0 -5.920306  23.0 -5.586688      0.0 -5.599098   \n",
       "1       79.0 -6.130860      1.0 -6.132979  19.0 -6.079284      1.0 -6.078047   \n",
       "2       56.0 -6.134938      2.0 -6.144638  26.0 -6.161404      2.0 -6.160436   \n",
       "3       88.0 -6.158427      3.0 -6.168409  43.0 -6.740957     42.0 -6.160839   \n",
       "4       62.0 -6.228091      4.0 -6.226757  58.0 -6.238058      3.0 -6.229351   \n",
       "...      ...       ...      ...       ...   ...       ...      ...       ...   \n",
       "95      17.0 -9.727593     88.0 -9.728438   3.0 -9.734209     98.0 -9.729780   \n",
       "96      51.0 -9.734209     96.0 -9.731973  12.0 -9.734209     97.0 -9.736673   \n",
       "97      50.0 -9.727593     89.0 -9.733766   4.0 -9.734209     95.0 -9.740248   \n",
       "98       4.0 -9.734209     95.0 -9.735850   2.0 -9.734209     92.0 -9.752468   \n",
       "99       3.0 -9.734209     98.0 -9.741558  51.0 -9.734209     96.0 -9.755829   \n",
       "\n",
       "label     48                                 49                               \\\n",
       "key2     idx      info act_rank      pred   idx      info act_rank      pred   \n",
       "tokens                                                                         \n",
       "0       56.0 -6.134938      0.0 -6.145071  95.0 -6.196057      0.0 -6.221061   \n",
       "1       54.0 -6.143131      1.0 -6.156491  80.0 -6.313117      1.0 -6.321089   \n",
       "2       82.0 -6.253382      3.0 -6.261949  53.0 -6.359660      2.0 -6.384645   \n",
       "3       75.0 -6.275874      4.0 -6.286424  97.0 -6.464425      4.0 -6.451509   \n",
       "4       38.0 -6.500648      6.0 -6.489244  99.0 -6.455512      3.0 -6.454201   \n",
       "...      ...       ...      ...       ...   ...       ...      ...       ...   \n",
       "95      12.0 -9.734209     97.0 -9.737868   3.0 -9.734209     98.0 -9.737443   \n",
       "96      51.0 -9.734209     96.0 -9.738751   4.0 -9.734209     95.0 -9.743547   \n",
       "97      16.0 -9.727593     90.0 -9.751773   2.0 -9.734209     92.0 -9.744169   \n",
       "98       7.0 -9.734209     94.0 -9.760437  12.0 -9.734209     97.0 -9.746710   \n",
       "99      14.0 -9.727593     87.0 -9.845098   8.0 -9.734209     99.0 -9.754340   \n",
       "\n",
       "label     50                                 51                               \\\n",
       "key2     idx      info act_rank      pred   idx      info act_rank      pred   \n",
       "tokens                                                                         \n",
       "0       86.0 -6.365438      2.0 -6.397769  75.0 -6.111759      0.0 -6.105064   \n",
       "1       36.0 -6.643223     12.0 -6.443179   5.0 -6.976879     71.0 -6.221260   \n",
       "2       99.0 -6.455512      3.0 -6.459650  92.0 -6.261224      1.0 -6.256142   \n",
       "3       85.0 -6.468452      6.0 -6.470255  46.0 -6.477499     30.0 -6.262456   \n",
       "4       97.0 -6.464425      4.0 -6.471692  72.0 -6.268482      2.0 -6.279225   \n",
       "...      ...       ...      ...       ...   ...       ...      ...       ...   \n",
       "95      51.0 -9.734209     96.0 -9.737937   3.0 -9.734209     98.0 -9.736635   \n",
       "96      12.0 -9.734209     97.0 -9.739881  51.0 -9.734209     96.0 -9.738748   \n",
       "97       8.0 -9.734209     99.0 -9.741224   7.0 -9.734209     94.0 -9.749399   \n",
       "98      14.0 -9.727593     87.0 -9.741903   2.0 -9.734209     92.0 -9.754594   \n",
       "99       7.0 -9.734209     94.0 -9.743132   1.0 -9.734209     93.0 -9.756827   \n",
       "\n",
       "label     52                                 53                               \\\n",
       "key2     idx      info act_rank      pred   idx      info act_rank      pred   \n",
       "tokens                                                                         \n",
       "0       56.0 -6.134938      0.0 -6.150967  24.0 -6.476279      0.0 -6.491059   \n",
       "1       84.0 -6.195378      1.0 -6.209301  42.0 -6.548010      1.0 -6.546859   \n",
       "2       65.0 -6.329358      3.0 -6.338180  96.0 -6.549222      2.0 -6.557016   \n",
       "3       24.0 -6.476279      4.0 -6.472601  29.0 -6.566573      3.0 -6.569449   \n",
       "4       94.0 -6.677761     15.0 -6.478706  52.0 -6.627138      5.0 -6.619775   \n",
       "...      ...       ...      ...       ...   ...       ...      ...       ...   \n",
       "95       7.0 -9.734209     94.0 -9.733876   4.0 -9.734209     95.0 -9.735138   \n",
       "96       3.0 -9.734209     98.0 -9.735381   3.0 -9.734209     98.0 -9.735368   \n",
       "97      14.0 -9.727593     87.0 -9.747395  51.0 -9.734209     96.0 -9.737254   \n",
       "98       2.0 -9.734209     92.0 -9.754959   2.0 -9.734209     92.0 -9.739686   \n",
       "99      16.0 -9.727593     90.0 -9.766459   7.0 -9.734209     94.0 -9.740934   \n",
       "\n",
       "label     54                                 55                               \\\n",
       "key2     idx      info act_rank      pred   idx      info act_rank      pred   \n",
       "tokens                                                                         \n",
       "0        6.0 -5.983247      0.0 -5.978225  26.0 -5.849577      0.0 -5.874820   \n",
       "1       62.0 -6.228091      2.0 -6.227862  83.0 -6.300496      2.0 -6.304437   \n",
       "2       83.0 -6.300496      3.0 -6.303432  42.0 -6.603251      9.0 -6.348537   \n",
       "3       86.0 -6.365438      4.0 -6.368578  86.0 -6.365438      3.0 -6.370860   \n",
       "4       24.0 -6.476279      8.0 -6.437866  24.0 -6.476279      4.0 -6.476952   \n",
       "...      ...       ...      ...       ...   ...       ...      ...       ...   \n",
       "95       7.0 -9.734209     94.0 -9.733182   4.0 -9.734209     95.0 -9.737107   \n",
       "96       1.0 -9.734209     93.0 -9.736837   7.0 -9.734209     94.0 -9.737754   \n",
       "97       8.0 -9.734209     99.0 -9.739243  50.0 -9.727593     89.0 -9.744504   \n",
       "98      50.0 -9.727593     89.0 -9.763390  12.0 -9.734209     97.0 -9.748107   \n",
       "99      11.0 -9.727593     91.0 -9.784596   8.0 -9.734209     99.0 -9.748240   \n",
       "\n",
       "label     56                                 57                               \\\n",
       "key2     idx      info act_rank      pred   idx      info act_rank      pred   \n",
       "tokens                                                                         \n",
       "0       75.0 -6.470216     45.0 -5.782003  20.0 -5.077253      0.0 -5.086127   \n",
       "1       35.0 -5.946779      4.0 -5.866240  55.0 -5.116639      1.0 -5.119152   \n",
       "2       72.0 -5.920697      1.0 -5.922410  89.0 -5.262535      2.0 -5.258935   \n",
       "3       53.0 -5.926846      2.0 -5.925239  34.0 -5.688428      4.0 -5.692982   \n",
       "4        0.0 -5.943341      3.0 -5.945606  19.0 -6.399796     26.0 -5.937406   \n",
       "...      ...       ...      ...       ...   ...       ...      ...       ...   \n",
       "95       3.0 -9.734209     98.0 -9.725956  50.0 -9.727593     89.0 -9.730526   \n",
       "96       1.0 -9.734209     93.0 -9.729997   1.0 -9.734209     93.0 -9.734992   \n",
       "97       4.0 -9.734209     95.0 -9.731354  12.0 -9.734209     97.0 -9.736590   \n",
       "98       2.0 -9.734209     92.0 -9.742269   7.0 -9.734209     94.0 -9.736703   \n",
       "99       7.0 -9.734209     94.0 -9.749172   8.0 -9.734209     99.0 -9.740966   \n",
       "\n",
       "label     58                                 59                               \\\n",
       "key2     idx      info act_rank      pred   idx      info act_rank      pred   \n",
       "tokens                                                                         \n",
       "0       67.0 -6.091972      0.0 -6.144179  88.0 -6.158427      0.0 -6.189448   \n",
       "1       87.0 -6.146663      1.0 -6.144928  35.0 -6.331505      3.0 -6.334547   \n",
       "2       84.0 -6.195378      2.0 -6.192425  77.0 -6.324052      2.0 -6.335538   \n",
       "3       82.0 -6.253382      5.0 -6.262632  99.0 -6.455512      4.0 -6.466951   \n",
       "4       35.0 -6.331505      7.0 -6.330876  57.0 -6.619365      9.0 -6.499627   \n",
       "...      ...       ...      ...       ...   ...       ...      ...       ...   \n",
       "95      50.0 -9.727593     89.0 -9.734692  11.0 -9.727593     91.0 -9.738967   \n",
       "96       7.0 -9.734209     94.0 -9.738513   8.0 -9.734209     99.0 -9.745404   \n",
       "97      17.0 -9.727593     88.0 -9.739035   3.0 -9.734209     98.0 -9.745927   \n",
       "98       1.0 -9.734209     93.0 -9.740134  16.0 -9.727593     90.0 -9.756600   \n",
       "99       2.0 -9.734209     92.0 -9.741769  12.0 -9.734209     97.0 -9.762114   \n",
       "\n",
       "label     60                                 61                               \\\n",
       "key2     idx      info act_rank      pred   idx      info act_rank      pred   \n",
       "tokens                                                                         \n",
       "0       65.0 -9.727593     90.0 -5.534416  26.0 -6.161404      0.0 -6.165813   \n",
       "1       69.0 -6.200252     23.0 -5.721776  85.0 -6.223698      1.0 -6.219057   \n",
       "2       32.0 -6.084221     13.0 -5.723261  58.0 -6.238058      2.0 -6.241138   \n",
       "3       98.0 -5.826158      0.0 -5.831306  49.0 -6.258624      3.0 -6.264011   \n",
       "4       42.0 -5.890154      2.0 -5.886685   6.0 -6.338272      4.0 -6.339762   \n",
       "...      ...       ...      ...       ...   ...       ...      ...       ...   \n",
       "95       3.0 -9.734209     98.0 -9.734627   7.0 -9.734209     94.0 -9.734056   \n",
       "96       4.0 -9.734209     95.0 -9.734641   8.0 -9.734209     99.0 -9.734358   \n",
       "97      12.0 -9.734209     97.0 -9.740032   2.0 -9.734209     92.0 -9.735862   \n",
       "98       2.0 -9.734209     92.0 -9.740938  12.0 -9.734209     97.0 -9.738839   \n",
       "99       1.0 -9.734209     93.0 -9.741757   4.0 -9.734209     95.0 -9.827401   \n",
       "\n",
       "label     62                                 63                               \\\n",
       "key2     idx      info act_rank      pred   idx      info act_rank      pred   \n",
       "tokens                                                                         \n",
       "0       74.0 -5.728308      0.0 -5.728362  58.0 -6.238058      1.0 -6.234853   \n",
       "1       72.0 -5.730041      1.0 -5.736298  29.0 -6.328879      4.0 -6.331000   \n",
       "2        0.0 -5.753846      2.0 -5.751369  68.0 -6.637797     34.0 -6.346422   \n",
       "3       64.0 -5.867534      3.0 -5.870164  52.0 -6.392454      6.0 -6.385984   \n",
       "4       67.0 -5.895584      4.0 -5.892632  57.0 -6.385644      5.0 -6.397864   \n",
       "...      ...       ...      ...       ...   ...       ...      ...       ...   \n",
       "95       7.0 -9.734209     94.0 -9.730145  12.0 -9.734209     97.0 -9.726907   \n",
       "96      12.0 -9.734209     97.0 -9.735061   3.0 -9.734209     98.0 -9.741680   \n",
       "97       8.0 -9.734209     99.0 -9.739370   4.0 -9.734209     95.0 -9.742630   \n",
       "98       4.0 -9.734209     95.0 -9.742396   8.0 -9.734209     99.0 -9.748480   \n",
       "99       3.0 -9.734209     98.0 -9.745969   1.0 -9.734209     93.0 -9.766183   \n",
       "\n",
       "label     64                                 65                               \\\n",
       "key2     idx      info act_rank      pred   idx      info act_rank      pred   \n",
       "tokens                                                                         \n",
       "0       98.0 -5.779319      0.0 -5.781091  37.0 -5.751393      0.0 -5.752360   \n",
       "1       22.0 -5.814093      1.0 -5.811885  88.0 -6.618454     39.0 -6.066234   \n",
       "2       26.0 -5.894030      2.0 -5.902766  43.0 -6.070198      3.0 -6.067514   \n",
       "3       99.0 -5.945722      3.0 -5.951723  77.0 -6.068260      2.0 -6.067588   \n",
       "4       93.0 -6.284039     16.0 -5.981733  22.0 -6.086826      4.0 -6.088886   \n",
       "...      ...       ...      ...       ...   ...       ...      ...       ...   \n",
       "95      51.0 -9.734209     96.0 -9.727899   7.0 -9.734209     94.0 -9.729106   \n",
       "96       4.0 -9.734209     95.0 -9.728675   8.0 -9.734209     99.0 -9.735067   \n",
       "97       1.0 -9.734209     93.0 -9.739616   1.0 -9.734209     93.0 -9.735845   \n",
       "98       3.0 -9.734209     98.0 -9.741485   4.0 -9.734209     95.0 -9.742193   \n",
       "99      12.0 -9.734209     97.0 -9.756805   3.0 -9.734209     98.0 -9.751494   \n",
       "\n",
       "label     66                                 67                               \\\n",
       "key2     idx      info act_rank      pred   idx      info act_rank      pred   \n",
       "tokens                                                                         \n",
       "0       62.0 -6.228091      1.0 -6.245189  60.0 -5.562530      0.0 -5.576107   \n",
       "1       90.0 -6.221301      0.0 -6.250992   6.0 -5.702554      1.0 -5.704838   \n",
       "2       66.0 -6.317314      3.0 -6.313829  79.0 -5.861041      3.0 -5.867354   \n",
       "3        0.0 -6.336077      4.0 -6.344388  64.0 -5.882263      4.0 -5.878037   \n",
       "4       24.0 -6.476279      5.0 -6.453732  84.0 -5.930282      5.0 -5.930590   \n",
       "...      ...       ...      ...       ...   ...       ...      ...       ...   \n",
       "95      11.0 -9.727593     91.0 -9.726359   3.0 -9.734209     98.0 -9.779644   \n",
       "96       3.0 -9.734209     98.0 -9.736040   8.0 -9.734209     99.0 -9.797387   \n",
       "97       8.0 -9.734209     99.0 -9.749786   7.0 -9.734209     94.0 -9.811434   \n",
       "98      51.0 -9.734209     96.0 -9.749865   2.0 -9.734209     92.0 -9.859339   \n",
       "99      17.0 -9.727593     88.0 -9.752332   1.0 -9.734209     93.0 -9.920288   \n",
       "\n",
       "label     68                                 69                               \\\n",
       "key2     idx      info act_rank      pred   idx      info act_rank      pred   \n",
       "tokens                                                                         \n",
       "0       62.0 -6.228091      0.0 -6.249218  17.0 -6.337839     35.0 -5.600626   \n",
       "1       74.0 -6.370296      2.0 -6.389161  64.0 -5.892754      0.0 -5.894948   \n",
       "2       97.0 -6.464425      5.0 -6.459091  54.0 -5.896406      1.0 -5.898395   \n",
       "3       99.0 -6.455512      4.0 -6.459464  56.0 -5.900869      2.0 -5.900371   \n",
       "4       84.0 -6.819258     40.0 -6.476225  71.0 -5.942923      3.0 -5.946708   \n",
       "...      ...       ...      ...       ...   ...       ...      ...       ...   \n",
       "95       3.0 -9.734209     98.0 -9.734325   1.0 -9.734209     93.0 -9.735436   \n",
       "96      17.0 -9.727593     88.0 -9.735950   3.0 -9.734209     98.0 -9.736580   \n",
       "97      11.0 -9.727593     91.0 -9.742347   8.0 -9.734209     99.0 -9.737024   \n",
       "98       7.0 -9.734209     94.0 -9.746536   2.0 -9.734209     92.0 -9.737098   \n",
       "99      51.0 -9.734209     96.0 -9.747378   7.0 -9.734209     94.0 -9.743192   \n",
       "\n",
       "label     70                                 71                               \\\n",
       "key2     idx      info act_rank      pred   idx      info act_rank      pred   \n",
       "tokens                                                                         \n",
       "0       24.0 -6.289662      0.0 -6.283247  37.0 -6.358535     26.0 -5.970652   \n",
       "1       85.0 -6.296583      1.0 -6.306055  57.0 -6.049853      0.0 -6.049420   \n",
       "2       99.0 -6.307454      3.0 -6.307156  52.0 -6.057899      1.0 -6.054447   \n",
       "3       92.0 -6.364440      4.0 -6.365460  49.0 -6.393664     29.0 -6.097986   \n",
       "4       72.0 -6.370844      5.0 -6.376408   5.0 -6.108628      2.0 -6.106754   \n",
       "...      ...       ...      ...       ...   ...       ...      ...       ...   \n",
       "95      12.0 -9.734209     97.0 -9.734463   4.0 -9.734209     95.0 -9.733220   \n",
       "96       8.0 -9.734209     99.0 -9.734673   8.0 -9.734209     99.0 -9.735376   \n",
       "97       7.0 -9.734209     94.0 -9.737544   3.0 -9.734209     98.0 -9.737366   \n",
       "98      51.0 -9.734209     96.0 -9.745873   1.0 -9.734209     93.0 -9.873932   \n",
       "99       4.0 -9.734209     95.0 -9.746218   2.0 -9.734209     92.0 -9.891241   \n",
       "\n",
       "label     72                                 73                               \\\n",
       "key2     idx      info act_rank      pred   idx      info act_rank      pred   \n",
       "tokens                                                                         \n",
       "0       90.0 -6.354778     11.0 -5.834315  63.0 -6.286141      1.0 -6.307220   \n",
       "1       31.0 -6.583543     38.0 -6.090720  80.0 -6.313117      2.0 -6.328473   \n",
       "2       25.0 -6.660027     45.0 -6.236250  65.0 -6.329358      3.0 -6.344307   \n",
       "3       21.0 -6.237360      1.0 -6.239362  92.0 -6.378193      5.0 -6.372351   \n",
       "4       76.0 -6.354255     10.0 -6.253492  74.0 -6.370296      4.0 -6.394812   \n",
       "...      ...       ...      ...       ...   ...       ...      ...       ...   \n",
       "95       4.0 -9.734209     95.0 -9.734617   2.0 -9.734209     92.0 -9.736211   \n",
       "96      51.0 -9.734209     96.0 -9.738548  11.0 -9.727593     91.0 -9.737751   \n",
       "97       7.0 -9.734209     94.0 -9.740950  12.0 -9.734209     97.0 -9.738492   \n",
       "98       8.0 -9.734209     99.0 -9.790779   4.0 -9.734209     95.0 -9.741970   \n",
       "99       2.0 -9.734209     92.0 -9.817722  17.0 -9.727593     88.0 -9.746170   \n",
       "\n",
       "label     74                                 75                               \\\n",
       "key2     idx      info act_rank      pred   idx      info act_rank      pred   \n",
       "tokens                                                                         \n",
       "0       86.0 -4.806090      0.0 -4.806323  34.0 -5.789685      0.0 -5.785295   \n",
       "1       70.0 -4.925375      1.0 -4.932771  97.0 -5.810853      2.0 -5.802298   \n",
       "2        0.0 -5.046092      4.0 -5.046387  25.0 -5.791017      1.0 -5.806690   \n",
       "3       18.0 -7.200351     88.0 -5.091616  99.0 -5.832678      3.0 -5.833019   \n",
       "4       55.0 -5.102791      5.0 -5.103552  89.0 -5.834847      4.0 -5.835123   \n",
       "...      ...       ...      ...       ...   ...       ...      ...       ...   \n",
       "95       7.0 -9.734209     94.0 -9.731956   8.0 -9.734209     99.0 -9.735878   \n",
       "96       2.0 -9.734209     92.0 -9.738721  12.0 -9.734209     97.0 -9.737626   \n",
       "97       4.0 -9.734209     95.0 -9.739339   3.0 -9.734209     98.0 -9.741815   \n",
       "98       1.0 -9.734209     93.0 -9.740574   1.0 -9.734209     93.0 -9.745522   \n",
       "99      12.0 -9.734209     97.0 -9.742110   2.0 -9.734209     92.0 -9.748466   \n",
       "\n",
       "label     76                                 77                               \\\n",
       "key2     idx      info act_rank      pred   idx      info act_rank      pred   \n",
       "tokens                                                                         \n",
       "0       72.0 -4.939904      2.0 -4.936449   0.0 -5.282387     12.0 -5.004199   \n",
       "1       48.0 -4.960993      3.0 -4.964064  53.0 -5.115004      0.0 -5.109031   \n",
       "2       83.0 -4.978734      4.0 -4.975702  66.0 -5.133687      1.0 -5.133000   \n",
       "3       61.0 -4.990871      5.0 -4.980806  35.0 -5.157475      3.0 -5.156066   \n",
       "4       30.0 -5.006681      6.0 -5.013330  83.0 -5.156481      2.0 -5.165165   \n",
       "...      ...       ...      ...       ...   ...       ...      ...       ...   \n",
       "95      51.0 -9.734209     96.0 -9.733600  51.0 -9.734209     96.0 -9.730465   \n",
       "96      77.0 -9.727593     91.0 -9.735757  12.0 -9.734209     97.0 -9.731701   \n",
       "97       4.0 -9.734209     95.0 -9.736565   2.0 -9.734209     92.0 -9.738268   \n",
       "98      12.0 -9.734209     97.0 -9.738027   4.0 -9.734209     95.0 -9.739939   \n",
       "99       3.0 -9.734209     98.0 -9.742846   1.0 -9.734209     93.0 -9.741092   \n",
       "\n",
       "label     78                                 79                               \\\n",
       "key2     idx      info act_rank      pred   idx      info act_rank      pred   \n",
       "tokens                                                                         \n",
       "0       50.0 -4.841265      0.0 -4.838974   9.0 -5.101474      0.0 -5.154175   \n",
       "1       17.0 -5.132164      1.0 -5.136580  33.0 -5.350877      2.0 -5.353909   \n",
       "2        9.0 -5.485133      3.0 -5.478284  86.0 -5.392830      3.0 -5.398625   \n",
       "3       61.0 -5.504945      4.0 -5.516371  60.0 -5.462380      5.0 -5.452002   \n",
       "4       66.0 -6.225242     51.0 -5.516737  13.0 -5.448400      4.0 -5.452592   \n",
       "...      ...       ...      ...       ...   ...       ...      ...       ...   \n",
       "95       8.0 -9.734209     99.0 -9.728991   2.0 -9.734209     92.0 -9.739847   \n",
       "96       3.0 -9.734209     98.0 -9.735601   1.0 -9.734209     93.0 -9.740124   \n",
       "97       4.0 -9.734209     95.0 -9.735865   8.0 -9.734209     99.0 -9.747544   \n",
       "98      51.0 -9.734209     96.0 -9.740923  51.0 -9.734209     96.0 -9.747748   \n",
       "99       1.0 -9.734209     93.0 -9.742655   7.0 -9.734209     94.0 -9.751253   \n",
       "\n",
       "label     80                                 81                               \\\n",
       "key2     idx      info act_rank      pred   idx      info act_rank      pred   \n",
       "tokens                                                                         \n",
       "0       30.0 -4.732475      0.0 -4.740738  26.0 -5.446089      1.0 -5.444146   \n",
       "1       40.0 -5.365047      2.0 -5.359856  82.0 -5.758117      3.0 -5.755056   \n",
       "2       27.0 -5.364374      1.0 -5.371370  34.0 -5.764222      4.0 -5.763424   \n",
       "3       86.0 -5.521792      4.0 -5.517632  43.0 -5.785132      5.0 -5.782202   \n",
       "4       20.0 -5.512191      3.0 -5.522265   9.0 -6.003216     16.0 -5.809843   \n",
       "...      ...       ...      ...       ...   ...       ...      ...       ...   \n",
       "95      51.0 -9.734209     96.0 -9.728591   7.0 -9.734209     94.0 -9.731763   \n",
       "96       2.0 -9.734209     92.0 -9.729023   3.0 -9.734209     98.0 -9.733335   \n",
       "97      12.0 -9.734209     97.0 -9.734732  12.0 -9.734209     97.0 -9.734577   \n",
       "98       3.0 -9.734209     98.0 -9.740969  51.0 -9.734209     96.0 -9.735214   \n",
       "99       7.0 -9.734209     94.0 -9.742553   4.0 -9.734209     95.0 -9.742047   \n",
       "\n",
       "label     82                                 83                               \\\n",
       "key2     idx      info act_rank      pred   idx      info act_rank      pred   \n",
       "tokens                                                                         \n",
       "0       34.0 -5.688428      0.0 -5.677827  86.0 -4.690676      0.0 -4.702187   \n",
       "1       45.0 -5.691516      1.0 -5.698607  70.0 -4.875638      1.0 -4.857039   \n",
       "2       58.0 -5.975066      3.0 -5.973187  22.0 -4.989712      3.0 -4.936715   \n",
       "3       96.0 -6.113518      4.0 -6.110330  83.0 -4.971684      2.0 -4.973066   \n",
       "4       77.0 -6.184637      5.0 -6.187559  67.0 -5.016879      4.0 -5.008533   \n",
       "...      ...       ...      ...       ...   ...       ...      ...       ...   \n",
       "95      10.0 -9.727593     89.0 -9.741066   1.0 -9.734209     93.0 -9.729050   \n",
       "96       2.0 -9.734209     92.0 -9.741921   7.0 -9.734209     94.0 -9.729384   \n",
       "97      51.0 -9.734209     96.0 -9.742312   4.0 -9.734209     95.0 -9.735067   \n",
       "98       1.0 -9.734209     93.0 -9.814561  12.0 -9.734209     97.0 -9.738807   \n",
       "99       4.0 -9.734209     95.0 -9.819942  51.0 -9.734209     96.0 -9.739064   \n",
       "\n",
       "label     84                                 85                               \\\n",
       "key2     idx      info act_rank      pred   idx      info act_rank      pred   \n",
       "tokens                                                                         \n",
       "0       50.0 -4.719597      0.0 -4.720963  18.0 -4.849282      0.0 -4.856553   \n",
       "1       14.0 -4.913362      1.0 -4.917946  23.0 -5.686593      1.0 -5.682671   \n",
       "2       48.0 -5.255649      2.0 -5.254946  46.0 -5.778876      5.0 -5.777435   \n",
       "3       61.0 -5.355205      4.0 -5.319679  61.0 -5.775953      4.0 -5.778328   \n",
       "4       40.0 -5.339369      3.0 -5.344602  22.0 -5.818552      8.0 -5.816669   \n",
       "...      ...       ...      ...       ...   ...       ...      ...       ...   \n",
       "95       2.0 -9.734209     92.0 -9.734412  90.0 -9.727593     91.0 -9.732044   \n",
       "96      12.0 -9.734209     97.0 -9.736106   3.0 -9.734209     98.0 -9.733565   \n",
       "97       3.0 -9.734209     98.0 -9.737699   2.0 -9.734209     92.0 -9.733966   \n",
       "98       4.0 -9.734209     95.0 -9.737867   4.0 -9.734209     95.0 -9.735077   \n",
       "99       7.0 -9.734209     94.0 -9.742714   7.0 -9.734209     94.0 -9.735802   \n",
       "\n",
       "label     86                                  87                               \\\n",
       "key2     idx      info act_rank       pred   idx      info act_rank      pred   \n",
       "tokens                                                                          \n",
       "0       22.0 -4.846297      0.0  -4.843096  18.0 -4.961876      1.0 -4.961539   \n",
       "1       70.0 -5.158384      2.0  -4.854918  72.0 -5.289307      2.0 -5.292850   \n",
       "2       68.0 -6.042273     48.0  -4.873288  13.0 -5.386950      4.0 -5.380569   \n",
       "3       86.0 -5.071329      1.0  -5.069152  86.0 -5.384874      3.0 -5.382573   \n",
       "4       91.0 -5.262899      4.0  -5.264668  82.0 -5.794258     27.0 -5.400883   \n",
       "...      ...       ...      ...        ...   ...       ...      ...       ...   \n",
       "95       7.0 -9.734209     94.0  -9.735693  12.0 -9.734209     97.0 -9.727961   \n",
       "96       2.0 -9.734209     92.0  -9.744117   1.0 -9.734209     93.0 -9.729630   \n",
       "97       3.0 -9.734209     98.0  -9.750275  51.0 -9.734209     96.0 -9.735549   \n",
       "98      12.0 -9.734209     97.0  -9.933986   4.0 -9.734209     95.0 -9.740797   \n",
       "99      51.0 -9.734209     96.0 -10.009768   2.0 -9.734209     92.0 -9.742281   \n",
       "\n",
       "label     88                                 89                               \\\n",
       "key2     idx      info act_rank      pred   idx      info act_rank      pred   \n",
       "tokens                                                                         \n",
       "0       86.0 -3.930341      0.0 -3.929331  82.0 -6.873151     63.0 -5.735173   \n",
       "1       82.0 -3.970979      1.0 -3.969098  55.0 -6.512794     42.0 -6.092467   \n",
       "2       90.0 -4.010327      2.0 -4.009221  98.0 -6.098917      2.0 -6.095601   \n",
       "3       70.0 -4.102417      3.0 -4.103675  83.0 -6.104436      3.0 -6.100065   \n",
       "4       93.0 -4.195492      5.0 -4.199290  80.0 -6.096283      1.0 -6.102666   \n",
       "...      ...       ...      ...       ...   ...       ...      ...       ...   \n",
       "95       3.0 -9.734209     98.0 -9.729873  12.0 -9.734209     97.0 -9.736639   \n",
       "96       7.0 -9.734209     94.0 -9.730498   8.0 -9.734209     99.0 -9.736886   \n",
       "97      12.0 -9.734209     97.0 -9.731700   2.0 -9.734209     92.0 -9.737006   \n",
       "98      51.0 -9.734209     96.0 -9.734659   3.0 -9.734209     98.0 -9.737267   \n",
       "99       8.0 -9.734209     99.0 -9.735478  51.0 -9.734209     96.0 -9.848498   \n",
       "\n",
       "label     90                                 91                               \\\n",
       "key2     idx      info act_rank      pred   idx      info act_rank      pred   \n",
       "tokens                                                                         \n",
       "0       89.0 -4.908507      0.0 -4.912350  37.0 -6.534252     32.0 -6.265768   \n",
       "1       58.0 -5.908459      1.0 -5.917121  84.0 -6.467428     20.0 -6.272301   \n",
       "2       64.0 -6.150773      2.0 -6.141796  71.0 -6.530053     31.0 -6.275233   \n",
       "3       44.0 -6.182239      3.0 -6.185272  34.0 -6.627779     42.0 -6.332142   \n",
       "4       84.0 -6.195378      4.0 -6.191125  86.0 -6.372328      4.0 -6.369525   \n",
       "...      ...       ...      ...       ...   ...       ...      ...       ...   \n",
       "95      14.0 -9.727593     87.0 -9.732518  11.0 -9.727593     86.0 -9.733492   \n",
       "96      50.0 -9.727593     89.0 -9.737035   2.0 -9.734209     92.0 -9.734220   \n",
       "97       8.0 -9.734209     99.0 -9.740822  51.0 -9.734209     96.0 -9.738712   \n",
       "98       4.0 -9.734209     95.0 -9.749562   4.0 -9.734209     95.0 -9.739071   \n",
       "99      51.0 -9.734209     96.0 -9.766806  12.0 -9.734209     97.0 -9.750229   \n",
       "\n",
       "label     92                                 93                               \\\n",
       "key2     idx      info act_rank      pred   idx      info act_rank      pred   \n",
       "tokens                                                                         \n",
       "0       31.0 -5.719459      0.0 -5.722820  20.0 -6.070054     15.0 -5.759802   \n",
       "1       72.0 -5.962330      1.0 -5.962266  98.0 -5.797782      2.0 -5.794234   \n",
       "2       91.0 -6.039551      2.0 -6.039298  46.0 -5.795397      1.0 -5.797428   \n",
       "3       39.0 -6.096475      3.0 -6.095916  22.0 -5.833680      3.0 -5.833448   \n",
       "4       40.0 -6.343839     15.0 -6.110246  34.0 -5.863704      5.0 -5.861121   \n",
       "...      ...       ...      ...       ...   ...       ...      ...       ...   \n",
       "95       8.0 -9.734209     99.0 -9.734880   3.0 -9.734209     98.0 -9.732196   \n",
       "96       4.0 -9.734209     95.0 -9.736528   2.0 -9.734209     92.0 -9.734276   \n",
       "97      12.0 -9.734209     97.0 -9.740213  51.0 -9.734209     96.0 -9.735433   \n",
       "98       3.0 -9.734209     98.0 -9.843504   1.0 -9.734209     93.0 -9.735964   \n",
       "99       7.0 -9.734209     94.0 -9.934364   4.0 -9.734209     95.0 -9.742120   \n",
       "\n",
       "label     94                                 95                               \\\n",
       "key2     idx      info act_rank      pred   idx      info act_rank      pred   \n",
       "tokens                                                                         \n",
       "0       89.0 -5.099403      0.0 -5.100125  84.0 -5.623547      0.0 -5.616222   \n",
       "1       28.0 -6.024241      2.0 -6.027439  37.0 -5.727442      1.0 -5.726045   \n",
       "2       80.0 -6.056923      3.0 -6.049633  28.0 -6.649372     48.0 -5.730793   \n",
       "3       85.0 -6.223698      5.0 -6.217341  48.0 -5.782372      2.0 -5.777117   \n",
       "4        5.0 -6.222270      4.0 -6.219057  69.0 -5.969609      8.0 -5.813753   \n",
       "...      ...       ...      ...       ...   ...       ...      ...       ...   \n",
       "95       1.0 -9.734209     93.0 -9.732396   4.0 -9.734209     95.0 -9.731427   \n",
       "96       7.0 -9.734209     94.0 -9.736037  51.0 -9.734209     96.0 -9.731923   \n",
       "97       2.0 -9.734209     92.0 -9.742655  12.0 -9.734209     97.0 -9.732054   \n",
       "98      51.0 -9.734209     96.0 -9.743469   2.0 -9.734209     92.0 -9.738361   \n",
       "99      12.0 -9.734209     97.0 -9.749361   7.0 -9.734209     94.0 -9.744190   \n",
       "\n",
       "label     96                                 97                               \\\n",
       "key2     idx      info act_rank      pred   idx      info act_rank      pred   \n",
       "tokens                                                                         \n",
       "0       68.0 -6.771860     69.0 -5.219338  61.0 -5.299616      0.0 -5.303560   \n",
       "1        0.0 -6.245051     45.0 -5.355571  34.0 -5.383586      2.0 -5.384628   \n",
       "2       98.0 -6.324862     52.0 -5.527510  31.0 -5.452182      3.0 -5.452777   \n",
       "3       61.0 -5.641105      1.0 -5.642994  60.0 -5.492447      4.0 -5.496153   \n",
       "4       70.0 -7.245023     81.0 -5.683380  45.0 -5.510035      5.0 -5.513776   \n",
       "...      ...       ...      ...       ...   ...       ...      ...       ...   \n",
       "95       7.0 -9.734209     94.0 -9.733614   8.0 -9.734209     99.0 -9.732234   \n",
       "96       3.0 -9.734209     98.0 -9.734844   1.0 -9.734209     93.0 -9.733034   \n",
       "97       8.0 -9.734209     99.0 -9.737084   4.0 -9.734209     95.0 -9.734362   \n",
       "98      88.0 -9.727593     89.0 -9.737380   3.0 -9.734209     98.0 -9.734757   \n",
       "99      12.0 -9.734209     97.0 -9.742350  12.0 -9.734209     97.0 -9.745341   \n",
       "\n",
       "label     98                                 99                               \n",
       "key2     idx      info act_rank      pred   idx      info act_rank      pred  \n",
       "tokens                                                                        \n",
       "0       36.0 -7.353806     80.0 -5.779118  86.0 -4.959163     26.0 -4.489558  \n",
       "1       48.0 -6.480022     44.0 -5.847100  54.0 -4.977380     27.0 -4.505052  \n",
       "2       99.0 -5.945722      0.0 -5.945576  35.0 -4.511155      0.0 -4.519406  \n",
       "3       97.0 -5.955873      1.0 -5.959344  28.0 -4.586379      1.0 -4.583322  \n",
       "4       52.0 -6.142846      3.0 -6.135499  53.0 -4.603284      2.0 -4.602691  \n",
       "...      ...       ...      ...       ...   ...       ...      ...       ...  \n",
       "95       8.0 -9.734209     99.0 -9.735483   8.0 -9.734209     99.0 -9.733828  \n",
       "96       1.0 -9.734209     93.0 -9.736398   1.0 -9.734209     93.0 -9.735018  \n",
       "97       2.0 -9.734209     92.0 -9.736816   3.0 -9.734209     98.0 -9.737608  \n",
       "98      51.0 -9.734209     96.0 -9.739737   2.0 -9.734209     92.0 -9.742069  \n",
       "99       3.0 -9.734209     98.0 -9.739832   7.0 -9.734209     94.0 -9.742805  \n",
       "\n",
       "[100 rows x 400 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_df = pd.DataFrame(sorted_toks.reshape(sorted_toks.shape[0], sorted_toks.shape[1]*4), \n",
    "                                  index=range(sorted_toks.shape[0]), columns=columns)\n",
    "_df.index.name = 'tokens'\n",
    "# pd.set_option('display.max_rows',None)\n",
    "_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e21336-b43f-41b9-9c9a-1e5293f08844",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| rmse: tensor(0.2114, device='cuda:0', dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "rmse = torch.sqrt(torch.mean(torch.square(sorted_tok_vals.to(torch.device(\"cuda:0\")) - torch.as_tensor(sorted_toks_bcx_info, device=default_device()))))\n",
    "ic(rmse);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa9b94a-5c65-4f01-9cb9-676f68d43647",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| r_sqr: tensor(0.9634, device='cuda:0', dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "ss_res = torch.sum(torch.square(sorted_tok_vals.to(torch.device(\"cuda:0\")) - torch.as_tensor(sorted_toks_bcx_info, device=default_device())))\n",
    "y_bar = np.mean(sorted_toks_bcx_info)\n",
    "ss_tot = torch.sum(torch.square(torch.as_tensor(y_bar, device=default_device()) - torch.as_tensor(sorted_toks_bcx_info, device=default_device())))\n",
    "r_sqr = 1 - ss_res/ss_tot\n",
    "ic(r_sqr);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95555428-7967-4d9d-9cf8-f18348eea3f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.718281828459045"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5b4815-8ef0-4ffe-a2ed-35062c980c71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>key2</th>\n",
       "      <th>idx</th>\n",
       "      <th>info</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>gain</th>\n",
       "      <th>discount</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokens</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20625.0</td>\n",
       "      <td>-6.853976</td>\n",
       "      <td>7735.0</td>\n",
       "      <td>-5.672819</td>\n",
       "      <td>49617.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20570.0</td>\n",
       "      <td>-6.853976</td>\n",
       "      <td>7732.0</td>\n",
       "      <td>-5.675634</td>\n",
       "      <td>49620.0</td>\n",
       "      <td>0.761463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20666.0</td>\n",
       "      <td>-6.853976</td>\n",
       "      <td>7740.0</td>\n",
       "      <td>-5.734933</td>\n",
       "      <td>49612.0</td>\n",
       "      <td>0.644561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20639.0</td>\n",
       "      <td>-6.853976</td>\n",
       "      <td>7738.0</td>\n",
       "      <td>-5.753483</td>\n",
       "      <td>49614.0</td>\n",
       "      <td>0.573504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20619.0</td>\n",
       "      <td>-6.853976</td>\n",
       "      <td>7734.0</td>\n",
       "      <td>-5.778982</td>\n",
       "      <td>49618.0</td>\n",
       "      <td>0.524981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57347</th>\n",
       "      <td>19758.0</td>\n",
       "      <td>-9.727593</td>\n",
       "      <td>57220.0</td>\n",
       "      <td>-8.628090</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.091266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57348</th>\n",
       "      <td>18966.0</td>\n",
       "      <td>-9.727593</td>\n",
       "      <td>57149.0</td>\n",
       "      <td>-8.680562</td>\n",
       "      <td>203.0</td>\n",
       "      <td>0.091266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57349</th>\n",
       "      <td>19035.0</td>\n",
       "      <td>-9.727593</td>\n",
       "      <td>57153.0</td>\n",
       "      <td>-8.726480</td>\n",
       "      <td>199.0</td>\n",
       "      <td>0.091266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57350</th>\n",
       "      <td>19075.0</td>\n",
       "      <td>-9.727593</td>\n",
       "      <td>57155.0</td>\n",
       "      <td>-8.748960</td>\n",
       "      <td>197.0</td>\n",
       "      <td>0.091266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57351</th>\n",
       "      <td>18857.0</td>\n",
       "      <td>-9.727593</td>\n",
       "      <td>57142.0</td>\n",
       "      <td>-8.804357</td>\n",
       "      <td>210.0</td>\n",
       "      <td>0.091266</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>57352 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "key2        idx      info  act_rank      pred     gain  discount\n",
       "tokens                                                          \n",
       "0       20625.0 -6.853976    7735.0 -5.672819  49617.0  1.000000\n",
       "1       20570.0 -6.853976    7732.0 -5.675634  49620.0  0.761463\n",
       "2       20666.0 -6.853976    7740.0 -5.734933  49612.0  0.644561\n",
       "3       20639.0 -6.853976    7738.0 -5.753483  49614.0  0.573504\n",
       "4       20619.0 -6.853976    7734.0 -5.778982  49618.0  0.524981\n",
       "...         ...       ...       ...       ...      ...       ...\n",
       "57347   19758.0 -9.727593   57220.0 -8.628090    132.0  0.091266\n",
       "57348   18966.0 -9.727593   57149.0 -8.680562    203.0  0.091266\n",
       "57349   19035.0 -9.727593   57153.0 -8.726480    199.0  0.091266\n",
       "57350   19075.0 -9.727593   57155.0 -8.748960    197.0  0.091266\n",
       "57351   18857.0 -9.727593   57142.0 -8.804357    210.0  0.091266\n",
       "\n",
       "[57352 rows x 6 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_0 = _df[0].copy()\n",
    "df_0.loc[:, ['gain']] = len(df_0) - df_0['act_rank']\n",
    "df_0.loc[:, ['discount']] = 1/np.log(math.e + df_0.index)\n",
    "\n",
    "# df_0.loc[:, 'gain'] = len(df_0) - df_0['act_rank']\n",
    "# df_0.loc[:, 'act_rank']\n",
    "df_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985ba329-8617-4b3d-afdb-8a0833e3294b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "172432851.6650227"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df_0['gain'] * df_0['discount']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0beb7bf2-510c-438a-a0ef-bee58ddb0bd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_0['info'].plot.kde();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffa5ba0-2ae4-4688-9f42-44b83ba88cd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_0['pred'].plot.kde();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f376b3b-e22e-4eb4-a571-db5e066d8fe4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9087e633-a4a0-4017-89c3-cd8bccbc3f44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba3a6c3-8699-4246-8b55-e274a271b577",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc4b646-fb55-4b65-b711-ef07f6b71031",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffad0e3-284a-4dfe-832d-cba205782664",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74e079a-b2b1-457f-85f6-4a8df729e310",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0002981424331665039"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_df.memory_usage().sum()/1024**3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397630ff-b124-4e69-8395-54c82640c67f",
   "metadata": {},
   "source": [
    "Fancy Indexing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8161cbb6-8ded-4d3e-8100-4e2b28a67c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.empty((8,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de54b7c-8cc8-42d1-89f6-420d6ff57ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(8):\n",
    "    arr[i] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b24dd7c-e6f5-481e-b3d2-f1fb63346427",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [2., 2., 2., 2.],\n",
       "       [3., 3., 3., 3.],\n",
       "       [4., 4., 4., 4.],\n",
       "       [5., 5., 5., 5.],\n",
       "       [6., 6., 6., 6.],\n",
       "       [7., 7., 7., 7.]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ad56a4-7f1e-4184-b559-b2533d99ff0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4., 4., 4., 4.],\n",
       "       [3., 3., 3., 3.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [6., 6., 6., 6.]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr[[4, 3, 0, 6]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec706fc-075d-44f9-9270-a326161c84ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5., 5., 5., 5.],\n",
       "       [3., 3., 3., 3.],\n",
       "       [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr[[-3, -5, -7]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded9276e-46ce-4efb-8742-205376e24c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.arange(32).reshape((8,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e25be4-b5ab-4ef1-8ab0-8ab400262802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  2,  3],\n",
       "       [ 4,  5,  6,  7],\n",
       "       [ 8,  9, 10, 11],\n",
       "       [12, 13, 14, 15],\n",
       "       [16, 17, 18, 19],\n",
       "       [20, 21, 22, 23],\n",
       "       [24, 25, 26, 27],\n",
       "       [28, 29, 30, 31]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f486b957-1100-4e3f-ad92-253416962ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(arr[[1, 5, 7, 2], [0, 3, 1, 2]] , [arr[o] for o in list(zip([1, 5, 7, 2], [0, 3, 1, 2]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f72a6d-612e-4aa6-8bfd-2e7712c79795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4,  5,  6,  7],\n",
       "       [20, 21, 22, 23],\n",
       "       [28, 29, 30, 31],\n",
       "       [ 8,  9, 10, 11]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr[[1, 5, 7, 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5472ca6-3d96-458e-b0de-3207db35c547",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4,  7,  5,  6],\n",
       "       [20, 23, 21, 22],\n",
       "       [28, 31, 29, 30],\n",
       "       [ 8, 11,  9, 10]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr[[1, 5, 7, 2]][:, [0, 3, 1, 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f541d89-4bde-400a-8edb-07f8eafa1ed8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 3, 1, 2], [2, 1, 3, 0], [0, 1, 2, 3], [3, 1, 2, 0]]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind = [[0,3,1,2], [2,1,3,0], [0,1,2,3], [3,1,2,0]]\n",
    "ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b63089-93dd-4af3-bb0f-108a262e25cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 4,  7,  5,  6],\n",
       "        [ 6,  5,  7,  4],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 7,  5,  6,  4]],\n",
       "\n",
       "       [[20, 23, 21, 22],\n",
       "        [22, 21, 23, 20],\n",
       "        [20, 21, 22, 23],\n",
       "        [23, 21, 22, 20]],\n",
       "\n",
       "       [[28, 31, 29, 30],\n",
       "        [30, 29, 31, 28],\n",
       "        [28, 29, 30, 31],\n",
       "        [31, 29, 30, 28]],\n",
       "\n",
       "       [[ 8, 11,  9, 10],\n",
       "        [10,  9, 11,  8],\n",
       "        [ 8,  9, 10, 11],\n",
       "        [11,  9, 10,  8]]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr[[1, 5, 7, 2]][ :, ind ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8b61ff-5ae6-4248-a77d-1bdc0c66e968",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4,  5,  6,  7],\n",
       "       [ 8,  9, 10, 11],\n",
       "       [20, 21, 22, 23],\n",
       "       [28, 29, 30, 31]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr[[1, 5, 7, 2]][[0, 3, 1, 2], :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c584ac9b-037c-4653-bfae-330235367156",
   "metadata": {},
   "source": [
    "Rearrange columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29448c55-8331-4d1d-9f47-b65e891b1ff7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10, 20, 30, 40, 50],\n",
       "       [ 6,  7,  8,  9, 10]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = array([[10, 20, 30, 40, 50],\n",
    "       [ 6,  7,  8,  9, 10]])\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404e9505-d079-436b-867b-1bde9611b3c3",
   "metadata": {},
   "source": [
    "change it to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8add3ada-f0ac-4291-a30f-be01b7b79f1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10, 30, 50, 40, 20],\n",
       "       [ 6,  8, 10,  9,  7]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array([[10, 30, 50, 40, 20],\n",
    "       [ 6,  8, 10,  9,  7]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8736fe-85f0-46f1-a194-2508208b824a",
   "metadata": {},
   "source": [
    "by applying the permutation\n",
    "\n",
    "0 -> 0 \n",
    "\n",
    "1 -> 4\n",
    "\n",
    "2 -> 1\n",
    "\n",
    "3 -> 3\n",
    "\n",
    "4 -> 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587451de-7e64-4b9a-ac64-b556ccb79b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "permutation = [0,4,1,3,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b70f50-4986-426e-8557-79302a3b54bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.empty_like(permutation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df640d56-be91-4b9c-9c3a-b5de1d3146c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 4, 1, 3, 2])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5d33b3-b6d1-42bf-afb8-fd7bc30f087f",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx[permutation] = np.arange(len(permutation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf5fb79-7e12-4734-ba0b-a2006947c658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 4, 3, 1])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82527563-4de5-4365-9155-85e0e01be0c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10, 20, 30, 40, 50],\n",
       "       [ 6,  7,  8,  9, 10]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e937fa54-43c6-44d4-b689-34dc97f00f67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10, 30, 50, 40, 20],\n",
       "       [ 6,  8, 10,  9,  7]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:,idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04490561-bd09-442f-b676-d6bf6c7fa888",
   "metadata": {},
   "outputs": [],
   "source": [
    "a[:] = a[:,idx] # in-place modifcation of a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d27050e-15c6-4142-a3c1-288e26a3ddab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10, 30, 50, 40, 20],\n",
       "       [ 6,  8, 10,  9,  7]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
