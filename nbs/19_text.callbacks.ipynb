{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075fc076-3208-45e3-a222-0f7163ea7183",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "! [ -e /content ] && pip install -Uqq xcube  # upgrade xcube on colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f55fa0-dbb1-446d-a10a-c9e69f688478",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from fastai.torch_imports import *\n",
    "from fastai.torch_core import *\n",
    "from fastai.callback.core import *\n",
    "from fastcore.all import *\n",
    "from xcube.imports import *\n",
    "from xcube.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267ed315-de11-42af-83da-b3a2f1898dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f49aba8-c46c-4033-b213-9f80399a4a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp text.callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17508b9f-314e-43de-ab1c-ee6a53e52bb6",
   "metadata": {},
   "source": [
    "# XML Callbacks\n",
    "\n",
    "> General purpose callbacks needed for XML TextLearner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a038af95-e700-4461-814a-76867e3b9e25",
   "metadata": {},
   "source": [
    "When the target is 1 we want the input to be close to 1 to incur low loss. So when the target is 1 we need to find out how much we need to boost the pred such that its sigmoid is close to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e83dc0-ea28-4c78-b8c9-b0d92920acc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class LabelForcing(Callback):\n",
    "    def __init__(self, end_epoch):\n",
    "        self.end_epoch = end_epoch\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def after_pred(self):\n",
    "        if self.training and self.epoch <= self.end_epoch:\n",
    "            pred, attn, wgts = self.learn.pred\n",
    "            pos = Tensor(self.y) == 1\n",
    "            # pred[pos] += 3*pred.std()\n",
    "            # pred[~pos] -= 3*pred.std()\n",
    "            attn[pos] += attn[pos].std(dim=-1).unsqueeze(-1)\n",
    "            attn[~pos] -= attn[~pos].std(dim=-1).unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31ebc9c-6366-4f3e-b9a9-5b7bb8b5e683",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "# xb, yb = dls_clas.one_batch()\n",
    "# pred, attn, wgts = learn.model.to(0)(xb)\n",
    "# L(pred, yb, attn, wgts).map(Self.shape())\n",
    "# pos = Tensor(yb) == 1\n",
    "# before_pos, before_neg = attn[pos].sum(-1).mean(), attn[~pos].sum(-1).mean()\n",
    "# after_pos, after_neg = attn[pos].sum(-1).mean(), attn[~pos].sum(-1).mean()\n",
    "# assert before_pos < after_pos and before_neg > after_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965885e7-b721-443d-82f7-0f01fb912c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "lf = LabelForcing(1) \n",
    "# cbs = L( [cbs]+ [lf])\n",
    "# cbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a5610d-f589-489a-8518-eba2ec00c703",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from fastai.callback.progress import *\n",
    "from fastai.learner import Recorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec71c32-2f00-4d52-8853-c4cde4b8e15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class _FakeLearner: \n",
    "    def to_detach(self,b,cpu=True,gather=True):\n",
    "        return to_detach(b,cpu,gather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db51fe24-202b-4855-8acb-ddb9e8e0524a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class RunningCallback(Callback):\n",
    "    order=ProgressCallback.order-1\n",
    "    \n",
    "    def __init__(self, mets):\n",
    "        self.mets = mets\n",
    "        \n",
    "    def before_train(self): \n",
    "        self.val_cyclit = iter(self.dls.valid) #itertools.cycle(self.dls.valid) \n",
    "        self._fake_l = _FakeLearner()\n",
    "        self.mets.map(Self.reset())\n",
    "        self.i = 0\n",
    "        \n",
    "    def _batch_gen():\n",
    "        \n",
    "    \n",
    "    def before_batch(self):\n",
    "        self._btch = next(self.val_cyclit)\n",
    "    \n",
    "    def after_batch(self):\n",
    "        if not self.training: return\n",
    "        self.model.eval()\n",
    "        self.learn.training=False\n",
    "        xb, yb = self._btch\n",
    "        print(self.i, end=' ')\n",
    "        self.i += 1k\n",
    "        self._fake_l.yb = (yb,)\n",
    "        self._fake_l.pred, *_ = self.model(xb) \n",
    "        self._fake_l.loss = Tensor(self.loss_func(self._fake_l.pred, yb))\n",
    "        for met in self.mets: met.accumulate(self._fake_l)\n",
    "        self.model.train()\n",
    "        self.learn.training=True\n",
    "    \n",
    "    def after_train(self):\n",
    "        pdb.set_trace()\n",
    "        if hasattr(self, 'val_cyclit'): delattr(self, 'val_cyclit')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c8e171-170b-41c1-8291-99b6455d4604",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b23de4b-4655-42fa-ab8b-f2d85ae15fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepk",
   "language": "python",
   "name": "deepk"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
