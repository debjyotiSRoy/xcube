from fastcore.script import *
from fastai.distributed import *
from fastai.data.load import DataLoader
from fastai.data.core import DataLoaders
from fastai.optimizer import *
from fastai.callback.schedule import valley, slide, steep
from fastprogress import fastprogress
from xcube.l2r.all import *

torch.backends.cudnn.benchmark = True
fastprogress.MAX_COLS = 80
def pr(s):
    if rank_distrib()==0: print(s)

@patch
def after_batch(self: ProgressBarCallback):
        self.pbar.update(self.iter_num+1)
        mets = ('_valid_mets', '_train_mets')[self.training]
        if getattr(self.track_results, 'losses'): 
            try:
                # self.pbar.comment = ', '.join([f'{name} = {val: .4f}' for name, val in L(self.track_results.names).zipwith(L(getattrs(self.track_results, 'losses', 'ndcgs', 'ndcgs_at_6', 'accs')).itemgot(-1).map(Self.item()))]) + f', smooth_moi = {self.track_results.smooth_moi: .4f}'
                self.pbar.comment = ', '.join([f'{name} = {val: .4f}' for name, val in L(self.track_results.names).zipwith(L(getattrs(self.track_results, 'losses', 'ndcgs', 'ndcgs_at_6', 'accs')).itemgot(-1).map(Self.item()))]) + f", lr={self.opt.hypers[0]['lr']:.6f}"
            except TypeError as e: pass


generated_files = [] # to keep track of files generated by proprocessing

def grab_learner_params(model_algo, fname, monitor, train_metrics=False):
    "Get relevant `learner` params depending on the `fname`"
    
    nn, lambrank, tiny =  [sp == n for sp, n in zip(model_algo.split('_'), ['nn', 'lambdarank', 'tiny'])]
    # create a dictionary that maps binary conditions to tuple (nn, lambdarank, tiny)
    conditions = {
        (True, True, True):  dict(lr = 1e-3, lambrank = lambrank, opt_func = partial(RMSProp, mom=0.9, wd=0.0)),   # nn_lambdarank_tiny
        (True, True, False): dict(lr = 1e-2, lambrank = lambrank, opt_func = partial(Adam, mom=0.9, wd=0.1)),   # nn_lambdarank_full
        (True, False, True):  dict(lr = 1e-2, lambrank = lambrank, opt_func = partial(RMSProp, mom=0.9, wd=0.2)),  # nn_ranknet_tiny
        (True, False, False): dict(lr = None, lambrank = lambrank, opt_func = None),  # nn_ranknet_full
        (False, True, True): dict(lr = 7e-3, lambrank = lambrank, opt_func = partial(RMSProp, mom=0.9, wd=0.0)),   # lin_lambdarank_tiny
        (False, True, False): dict(lr = 7e-3, lambrank = lambrank, opt_func = partial(Adam, mom=0.9, wd=0.1)),  # lin_lambdarank_full
        (False, False, True): dict(lr = 1e-4, lambrank = lambrank, opt_func = None),  # lin_ranknet_tiny
        (False, False, False): dict(lr = None, lambrank = lambrank, opt_func = None), # lin_ranknet_full
    }
    learner_params = conditions.get((nn, lambrank, tiny), (True, True, True))
    default_cbs = [TrainEval(), TrackResults(train_metrics=train_metrics, beta=0.98), ProgressBarCallback(), Monitor(), SaveCallBack(fname, monitor=monitor), MixedPrecision(), GradientClip()]
    grad_fn = partial(rank_loss3, gain_fn='exp', k=15)
    learner_params = {**learner_params, **{'cbs':default_cbs, 'grad_fn':grad_fn}}
    return learner_params

def get_dls(source, data, bs, lbs_chunks, sl, save_dir, workers=None,):
    workers = ifnone(workers, min(8, num_cpus()))
    data = join_path_file(data, source, ext='.ft')
    df_l2r = pd.read_feather(data)
    col_drops = [c for c in df_l2r.columns if 'mutual_info' in c]
    # df_l2r = df_l2r.drop(['bcx_mutual_info'], axis=1)
    df_l2r = df_l2r.drop(col_drops, axis=1)
    pdl = PreLoadTrans(df_l2r, device=torch.device('cpu'))
    scored_toks = pdl.quantized_score()
    # torch.save(scored_toks, save_dir/'scored_tokens.pth'); generated_files.append(save_dir/'scored_tokens.pth')
    test_eqs(scored_toks.shape, 
        (df_l2r.label.nunique(), df_l2r.token.nunique(), 4), 
        (pdl.num_lbs, pdl.num_toks, 4))
    scored_toks, binned_toks, probs, is_valid, bin_size, bin_bds = pdl.train_val_split()
    val_sl = pdl.pad_split()
    test_eq(is_valid.sum(dim=-1).unique().item(), val_sl)
    print(f"{val_sl=}")
    top_lens = pdl.count_topbins()
    test_eq(top_lens.shape, [pdl.num_lbs])
    print(f"For {torch.where(top_lens >= 1)[0].numel()} labels out of total {pdl.num_lbs}, in the validation set we have at least one top 2 bin")
    
    # prepare train/val datasets
    trn_dset, val_dset = pdl.datasets()
    test_eq(val_dset.shape, (scored_toks.shape[0], val_sl, scored_toks.shape[2]))
    test_eq(trn_dset.shape, scored_toks.shape) 
    # torch.save((trn_dset, val_dset), save_dir/'trn_val_split.pkl'); generated_files.append(save_dir/'trn_val_split.pkl')

    # ready to create train/val dataloaders
    trn_dl = L2RDataLoader(dataset=trn_dset, sl=sl, bs=bs, lbs_chunks=lbs_chunks, shuffle=False, after_batch=partial(to_device, device=default_device(use=True)), num_workers=0)
    val_dset = val_dset.unsqueeze(0)
    val_dl = DataLoader(val_dset, bs=1, shuffle=False, after_batch=partial(to_device, device=default_device()), num_workers=0)
    dls = DataLoaders(trn_dl, val_dl)
    return dls

@call_parse
def main(
    source_url: Param("Source url", str)="XURLs.MIMIC4_L2R",
    data: Param("Bootstrapped data", str)="",
    root_dir: Param("Root dir for saving models", str)="..",
    workers:   Param("Number of workers", int)=None,
    force_build_dls: Param("Build dataloaders even if it exists in `root_dir/tmp`", store_true)=False,  
    epochs:Param("Number of epochs", int)=10,
    bs: Param("Batch size", int)=32,
    lr: Param("base Learning rate", float)=1e-2,
    lbs_chunks:    Param("Batch size", int)=4,
    sl:    Param("Batch size", int)=64,
    model_algo: Param("Model (lin/nn) Algo (lambdarank/ranknet)", str)="lin_lambdarank",
    fname: Param("Save model file", str)="xxx",
    track_train: Param("Record training metrics", store_true)=False,
    emb_sz: Param("Embedding size of the L2R model", int)=200,
    log: Param("Log loss and metrics after each epoch", store_true)=False,
    trn_frm_cpt: Param("Train from saved checkpoint", store_true)=False,
    infer: Param("Don't train, just validate", int)=0,
):
    "Training a learning-to-rank model."

    source = rank0_first(untar_xxx, eval(source_url))
    # make tmp directory to save and load models and dataloaders
    tmp = Path(root_dir)/'tmp/models'
    tmp.mkdir(exist_ok=True, parents=True)
    tmp = tmp.parent

    # loading dataloaders
    dls_file = join_path_file(data+'_dls_clas_'+str(bs), tmp, ext='.pkl')
    if force_build_dls or not dls_file.exists():
        dls_l2r = get_dls(source, data, bs, lbs_chunks, sl, save_dir=tmp, workers=workers)
        torch.save(dls_l2r, dls_file); generated_files.append(dls_file)
    elif dls_file.exists(): 
        dls_l2r = torch.load(dls_file, map_location=torch.device('cpu'))

    # import IPython; IPython.embed()
    # dummy run of the train and valid dataloader <remove later>

    print(f"length of training dataloader = {len(dls_l2r.train)}")
    # for _ in range(2):
    #     for xb in progress_bar(dls_l2r.train): time.sleep(0.01)
    # for _ in range(2):
    #     for xb in progress_bar(dls_l2r.valid): time.sleep(0.01)
    # end <remove later>
    # import IPython; IPython.embed()

    monitor = 'ndcg_at_6' if 'lambda' in model_algo else 'acc'
    s = model_algo.split('_')
    print(f'We will run a {s[0]} model using the {s[1]} algorithm. And our metric of interest(moi) is {monitor}.')

    # Make the model
    Datasizes = namedtuple("Datasizes", ('num_lbs', 'num_toks', 'num_factors'))
    sizes = Datasizes(*dls_l2r.dataset.shape[:-1], emb_sz) # or pdl.num_lbs, pdl.num_toks, 200
    ic(sizes)
    print("Creating Model...")
    model = (L2R_NN(*sizes, layers=[100], embed_p=0.2, ps=[0.1], bn_final=False, y_range=None) if 'nn' in model_algo else L2R_DotProductBias(*sizes,y_range=None)).to(default_device())

    # Create Learner
    learner_params = grab_learner_params(model_algo+'_full', fname=fname+'_'+model_algo, monitor=monitor, train_metrics=track_train)
    if not infer and log: 
        logfname = join_path_file(fname+'_'+model_algo, tmp, ext='.csv')
        if not trn_frm_cpt and logfname.exists(): logfname.unlink()
        learner_params['cbs'] += L(CSVLog(fname=logfname, append=True))
    print("Creating Learner...")
    learner = get_learner(model, dls_l2r, **learner_params, path=tmp)
    ic(learner_params)

    # Find learning rate
    # print("Computing learning rate..")
    # track_cb_idx = learner.cbs.argfirst(lambda o: isinstance(o, TrackResults))
    # cms = L(learner.removed_cbs(learner.cbs[track_cb_idx]), learner.added_cbs(TrackResults(train_metrics=True, beta=0.98)))
    # with ContextManagers(cms): 
    #     lrs = learner.xrl_find(num_it=300, suggest_funcs=(valley, slide, steep))
    # print(f"learning rate = {lrs.valley}")
    
    # Inference
    if infer:
        try:
            learner = learner.load(learner.save_call_back.fname, device=default_device())
            learner.validate()
            dataset = to_device(learner.dls.train.dataset)
            _ndcg_at_k = ndcg_at_k(dataset, learner.model, k=15)
            ic(_ndcg_at_k.min(), _ndcg_at_k.mean(), _ndcg_at_k.max(), _ndcg_at_k.median(), _ndcg_at_k.std())
            acc = accuracy(dataset, learner.model)
            ic(acc.min(), acc.mean(), acc.max(), acc.median(), acc.std())
        except FileNotFoundError as e:
            print("Exception:", e)
            print("Trained model not found!")
        finally: exit()
    
    # Train from Checkpoint
    if trn_frm_cpt:
        try:
            ic(learner.save_call_back.fname)
            # ic(learn.save_model.reset_on_fit)
            assert learner.save_call_back.reset_on_fit is False
            learner = learner.load(learner.save_call_back.fname, device= default_device())
            print("Validating the checkpointed model so that we can run from where we left of...")
            learner.validate()
            print(f"We are monitoring {learner.save_call_back.monitor}. Set the best so far = {learner.track_results.ndcgs_at_6[-1]}")
            learner.save_call_back.best = learner.track_results.ndcgs_at_6[-1].item()
        except FileNotFoundError as e: 
            print("Exception:", e)
            print("Checkpoint model not found!")

    # Training 
    print("Time to train!")
    set_seed(1, reproducible=True)
    epochs_so_far = 0
    while epochs_so_far < epochs:
        try:
            # learner.fit_one_cycle(epochs, lr_max=0.07, start_epoch=epochs_so_far, )
            print(f"Training with {lr=}.")
            # learner.fit_one_cycle(epochs, lr_max=lr, start_epoch=epochs_so_far, )
            # learner.fit(epochs, lr=lr, wd=0.1, start_epoch=epochs_so_far, )

            # learner.fit_sgdr(4, 1, lr_max=0.1, start_epoch=epochs_so_far) # 1 lr=0.1
            # learner.fit_one_cycle(epochs, lr_max=lr, start_epoch=epochs_so_far, ) #2 lr=7e-2
            # learner.fit(epochs, lr=lr, wd=0.1, start_epoch=epochs_so_far, ) #lr=7e-3
            # learner.fit(epochs, lr=lr, wd=0.1, start_epoch=epochs_so_far, ) #lr=7e-3
            # learner.fit(epochs, lr=lr, wd=0.1, start_epoch=epochs_so_far, ) #lr=7e-4
            learner.fit(epochs, lr=lr, wd=0.1, start_epoch=epochs_so_far, ) #lr=7e-5

            # epochs_so_far = learner.epoch+1
            break
        except torch.cuda.OutOfMemoryError as e:
            print("Memory Error!!! Saving the Crashed Model!")
            learner.save('crashed')
            best_so_far = learner.save_call_back.best
            epochs_so_far = learner.epoch
            learner.model = None
            torch.cuda.empty_cache()
            print("Memory Error Occurred, Loading Crashed Model and Resume Training...")
            learner = get_learner(model, dls_l2r, **learner_params, path=tmp)
            # learner = learner.load(learner.save_call_back.fname, device= default_device())
            learner = learner.load('crashed', device= default_device())
            print(f"We are monitoring {learner.save_call_back.monitor}. Set the best so far = {best_so_far}")
            learner.save_call_back.best = best_so_far
            continue
    
    print("Generated Files:")
    for file_path in generated_files: print(file_path)