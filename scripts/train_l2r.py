from fastcore.script import *
from fastai.distributed import *
from fastai.data.load import DataLoader
from fastai.data.core import DataLoaders
from fastai.optimizer import *
from fastai.callback.schedule import valley, slide, steep
from fastprogress import fastprogress
from xcube.l2r.all import *

torch.backends.cudnn.benchmark = True
fastprogress.MAX_COLS = 80

generated_files = [] # to keep track of files generated by proprocessing

def grab_learner_params(model_algo, fname, monitor, train_metrics=False):
    "Get relevant `learner` params depending on the `fname`"
    
    nn, lambrank, tiny =  [sp == n for sp, n in zip(model_algo.split('_'), ['nn', 'lambdarank', 'tiny'])]
    # create a dictionary that maps binary conditions to tuple (nn, lambdarank, tiny)
    conditions = {
        (True, True, True):  dict(lr = 1e-3, lambrank = lambrank, opt_func = partial(RMSProp, mom=0.9, wd=0.0)),   # nn_lambdarank_tiny
        (True, True, False): dict(lr = 1e-2, lambrank = lambrank, opt_func = partial(Adam, mom=0.9, wd=0.1)),   # nn_lambdarank_full
        (True, False, True):  dict(lr = 1e-2, lambrank = lambrank, opt_func = partial(RMSProp, mom=0.9, wd=0.2)),  # nn_ranknet_tiny
        (True, False, False): dict(lr = None, lambrank = lambrank, opt_func = None),  # nn_ranknet_full
        (False, True, True): dict(lr = 7e-3, lambrank = lambrank, opt_func = partial(RMSProp, mom=0.9, wd=0.0)),   # lin_lambdarank_tiny
        (False, True, False): dict(lr = 7e-3, lambrank = lambrank, opt_func = partial(RMSProp, mom=0.9, wd=0.0)),  # lin_lambdarank_full
        (False, False, True): dict(lr = 1e-4, lambrank = lambrank, opt_func = None),  # lin_ranknet_tiny
        (False, False, False): dict(lr = None, lambrank = lambrank, opt_func = None), # lin_ranknet_full
    }
    learner_params = conditions.get((nn, lambrank, tiny), (True, True, True))
    default_cbs = [TrainEval(), TrackResults(train_metrics=train_metrics, beta=0.98), ProgressBarCallback(), Monitor(), SaveCallBack(fname, monitor=monitor)]
    grad_fn = partial(rank_loss3, gain_fn='exp', k=15)
    learner_params = {**learner_params, **{'cbs':default_cbs, 'grad_fn':grad_fn}}
    return learner_params

def get_dls(source, data, bs, lbs_chunks, sl, save_dir, workers=None,):
    workers = ifnone(workers, min(8, num_cpus()))
    data = join_path_file(data, source, ext='.ft')
    df_l2r = pd.read_feather(data)
    df_l2r = df_l2r.drop(['bcx_mutual_info'], axis=1)
    pdl = PreLoadTrans(df_l2r, device=torch.device('cpu'))
    scored_toks = pdl.quantized_score()
    torch.save(scored_toks, save_dir/'scored_tokens.pth'); generated_files.append(save_dir/'scored_tokens.pth')
    test_eqs(scored_toks.shape, 
        (df_l2r.label.nunique(), df_l2r.token.nunique(), 4), 
        (pdl.num_lbs, pdl.num_toks, 4))
    scored_toks, binned_toks, probs, is_valid, bin_size, bin_bds = pdl.train_val_split()
    val_sl = pdl.pad_split()
    test_eq(is_valid.sum(dim=-1).unique().item(), val_sl)
    print(f"{val_sl=}")
    top_lens = pdl.count_topbins()
    test_eq(top_lens.shape, [pdl.num_lbs])
    print(f"For {torch.where(top_lens >= 1)[0].numel()} labels out of total {pdl.num_lbs}, in the validation set we have at least one top 2 bin")
    
    # prepare train/val datasets
    trn_dset, val_dset = pdl.datasets()
    test_eq(val_dset.shape, (scored_toks.shape[0], val_sl, scored_toks.shape[2]))
    test_eq(trn_dset.shape, scored_toks.shape) 
    torch.save((trn_dset, val_dset), save_dir/'trn_val_split.pkl'); generated_files.append(save_dir/'trn_val_split.pkl')

    # ready to create train/val dataloaders
    trn_dl = L2RDataLoader(dataset=trn_dset, sl=sl, bs=bs, lbs_chunks=lbs_chunks, shuffle=False, after_batch=partial(to_device, device=default_device(use=True)), num_workers=0)
    val_dset = val_dset.unsqueeze(0)
    val_dl = DataLoader(val_dset, bs=1, shuffle=False, after_batch=partial(to_device, device=default_device()), num_workers=0)
    dls = DataLoaders(trn_dl, val_dl)
    return dls

@call_parse
def main(
    source_url: Param("Source url", str)="XURLs.MIMIC4_L2R",
    data: Param("Bootstrapped data", str)="",
    root_dir: Param("Root dir for saving models", str)="..",
    workers:   Param("Number of workers", int)=None,
    force_build_dls: Param("Build dataloaders even if it exists in `root_dir/tmp`", store_true)=False,  
    bs:    Param("Batch size", int)=32,
    lbs_chunks:    Param("Batch size", int)=4,
    sl:    Param("Batch size", int)=64,
    model_algo: Param("Model (lin/nn) Algo (lambdarank/ranknet)", str)="lin_lambdarank",
    fname: Param("Save model file", str)="xxx",
    track_train: Param("Record training metrics", store_true)=False,

):
    "Training a learning-to-rank model."

    source = rank0_first(untar_xxx, eval(source_url))
    # make tmp directory to save and load models and dataloaders
    tmp = Path(root_dir)/'tmp/models'
    tmp.mkdir(exist_ok=True, parents=True)
    tmp = tmp.parent

    # loading dataloaders
    dls_file = join_path_file(data+'_dls_clas_'+str(bs), tmp, ext='.pkl')
    if force_build_dls or not dls_file.exists():
        dls_l2r = get_dls(source, data, bs, lbs_chunks, sl, save_dir=tmp, workers=workers)
        torch.save(dls_l2r, dls_file); generated_files.append(dls_file)
    elif dls_file.exists(): 
        dls_l2r = torch.load(dls_file, map_location=torch.device('cpu'))

    # dummy run of the train and valid dataloader <remove later>
    for _ in range(2):
        for xb in progress_bar(dls_l2r.train): time.sleep(0.01)
    for _ in range(2):
        for xb in progress_bar(dls_l2r.valid): time.sleep(0.01)
    # end <remove later>

    monitor = 'ndcg_at_6' if 'lambda' in model_algo else 'acc'
    s = model_algo.split('_')
    print(f'We will run a {s[0]} model using the {s[1]} algorithm. And our metric of interest(moi) is {monitor}.')

    # Make the model
    Datasizes = namedtuple("Datasizes", ('num_lbs', 'num_toks', 'num_factors'))
    sizes = Datasizes(*dls_l2r.dataset.shape[:-1], 200) # or pdl.num_lbs, pdl.num_toks, 200
    ic(sizes)
    model = (L2R_NN(*sizes, layers=[100], embed_p=0.2, ps=[0.1], bn_final=False, y_range=None) if 'nn' in model_algo else L2R_DotProductBias(*sizes,y_range=None)).to(default_device())

    # Create Learner
    learner_params = grab_learner_params(model_algo+'_tiny', fname=fname+'_'+model_algo, monitor=monitor, train_metrics=track_train)
    ic(learner_params)
    learner = get_learner(model, dls_l2r, **learner_params, path=tmp)

    # Find learning rate
    track_cb_idx = learner.cbs.argfirst(lambda o: isinstance(o, TrackResults))
    cms = L(learner.removed_cbs(learner.cbs[track_cb_idx]), learner.added_cbs(TrackResults(train_metrics=True, beta=0.98)))
    with ContextManagers(cms): 
        lrs = learner.xrl_find(num_it=300, suggest_funcs=(valley, slide, steep))

    
    # Training 
    learner.fit_one_cycle(1, lr_max=lrs.valley)

    import IPython; IPython.embed()
    print("Generated Files:")
    for file_path in generated_files: print(file_path)