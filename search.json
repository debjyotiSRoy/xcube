[
  {
    "objectID": "text.callbacks.html",
    "href": "text.callbacks.html",
    "title": "XML Callbacks",
    "section": "",
    "text": "When the target is 1 we want the input to be close to 1 to incur low loss. So when the target is 1 we need to find out how much we need to boost the pred such that its sigmoid is close to 1.\n\nsource\n\nLabelForcing\n\n LabelForcing (end_epoch)\n\nBasic class handling tweaks of the training loop by changing a Learner in various events\n\nsource\n\n\nRunvalCallback\n\n RunvalCallback (mets)\n\nBasic class handling tweaks of the training loop by changing a Learner in various events"
  },
  {
    "objectID": "data.external.html",
    "href": "data.external.html",
    "title": "Downloading…",
    "section": "",
    "text": "This module is the xcube downloading counterpart of fastai’s External data. Specifically, untar_data is repleaced with untar_xxx.\nTo download any of the datasets or pretrained weights, simply run untar_xxx by passing any dataset name mentioned above like so:\npath = untar_xxx(XURLs.MIMIC3_L2R)\npath.ls()\n\n&gt;&gt; (#1) [Path('/home/deb/.xcube/data/mimic3/l2r')]\nTo download model pretrained weights:\npath = untar_xxx(XURLs.)\npath.ls()\n\n&gt;&gt; (#2) []\n\nsource\n\nxcube_cfg\n\n xcube_cfg ()\n\nConfig object for xcube’s config.ini\nThis is a basic Config file that consists of data, model, storage and archive. All future downloads occur at the paths defined in the config file based on the type of download. For example, all future xcube datasets are downloaded to the data while all pretrained model weights are download to model unless the default download location is updated.\n\ncfg = xcube_cfg()\ncfg.data, cfg.path('archive')\n\n('data', Path('/home/deb/.xcube/archive'))\n\n\n\nsource\n\n\nxcube_path\n\n xcube_path (folder:str)\n\nLocal path to folder in Config\n\nxcube_path('archive')\n\nPath('/home/deb/.xcube/archive')\n\n\n\nsource\n\n\nXURLs\n\n XURLs ()\n\nGlobal cosntants for datasets and model URLs.\nThe default local path is at ~/.xcube/archive/ but this can be updated by passing a different c_key. Note: c_key should be one of 'archive', 'data', 'model', 'storage'.\n\nurl = XURLs.MIMIC3_L2R\nlocal_path = XURLs.path(url)\ntest_eq(local_path.parent, xcube_path('archive'))\nlocal_path\n\nPath('/home/deb/.xcube/archive/mimic3_l2r.tgz')\n\n\n\nlocal_path = XURLs.path(url, c_key='model')\ntest_eq(local_path.parent, xcube_path('model'))\nlocal_path\n\nPath('/home/deb/.xcube/models/mimic3_l2r.tgz')\n\n\n\nsource\n\n\nuntar_xxx\n\n untar_xxx (url:str, archive:pathlib.Path=None, data:pathlib.Path=None,\n            c_key:str='data', force_download:bool=False,\n            base:str='~/.xcube')\n\nDownload url using FastDownload.get\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nurl\nstr\n\nFile to download\n\n\narchive\nPath\nNone\nOptional override for Config’s archive key\n\n\ndata\nPath\nNone\nOptional override for Config’s data key\n\n\nc_key\nstr\ndata\nKey in Config where to extract file\n\n\nforce_download\nbool\nFalse\nSetting to True will overwrite any existing copy of data\n\n\nbase\nstr\n~/.xcube\nDirectory containing config file and base of relative paths\n\n\nReturns\nPath\n\nPath to extracted file(s)\n\n\n\nuntar_xxx is a thin wrapper for FastDownload.get. It downloads and extracts url, by default to subdirectories of ~/.xcube, and returns the path to the extracted data. Setting the force_download flag to ‘True’ will overwrite any existing copy of the data already present. For an explanation of the c_key parameter, see XURLs.\n\np = untar_xxx(XURLs.MIMIC3_L2R)\np\n\nPath('/home/deb/.xcube/data/mimic3_l2r')\n\n\n\nlist(p.glob('**/*.csv'))\n\n[Path('/home/deb/.xcube/data/mimic3_l2r/code_descriptions.csv'),\n Path('/home/deb/.xcube/data/mimic3_l2r/mimic3-9k.csv')]"
  },
  {
    "objectID": "l2r.callbacks.html",
    "href": "l2r.callbacks.html",
    "title": "L2R Callbacks",
    "section": "",
    "text": "source\n\n\n\n TrainEval (after_create=None, before_fit=None, before_epoch=None,\n            before_train=None, before_batch=None, after_pred=None,\n            after_loss=None, before_backward=None,\n            after_cancel_backward=None, after_backward=None,\n            before_step=None, after_cancel_step=None, after_step=None,\n            after_cancel_batch=None, after_batch=None,\n            after_cancel_train=None, after_train=None,\n            before_validate=None, after_cancel_validate=None,\n            after_validate=None, after_cancel_epoch=None,\n            after_epoch=None, after_cancel_fit=None, after_fit=None)\n\nCallback that tracks the number of iterations done and properly sets training/eval mode"
  },
  {
    "objectID": "l2r.callbacks.html#essential",
    "href": "l2r.callbacks.html#essential",
    "title": "L2R Callbacks",
    "section": "",
    "text": "source\n\n\n\n TrainEval (after_create=None, before_fit=None, before_epoch=None,\n            before_train=None, before_batch=None, after_pred=None,\n            after_loss=None, before_backward=None,\n            after_cancel_backward=None, after_backward=None,\n            before_step=None, after_cancel_step=None, after_step=None,\n            after_cancel_batch=None, after_batch=None,\n            after_cancel_train=None, after_train=None,\n            before_validate=None, after_cancel_validate=None,\n            after_validate=None, after_cancel_epoch=None,\n            after_epoch=None, after_cancel_fit=None, after_fit=None)\n\nCallback that tracks the number of iterations done and properly sets training/eval mode"
  },
  {
    "objectID": "l2r.callbacks.html#tracking",
    "href": "l2r.callbacks.html#tracking",
    "title": "L2R Callbacks",
    "section": "Tracking",
    "text": "Tracking\n\nsource\n\nAvgSmoothMetric\n\n AvgSmoothMetric (beta=0.98)\n\nSmooth average of the losses (exponentially weighted with beta)\n\nclass _TstLearner(): pass\nlearn = _TstLearner()\n_data = torch.randn(24)\n\ntst = AvgSmoothMetric()\ntst.reset()\nval = tensor(0.)\nfor i,o in enumerate(_data): \n    learn.moi = o\n    tst.accumulate(learn)\n    val = val*0.98 + o*(1-0.98)\n    test_close(val/(1-0.98**(i+1)), tst.value)\n\n\nsource\n\n\nTrackResults\n\n TrackResults (train_metrics=False, beta=0.98)\n\nBasic class handling tweaks of the training loop by changing a Learner in various events\n\nsource\n\n\nMonitor\n\n Monitor (monitor='ndcg_at_6', comp=None, min_delta=0.0,\n          reset_on_fit=False)\n\nBasic class handling tweaks of the training loop by changing a Learner in various events\n\nsource\n\n\nXParamScheduler\n\n XParamScheduler (scheds)\n\nSchedule hyper-parameters according to scheds\nscheds is a dictionary with one key for each hyper-parameter you want to schedule, with either a scheduler or a list of schedulers as values (in the second case, the list must have the same length as the the number of parameters groups of the optimizer).\n\nsource\n\n\nXParamScheduler.before_fit\n\n XParamScheduler.before_fit ()\n\nInitialize container for hyper-parameters\n\nsource\n\n\nXParamScheduler.before_batch\n\n XParamScheduler.before_batch ()\n\nSet the proper hyper-parameters in the optimizer\n\nsource\n\n\nXParamScheduler.after_batch\n\n XParamScheduler.after_batch ()\n\nRecord hyper-parameters of this batch\n\nsource\n\n\nXParamScheduler.after_fit\n\n XParamScheduler.after_fit ()\n\nSave the hyper-parameters in the track_results if there is one\n\nsource\n\n\nL2RLearner.fit_one_cycle\n\n L2RLearner.fit_one_cycle (n_epoch, lr_max=None, div=25.0,\n                           div_final=100000.0, pct_start=0.25, moms=None,\n                           cbs=None)\n\nFit self.model for n_epoch using the 1cycle policy.\nThe 1cycle policy was introduced by Leslie N. Smith et al. in Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates. It schedules the learning rate with a cosine annealing from lr_max/div to lr_max then lr_max/div_final (pass an array to lr_max if you want to use differential learning rates) and the momentum with cosine annealing according to the values in moms. The first phase takes pct_start of the training. You can optionally pass additional cbs and reset_opt.\n\nsource\n\n\nTrackResults.plot_sched\n\n TrackResults.plot_sched (keys=None, figsize=None)"
  },
  {
    "objectID": "l2r.callbacks.html#xlrfind",
    "href": "l2r.callbacks.html#xlrfind",
    "title": "L2R Callbacks",
    "section": "XLRFind",
    "text": "XLRFind\n\nsource\n\nXLRFinder\n\n XLRFinder (start_lr=1e-07, end_lr=10, num_it=100, stop_div=True)\n\nTraining with exponentially growing learning rate\n\nsource\n\n\nTrackResults.plot_xlr_find\n\n TrackResults.plot_xlr_find (skip_end=5, return_fig=True,\n                             suggestions=None, nms=None, **kwargs)\n\nPlot the result of an LR Finder test (won’t work if you didn’t do learn.xlr_find() before)\n\nsource\n\n\nL2RLearner.xrl_find\n\n L2RLearner.xrl_find (start_lr=1e-05, end_lr=0.1, num_it=400,\n                      stop_div=True, show_plot=True,\n                      suggest_funcs=(&lt;function valley at\n                      0x7f2f4c36d670&gt;,))\n\nLaunch a mock training to find a good learning rate and return suggestions based on suggest_funcs as a named tuple"
  },
  {
    "objectID": "l2r.callbacks.html#progress",
    "href": "l2r.callbacks.html#progress",
    "title": "L2R Callbacks",
    "section": "Progress",
    "text": "Progress\n\nsource\n\nProgressBarCallback\n\n ProgressBarCallback (after_create=None, before_fit=None,\n                      before_epoch=None, before_train=None,\n                      before_batch=None, after_pred=None, after_loss=None,\n                      before_backward=None, after_cancel_backward=None,\n                      after_backward=None, before_step=None,\n                      after_cancel_step=None, after_step=None,\n                      after_cancel_batch=None, after_batch=None,\n                      after_cancel_train=None, after_train=None,\n                      before_validate=None, after_cancel_validate=None,\n                      after_validate=None, after_cancel_epoch=None,\n                      after_epoch=None, after_cancel_fit=None,\n                      after_fit=None)\n\nBasic class handling tweaks of the training loop by changing a Learner in various events"
  },
  {
    "objectID": "l2r.callbacks.html#saving",
    "href": "l2r.callbacks.html#saving",
    "title": "L2R Callbacks",
    "section": "Saving",
    "text": "Saving\n\nsource\n\nSaveCallBack\n\n SaveCallBack (fname, monitor='ndcg_at_6', comp=None, min_delta=0.0,\n               reset_on_fit=False)\n\nBasic class handling tweaks of the training loop by changing a Learner in various events"
  },
  {
    "objectID": "l2r.data.load.html",
    "href": "l2r.data.load.html",
    "title": "L2R DataLoader",
    "section": "",
    "text": "! [ -e /content ] && pip install -Uqq fastai # upgrade fastai on colab\n\nThis module contains all the classes and supporting fuctions to build a L2RDataLoader.\nThe method quantized_score simulates Pandas groupby using Numpy/PyTorch: (Why? Pandas are cute but speed thrills! More importantly: “Memory”)\n\nsource\n\nPreLoadTrans\n\n PreLoadTrans (df, qnts=None, device=None)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nPreLoadTrans.pad_split\n\n PreLoadTrans.pad_split ()\n\npads the validation set for each label to the nearest multiple of 16\n\nsource\n\n\nPreLoadTrans.count_topbins\n\n PreLoadTrans.count_topbins ()\n\ncounts the number of top 2 bins in the val set for each label\n\nsource\n\n\nPreLoadTrans.datasets\n\n PreLoadTrans.datasets ()\n\nprepare the train/val dataset from self.scored_toks\n\nimg = Image.open(Path.cwd()/'pics'/'LRdataloader.png').resize((500,250))\nimg\n\n\n\n\n\nsource\n\n\nL2RDataLoader\n\n L2RDataLoader (*args, **kwargs)\n\nAPI compatible with PyTorch DataLoader, with a lot more callbacks and flexibility"
  },
  {
    "objectID": "layers.html",
    "href": "layers.html",
    "title": "Layers",
    "section": "",
    "text": "One can easily create a beautiful layer with minimum boilerplate using fastai utilities. We will show a few simple examples here. For details and extensive illustrations please refer to decorated fastai layers.\nAn easy way to create a pytorch layer for a simple func\n\nsource\n\n\n\n Lambda (func)\n\nAn easy way to create a pytorch layer for a simple func\n\ndef _add2(x): return x+2\ntst = Lambda(_add2)\nx = torch.randn(10,20)\ntest_eq(tst(x), x+2)\ntst2 = pickle.loads(pickle.dumps(tst))\ntest_eq(tst2(x), x+2)\n\n\nsource\n\n\n\n\n PartialLambda (func)\n\nLayer that applies partial(func, **kwargs)\n\ndef test_func(a,b=2): return a+b\ntst = PartialLambda(test_func, b=5)\ntest_eq(tst(x), x+5)"
  },
  {
    "objectID": "layers.html#basic-manipulations-and-resizing",
    "href": "layers.html#basic-manipulations-and-resizing",
    "title": "Layers",
    "section": "",
    "text": "One can easily create a beautiful layer with minimum boilerplate using fastai utilities. We will show a few simple examples here. For details and extensive illustrations please refer to decorated fastai layers.\nAn easy way to create a pytorch layer for a simple func\n\nsource\n\n\n\n Lambda (func)\n\nAn easy way to create a pytorch layer for a simple func\n\ndef _add2(x): return x+2\ntst = Lambda(_add2)\nx = torch.randn(10,20)\ntest_eq(tst(x), x+2)\ntst2 = pickle.loads(pickle.dumps(tst))\ntest_eq(tst2(x), x+2)\n\n\nsource\n\n\n\n\n PartialLambda (func)\n\nLayer that applies partial(func, **kwargs)\n\ndef test_func(a,b=2): return a+b\ntst = PartialLambda(test_func, b=5)\ntest_eq(tst(x), x+5)"
  },
  {
    "objectID": "layers.html#linear",
    "href": "layers.html#linear",
    "title": "Layers",
    "section": "Linear",
    "text": "Linear\n\nsource\n\nElemWiseLin\n\n ElemWiseLin (dim0, dim1, add_bias=False, **kwargs)\n\nSame as nn.Module, but no need for subclasses to call super().__init__\n\nbs, dim0, dim1 = 10, 1271, 400\ntst = ElemWiseLin(dim0, dim1)\ntest_eq(tst.lin.weight.shape, (dim0, dim1))\nx = torch.randn(bs, dim0, dim1)\ntest_eq(tst(x).shape, (bs, dim0, dim1))"
  },
  {
    "objectID": "layers.html#batchnorm-layers",
    "href": "layers.html#batchnorm-layers",
    "title": "Layers",
    "section": "BatchNorm Layers",
    "text": "BatchNorm Layers\n\nsource\n\nLinBnFlatDrop\n\n LinBnFlatDrop (n_in, n_out, bn=True, p=0.0, act=None, lin_first=False)\n\nModule grouping BatchNorm1dFlat, Dropout and Linear layers\n\nsource\n\n\nLinBnDrop\n\n LinBnDrop (n_in, n_out=None, bn=True, ln=True, p=0.0, act=None,\n            lin_first=False, ndim=1)\n\nModule grouping BatchNorm1d, Dropout and Linear layers\nLinBnDrop is just like fastai’s LinBnDrop with an extra modality ln which provides the option of skipping the linear layer. That is, BatchNorm or the Linear layer is skipped if bn=False or ln=False, as is the dropout if p=0. Optionally, you can add an activation for after the linear layer with act.\n\ntst = LinBnDrop(10, 20)\nmods = list(tst.children())\nassert isinstance(mods[0], nn.BatchNorm1d)\nassert isinstance(mods[1], nn.Linear)\n\nThe LinBnDrop layer is not going to add an activation (even if provided) if ln is False but raise an error if not ln and ln_first:\n\ntst = LinBnDrop(10, 20, ln=False, p=0.02, act=nn.ReLU(inplace=True))\nmods = list(tst.children())\nassert isinstance(mods[0], nn.BatchNorm1d)\nassert isinstance(mods[1], nn.Dropout)\ntest_fail(lambda : LinBnDrop(10, 20, ln=False, lin_first=True), contains='AssertionError')"
  },
  {
    "objectID": "layers.html#embeddings",
    "href": "layers.html#embeddings",
    "title": "Layers",
    "section": "Embeddings",
    "text": "Embeddings\n\nsource\n\nEmbedding\n\n Embedding (ni, nf, std=0.01, **kwargs)\n\nEmbedding layer with truncated normal initialization"
  },
  {
    "objectID": "layers.html#attention-layers-for-extreme-multi-label-classification",
    "href": "layers.html#attention-layers-for-extreme-multi-label-classification",
    "title": "Layers",
    "section": "Attention Layers for Extreme Multi-Label Classification",
    "text": "Attention Layers for Extreme Multi-Label Classification\n\nsource\n\n_linear_attention\n\n _linear_attention (sentc:torch.Tensor, based_on:Union[torch.nn.modules.sp\n                    arse.Embedding,fastai.torch_core.Module])\n\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nsentc\nTensor\nSentence typically (bs, bptt, nh)\n\n\nbased_on\nnn.Embedding | Module\nxcube’s Embedding(n_lbs, nh) layer holding the label embeddings or a full fledged model\n\n\n\n\nsource\n\n\n_planted_attention\n\n _planted_attention (sentc:torch.Tensor, brain:torch.Tensor)\n\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nsentc\nTensor\nSentence typically (bs, bptt) containing the vocab idxs that goes inside the encoder\n\n\nbrain\nTensor\nlabel specific attn wgts for each token in vocab, typically of shape (vocab_sz, n_lbs)\n\n\n\n\nsource\n\n\n_diffntble_attention\n\n _diffntble_attention (inp:torch.Tensor,\n                       based_on:torch.nn.modules.container.ModuleDict)\n\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ninp\nTensor\nSentence typically (bs, bptt) containing the vocab idxs that goes inside the encoder\n\n\nbased_on\nnn.ModuleDict\ndictionary of pretrained nn.Embedding from l2r model\n\n\n\n\nsource\n\n\nLinear_Attention\n\n Linear_Attention (based_on:fastai.torch_core.Module)\n\n\nbs, bptt, nh, n_lbs = 16, 72, 100, 10\ntst_lbs = Embedding(n_lbs, nh)\ntst_Lin_Attn = Linear_Attention(tst_lbs)\nattn_layer = Lambda(tst_Lin_Attn)\nsentc = torch.randn(bs, bptt, nh)\ntest_eq(tst_Lin_Attn(sentc).shape , (bs, bptt, n_lbs))\ntest_eqs(attn_layer(sentc), tst_Lin_Attn(sentc), sentc @ tst_lbs.weight.transpose(0,1))\n\nattn_layer2 = pickle.loads(pickle.dumps(attn_layer))\ntest_eqs(attn_layer2(sentc), sentc @ tst_lbs.weight.transpose(0,1))\n\n\nsource\n\n\nPlanted_Attention\n\n Planted_Attention (brain:torch.Tensor)\n\n\nbs, bptt, vocab_sz, n_lbs = 16, 72, 100, 10\ninp = torch.zeros((bs, bptt)).random_(vocab_sz)\nbrain = torch.randn(vocab_sz, n_lbs)\ntst_planted_Attn = Planted_Attention(brain)\nattn_layer = Lambda(tst_planted_Attn)\nattn = brain[inp.long()]\ntest_eq(attn.shape, (bs, bptt, n_lbs))\ntest_eqs(attn, tst_planted_Attn(inp), attn_layer(inp))\n# test_eq(brain[sentc[8].long()][:, 4], attn[8, :, 4]) # looking at the attn wgts of the 8th sentence and 4th label\n\n\nsource\n\n\nDiff_Planted_Attention\n\n Diff_Planted_Attention (based_on:fastai.torch_core.Module)\n\n\nsource\n\n\nlincomb\n\n lincomb (t, wgts=None)\n\nreturns the linear combination of the dim1 of a 3d tensor of t based on wgts (if wgts is None just adds the rows)\n\nt = torch.randn(16, 72, 100)\nwgts = t.new_ones(t.size(0), 1, t.size(1))\ntest_eq(torch.bmm(wgts, t), lincomb(t))\nrand_wgts = t.new_empty(t.size(0), 15, t.size(1)).random_(10)\n# test_eq(lincomb(t, wgts=rand_wgts), torch.bmm(rand_wgts, t))\ntst_LinComb = PartialLambda(lincomb, wgts=rand_wgts)\ntest_eq(tst_LinComb(t), torch.bmm(rand_wgts, t))\n\n\n\n\ntopkmax\n\n topkmax (k=None, dim=1)\n\nreturns softmax of the 1th dim of 3d tensor x after zeroing out values in x smaller than kth largest. If k is None behaves like x.softmax(dim=dim). Intuitively,topkmaxhedges more compared toF.softmax``\n\nsource\n\n\nsplit_sort\n\n split_sort (t, sp_dim, sort_dim, sp_sz=500, **kwargs)\n\n\nt = torch.randn(16, 106, 819)\ns_t = split_sort(t, sp_dim=1, sort_dim=-1, sp_sz=14)\ntest_eq(t.sort(dim=-1).values, s_t)\n\n\n\n\ninattention\n\n inattention (k=None, sort_dim=0, sp_dim=0)\n\nreturns self after zeroing out values smaller than kth largest in dimension dim. If k is None behaves like returns self.\nTODO: DEB - Make it work for other dims - Hyperparmam schedule the k in topkmax (start with high gradually decrease)\n\nx = torch.randn((2, 7, 3))\ntest_eq(x.topkmax() , F.softmax(x, dim=1))\n# test_fail(topkmax, args=(x, ), kwargs=dict(dim=-1)) # NotImplemented\ntest_fail(x.topkmax, kwargs=dict(dim=-1)) # NotImplemented\ntest_eq(x.inattention(k=2, sort_dim=-1), \n        torch.where(x &lt; x.sort(dim=-1, descending=True).values[:, :, 2].unsqueeze(dim=-1), 0, x))\n\n\nx = torch.randn((8820,) )\nx_inattn = torch.where(x &lt; x.sort(dim=0, descending=True).values[2].unsqueeze(dim=0), 0, x)\nx_inattn1 = x.inattention(k=2, sort_dim=0)\ntest_eq(x_inattn, x_inattn1)\n\n\nsource\n\n\nXMLAttention\n\n XMLAttention (n_lbs, emb_sz, embed_p=0.0)\n\nCompute label specific attention weights for each token in a sequence\n\n# testing linear attention\ninp = torch.zeros(bs, bptt).random_(100)\nsentc = torch.randn(bs, bptt, nh)\nmask = sentc.new_empty(sentc.size()[:-1]).random_(2).bool()\ntest_eq(mask.unique(), tensor([0., 1.]))\nxml_attn = XMLAttention(n_lbs, nh)\nattn, tok_wgts, lbs_cf = xml_attn(inp, sentc, mask)\ntest_eq(attn.shape, (bs, n_lbs, nh))\ntst_lbs = xml_attn.lbs\ntst_Lin_Attn = Linear_Attention(tst_lbs)\nlin_attn_layer = Lambda(tst_Lin_Attn)\nattn_wgts = F.softmax(lin_attn_layer(sentc), dim=1) # topkmax(attn_layer(sentc), dim=1)\ntest_eq(attn, torch.bmm(attn_wgts.masked_fill(mask[:, :, None], 0).transpose(1,2), sentc))\n\n# testing planted attention followed by inattention\nassert xml_attn.attn.func.f is _linear_attention\ninp = torch.zeros((bs, bptt)).random_(vocab_sz)\nbrain = torch.randn(vocab_sz, n_lbs)\nplant_attn_layer = Lambda(Planted_Attention(brain))\n# xml_attn.attn = plant_attn_layer\nsetattr(xml_attn, 'attn', plant_attn_layer)\nassert xml_attn.attn.func.f is _planted_attention\nattn, tok_wgts, lbs_cf = xml_attn(inp, sentc, mask)\ntest_eqs(tok_wgts, \n         plant_attn_layer(inp).masked_fill(mask[:,:,None], 0).inattention(k=15, sort_dim=1), \n         brain[inp.long()].masked_fill(mask[:,:,None], 0).inattention(k=15, sort_dim=1)\n        )\ntest_eq(attn, \n        lincomb(sentc, \n                wgts=brain[inp.long()].masked_fill(mask[:,:,None], 0).inattention(k=15, sort_dim=1).transpose(1,2)\n               )\n       )\n\nTest masking works:\n\nfor attn_layer in (lin_attn_layer, plant_attn_layer):\n    setattr(xml_attn, 'attn', attn_layer)\n    inp = torch.zeros(bs, bptt).random_(100)\n    sentc = torch.randn(bs, bptt, nh)\n    sentc = sentc.masked_fill(mask[:, :, None], 0)\n    assert sentc[mask].sum().item() == 0\n    attn, tok_wgts, lbs_cf = xml_attn(inp, sentc, mask)\n    assert sentc[mask].sum().item() == 0\n    attn_wgts = F.softmax(attn_layer(sentc), dim=1) if attn_layer is lin_attn_layer else attn_layer(inp).masked_fill(mask[:,:,None], 0).inattention(k=15, sort_dim=1)# topkmax(attn_layer(sentc), dim=1)\n    test_eq(attn, torch.bmm(attn_wgts.transpose(1,2), sentc))"
  },
  {
    "objectID": "tutorial.train_mimic3.html",
    "href": "tutorial.train_mimic3.html",
    "title": "Training an XML Text Classifier",
    "section": "",
    "text": "! [ -e /content ] && pip install -Uqq xcube # upgrade xcube on colab\nfrom fastai.text.all import *\nfrom xcube.text.all import *\nfrom fastai.metrics import accuracy # there's an 'accuracy' metric in xcube as well (Deb fix name conflict later)\nMake sure we have that “beast”:\nic(torch.cuda.get_device_name(default_device()));\ntest_eq(torch.cuda.get_device_name(0), torch.cuda.get_device_name(default_device()))\ntest_eq(default_device(), torch.device(0))\nprint(f\"GPU memory = {torch.cuda.get_device_properties(default_device()).total_memory/1024**3}GB\")\n\nic| torch.cuda.get_device_name(default_device()): 'Quadro RTX 8000'\n\n\nGPU memory = 44.99969482421875GB\nsource = untar_xxx(XURLs.MIMIC3)\nsource_l2r = untar_xxx(XURLs.MIMIC3_L2R)\nSetting some environment variables:\nos.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\nSetting defaults for pandas and matplotlib:\n# Set the default figure size\nplt.rcParams[\"figure.figsize\"] = (8, 4)\nAltering some default jupyter settings:\nfrom IPython.core.interactiveshell import InteractiveShell\n# InteractiveShell.ast_node_interactivity = \"last\" # \"all\""
  },
  {
    "objectID": "tutorial.train_mimic3.html#dataloaders-for-the-language-model",
    "href": "tutorial.train_mimic3.html#dataloaders-for-the-language-model",
    "title": "Training an XML Text Classifier",
    "section": "DataLoaders for the Language Model",
    "text": "DataLoaders for the Language Model\nTo be able to use Transfer Learning, first we need to fine-tune our Language Model (which we pretrained on Wikipedia) on the corpus of Wiki-500k (the one we downloaded). Here we will build the DataLoaders object using fastai’s DataBlock API:\n\ndata = source/'mimic3-9k.csv'\n!head -n 1 {data}\n\nsubject_id,hadm_id,text,labels,length,is_valid\n\n\n\ndf = pd.read_csv(data,\n                 header=0,\n                 names=['subject_id', 'hadm_id', 'text', 'labels', 'length', 'is_valid'],\n                 dtype={'subject_id': str, 'hadm_id': str, 'text': str, 'labels': str, 'length': np.int64, 'is_valid': bool})\nlen(df)\n\n52726\n\n\n\ndf[['text', 'labels']] = df[['text', 'labels']].astype(str)\n\nLet’s take a look at the data:\n\ndf.head(3)\n\n\n\n\n\n\n\n\nsubject_id\nhadm_id\ntext\nlabels\nlength\nis_valid\n\n\n\n\n0\n86006\n111912\nadmission date discharge date date of birth sex f service surgery allergies patient recorded as having no known allergies to drugs attending first name3 lf chief complaint 60f on coumadin was found slightly drowsy tonight then fell down stairs paramedic found her unconscious and she was intubated w o any medication head ct shows multiple iph transferred to hospital1 for further eval major surgical or invasive procedure none past medical history her medical history is significant for hypertension osteoarthritis involving bilateral knee joints with a dependence on cane for ambulation chronic...\n801.35;348.4;805.06;807.01;998.30;707.24;E880.9;427.31;414.01;401.9;V58.61;V43.64;707.00;E878.1;96.71\n230\nFalse\n\n\n1\n85950\n189769\nadmission date discharge date service neurosurgery allergies sulfa sulfonamides attending first name3 lf chief complaint cc cc contact info major surgical or invasive procedure none history of present illness hpi 88m who lives with family had fall yesterday today had decline in mental status ems called pt was unresponsive on arrival went to osh head ct showed large r sdh pt was intubated at osh and transferred to hospital1 for further care past medical history cad s p mi in s p cabg in ventricular aneurysm at that time cath in with occluded rca unable to intervene chf reported ef 1st degre...\n852.25;E888.9;403.90;585.9;250.00;414.00;V45.81;96.71\n304\nFalse\n\n\n2\n88025\n180431\nadmission date discharge date date of birth sex f service surgery allergies no known allergies adverse drug reactions attending first name3 lf chief complaint s p fall major surgical or invasive procedure none history of present illness 45f etoh s p fall from window at feet found ambulating and slurring speech on scene intubated en route for declining mental status in the er the patient was found to be bradycardic to the s with bp of systolic she was given atropine dilantin and was started on saline past medical history unknown social history unknown family history unknown physical exam ex...\n518.81;348.4;348.82;801.25;427.89;E882;V49.86;305.00;96.71;38.93\n359\nFalse\n\n\n\n\n\n\n\nWe will now create the DataLoaders using DataBlock API:\n\ndls_lm = DataBlock(\n    blocks   = TextBlock.from_df('text', is_lm=True),\n    get_x    = ColReader('text'),\n    splitter = RandomSplitter(0.1)\n).dataloaders(df, bs=384, seq_len=80)\n\n\n\n\n\n\n\n\nFor the backward LM:\n\ndls_lm_r = DataBlock(\n    blocks   = TextBlock.from_df('text', is_lm=True, backwards=True),\n    get_x    = ColReader('text'),\n    splitter = RandomSplitter(0.1)\n).dataloaders(df, bs=384, seq_len=80)\n\n\n\n\n\n\n\n\nLet’s take a look at the batches:\n\ndls_lm.show_batch(max_n=2)\n\n\n\n\n\ntext\ntext_\n\n\n\n\n0\nxxbos admission date discharge date date of birth sex m history of present illness first name8 namepattern2 known lastname is the firstborn of triplets at week gestation born to a year old gravida para woman immune rpr non reactive hepatitis b surface antigen negative group beta strep status unknown this was an intrauterine insemination achieved pregnancy the pregnancy was uncomplicated until when mother was admitted for evaluation of pregnancy induced hypertension she was treated with betamethasone and discharged home she\nadmission date discharge date date of birth sex m history of present illness first name8 namepattern2 known lastname is the firstborn of triplets at week gestation born to a year old gravida para woman immune rpr non reactive hepatitis b surface antigen negative group beta strep status unknown this was an intrauterine insemination achieved pregnancy the pregnancy was uncomplicated until when mother was admitted for evaluation of pregnancy induced hypertension she was treated with betamethasone and discharged home she was\n\n\n1\noccluded rca with l to r collaterals the femoral sheath was sewn in started on heparin and transferred to hospital1 for surgical revascularization past medical history coronary artery diease s p myocardial infarction s p pci to rca subarachnoid hemorrhage secondary to streptokinase hypertension hyperlipidemia hiatal hernia gastritis depression reactive airway disease s p ppm placement for 2nd degree av block social history history of smoking having quit in with a pack year history family history strong family history of\nrca with l to r collaterals the femoral sheath was sewn in started on heparin and transferred to hospital1 for surgical revascularization past medical history coronary artery diease s p myocardial infarction s p pci to rca subarachnoid hemorrhage secondary to streptokinase hypertension hyperlipidemia hiatal hernia gastritis depression reactive airway disease s p ppm placement for 2nd degree av block social history history of smoking having quit in with a pack year history family history strong family history of premature\n\n\n\n\n\n\ndls_lm_r.show_batch(max_n=2)\n\n\n\n\n\ntext\ntext_\n\n\n\n\n0\nsb west d i basement name first doctor bldg name ward hospital lm lf name3 first lf name last name last doctor d i 30a visit fellow sb west d i name first doctor name last doctor d i 30p visit attending opat appointments up follow disease infectious garage name ward hospital parking best east campus un location ctr clinical name ward hospital sc building fax telephone md namepattern4 name last pattern1 name name11 first with am at wednesday when\nwest d i basement name first doctor bldg name ward hospital lm lf name3 first lf name last name last doctor d i 30a visit fellow sb west d i name first doctor name last doctor d i 30p visit attending opat appointments up follow disease infectious garage name ward hospital parking best east campus un location ctr clinical name ward hospital sc building fax telephone md namepattern4 name last pattern1 name name11 first with am at wednesday when services\n\n\n1\nbare and catheterization cardiac a for hospital1 to flown were you infarction myocardial or attack heart a and pain chest had you instructions discharge independent ambulatory status activity interactive and alert consciousness of level coherent and clear status mental condition discharge hyperglycemia hyperlipidemia hypertension infarction myocardial inferior acute diagnosis discharge home disposition discharge refills tablets disp pain chest for needed as year month of total a for minutes every sublingual tablet one sig sublingual tablet mg nitroglycerin day a once\nand catheterization cardiac a for hospital1 to flown were you infarction myocardial or attack heart a and pain chest had you instructions discharge independent ambulatory status activity interactive and alert consciousness of level coherent and clear status mental condition discharge hyperglycemia hyperlipidemia hypertension infarction myocardial inferior acute diagnosis discharge home disposition discharge refills tablets disp pain chest for needed as year month of total a for minutes every sublingual tablet one sig sublingual tablet mg nitroglycerin day a once po\n\n\n\n\n\nThe length of our vocabulary is:\n\nlen(dls_lm.vocab)\n\n57376\n\n\nLet’s take a look at some words of the vocab:\n\nprint(coll_repr(L(dls_lm.vocab), 30))\n\n(#57376) ['xxunk','xxpad','xxbos','xxeos','xxfld','xxrep','xxwrep','xxup','xxmaj','the','and','to','of','was','with','a','on','in','for','mg','no','tablet','patient','is','he','at','blood','name','po','she'...]\n\n\nCreating the DataLaoders takes some time, so smash that save button (also a good idea to save the dls_lm.vocab for later use) if you are working on your own dataset. In this case though untar_xxx has got it for you:\n\nprint(\"\\n\".join(L(source.glob(\"**/*dls*lm*.pkl\")).map(str))) # the ones with _r are for the reverse language model\n\n/home/deb/.xcube/data/mimic3/mimic3-9k_dls_lm.pkl\n/home/deb/.xcube/data/mimic3/mimic3-9k_dls_lm_vocab_r.pkl\n/home/deb/.xcube/data/mimic3/mimic3-9k_dls_lm_vocab.pkl\n/home/deb/.xcube/data/mimic3/mimic3-9k_dls_lm_r.pkl\n/home/deb/.xcube/data/mimic3/mimic3-9k_dls_lm_old.pkl\n\n\nTo load back the dls_lm later on:\n\ndls_lm = torch.load(source/'mimic3-9k_dls_lm.pkl')\n\n\ndls_lm_r = torch.load(source/'mimic3-9k_dls_lm_r.pkl')"
  },
  {
    "objectID": "tutorial.train_mimic3.html#learner-for-the-language-model-fine-tuning",
    "href": "tutorial.train_mimic3.html#learner-for-the-language-model-fine-tuning",
    "title": "Training an XML Text Classifier",
    "section": "Learner for the Language Model Fine-Tuning:",
    "text": "Learner for the Language Model Fine-Tuning:\n\nlearn = language_model_learner(\n    dls_lm, AWD_LSTM, drop_mult=0.3,\n    metrics=[accuracy, Perplexity()]).to_fp16()\n\nAnd, one more for the reverse:\n\nlearn_r = language_model_learner(\n    dls_lm_r, AWD_LSTM, drop_mult=0.3, backwards=True,\n    metrics=[accuracy, Perplexity()]).to_fp16()\n\nTraining a language model on the full datset takes a lot of time. So you can train one on a tiny dataset for illustration. Or you can skip the training and just load up the one that’s pretrained and downloaded by untar_xxx and just do the validation.\nLet’s compute the learning rate using the lr_find:\n\nlr_min, lr_steep, lr_valley, lr_slide = learn.lr_find(suggest_funcs=(minimum, steep, valley, slide))\nlr_min, lr_steep, lr_valley, lr_slide\n\n\nlearn.fit_one_cycle(1, lr_min)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\nperplexity\ntime\n\n\n\n\n0\n3.646323\n3.512013\n0.382642\n33.515659\n2:27:57\n\n\n\n\n\nIt takes quite a while to train each epoch, so we’ll be saving the intermediate model results during the training process:\nSince we have completed the initial training, we will now continue fine-tuning the model after unfreezing:\n\nlearn.unfreeze()\n\nand run lr_find again, because we now have more layers to train, and the last layers weight have already been trained for one epoch:\n\nlr_min, lr_steep, lr_valley, lr_slide = learn.lr_find(suggest_funcs=(minimum, steep, valley, slide))\nlr_min, lr_steep, lr_valley, lr_slide\n\nLet’s now traing with a suitable learning rate:\n\nlearn.fit_one_cycle(10, lr_max=2e-3, cbs=SaveModelCallback(fname='lm'))\n\nNote: Make sure if you have trained the most recent language model Learner for more epochs (then you need to save that version)\nHere you can load the pretrained language model which untar_xxx downloaded:\n\nlearn = learn.load(source/'mimic3-9k_lm')\n\n\nlearn_r = learn_r.load(source/'mimic3-9k_lm_r')\n\nLet’s validate the Learner to make sure we loaded the correct version:\n\nlearn.validate()\n\n\n\n\n\n\n\n\n(#3) [2.060459852218628,0.5676611661911011,7.849578857421875]\n\n\nand the reverse…\n\nlearn_r.validate()\n\n\n\n\n\n\n\n\n(#3) [2.101907968521118,0.5691556334495544,8.18176555633545]\n\n\n\nSaving the encoder of the Language Model\nCrucial: Once we have trained our LM we will save all of our model except the final layer that converts activation to probabilities of picking each token in our vocabulary. The model not including the final layer has a sexy name - encoder. We will save it using save_encoder method of the Learner:\n\n# learn.save_encoder('lm_finetuned')\n# learn_r.save_encoder('lm_finetuned_r')\n\n\n\nSaving the decoder of the Language Model\n\nlearn.save_decoder('mimic3-9k_lm_decoder')\nlearn_r.save_decoder('mimic3-9k_lm_decoder_r')\n\nThis completes the second stage of the text classification process - fine-tuning the Language Model pretrained on Wikipedia corpus. We will now use it to fine-tune a text multi-label text classifier."
  },
  {
    "objectID": "tutorial.train_mimic3.html#dataloaders-for-the-multi-label-classifier-using-fastais-mid-level-data-api",
    "href": "tutorial.train_mimic3.html#dataloaders-for-the-multi-label-classifier-using-fastais-mid-level-data-api",
    "title": "Training an XML Text Classifier",
    "section": "DataLoaders for the Multi-Label Classifier (using fastai’s Mid-Level Data API)",
    "text": "DataLoaders for the Multi-Label Classifier (using fastai’s Mid-Level Data API)\nFastai’s midlevel data api is the swiss army knife of data preprocessing. Detailed tutorials can be found here intermediate, advanced. We will use it here to create our dataloaders for the classifier.\n\nLoading Raw Data\n\ndata = source/'mimic3-9k.csv'\n!head -n 1 {data}\n\nsubject_id,hadm_id,text,labels,length,is_valid\n\n\n\n# !shuf -n 200000 {data} &gt; {data_sample}\n\n\ndf = pd.read_csv(data,\n                 header=0,\n                 names=['subject_id', 'hadm_id', 'text', 'labels', 'length', 'is_valid'],\n                 dtype={'subject_id': str, 'hadm_id': str, 'text': str, 'labels': str, 'length': np.int64, 'is_valid': bool})\n\n\ndf[['text', 'labels']] = df[['text', 'labels']].astype(str)\n\n\ndf.head(3)\n\n\n\n\n\n\n\n\nsubject_id\nhadm_id\ntext\nlabels\nlength\nis_valid\n\n\n\n\n0\n86006\n111912\nadmission date discharge date date of birth sex f service surgery allergies patient recorded as having no known allergies to drugs attending first name3 lf chief complaint 60f on coumadin was found slightly drowsy tonight then fell down stairs paramedic found her unconscious and she was intubated w o any medication head ct shows multiple iph transferred to hospital1 for further eval major surgical or invasive procedure none past medical history her medical history is significant for hypertension osteoarthritis involving bilateral knee joints with a dependence on cane for ambulation chronic...\n801.35;348.4;805.06;807.01;998.30;707.24;E880.9;427.31;414.01;401.9;V58.61;V43.64;707.00;E878.1;96.71\n230\nFalse\n\n\n1\n85950\n189769\nadmission date discharge date service neurosurgery allergies sulfa sulfonamides attending first name3 lf chief complaint cc cc contact info major surgical or invasive procedure none history of present illness hpi 88m who lives with family had fall yesterday today had decline in mental status ems called pt was unresponsive on arrival went to osh head ct showed large r sdh pt was intubated at osh and transferred to hospital1 for further care past medical history cad s p mi in s p cabg in ventricular aneurysm at that time cath in with occluded rca unable to intervene chf reported ef 1st degre...\n852.25;E888.9;403.90;585.9;250.00;414.00;V45.81;96.71\n304\nFalse\n\n\n2\n88025\n180431\nadmission date discharge date date of birth sex f service surgery allergies no known allergies adverse drug reactions attending first name3 lf chief complaint s p fall major surgical or invasive procedure none history of present illness 45f etoh s p fall from window at feet found ambulating and slurring speech on scene intubated en route for declining mental status in the er the patient was found to be bradycardic to the s with bp of systolic she was given atropine dilantin and was started on saline past medical history unknown social history unknown family history unknown physical exam ex...\n518.81;348.4;348.82;801.25;427.89;E882;V49.86;305.00;96.71;38.93\n359\nFalse\n\n\n\n\n\n\n\nSample a small fraction of the dataset to ensure quick iteration (Skip this if you want to do this on the full dataset)\n\n# df = df.sample(frac=0.3, random_state=89, ignore_index=True)\n# df = df.sample(frac=0.025, random_state=89, ignore_index=True)\ndf = df.sample(frac=0.005, random_state=89, ignore_index=True)\nlen(df)\n\n264\n\n\nLet’s now gather the labels from the ‘labels’ columns of the df:\n\nlbl_freqs = Counter()\nfor labels in df.labels: lbl_freqs.update(labels.split(';'))\n\nThe total number of labels are:\n\nlen(lbl_freqs)\n\n8922\n\n\nLet’s take a look at the most common labels:\n\npd.DataFrame(lbl_freqs.most_common(20), columns=['label', 'frequency'])\n\n\n\n\n\n\n\n\nlabel\nfrequency\n\n\n\n\n0\n401.9\n20053\n\n\n1\n38.93\n14444\n\n\n2\n428.0\n12842\n\n\n3\n427.31\n12594\n\n\n4\n414.01\n12179\n\n\n5\n96.04\n9932\n\n\n6\n96.6\n9161\n\n\n7\n584.9\n8907\n\n\n8\n250.00\n8784\n\n\n9\n96.71\n8619\n\n\n10\n272.4\n8504\n\n\n11\n518.81\n7249\n\n\n12\n99.04\n7147\n\n\n13\n39.61\n6809\n\n\n14\n599.0\n6442\n\n\n15\n530.81\n6156\n\n\n16\n96.72\n5926\n\n\n17\n272.0\n5766\n\n\n18\n285.9\n5296\n\n\n19\n88.56\n5240\n\n\n\n\n\n\n\nLet’s make a list of all labels (We will use it later while creating the DataLoader)\n\nlbls = list(lbl_freqs.keys())\n\n\n\nDataset Statistics (Optional)\n\nLet’s try to understand what captures the hardness of an xml dataset\n\n\nCheck #1: Number of instances (train/valid split)\n\ntrain, valid = df.index[~df['is_valid']], df.index[df['is_valid']]\nlen(train), len(valid)\n\n(244, 20)\n\n\n\n\nCheck #2: Avg number of instances per label\n\narray(list(lbl_freqs.values())).mean()\n\n3.341463414634146\n\n\n\n\nCheck #3: Plotting the label distribution\n\nlbl_count = []\nfor lbls in df.labels: lbl_count.append(len(lbls.split(',')))\n\n\ndf_copy = df.copy()\ndf_copy['label_count'] = lbl_count\n\n\ndf_copy.head(2)\n\n\n\n\n\n\n\n\ntext\nlabels\nis_valid\nlabel_count\n\n\n\n\n0\nMethodical Bible study: A new approach to hermeneutics /SEP/ Methodical Bible study: A new approach to hermeneutics. Inductive study compares related Bible texts in order to let the Bible interpret itself, rather than approaching Scripture with predetermined notions of what it will say. Dr. Trainas Methodical Bible Study was not intended to be the last word in inductive Bible study; but since its first publication in 1952, it has become a foundational text in this field. Christian colleges and seminaries have made it required reading for beginning Bible students, while many churches have...\n34141,119299,126600,128716,187372,218742\nFalse\n6\n\n\n1\nSoutheastern Mills Roast Beef Gravy Mix, 4.5-Ounce Packages (Pack of 24) /SEP/ Southeastern Mills Roast Beef Gravy Mix, 4.5-Ounce Packages (Pack of 24). Makes 3-1/2 cups. Down home taste. Makes hearty beef stew base.\n465536,465553,615429\nFalse\n3\n\n\n\n\n\n\n\nThe average number of labels per instance is:\n\ndf_copy.label_count.mean()\n\n5.385563363865518\n\n\n\nimport seaborn as sns\nsns.distplot(df_copy.label_count, bins=10, color='b');\n\n/home/deb/miniconda3/lib/python3.9/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n  warnings.warn(msg, FutureWarning)\n\n\n\n\n\n\nlbls_sorted = sorted(lbl_freqs.items(), key=lambda item: item[1], reverse=True)\n\n\nlbls_sorted[:20]\n\n[('455619', 2258),\n ('455662', 2176),\n ('547041', 2160),\n ('516790', 1214),\n ('455712', 1203),\n ('455620', 1133),\n ('632786', 1132),\n ('632789', 1132),\n ('632785', 1030),\n ('632788', 1030),\n ('492255', 938),\n ('455014', 872),\n ('670034', 850),\n ('427871', 815),\n ('599701', 803),\n ('308331', 801),\n ('581325', 801),\n ('649272', 799),\n ('455704', 762),\n ('666760', 733)]\n\n\n\nranked_lbls = L(lbls_sorted).itemgot(0)\nranked_freqs = L(lbls_sorted).itemgot(1)\nranked_lbls, ranked_freqs\n\n((#670091) ['455619','455662','547041','516790','455712','455620','632786','632789','632785','632788'...],\n (#670091) [2258,2176,2160,1214,1203,1133,1132,1132,1030,1030...])\n\n\n\nfig = plt.figure(figsize=(10,5))\nax = fig.add_subplot(1,1,1)\nax.plot(ranked_freqs)\nax.set_xlabel('Labels ranked by frequency')\nax.set_ylabel('Text Count')\nax.set_yscale('log');\n\n\n\n\n\n\nCheck #4: Computing the min label freq for each text\n\ndf_copy.head(10)\n\n\n\n\n\n\n\n\ntext\nlabels\nis_valid\nlabel_count\nmin_code_freq\nmax_code_freq\n90pct_code_freq\n\n\n\n\n0\nMethodical Bible study: A new approach to hermeneutics /SEP/ Methodical Bible study: A new approach to hermeneutics. Inductive study compares related Bible texts in order to let the Bible interpret itself, rather than approaching Scripture with predetermined notions of what it will say. Dr. Trainas Methodical Bible Study was not intended to be the last word in inductive Bible study; but since its first publication in 1952, it has become a foundational text in this field. Christian colleges and seminaries have made it required reading for beginning Bible students, while many churches have...\n34141,119299,126600,128716,187372,218742\nFalse\n6\n2\n29\n25.5\n\n\n1\nSoutheastern Mills Roast Beef Gravy Mix, 4.5-Ounce Packages (Pack of 24) /SEP/ Southeastern Mills Roast Beef Gravy Mix, 4.5-Ounce Packages (Pack of 24). Makes 3-1/2 cups. Down home taste. Makes hearty beef stew base.\n465536,465553,615429\nFalse\n3\n2\n3\n3.0\n\n\n2\nMMF Industries 24-Key Portable Zippered Key Case (201502417) /SEP/ MMF Industries 24-Key Portable Zippered Key Case (201502417). The MMF Industries 201502417 24-Key Portable Zippered Key Case is an attractive burgundy-colored leather-like vinyl case with brass corners and looks like a portfolio. Its easy-slide zipper keeps keys enclosed, and a hook-and-loop fastener strips keep keys securely in place. Key tags are included.\\tZippered key case offers a portable alternative to metal wall key cabinets. Included key tags are backed by hook-and-loop closures. Easy slide zipper keeps keys enclos...\n393828\nFalse\n1\n2\n2\n2.0\n\n\n3\nHoover the Fishing President /SEP/ Hoover the Fishing President. Hal Elliott Wert has spent years researching Herbert Hoover, Franklin Roosevelt, and Harry Truman. He holds a Ph.D. from the University of Kansas and currently teaches at the Kansas City Art Institute.\n167614,223686\nFalse\n2\n4\n4\n4.0\n\n\n4\nGeoPuzzle U.S.A. and Canada - Educational Geography Jigsaw Puzzle (69 pcs) /SEP/ GeoPuzzle U.S.A. and Canada - Educational Geography Jigsaw Puzzle (69 pcs). GeoPuzzles are jigsaw puzzles that make learning world geography fun. The pieces of a GeoPuzzle are shaped like individual countries, so children learn as they put the puzzle together. Award-winning Geopuzzles help to build fine motor, cognitive, language, and problem-solving skills, and are a great introduction to world geography for children 4 and up. Designed by an art professor, jumbo sized and brightly colored GeoPuzzles are avail...\n480528,480530,480532,485144,485146,598793\nFalse\n6\n5\n10\n8.5\n\n\n5\nAmazon.com: Paul Fredrick Men's Cotton Pinpoint Oxford Straight Collar Dress Shirt: Clothing /SEP/ Amazon.com: Paul Fredrick Men's Cotton Pinpoint Oxford Straight Collar Dress Shirt: Clothing. Pinpoint Oxford Cotton. Traditional Straight Collar, 1/4 Topstitched. Button Cuffs, 1/4 Topstitched. Embedded Collar Stay. Regular, Big and Tall. Top Center Placket. Split Yoke. Single Front Rounded Pocket. Machine Wash Or Dry Clean. Imported. * Big and Tall Sizes - addl $5.00\n516790,567615,670034\nFalse\n3\n256\n1214\n1141.2\n\n\n6\nDarkest Fear : A Myron Bolitar Novel /SEP/ Darkest Fear : A Myron Bolitar Novel. Myron Bolitar's father's recent heart attack brings Myron smack into a midlife encounter with issues of adulthood and mortality. And if that's not enough to turn his life upside down, the reappearance of his first serious girlfriend is. The basketball star turned sports agent, who does a little detecting when business is slow, is saddened by the news that Emily Downing's 13-year-old son is dying and desperately needs a bone marrow transplant; even if she did leave him for the man who destroyed his basketball c...\n50442,50516,50647,50672,50680,662538\nFalse\n6\n2\n3\n2.5\n\n\n7\nIn Debt We Trust (2007) /SEP/ In Debt We Trust (2007). Just a few decades ago, owing more money than you had in your bank account was the exception, not the rule. Yet, in the last 10 years, consumer debt has doubled and, for the first time, Americans are spending more than they're saving -- or making. This April, award-winning former ABC News and CNN producer Danny Schechter investigates America's mounting debt crisis in his latest hard-hitting expose, IN DEBT WE TRUST. While many Americans are \"maxing out\" on credit cards, there is a much deeper story: power is shifting into few...\n493095,499382,560691,589867,591343,611615,619231\nFalse\n7\n4\n50\n47.0\n\n\n8\nCraftsman 9-34740 32 Piece 1/4-3/8 Drive Standard/Metric Socket Wrench Set /SEP/ Craftsman 9-34740 32 Piece 1/4-3/8 Drive Standard/Metric Socket Wrench Set. Craftsman 9-34740 32 Pc. 1/4-3/8 Drive Standard/Metric Socket Wrench Set. The Craftsman 9-34740 32 Pc. 1/4-3/8 Drive Standard/Metric Socket Wrench Set includes 1/4 Inch Drive Sockets; 8 Six-Point Standard Sockets:9-43492 7/32-Inch, 9-43496 11/32,9-43493 1/4-Inch, 9-43497 3/8-Inch, 9-43494 9/32-Inch, 9-43498 7/16-Inch,9-43495 5/16-Inch, 9-43499 1/2-Inch; 7 Six-Point Metric Sockets:9-43505 4mm, 9-43504 8mm, 9-43501 5mm, 9-43507 9mm,9-435...\n590298,609038\nFalse\n2\n2\n2\n2.0\n\n\n9\n2013 Hammer Nutrition Complex Carbohydrate Energy Gel - 12-Pack /SEP/ 2013 Hammer Nutrition Complex Carbohydrate Energy Gel - 12-Pack. Hammer Nutrition made the Complex Carbohydrate Energy Gel with natural ingredients and real fruit, so you won't have those unhealthy insulin-spike sugar highs. Instead, you get prolonged energy levels from the complex carbohydrates and amino acids in this Hammer Gel. The syrup-like consistency makes it easy to drink straight or add to your water. Unlike nutrition bars that freeze on your winter treks or melt during hot adventure races, the Complex Carbohydr...\n453268,453277,510450,516828,520780,542684,634756\nFalse\n7\n3\n11\n9.2\n\n\n\n\n\n\n\n\ndf_copy['min_code_freq'] = df_copy.apply(\n    lambda row: min([lbl_freqs[lbl] for lbl in row.labels.split(',')]), axis=1)\n\n\ndf_copy['max_code_freq'] = df_copy.apply(\n    lambda row: max([lbl_freqs[lbl] for lbl in row.labels.split(',')]), axis=1)\n\n\ndf_copy['90pct_code_freq'] = df_copy.apply(\n    lambda row: np.percentile([lbl_freqs[lbl] for lbl in row.labels.split(',')], 90), axis=1)\n\n\nfig, axes = plt.subplots(nrows=3, ncols=2, figsize=(20,20))\n\nfor freq, axis in zip(['min_code_freq', 'max_code_freq', '90pct_code_freq'], axes):\n    df_copy[freq].hist(ax=axis[0], bins=25)\n    axis[0].set_xlabel(freq)\n    axis[0].set_ylabel('Text Count')\n    df_copy[freq].plot.density(ax=axis[1])\n    axis[1].set_xlabel(freq)\n\n\n\n\n\nmin_code_freqs = Counter(df_copy['min_code_freq'])\nmax_code_freqs = Counter(df_copy['max_code_freq'])\nnintypct_code_freqs = Counter(df_copy['90pct_code_freq'])\n\n\ntotal_notes = L(min_code_freqs.values()).sum()\ntotal_notes\n\n643474\n\n\n\nfor kmin in min_code_freqs:\n    min_code_freqs[kmin] = (min_code_freqs[kmin]/total_notes) * 100\n    \nfor kmax in max_code_freqs:\n    max_code_freqs[kmax] = (max_code_freqs[kmax]/total_notes) * 100\n    \nfor k90pct in nintypct_code_freqs:\n    nintypct_code_freqs[k90pct] = (nintypct_code_freqs[k90pct]/total_notes) * 100\n\n\nmin_code_freqs = dict(sorted(min_code_freqs.items(), key=lambda item: item[0]))\nmax_code_freqs = dict(sorted(max_code_freqs.items(), key=lambda item: item[0]))\nnintypct_code_freqs = dict(sorted(nintypct_code_freqs.items(), key=lambda item: item[0]))\n\n\nfig, axes = plt.subplots(nrows=3, ncols=2, figsize=(20,20))\n\nfor axis, freq_dict, label in zip(axes, (min_code_freqs, max_code_freqs, nintypct_code_freqs), ('min', 'max', '90pct')):\n    axis[0].plot(freq_dict.keys(), freq_dict.values())\n    axis[0].set_xlabel(f\"{label} label freq (f)\")\n    axis[0].set_ylabel(\"% of texts (P[f])\");\n    \n    axis[1].plot(freq_dict.keys(), np.cumsum(list(freq_dict.values())))\n    axis[1].set_xlabel(f\"{label} code freq (f)\")\n    axis[1].set_ylabel(\"P[f&lt;=t]\");\n\n\n\n\n\n\n\nSteps for creating the classifier DataLoaders using fastai’s Transforms:\n\n1. train/valid splitter:\nOkay, based on the is_valid column of our Dataframe, let’s create a splitter:\n\ndef splitter(df):\n    train = df.index[~df['is_valid']].tolist()\n    valid = df.index[df['is_valid']].to_list()\n    return train, valid\n\nLet’s check the train/valid split\n\nsplits = [train, valid] = splitter(df)\nL(splits[0]), L(splits[1])\n\n((#49354) [0,1,2,3,4,5,6,7,8,9...],\n (#3372) [1631,1632,1633,1634,1635,1636,1637,1638,1639,1640...])\n\n\n\n\n2. Making the Datasets object:\nCrucial: We need the vocab of the language model so that we can make sure we use the same correspondence of token to index. Otherwise, the embeddings we learned in our fine-tuned language model won’t make any sense to our classifier model, and the fine-tuning won’t be of any use. So we need to pass the lm_vocab to the Numericalize transform:\nSo let’s load the vocab of the language model:\n\nlm_vocab = torch.load(source/'mimic3-9k_dls_lm_vocab.pkl')\nlm_vocab_r = torch.load(source/'mimic3-9k_dls_lm_vocab_r.pkl')\n\n\nall_equal(lm_vocab, lm_vocab_r)\nL(lm_vocab)\n\n(#57376) ['xxunk','xxpad','xxbos','xxeos','xxfld','xxrep','xxwrep','xxup','xxmaj','the'...]\n\n\n\nx_tfms = [Tokenizer.from_df('text'), attrgetter(\"text\"), Numericalize(vocab=lm_vocab)]\nx_tfms_r = [Tokenizer.from_df('text', ), attrgetter(\"text\"), Numericalize(vocab=lm_vocab), reverse_text]\ny_tfms = [ColReader('labels', label_delim=';'), MultiCategorize(vocab=lbls), OneHotEncode()]\ntfms = [x_tfms, y_tfms]\ntfms_r = [x_tfms_r, y_tfms]\n\n\ndsets = Datasets(df, tfms, splits=splits)\n\n\n\n\n\n\n\n\nCPU times: user 17.9 s, sys: 3.27 s, total: 21.2 s\nWall time: 2min 38s\n\n\n\ndsets_r = Datasets(df, tfms_r, splits=splits)\n\n\n\n\n\n\n\n\nCPU times: user 18.2 s, sys: 4.67 s, total: 22.9 s\nWall time: 2min 54s\n\n\nLet’s now check if our Datasets got created alright:\n\nlen(dsets.train), len(dsets.valid)\n\n(49354, 3372)\n\n\nLet’s check a random data point:\n\nidx = random.randint(0, len(dsets))\nx = dsets.train[idx]\nassert isinstance(x, tuple) # tuple if independent and dependent variable\nx_r = dsets_r.train[idx]\n\n\ndsets.decode(x)\n\n('xxbos admission date discharge date date of birth sex f service medicine allergies nsaids aspirin influenza virus vaccine attending first name3 lf chief complaint hypotension resp distress major surgical or invasive procedure rij placement and removal last name un gastric tube placement and removal rectal tube placement and removal picc line placement and removal history of present illness ms known lastname is a 50f with cirrhosis of unproven etiology h o perforated duodenal ulcer who has had weakness and back pain with cough and shortness of breath for days she has chronic low back pain but this is in a different location she also reports fever and chills she reports increasing weakness and fatigue over the same amount of time she is followed by vna who saw her at home and secondary to hypotension and hypoxia requested she go to ed in the ed initial vs were afebrile and o2 sat unable to be read ct of torso showed pneumonia but no abdominal pathology she was seen by surgery she was found to be significantly hypoxic and started on nrb a right ij was placed she was started on levophed after a drop in her pressures in the micu she is conversant and able to tell her story past medical history asthma does not use inhalers htn off meds for several years rheumatoid arthritis seronegative chronic severe back pain s p c sections history of secondary syphilis treated polysubstance abuse notably cocaine but no drug or alcohol use for at least past months depression pulmonary hypertension severe on cardiac cath restrictive lung disease seizures in childhood cirrhosis by liver biopsy etiology as yet unknown duodenal ulcer s p surgical repair xxunk location un in summer social history lives with boyfriend who helps in her healthcare has children past h o cocaine abuse but not current denies any etoh drug or tobacco use since cirrhosis diagnosis in month only previously smoked up to pack per wk family history noncontributory physical exam on admission t hr bp r o2 sat on nrb general alert oriented moderate respiratory distress heent sclera anicteric drymm oropharynx clear neck supple jvp not elevated no lad lungs rhonchi and crackles in bases xxrep 3 l with egophany increased xxunk cv tachy rate and rhythm normal s1 s2 no murmurs rubs gallops abdomen soft diffusely tender non distended bowel sounds present healing midline ex lap wound dehiscence on caudal end but without erythema drainage ext warm well perfused pulses no clubbing cyanosis or edema pertinent results admission labs 13pm blood wbc rbc hgb hct mcv mch mchc rdw plt ct 13pm blood neuts bands lymphs monos eos baso 13pm blood pt ptt inr pt 13pm blood plt smr high plt ct 13pm blood glucose urean creat na k cl hco3 angap 13pm blood alt ast ld ldh alkphos amylase totbili 22am blood ck mb ctropnt 42pm blood probnp numeric identifier 13pm blood albumin calcium phos mg 22am blood cortsol 28pm blood type art o2 flow po2 pco2 ph caltco2 base xs intubat not intuba vent spontaneou comment non rebrea 20pm blood lactate discharge labs 58am blood wbc rbc hgb hct mcv mch mchc rdw plt ct 58am blood plt ct 58am blood pt ptt inr pt 58am blood glucose urean creat na k cl hco3 angap 58am blood alt ast ld ldh alkphos totbili 58am blood calcium phos mg imaging ct chest abd pelvis impression no etiology for abdominal pain identified no free air is noted within the abdomen interval increase in diffuse anasarca and bilateral pleural effusions small on the right and small to moderate on the left compressive atelectasis and new bilateral lower lobe pneumonias interval improvement in the degree of mediastinal lymphadenopathy interval development of small pericardial effusion no ct findings to suggest tamponade tte the left atrium is normal in size there is mild symmetric left ventricular hypertrophy with normal cavity size and regional global systolic function lvef the estimated cardiac index is borderline low 5l min m2 tissue doppler imaging suggests a normal left ventricular filling pressure pcwp 12mmhg the right ventricular free wall is hypertrophied the right ventricular cavity is markedly dilated with severe global free wall hypokinesis there is abnormal septal motion position consistent with right ventricular pressure volume overload the aortic valve leaflets appear structurally normal with good leaflet excursion and no aortic regurgitation the mitral valve leaflets are structurally normal mild mitral regurgitation is seen the tricuspid valve leaflets are mildly thickened there is mild moderate tricuspid xxunk there is severe pulmonary artery systolic hypertension there is a small circumferential pericardial effusion without echocardiographic signs of tamponade compared with the prior study images reviewed of the overall findings are similar ct abd pelvis impression anasarca increased bilateral pleural effusions small to moderate right greater than left trace pericardial fluid increased left lower lobe collapse consolidation right basilar atelectasis has also increased new ascites no free air dilated fluid filled colon and rectum no wall thickening to suggest colitis no evidence of small bowel obstruction bilat upper extremities venous ultrasound impression non occlusive right subclavian deep vein thrombosis cta chest impression no pulmonary embolism to the subsegmental level anasarca moderate to large left and moderate right pleural effusion small pericardial effusion unchanged right ventricle and right atrium enlargement since enlarged main pulmonary artery could be due to pulmonary hypertension complete collapse of the left lower lobe pneumonia can not be ruled out right basilar atelectasis kub impression unchanged borderline distended colon no evidence of small bowel obstruction or ileus brief hospital course this is a 50f with severe pulmonary hypertension h o duodenal perf s p surgical repair cirrhosis of unknown etiology and h o polysubstance abuse who presented with septic shock secondary pneumonia with respiratory distress septic shock pneumonia fever hypotension tachycardia hypoxia leukocytosis with xxrep 3 l infiltrate initially pt was treated for both health care associated pneumonia and possible c diff infection given wbc on admission however c diff treatment was stopped after patient tested negative x3 she completed a day course of vancomycin zosyn on for pneumonia she did initially require non rebreather in the icu but for the last week prior to discharge she has had stable o2 sats to mid s on nasal cannula she did also require pressors for the first several days of this hospitalization but was successfully weaned off pressors approximately wk prior to discharge at the time of discharge she is maintaining stable bp s 100s 110s doppler due to repeated ivf boluses for hypotension during this admission the pt developed anasarca and at the time of discharge is being slowly diuresed with lasix iv daily ogilvies pseudo obstruction the pt experience abd pain and distention with radiologic evidence of dilated loops of small bowel and large bowel during this hospitalization there was never any evidence of a transition point of stricture both a rectal and ngt were placed for decompression the pts symptoms and abd exam improved during the days prior to discharge and at the time of discharge she is tolerating a regular po diet and having normal bowel movements chronic back pain the pt has a long h o chronic back pain she was treated with iv morphine here for acute on chronic back pain likely exacerbated by bed rest and ileus at discharge she was transitioned to po morphine cirrhosis the etiology of this is unknown but the pt has biopsy proven cirrhosis from liver biopsy this likely explains her baseline coagulopathy and hypoalbuminemia here she needs to have outpt f u with hepatology she has no known complications of cirrhosis at this time rue dvt the pt has a rue dvt seen on u s associated with rij cvl which was placed in house she was started on a heparin gtt which was transitioned to coumadin prior to discharge at discharge her inr is she should have follow up lab work on friday of note at discharge the pt had several healing boils on her inner upper thighs thought to be trauma from her foley rubbing against the skin no further treatment was thought to be neccessary for these medications on admission clobetasol cream apply to affected area twice a day as needed gabapentin neurontin mg tablet tablet s by mouth three times a day metoprolol tartrate mg tablet tablet s by mouth twice a day omeprazole mg capsule delayed release e c capsule delayed release e c s by mouth once a day oxycodone mg capsule capsule s by mouth q hours physical therapy dx first name9 namepattern2 location un evaluation and management of motor skills tramadol mg tablet tablet s by mouth twice a day as needed for pain trazodone mg tablet tablet s by mouth hs medications otc docusate sodium mg ml liquid teaspoon by mouth twice a day as needed magnesium oxide mg tablet tablet s by mouth twice a day discharge medications ipratropium bromide solution sig one nebulizer inhalation q6h every hours clobetasol ointment sig one appl topical hospital1 times a day lidocaine mg patch adhesive patch medicated sig one adhesive patch medicated topical qday as needed for back pain please wear on skin hrs then have hrs with patch off hemorrhoidal suppository suppository sig one suppository rectal once a day as needed for pain pantoprazole mg tablet delayed release e c sig one tablet delayed release e c po twice a day outpatient lab work please have you inr checked saturday the results should be sent to your doctor at rehab multivitamin tablet sig one tablet po once a day morphine mg ml solution sig mg po every four hours as needed for pain warfarin mg tablet sig one tablet po once daily at pm discharge disposition extended care facility hospital3 hospital discharge diagnosis healthcare associated pneumonia colonic pseudo obstruction severe pulmonary hypertension cirrhosis htn chronic back pain right upper extremity dvt discharge condition good o2 sat high s on 3l nc bp stable 100s 110s doppler patient tolerating regular diet and having bowel movements not ambulatory discharge instructions you were admitted with a pneumonia which required a stay in our icu but did not require intubation or the use of a breathing tube you did need medications for a low blood pressure for the first several days of your hospitalization while you were here you also had problems with your intestines that caused them to stop working and food to get stuck in your intestines we treated you for this and you are now able to eat and move your bowels you got a blood clot in your arm while you were here we started you on iv medication for this initially but have now transitioned you to oral anticoagulants your doctor will need to monitor the levels of this medication at rehab you will likely need to remain on this medication for months please follow up as below you need to see a hepatologist liver doctor within the next month to follow up on your diagnosis of cirrhosis your list of medications is attached please call your doctor or return to the hospital if you have fevers shortness of breath chest pain abdominal pain vomitting inability to tolerate food or liquids by mouth dizziness or any other concerning symptoms followup instructions primary care first name11 name pattern1 last name namepattern4 md phone telephone fax date time dermatology name6 md name8 md md phone telephone fax date time you need to have your blood drawn to monitor your inr or coumadin level next on saturday the doctor at the rehab with change your coumadin dose accordingly please follow up with a hepatologist liver doctor within month if you would like to make an appointment at our liver center at hospital1 the number is telephone fax please follow up with a pulmonologist about your severe pulmonary hypertension within month if you do not have a pulmonologist and would like to follow up at hospital1 the number is telephone fax completed by',\n (#18) ['401.9','38.93','493.90','785.52','995.92','070.54','276.1','038.9','486','518.82'...])\n\n\n\ndsets.show(x)\n\nxxbos admission date discharge date date of birth sex f service medicine allergies nsaids aspirin influenza virus vaccine attending first name3 lf chief complaint hypotension resp distress major surgical or invasive procedure rij placement and removal last name un gastric tube placement and removal rectal tube placement and removal picc line placement and removal history of present illness ms known lastname is a 50f with cirrhosis of unproven etiology h o perforated duodenal ulcer who has had weakness and back pain with cough and shortness of breath for days she has chronic low back pain but this is in a different location she also reports fever and chills she reports increasing weakness and fatigue over the same amount of time she is followed by vna who saw her at home and secondary to hypotension and hypoxia requested she go to ed in the ed initial vs were afebrile and o2 sat unable to be read ct of torso showed pneumonia but no abdominal pathology she was seen by surgery she was found to be significantly hypoxic and started on nrb a right ij was placed she was started on levophed after a drop in her pressures in the micu she is conversant and able to tell her story past medical history asthma does not use inhalers htn off meds for several years rheumatoid arthritis seronegative chronic severe back pain s p c sections history of secondary syphilis treated polysubstance abuse notably cocaine but no drug or alcohol use for at least past months depression pulmonary hypertension severe on cardiac cath restrictive lung disease seizures in childhood cirrhosis by liver biopsy etiology as yet unknown duodenal ulcer s p surgical repair xxunk location un in summer social history lives with boyfriend who helps in her healthcare has children past h o cocaine abuse but not current denies any etoh drug or tobacco use since cirrhosis diagnosis in month only previously smoked up to pack per wk family history noncontributory physical exam on admission t hr bp r o2 sat on nrb general alert oriented moderate respiratory distress heent sclera anicteric drymm oropharynx clear neck supple jvp not elevated no lad lungs rhonchi and crackles in bases xxrep 3 l with egophany increased xxunk cv tachy rate and rhythm normal s1 s2 no murmurs rubs gallops abdomen soft diffusely tender non distended bowel sounds present healing midline ex lap wound dehiscence on caudal end but without erythema drainage ext warm well perfused pulses no clubbing cyanosis or edema pertinent results admission labs 13pm blood wbc rbc hgb hct mcv mch mchc rdw plt ct 13pm blood neuts bands lymphs monos eos baso 13pm blood pt ptt inr pt 13pm blood plt smr high plt ct 13pm blood glucose urean creat na k cl hco3 angap 13pm blood alt ast ld ldh alkphos amylase totbili 22am blood ck mb ctropnt 42pm blood probnp numeric identifier 13pm blood albumin calcium phos mg 22am blood cortsol 28pm blood type art o2 flow po2 pco2 ph caltco2 base xs intubat not intuba vent spontaneou comment non rebrea 20pm blood lactate discharge labs 58am blood wbc rbc hgb hct mcv mch mchc rdw plt ct 58am blood plt ct 58am blood pt ptt inr pt 58am blood glucose urean creat na k cl hco3 angap 58am blood alt ast ld ldh alkphos totbili 58am blood calcium phos mg imaging ct chest abd pelvis impression no etiology for abdominal pain identified no free air is noted within the abdomen interval increase in diffuse anasarca and bilateral pleural effusions small on the right and small to moderate on the left compressive atelectasis and new bilateral lower lobe pneumonias interval improvement in the degree of mediastinal lymphadenopathy interval development of small pericardial effusion no ct findings to suggest tamponade tte the left atrium is normal in size there is mild symmetric left ventricular hypertrophy with normal cavity size and regional global systolic function lvef the estimated cardiac index is borderline low 5l min m2 tissue doppler imaging suggests a normal left ventricular filling pressure pcwp 12mmhg the right ventricular free wall is hypertrophied the right ventricular cavity is markedly dilated with severe global free wall hypokinesis there is abnormal septal motion position consistent with right ventricular pressure volume overload the aortic valve leaflets appear structurally normal with good leaflet excursion and no aortic regurgitation the mitral valve leaflets are structurally normal mild mitral regurgitation is seen the tricuspid valve leaflets are mildly thickened there is mild moderate tricuspid xxunk there is severe pulmonary artery systolic hypertension there is a small circumferential pericardial effusion without echocardiographic signs of tamponade compared with the prior study images reviewed of the overall findings are similar ct abd pelvis impression anasarca increased bilateral pleural effusions small to moderate right greater than left trace pericardial fluid increased left lower lobe collapse consolidation right basilar atelectasis has also increased new ascites no free air dilated fluid filled colon and rectum no wall thickening to suggest colitis no evidence of small bowel obstruction bilat upper extremities venous ultrasound impression non occlusive right subclavian deep vein thrombosis cta chest impression no pulmonary embolism to the subsegmental level anasarca moderate to large left and moderate right pleural effusion small pericardial effusion unchanged right ventricle and right atrium enlargement since enlarged main pulmonary artery could be due to pulmonary hypertension complete collapse of the left lower lobe pneumonia can not be ruled out right basilar atelectasis kub impression unchanged borderline distended colon no evidence of small bowel obstruction or ileus brief hospital course this is a 50f with severe pulmonary hypertension h o duodenal perf s p surgical repair cirrhosis of unknown etiology and h o polysubstance abuse who presented with septic shock secondary pneumonia with respiratory distress septic shock pneumonia fever hypotension tachycardia hypoxia leukocytosis with xxrep 3 l infiltrate initially pt was treated for both health care associated pneumonia and possible c diff infection given wbc on admission however c diff treatment was stopped after patient tested negative x3 she completed a day course of vancomycin zosyn on for pneumonia she did initially require non rebreather in the icu but for the last week prior to discharge she has had stable o2 sats to mid s on nasal cannula she did also require pressors for the first several days of this hospitalization but was successfully weaned off pressors approximately wk prior to discharge at the time of discharge she is maintaining stable bp s 100s 110s doppler due to repeated ivf boluses for hypotension during this admission the pt developed anasarca and at the time of discharge is being slowly diuresed with lasix iv daily ogilvies pseudo obstruction the pt experience abd pain and distention with radiologic evidence of dilated loops of small bowel and large bowel during this hospitalization there was never any evidence of a transition point of stricture both a rectal and ngt were placed for decompression the pts symptoms and abd exam improved during the days prior to discharge and at the time of discharge she is tolerating a regular po diet and having normal bowel movements chronic back pain the pt has a long h o chronic back pain she was treated with iv morphine here for acute on chronic back pain likely exacerbated by bed rest and ileus at discharge she was transitioned to po morphine cirrhosis the etiology of this is unknown but the pt has biopsy proven cirrhosis from liver biopsy this likely explains her baseline coagulopathy and hypoalbuminemia here she needs to have outpt f u with hepatology she has no known complications of cirrhosis at this time rue dvt the pt has a rue dvt seen on u s associated with rij cvl which was placed in house she was started on a heparin gtt which was transitioned to coumadin prior to discharge at discharge her inr is she should have follow up lab work on friday of note at discharge the pt had several healing boils on her inner upper thighs thought to be trauma from her foley rubbing against the skin no further treatment was thought to be neccessary for these medications on admission clobetasol cream apply to affected area twice a day as needed gabapentin neurontin mg tablet tablet s by mouth three times a day metoprolol tartrate mg tablet tablet s by mouth twice a day omeprazole mg capsule delayed release e c capsule delayed release e c s by mouth once a day oxycodone mg capsule capsule s by mouth q hours physical therapy dx first name9 namepattern2 location un evaluation and management of motor skills tramadol mg tablet tablet s by mouth twice a day as needed for pain trazodone mg tablet tablet s by mouth hs medications otc docusate sodium mg ml liquid teaspoon by mouth twice a day as needed magnesium oxide mg tablet tablet s by mouth twice a day discharge medications ipratropium bromide solution sig one nebulizer inhalation q6h every hours clobetasol ointment sig one appl topical hospital1 times a day lidocaine mg patch adhesive patch medicated sig one adhesive patch medicated topical qday as needed for back pain please wear on skin hrs then have hrs with patch off hemorrhoidal suppository suppository sig one suppository rectal once a day as needed for pain pantoprazole mg tablet delayed release e c sig one tablet delayed release e c po twice a day outpatient lab work please have you inr checked saturday the results should be sent to your doctor at rehab multivitamin tablet sig one tablet po once a day morphine mg ml solution sig mg po every four hours as needed for pain warfarin mg tablet sig one tablet po once daily at pm discharge disposition extended care facility hospital3 hospital discharge diagnosis healthcare associated pneumonia colonic pseudo obstruction severe pulmonary hypertension cirrhosis htn chronic back pain right upper extremity dvt discharge condition good o2 sat high s on 3l nc bp stable 100s 110s doppler patient tolerating regular diet and having bowel movements not ambulatory discharge instructions you were admitted with a pneumonia which required a stay in our icu but did not require intubation or the use of a breathing tube you did need medications for a low blood pressure for the first several days of your hospitalization while you were here you also had problems with your intestines that caused them to stop working and food to get stuck in your intestines we treated you for this and you are now able to eat and move your bowels you got a blood clot in your arm while you were here we started you on iv medication for this initially but have now transitioned you to oral anticoagulants your doctor will need to monitor the levels of this medication at rehab you will likely need to remain on this medication for months please follow up as below you need to see a hepatologist liver doctor within the next month to follow up on your diagnosis of cirrhosis your list of medications is attached please call your doctor or return to the hospital if you have fevers shortness of breath chest pain abdominal pain vomitting inability to tolerate food or liquids by mouth dizziness or any other concerning symptoms followup instructions primary care first name11 name pattern1 last name namepattern4 md phone telephone fax date time dermatology name6 md name8 md md phone telephone fax date time you need to have your blood drawn to monitor your inr or coumadin level next on saturday the doctor at the rehab with change your coumadin dose accordingly please follow up with a hepatologist liver doctor within month if you would like to make an appointment at our liver center at hospital1 the number is telephone fax please follow up with a pulmonologist about your severe pulmonary hypertension within month if you do not have a pulmonologist and would like to follow up at hospital1 the number is telephone fax completed by\n401.9;38.93;493.90;785.52;995.92;070.54;276.1;038.9;486;518.82;453.8;714.0;571.5;996.74;560.1;96.07;416.0;96.09\n\n\n\nassert isinstance(dsets.tfms[0], Pipeline) # `Pipeline` of the `x_tfms`\nassert isinstance(dsets.tfms[0][0], Tokenizer)\nassert isinstance(dsets.tfms[0][1], Transform)\nassert isinstance(dsets.tfms[0][2], Numericalize)\n\nIf we just want to decode the one-hot encoded dependent variable:\n\n_ind, _dep = x\n_lbl = dsets.tfms[1].decode(_dep)\ntest_eq(_lbl, array(lbls)[_dep.nonzero().flatten().int().numpy()])\n\nLet’s extract the MultiCategorize transform applied by dsets on the dependent variable so that we can apply it ourselves:\n\ntfm_cat = dsets.tfms[1][1]\ntest_eq(str(tfm_cat.__class__), \"&lt;class 'fastai.data.transforms.MultiCategorize'&gt;\")\n\nvocab attribute of the MultiCategorize transform stores the category vocabulary. If it was specified from outside (in this case it was) then the MultiCategorize transform will not sort the vocabulary otherwise it will.\n\ntest_eq(lbls, tfm_cat.vocab)\ntest_ne(lbls, sorted(tfm_cat.vocab))\n\n\ntest_eq(str(_lbl.__class__), \"&lt;class 'fastai.data.transforms.MultiCategory'&gt;\")\ntest_eq(tfm_cat(_lbl), TensorMultiCategory([lbls.index(o) for o in _lbl]))\ntest_eq(tfm_cat.decode(tfm_cat(_lbl)), _lbl)\n\nLet’s check the reverse:\n\n# dsets_r.decode(x_r)\n\n\n# dsets_r.show(x_r)\n\nLooks pretty good!\n\n\n3. Making the DataLoaders object:\nWe need to pick the sequence length and the batch size (you might have to adjust this depending on you GPU size)\n\nbs, sl = 16, 72\n\nWe will use the dl_type argument of the DataLoaders. The purpose is to tell DataLoaders to use SortedDL class of the DataLoader, and not the usual one. SortedDL constructs batches by putting samples of roughly the same lengths into batches.\n\ndl_type = partial(SortedDL, shuffle=True)\n\nCrucial: - We will use pad_input_chunk because our encoder AWD_LSTM will be wrapped inside SentenceEncoder. - A SenetenceEncoder expects that all the documents are padded, - with most of the padding at the beginning of the document, with each sequence beginning at a round multiple of bptt - and the rest of the padding at the end.\n\ndls_clas = dsets.dataloaders(bs=bs, seq_len=sl, \n                             dl_type=dl_type,\n                             before_batch=pad_input_chunk)\n\nFor the reverse:\n\ndls_clas_r = dsets_r.dataloaders(bs=bs, seq_len=sl,\n                                 dl_type=dl_type,\n                                 before_batch=pad_input_chunk)\n\nCreating the DataLoaders object takes considerable amount of time, so do save it when working on your dataset. In this though, (as always) untar_xxx downloaded it for you:\n\n!tree -shLD 1 {source} -P *clas*\n# or using glob\n# L(source.glob(\"**/*clas*\"))\n\n/home/deb/.xcube/data/mimic3\n├── [192M May  1  2022]  mimic3-9k_clas.pth\n├── [758K Mar 27 17:59]  mimic3-9k_clas_full_vocab.pkl\n├── [1.6G Apr 21 18:29]  mimic3-9k_dls_clas.pkl\n├── [1.6G Apr  5 17:17]  mimic3-9k_dls_clas_old_remove_later.pkl\n├── [1.6G Apr 21 18:30]  mimic3-9k_dls_clas_r.pkl\n└── [1.6G Apr  5 17:18]  mimic3-9k_dls_clas_r_old_remove_later.pkl\n\n0 directories, 6 files\n\n\nAside: Some handy linux find tricks: 1. https://stackoverflow.com/questions/18312935/find-file-in-linux-then-report-the-size-of-file-searched 2. https://stackoverflow.com/questions/4210042/how-to-exclude-a-directory-in-find-command\n\n# !find -path ./models -prune -o -type f -name \"*caml*\" -exec du -sh {} \\;\n# !find -not -path \"./data/*\" -type f -name \"*caml*\" -exec du -sh {} \\;\n# !find {path_data} -type f -name \"*caml*\" | xargs du -sh\n\nIf you want to load the dls for the full dataset:\n\ndls_clas = torch.load(source/'mimic3-9k_dls_clas.pkl')\ndls_clas_r = torch.load(source/'mimic3-9k_dls_clas_r.pkl')\n\nCPU times: user 13.1 s, sys: 3.16 s, total: 16.2 s\nWall time: 16.6 s\n\n\nLet’s take a look at the data:\n\n# dls_clas.show_batch(max_n=3)\n\n\n# dls_clas_r.show_batch(max_n=3)\n\n\n\n4. (Optional) Making the DataLoaders using fastai’s DataBlock API:\nIt’s worth mentioning here that all the steps we performed to create the DataLoaders can be packaged together using fastai’s DataBlock API.\n\ndblock = DataBlock(\n        blocks = (TextBlock.from_df('text', seq_len=sl, vocab=lm_vocab), MultiCategoryBlock(vocab=lbls)),\n        get_x = ColReader('text'),\n        get_y = ColReader('labels', label_delim=';'),\n        splitter = splitter,\n        dl_type = dl_type,\n)\n\n\ndls_clas = dblock.dataloaders(df, bs=bs, before_batch=pad_input_chunk)\n\n\n\n\n\n\n\n\n\ndls_clas.show_batch(max_n=5)"
  },
  {
    "objectID": "tutorial.train_mimic3.html#learner-for-multi-label-classifier-fine-tuning",
    "href": "tutorial.train_mimic3.html#learner-for-multi-label-classifier-fine-tuning",
    "title": "Training an XML Text Classifier",
    "section": "Learner for Multi-Label Classifier Fine-Tuning",
    "text": "Learner for Multi-Label Classifier Fine-Tuning\n\n# set_seed(897997989, reproducible=True)\n# set_seed(67, reproducible=True)\nset_seed(1, reproducible=True)\n\nThis is where we have dls_clas(for the full dataset) we made in the previous section:\n\n!tree -shDL 1 {source} -P \"*clas*\"\n# or using glob\n# L(source.glob(\"**/*clas*\"))\n\n/home/deb/.xcube/data/mimic3\n├── [576M Jun  8 13:21]  mimic3-9k_clas_full.pth\n├── [576M May 26 12:00]  mimic3-9k_clas_full_r.pth\n├── [758K Mar 27 17:59]  mimic3-9k_clas_full_vocab.pkl\n├── [1.6G Apr 21 18:29]  mimic3-9k_dls_clas.pkl\n└── [1.6G Apr 21 18:30]  mimic3-9k_dls_clas_r.pkl\n\n0 directories, 5 files\n\n\nAnd this is where we have the finetuned language model:\n\n!tree -shDL 1 {source} -P '*fine*'\n\n/home/deb/.xcube/data/mimic3\n├── [165M Apr 30  2022]  mimic3-9k_lm_finetuned.pth\n└── [165M May  7  2022]  mimic3-9k_lm_finetuned_r.pth\n\n0 directories, 2 files\n\n\nAnd this is where we have the bootstrapped brain and the label biases:\n\n!tree -shDL 1 {source_l2r} -P \"*tok_lbl_info*|*p_L*\"\n\n/home/deb/.xcube/data/mimic3_l2r\n├── [3.8G Jun 24  2022]  mimic3-9k_tok_lbl_info.pkl\n└── [ 70K Apr  3 18:35]  p_L.pkl\n\n0 directories, 2 files\n\n\nNext we’ll create a tmp directory to store results. In order for our learner to have access to the finetuned language model we need to symlink to it.\n\ntmp = Path.cwd()/'tmp/models'\ntmp.mkdir(exist_ok=True, parents=True)\ntmp = tmp.parent\n# (tmp/'models'/'mimic3-9k_lm_decoder.pth').symlink_to(source/'mimic3-9k_lm_decoder.pth') # run this just once\n# (tmp/'models'/'mimic3-9k_lm_decoder_r.pth').symlink_to(source/'mimic3-9k_lm_decoder_r.pth') # run this just once\n# (tmp/'models'/'mimic3-9k_lm_finetuned.pth').symlink_to(source/'mimic3-9k_lm_finetuned.pth') # run this just once\n# (tmp/'models'/'mimic3-9k_lm_finetuned_r.pth').symlink_to(source/'mimic3-9k_lm_finetuned_r.pth') # run this just once\n# (tmp/'models'/'mimic3-9k_tok_lbl_info.pkl').symlink_to(join_path_file('mimic3-9k_tok_lbl_info', source_l2r, ext='.pkl')) #run this just once\n# (tmp/'models'/'p_L.pkl').symlink_to(join_path_file('p_L', source_l2r, ext='.pkl')) #run this just once\n# (tmp/'models'/'lin_lambdarank_full.pth').symlink_to(join_path_file('lin_lambdarank_full', source_l2r, ext='.pth')) #run this just once\n# list_files(tmp)\n!tree -shD {tmp}\n\n/home/deb/xcube/nbs/tmp\n├── [ 23G Feb 21 13:27]  dls_full.pkl\n├── [1.7M Feb 10 16:40]  dls_tiny.pkl\n├── [152M Mar  8 00:28]  lin_lambdarank_full.pth\n├── [1.0M Mar  7 16:52]  lin_lambdarank_tiny.pth\n├── [ 10M Apr 21 17:48]  mimic3-9k_dls_clas_tiny.pkl\n├── [ 10M Apr 21 17:48]  mimic3-9k_dls_clas_tiny_r.pkl\n├── [4.0K Jun  7 17:39]  models\n│   ├── [  56 Apr 17 17:29]  lin_lambdarank_full.pth -&gt; /home/deb/.xcube/data/mimic3_l2r/lin_lambdarank_full.pth\n│   ├── [ 11K Apr 17 13:17]  log.csv\n│   ├── [576M Jun  8 02:04]  mimic3-9k_clas_full.pth\n│   ├── [5.4G May 10 18:21]  mimic3-9k_clas_full_predslog.pkl\n│   ├── [576M May 26 11:55]  mimic3-9k_clas_full_r.pth\n│   ├── [758K May 26 11:55]  mimic3-9k_clas_full_r_vocab.pkl\n│   ├── [264K May 10 18:35]  mimic3-9k_clas_full_rank.csv\n│   ├── [758K Jun  8 02:04]  mimic3-9k_clas_full_vocab.pkl\n│   ├── [196M Jun  3 13:21]  mimic3-9k_clas_tiny.pth\n│   ├── [273M Apr  3 13:19]  mimic3-9k_clas_tiny_r.pth\n│   ├── [635K Apr  3 13:19]  mimic3-9k_clas_tiny_r_vocab.pkl\n│   ├── [635K Jun  3 13:21]  mimic3-9k_clas_tiny_vocab.pkl\n│   ├── [  53 Jun  7 17:39]  mimic3-9k_lm_decoder.pth -&gt; /home/deb/.xcube/data/mimic3/mimic3-9k_lm_decoder.pth\n│   ├── [  55 Jun  7 17:39]  mimic3-9k_lm_decoder_r.pth -&gt; /home/deb/.xcube/data/mimic3/mimic3-9k_lm_decoder_r.pth\n│   ├── [  55 Mar  8 16:09]  mimic3-9k_lm_finetuned.pth -&gt; /home/deb/.xcube/data/mimic3/mimic3-9k_lm_finetuned.pth\n│   ├── [  57 Mar 19 19:27]  mimic3-9k_lm_finetuned_r.pth -&gt; /home/deb/.xcube/data/mimic3/mimic3-9k_lm_finetuned_r.pth\n│   ├── [  59 Mar 29 17:36]  mimic3-9k_tok_lbl_info.pkl -&gt; /home/deb/.xcube/data/mimic3_l2r/mimic3-9k_tok_lbl_info.pkl\n│   ├── [758K Apr 15 14:29]  mimic5_tmp_vocab.pkl\n│   └── [  40 Apr  3 20:03]  p_L.pkl -&gt; /home/deb/.xcube/data/mimic3_l2r/p_L.pkl\n└── [1.2M Mar  7 16:46]  nn_lambdarank_tiny.pth\n\n1 directory, 26 files\n\n\nLet’s now get the dataloaders for the classifier. We’ll also save classifier with fname.\n\n# fname = 'mimic3-9k_clas_tiny'\nfname = 'mimic3-9k_clas_full'\n\n\nif 'tiny' in fname:\n    dls_clas = torch.load(tmp/'mimic3-9k_dls_clas_tiny.pkl', map_location=default_device())\n    dls_clas_r = torch.load(tmp/'mimic3-9k_dls_clas_tiny_r.pkl')\n    # dls_clas.show_batch(max_n=5)\nelif 'full' in fname:\n    dls_clas = torch.load(source/'mimic3-9k_dls_clas.pkl')\n    dls_clas_r = torch.load(source/'mimic3-9k_dls_clas_r.pkl')\n    # dls_clas.show_batch(max_n=5)\n\nLet’s create the saving callback upfront:\n\nfname_r = fname+'_r'\ncbs=SaveModelCallback(monitor='valid_precision_at_k', fname=fname, with_opt=True, reset_on_fit=True)\ncbs_r=SaveModelCallback(monitor='valid_precision_at_k', fname=fname_r, with_opt=True, reset_on_fit=True)\n\nWe will make the TextLearner (Here you can use pretrained=False to save time beacuse we are anyway going to load_encoder later which will replace the encoder wgts with the ones we that we have in the fine-tuned LM):\nTodo(Deb): Implement TTA - make max_len=None during validation - magnify important tokens\n\nlearn = xmltext_classifier_learner(dls_clas, AWD_LSTM, drop_mult=0.1, max_len=None,#72*40, \n                                   metrics=partial(precision_at_k, k=15), path=tmp, cbs=cbs,\n                                   pretrained=False,\n                                   splitter=None,\n                                   running_decoder=True).to_fp16()\nlearn_r = xmltext_classifier_learner(dls_clas_r, AWD_LSTM, drop_mult=0.1, max_len=None,#72*40, \n                                     metrics=partial(precision_at_k, k=15), path=tmp, cbs=cbs_r,\n                                     pretrained=False,\n                                     splitter=None,\n                                     running_decoder=True).to_fp16()\n\n\n# for i,g in enumerate(awd_lstm_xclas_split(learn.model)):\n    # print(f\"group: {i}, {g=}\")\n    # print(\"****\")\n\nNote: Don’t forget to check k in inattention\nA few customizations into fastai’s callbacks:\nTo tracks metrics on training bactches during an epoch:\n\n# tell `Recorder` to track `train_metrics`\nassert learn.cbs[1].__class__ is Recorder\nsetattr(learn.cbs[1], 'train_metrics', True)\nassert learn_r.cbs[1].__class__ is Recorder\nsetattr(learn_r.cbs[1], 'train_metrics', True)\n\n\nimport copy\nmets = copy.deepcopy(learn.recorder._valid_mets)\n# mets = L(AvgSmoothLoss(), AvgMetric(precision_at_k))\nrv = RunvalCallback(mets)\nlearn.add_cbs(rv)\nlearn.cbs\n\n(#9) [TrainEvalCallback,Recorder,CastToTensor,ProgressCallback,SaveModelCallback,ModelResetter,RNNCallback,MixedPrecision,RunvalCallback]\n\n\n\n@patch\ndef after_batch(self: ProgressCallback):\n        self.pbar.update(self.iter+1)\n        mets = ('_valid_mets', '_train_mets')[self.training]\n        self.pbar.comment = ' '.join([f'{met.name} = {met.value.item():.4f}' for met in getattr(self.recorder, mets)])\n\nThe following line essentially captures the magic of ULMFit’s transfer learning:\n\nlearn = learn.load_encoder('mimic3-9k_lm_finetuned')\nlearn_r = learn_r.load_encoder('mimic3-9k_lm_finetuned_r')\n\n\n# learn = learn.load_brain('mimic3-9k_tok_lbl_info', 'p_L')\n# learn_r = learn_r.load_brain('mimic3-9k_tok_lbl_info')\n\n\n\nsv_idx = learn.cbs.attrgot('__class__').index(SaveModelCallback)\nlearn.cbs[sv_idx]\nwith learn.removed_cbs(learn.cbs[sv_idx]):\n    learn.fit(1, lr=1e-2)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\ntrain_precision_at_k\nvalid_loss\nvalid_precision_at_k\ntime\n\n\n\n\n0\n0.012547\n0.403337\n0.013126\n0.470087\n41:55\n\n\n\n\n\n\n# os.getpid()\n# learn.fit(3, lr=3e-2)\n\n\n\n\n\n\n    \n      \n      66.67% [2/3 40:17&lt;20:08]\n    \n    \n\n\n\n\nepoch\ntrain_loss\ntrain_precision_at_k\nvalid_loss\nvalid_precision_at_k\ntime\n\n\n\n\n0\n0.010768\n0.434745\n0.011315\n0.491044\n20:15\n\n\n1\n0.010263\n0.465117\n0.011162\n0.494939\n20:01\n\n\n\n\n\n    \n      \n      3.50% [108/3084 00:40&lt;18:25 avg_smooth_loss = 0.010285338386893272 precision_at_k = 0.46471962616822415 ]\n    \n    \n\n\nBetter model found at epoch 0 with valid_precision_at_k value: 0.4910438908659549.\nBetter model found at epoch 1 with valid_precision_at_k value: 0.4949387109529458.\n\n\n\nlearn = learn.load((learn.path/learn.model_dir)/fname)\nvalidate(learn)\n\n\n# os.getpid()\n# sorted(learn.cbs.zipwith(learn.cbs.attrgot('order')), key=lambda tup: tup[1] )\n\n\n\nclass _FakeLearner: \n    def to_detach(self,b,cpu=True,gather=True):\n        return to_detach(b,cpu,gather)\n_fake_l = _FakeLearner()\n\ndef cpupy(t): return t.cpu().numpy() if isinstance(t, Tensor) else t\nlearn.model = learn.model.to('cuda:0')\n\nTODO: - Also print avg text lengths\n\n# import copy\n# mets = L(AvgLoss(), AvgMetric(partial(precision_at_k, k=15)))\nmets = L(F1ScoreMulti(thresh=0.14, average='macro'), F1ScoreMulti(thresh=0.14, average='micro'))  #(learn.recorder._valid_mets)\nlearn.model.eval()\nmets.map(Self.reset())\npbar = progress_bar(learn.dls.valid)\nlog_file = join_path_file('log', learn.path/learn.model_dir, ext='.csv')\nwith open(log_file, 'w', newline='') as csvfile:\n    writer = csv.writer(csvfile)\n    header = mets.attrgot('name') + L('bs', 'n_lbs', 'mean_nlbs')\n    writer.writerow(header)\n    for i, (xb, yb) in enumerate(pbar):\n        _fake_l.yb = (yb,)\n        _fake_l.y = yb\n        _fake_l.pred, *_ = learn.model(xb) \n        _fake_l.loss = Tensor(learn.loss_func(_fake_l.pred, yb))\n        for met in mets: met.accumulate(_fake_l)\n        pbar.comment = ' '.join(mets.attrgot('value').map(str))\n        yb_nlbs =  Tensor(yb.count_nonzero(dim=1)).float().cpu().numpy()\n        writer.writerow(mets.attrgot('value').map(cpupy) + L(find_bs(yb), yb_nlbs, yb_nlbs.mean()))\n\n\npd.set_option('display.max_rows', 100)\ndf = pd.read_csv(log_file)\ndf\n\n\n# learn.model[1].label_bias.data.min(),  learn.model[1].label_bias.data.max()\n# nn.init.kaiming_normal_(learn.model[1].label_bias.data.unsqueeze(-1))\n# # init_default??\n# with torch.no_grad(): \n    # learn.model[1].label_bias.data = learn.lbsbias\n# learn.model[1].label_bias.data.min(),  learn.model[1].label_bias.data.max()\n# learn.model[1].pay_attn.attn.func.f#, learn_r.model[1].pay_attn.attn.func.f\n# set_seed(1, reproducible=True)"
  },
  {
    "objectID": "tutorial.train_mimic3.html#dev-ignore",
    "href": "tutorial.train_mimic3.html#dev-ignore",
    "title": "Training an XML Text Classifier",
    "section": "Dev (Ignore)",
    "text": "Dev (Ignore)\n\ndf_des = pd.read_csv(source/'code_descriptions.csv')\n\n\nlearn = learn.load_brain('mimic3-9k_tok_lbl_info')\nlearn.brain.shape\n\nPerforming brainsplant...\nSuccessfull!\n\n\n\n\n\n\n\n\n\ntorch.Size([57376, 1271])\n\n\n\nL(learn.dls.vocab[0]), L(learn.dls.vocab[1])\n\n((#57376) ['xxunk','xxpad','xxbos','xxeos','xxfld','xxrep','xxwrep','xxup','xxmaj','the'...],\n (#1271) ['431','507.0','518.81','112.0','287.4','401.9','427.89','600.00','272.4','300.4'...])\n\n\n\nrnd_code = '733.00' #random.choice(learn.dls.vocab[1]) # '96.04'\ndes = load_pickle(join_path_file('code_desc', source, ext='.pkl'))\nk=20\nidx, *_ = dls_clas.vocab[1].map_objs([rnd_code])\nprint(f\"For the icd code {rnd_code} which means {des[rnd_code]}, the top {k} tokens in the brain are:\")\nprint('\\n'.join(L(array(learn.dls.vocab[0])[learn.brain[:, idx].topk(k=k).indices.cpu()], use_list=True)))\n\nFor the icd code 733.00 which means Osteoporosis, unspecified, the top 20 tokens in the brain are:\nosteoporosis\nfosamax\nalendronate\nd3\ncarbonate\ncholecalciferol\n70\nosteoperosis\nactonel\nshe\nher\nhe\nhis\nvitamin\nrisedronate\nqweek\ncompression\nraloxifene\nms\nmale\n\n\nLet’s pull out a text from the batch and take a look at the text and its codes:\n\nxb, yb = dls_clas.one_batch()\ni = 3\ntext = ' '.join([dls_clas.vocab[0][o] for o in xb[3] if o !=1])\ncodes = dls_clas.tfms[1].decode(yb[3])\nprint(f\"The text is {len(text)} words long and has {len(codes)} codes\")\n\nThe text is 25411 words long and has 36 codes\n\n\n\ndf_codes = pd.DataFrame(columns=['code', 'description', 'freq', 'top_toks'])\ndf_codes['code'] = codes\ndf_codes['description'] = mapt(des.get, codes)\ndf_codes['freq'] = mapt(lbl_freqs.get, codes)\nidxs = learn.dls.vocab[1].map_objs(codes)\ntop_toks = array(learn.dls.vocab[0])[learn.brain[:, idxs].topk(k=k, dim=0).indices.cpu()].T\ntop_vals = learn.brain[:, idxs].topk(k=k, dim=0).values.cpu().T\ndf_codes['top_toks'] = L(top_toks, use_list=True).map(list)\ndf_codes\n\n\n\n\n\n\n\n\ncode\ndescription\nfreq\ntop_toks\n\n\n\n\n0\n507.0\nPneumonitis due to inhalation of food or vomitus\n15\n[aspiration, pneumonia, pneumonitis, pna, flagyl, swallow, peg, sputum, intubation, secretions, levofloxacin, aspirated, suctioning, video, hypoxic, opacities, tube, zosyn, hypoxia, vancomycin]\n\n\n1\n518.81\nAcute respiratory failure\n42\n[intubation, intubated, peep, failure, respiratory, fio2, hypoxic, extubation, sputum, pneumonia, sedated, abg, endotracheal, hypercarbic, expired, vent, hypoxia, pco2, aspiration, vancomycin]\n\n\n2\n96.04\nInsertion of endotracheal tube\n48\n[intubated, intubation, extubation, surfactant, extubated, endotracheal, respiratory, peep, sedated, airway, ventilator, ventilation, protection, fio2, reintubated, tube, expired, vent, feeds, et]\n\n\n3\n96.72\nContinuous mechanical ventilation for 96 consecutive hours or more\n35\n[intubated, tracheostomy, ventilator, trach, vent, intubation, tube, sputum, peg, peep, extubation, ventilation, wean, feeds, secretions, bal, sedated, weaning, fio2, reintubated]\n\n\n4\n99.15\nParenteral infusion of concentrated nutritional substances\n22\n[tpn, nutrition, parenteral, prematurity, phototherapy, feeds, hyperbilirubinemia, immunizations, preterm, circumference, enteral, newborn, pediatrician, infant, caffeine, prenatal, infants, rubella, surfactant, percentile]\n\n\n5\n38.93\nVenous catheterization, not elsewhere classified\n77\n[picc, line, vancomycin, central, zosyn, cultures, grew, placement, flagyl, vanco, tpn, septic, recon, placed, flush, levophed, bacteremia, intubated, ij, soln]\n\n\n6\n785.52\nSeptic shock\n17\n[septic, shock, levophed, pressors, sepsis, hypotension, zosyn, vasopressin, hypotensive, myelos, meropenem, broad, metas, atyps, expired, pressor, spectrum, urosepsis, vancomycin, vanc]\n\n\n7\n995.92\nSevere sepsis\n23\n[septic, shock, sepsis, levophed, pressors, hypotension, zosyn, meropenem, expired, vancomycin, myelos, metas, atyps, vanco, bacteremia, vanc, broad, hypotensive, cefepime, spectrum]\n\n\n8\n518.0\nPulmonary collapse\n9\n[collapse, atelectasis, bronchoscopy, bronchus, plugging, plug, lobe, collapsed, bronch, pleural, secretions, spirometry, thoracentesis, incentive, newborn, rubella, infant, immunizations, hemithorax, pediatrician]\n\n\n9\n584.9\nAcute renal failure, unspecified\n49\n[renal, arf, failure, cr, prerenal, baseline, creatinine, kidney, medicine, acute, likely, setting, elevated, held, urine, cxr, ed, infant, chf, improved]\n\n\n10\n799.02\nHypoxemia\n8\n[hypoxia, hypoxemia, hypoxic, nrb, cxr, medquist36, invasive, 4l, major, attending, job, brief, medicine, chief, dictated, complaint, 6l, legionella, probnp, procedure]\n\n\n11\n427.31\nAtrial fibrillation\n56\n[fibrillation, atrial, afib, amiodarone, coumadin, rvr, fib, warfarin, digoxin, irregular, irregularly, diltiazem, converted, cardioversion, af, anticoagulation, paroxysmal, metoprolol, paf, irreg]\n\n\n12\n599.0\nUrinary tract infection, site not specified\n33\n[uti, tract, urinary, nitrofurantoin, coli, ciprofloxacin, urosepsis, urine, sensitivities, ua, tobramycin, organisms, sulbactam, sensitive, meropenem, infection, mic, cipro, ceftazidime, piperacillin]\n\n\n13\n285.9\nAnemia, unspecified\n32\n[anemia, newborn, infant, immunizations, rubella, pediatrician, prenatal, circumference, allergies, phototherapy, prematurity, gestation, normocytic, seat, medical, infants, medquist36, invasive, nonreactive, apgars]\n\n\n14\n486\nPneumonia, organism unspecified\n25\n[pneumonia, acquired, pna, levofloxacin, azithromycin, legionella, community, sputum, lobe, infiltrate, opacity, consolidation, productive, rll, opacities, cxr, multifocal, hcap, ceftriaxone, vancomycin]\n\n\n15\n038.42\nSepticemia due to escherichia coli [E. coli]\n6\n[coli, escherichia, urosepsis, sulbactam, ecoli, tazo, gnr, tobramycin, septicemia, pyelonephritis, cefazolin, piperacillin, cefuroxime, ceftazidime, interpretative, meropenem, bacteremia, sensitivities, mic, 57]\n\n\n16\n733.00\nOsteoporosis, unspecified\n20\n[osteoporosis, fosamax, alendronate, d3, carbonate, cholecalciferol, 70, osteoperosis, actonel, she, her, he, his, vitamin, risedronate, qweek, compression, raloxifene, ms, male]\n\n\n17\n591\nHydronephrosis\n3\n[nephrostomy, ureter, ureteral, hydroureter, hydronephrosis, urology, hydroureteronephrosis, obstructing, pyelonephritis, uropathy, collecting, nephrostogram, urosepsis, upj, ureteropelvic, calculus, perinephric, uvj, stone, hydro]\n\n\n18\n276.1\nHyposmolality and/or hyponatremia\n17\n[hyponatremia, hyponatremic, osmolal, hypovolemic, siadh, restriction, paracentesis, hypervolemic, ascites, cirrhosis, rifaximin, spironolactone, meld, ns, portal, icteric, likely, medicine, encephalopathy, hypovolemia]\n\n\n19\n560.1\nParalytic ileus\n5\n[ileus, kub, loops, flatus, tpn, ngt, distension, illeus, obstruction, sbo, exploratory, laparotomy, clears, decompression, distention, sips, adhesions, npo, ng, passing]\n\n\n20\nE849.7\nAccidents occurring in residential institution\n9\n[major, medquist36, attending, brief, invasive, job, surgical, procedure, chief, complaint, diagnosis, instructions, followup, pertinent, allergies, 8cm2, daily, dictated, disposition, results]\n\n\n21\nE870.8\nAccidental cut, puncture, perforation or hemorrhage during other specified medical care\n3\n[recannulate, suboptimald, whistle, trickle, insom, tumeric, erythematosis, advertisement, choledochotomy, disaese, xfusions, ameniable, patrial, intrasinus, reproduceable, adamsts, valgan, interscalene, duodenorrhaphy, strattice]\n\n\n22\n788.30\nUrinary incontinence, unspecified\n2\n[incontinence, incontinent, detrol, oxybutynin, urinary, aledronate, incontinance, tolterodine, t34, vesicare, condom, tenders, urinal, thyrodectomy, arthritides, urge, solifenacin, hospitalize, oxybutinin, sparc]\n\n\n23\n562.10\nDiverticulosis of colon (without mention of hemorrhage)\n4\n[diverticulosis, sigmoid, cecum, diverticulitis, colonoscopy, diverticula, diverticular, hemorrhoids, diverticuli, polypectomy, colon, colonic, polyp, prep, colonscopy, tagged, lgib, maroon, brbpr, polyps]\n\n\n24\nE879.6\nUrinary catheterization as the cause of abnormal reaction of patient, or of later complication, without mention of misadventure at time of procedure\n2\n[indwelling, urethra, insufficicency, hyposensitive, nonambulating, urology, methenamine, suprapubic, fosfomycin, neurogenic, utis, mirabilis, sporogenes, mdr, pyuria, urologist, urosepsis, policeman, uti, vse]\n\n\n25\n401.1\nBenign essential hypertension\n4\n[diarhea, medquist36, job, feline, ronchi, osteosclerotic, invasive, major, attending, septicemia, brief, dictated, benzodiazepene, unprovoked, caox3, rhinorrhea, metbolic, lak, lext, procedure]\n\n\n26\n250.22\nDiabetes mellitustype II [non-insulin dependent type] [NIDDM type] [adult-onset type] or unspecified type with hyperosmolarity, uncontrolled\n2\n[hyperosmolar, nonketotic, hhs, hhnk, ketotic, hyperosmotic, honk, honc, hyperglycemic, flatbush, sniffs, pelvocaliectasis, pseudohyponatremia, ha1c, furosemdie, 25gx1, nistatin, hypersomolar, 05u, guaaic]\n\n\n27\n345.80\nOther forms of epilepsy, without mention of intractable epilepsy\n1\n[forefinger, indentified, emu, overshunting, midac, gliosarcoma, foscarnate, signifcantly, mirroring, jamacia, 4tab, t34, fibrinolytic, 3tab, majestic, persecutory, uncomfirmed, polyspike, seperated, hoffmans]\n\n\n28\n996.76\nOther complications due to genitourinary device, implant, and graft\n1\n[paraphrasias, reinstuted, tonics, qoday, choreoathetosis, infilterates, extravesical, acetoacetate, uteral, hematuric, patell, clipboard, peform, cytomorphology, pirbuterol, athetosis, suborbital, axon, stumbles, summetric]\n\n\n29\n560.39\nOther impaction of intestine\n1\n[disimpaction, disimpacted, impaction, disimpact, suds, disempacted, cathartics, biscodyl, mebaral, enemas, divigel, addisonian, colocolostomy, fecal, manually, ileosigmoid, obstipation, periappendiceal, somatropin, liquied]\n\n\n30\n550.10\nUnilateral or unspecified inguinal hernia with obstruction, without mention of gangrene (not specified as recurrent)\n1\n[dwarfism, toilets, irreducible, nonreducible, intenal, diseection, reduceable, purp, lee, cerfactin, extravesical, stimata, hemiscrotal, pleuraleffusions, diversions, desireable, incarcerated, bogata, txplnt, plce]\n\n\n31\n595.9\nCystitis, unspecified\n1\n[bulsulfan, trabeculae, cystitis, heliotrope, roscea, ileous, intertitial, 66yrs, extravesical, dentention, cephalasporin, thyrodectomy, rangin, musculotendinous, valcade, postcricoid, replacemet, pericystic, pyocystitis, cylosporine]\n\n\n32\n319\nUnspecified mental retardation\n1\n[retardation, zonegran, hemispherectomy, gastaut, guardian, retarded, zonisamide, epilepsy, phenoba, hypoid, mentral, epileptologist, phenobarbital, valproic, crichoid, neurodermatitis, stimulator, aeds, tracheobroncomalacia, developmental]\n\n\n33\n542\nOther appendicitis\n1\n[atmospheric, vioptics, comedo, apriso, extravesical, revisional, osteoporsis, plce, resuscited, retrocecal, improvemed, impre, tubule, periappendiceal, castor, paratubal, nebullizer, pleomorphism, feamle, mebaral]\n\n\n34\n596.8\nOther specified disorders of bladder\n1\n[presbylaryngis, spout, urothelial, satellitosis, anascarca, melanophages, hemartoma, amplicillin, refexes, glassess, hypogammaglobulinemic, senstitve, pseuduomonas, deificts, chux, superpubic, adenoviremia, curator, valvulopathies, tristar]\n\n\n35\n87.77\nOther cystogram\n1\n[cystogram, hematurea, pupuric, movmement, pyocystitis, malencot, subperitoneal, mlc, divericuli, ould, hepaticojej, bilatearally, elluting, extravesical, remmained, replacemet, fluzone, ureterotomy, fulgaration, q3hour]\n\n\n\n\n\n\n\n\nprint('\\n'.join(L(zip(top_toks[16], top_vals[16])).map(str)))\n\n('osteoporosis', tensor(0.3675))\n('fosamax', tensor(0.0482))\n('alendronate', tensor(0.0479))\n('d3', tensor(0.0223))\n('carbonate', tensor(0.0183))\n('cholecalciferol', tensor(0.0181))\n('70', tensor(0.0147))\n('osteoperosis', tensor(0.0137))\n('actonel', tensor(0.0132))\n('she', tensor(0.0128))\n('her', tensor(0.0127))\n('he', tensor(0.0109))\n('his', tensor(0.0095))\n('vitamin', tensor(0.0092))\n('risedronate', tensor(0.0078))\n('qweek', tensor(0.0077))\n('compression', tensor(0.0076))\n('raloxifene', tensor(0.0070))\n('ms', tensor(0.0069))\n('male', tensor(0.0067))\n\n\n\ntok_list = top_toks[16]\nL(tok for tok in tok_list if tok in text.split())\n\n(#5) ['osteoporosis','he','his','vitamin','male']\n\n\n\npattern = r'\\b({0})\\b'.format('|'.join(list(tok_list)))\npattern = re.compile(pattern)\nfor i,match in enumerate(pattern.finditer(text)):\n    print(i, match.group())\n    print(\"-\"*len(match.group()))\n    print(text[match.start()-100: match.end()+100], end='\\n\\n')\n\n0 diabetes\n--------\nvement in numbers no episodes of bleeding dic labs negative he subsequent had platelets in 60s 100s diabetes the patient was placed on ssi in house and lantus due to persistent hypoglycemia in the morning he \n\n1 insulin\n-------\nmnia oxycodone mg tablet sig one tablet po q6h every hours as needed for pain disp tablet s refills insulin glargine unit ml solution sig twelve units subcutaneous at bedtime heparin flush units ml ml iv prn\n\n2 diabetes\n--------\n failure atrial fibrillation with rapid ventricular response portal gastropathy secondary cirrhosis diabetes mellitus discharge condition mental status clear and coherent level of consciousness alert and inte\n\n3 mellitus\n--------\natrial fibrillation with rapid ventricular response portal gastropathy secondary cirrhosis diabetes mellitus discharge condition mental status clear and coherent level of consciousness alert and interactive a\n\n\n\n\ntst_model = get_xmltext_classifier2(AWD_LSTM, 60000, 1271, seq_len=72, config=awd_lstm_clas_config, \n                               drop_mult=0.1, max_len=72*40).to(default_device())\n\nxb, yb = dls_clas.one_batch()\n\nL(xb, yb).map(lambda o: (o.shape, o.device))\n\nsent_enc = learn.model[0].to(default_device())\nattn_clas = learn.model[1].to(default_device())\n\npreds_by_sent_enc, mask = sent_enc(xb)\npreds_by_sent_enc.shape, mask.shape\n\no = tst_model(xb)\n\no[0].shape\n\ntst_model[1].hl.shape"
  },
  {
    "objectID": "tutorial.train_mimic3.html#fine-tuning-the-classifier",
    "href": "tutorial.train_mimic3.html#fine-tuning-the-classifier",
    "title": "Training an XML Text Classifier",
    "section": "Fine-Tuning the Classifier",
    "text": "Fine-Tuning the Classifier\n\nlearn.fit(2, lr=3e-2)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\ntrain_precision_at_k\nvalid_loss\nvalid_precision_at_k\ntime\n\n\n\n\n0\n0.006671\n0.427560\n0.007373\n0.501760\n18:52\n\n\n1\n0.006010\n0.486638\n0.007033\n0.518841\n19:13\n\n\n\n\n\n\nlearn.fit(9, lr=3e-2)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\ntrain_precision_at_k\nvalid_loss\nvalid_precision_at_k\ntime\n\n\n\n\n0\n0.006171\n0.496154\n0.006873\n0.524792\n19:04\n\n\n1\n0.006296\n0.499897\n0.006786\n0.526097\n19:07\n\n\n2\n0.006071\n0.501944\n0.006728\n0.529656\n21:40\n\n\n3\n0.005868\n0.503599\n0.006685\n0.529261\n19:53\n\n\n4\n0.005772\n0.504341\n0.006666\n0.530565\n18:51\n\n\n5\n0.005700\n0.504796\n0.006648\n0.529201\n19:29\n\n\n6\n0.005662\n0.504856\n0.006632\n0.530625\n18:40\n\n\n7\n0.006080\n0.505080\n0.006594\n0.531870\n19:07\n\n\n8\n0.005849\n0.505091\n0.006607\n0.532543\n18:40\n\n\n\n\n\nPath('/home/deb/xcube/nbs/tmp/models/mimic_tmp.pth')\n\n\n\nlearn.freeze_to(-2)\n\n\nlearn.fit(10, lr=1e-2)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\ntrain_precision_at_k\nvalid_loss\nvalid_precision_at_k\ntime\n\n\n\n\n0\n0.004908\n0.520075\n0.005996\n0.554903\n22:30\n\n\n1\n0.005100\n0.524777\n0.005895\n0.557651\n22:42\n\n\n2\n0.004864\n0.525998\n0.005869\n0.555219\n23:00\n\n\n3\n0.004873\n0.527586\n0.005766\n0.561507\n22:30\n\n\n4\n0.004755\n0.530503\n0.005780\n0.558323\n22:37\n\n\n5\n0.004760\n0.532162\n0.005718\n0.562060\n22:37\n\n\n6\n0.004906\n0.533651\n0.005617\n0.569632\n22:28\n\n\n7\n0.004725\n0.535399\n0.005622\n0.569553\n23:41\n\n\n8\n0.004727\n0.536854\n0.005613\n0.569968\n22:40\n\n\n9\n0.004670\n0.538211\n0.005558\n0.571372\n23:04\n\n\n\n\n\nPath('/home/deb/xcube/nbs/tmp/models/mimic2_tmp.pth')\n\n\n\nlearn.fit(10, lr=1e-2)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\ntrain_precision_at_k\nvalid_loss\nvalid_precision_at_k\ntime\n\n\n\n\n0\n0.004632\n0.539177\n0.005528\n0.571847\n22:36\n\n\n1\n0.004653\n0.540667\n0.005551\n0.568802\n22:27\n\n\n2\n0.004833\n0.541256\n0.005607\n0.566588\n24:31\n\n\n3\n0.004460\n0.541850\n0.005555\n0.573665\n26:17\n\n\n4\n0.004692\n0.540970\n0.005530\n0.570522\n23:43\n\n\n5\n0.004859\n0.541696\n0.005493\n0.574812\n22:50\n\n\n6\n0.004495\n0.543965\n0.005538\n0.570364\n22:46\n\n\n7\n0.004517\n0.544347\n0.005493\n0.572875\n23:18\n\n\n8\n0.004755\n0.543222\n0.005501\n0.572578\n23:29\n\n\n9\n0.004687\n0.544784\n0.005483\n0.571076\n22:44\n\n\n\n\n\nPath('/home/deb/xcube/nbs/tmp/models/mimic3_tmp.pth')\n\n\n\nlearn.freeze_to(-3)\n\n\nlearn.fit(2, lr=1e-2)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\ntrain_precision_at_k\nvalid_loss\nvalid_precision_at_k\ntime\n\n\n\n\n0\n0.004854\n0.550886\n0.005433\n0.578094\n1:09:40\n\n\n1\n0.004290\n0.554642\n0.005335\n0.582720\n1:13:00\n\n\n\n\n\nPath('/home/deb/xcube/nbs/tmp/models/mimic4_tmp.pth')\n\n\n\nlearn.freeze_to(-3)\n\n\nlearn.fit(5, lr=1e-2)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\ntrain_precision_at_k\nvalid_loss\nvalid_precision_at_k\ntime\n\n\n\n\n0\n0.004555\n0.558910\n0.005358\n0.583887\n1:08:32\n\n\n1\n0.004249\n0.559825\n0.005338\n0.582068\n1:06:38\n\n\n2\n0.004497\n0.557554\n0.005273\n0.588711\n1:08:03\n\n\n3\n0.004690\n0.557821\n0.005338\n0.582444\n1:04:41\n\n\n4\n0.004420\n0.559116\n0.005297\n0.585923\n1:04:19\n\n\n\n\n\nBetter model found at epoch 0 with valid_precision_at_k value: 0.5838869118228547.\nBetter model found at epoch 2 with valid_precision_at_k value: 0.5887109529458283.\n\n\n\nlearn.unfreeze()\n\n\nlearn.fit(12, lr=1e-6, wd=0.3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\ntrain_precision_at_k\nvalid_loss\nvalid_precision_at_k\ntime\n\n\n\n\n0\n0.004274\n0.571029\n0.005253\n0.589897\n1:22:37\n\n\n1\n0.003970\n0.573214\n0.005237\n0.590826\n1:35:24\n\n\n2\n0.003580\n0.576061\n0.005224\n0.592210\n1:28:15\n\n\n3\n0.004398\n0.573131\n0.005213\n0.592764\n1:21:15\n\n\n4\n0.004146\n0.573926\n0.005203\n0.593634\n1:21:23\n\n\n5\n0.004096\n0.575295\n0.005194\n0.594227\n1:20:27\n\n\n6\n0.004011\n0.575432\n0.005185\n0.594879\n1:20:34\n\n\n7\n0.003997\n0.576225\n0.005178\n0.595789\n1:23:31\n\n\n8\n0.003942\n0.577274\n0.005171\n0.596362\n1:21:20\n\n\n9\n0.004266\n0.577674\n0.005164\n0.596659\n1:21:25\n\n\n10\n0.004115\n0.578049\n0.005158\n0.597113\n1:21:17\n\n\n11\n0.003978\n0.578710\n0.005152\n0.597232\n1:21:51\n\n\n\n\n\nBetter model found at epoch 0 with valid_precision_at_k value: 0.5898971925662319.\nBetter model found at epoch 1 with valid_precision_at_k value: 0.5908264136022148.\nBetter model found at epoch 2 with valid_precision_at_k value: 0.5922103598260184.\nBetter model found at epoch 3 with valid_precision_at_k value: 0.59276393831554.\nBetter model found at epoch 4 with valid_precision_at_k value: 0.5936338473705026.\nBetter model found at epoch 5 with valid_precision_at_k value: 0.594226967180704.\nBetter model found at epoch 6 with valid_precision_at_k value: 0.5948793989719259.\nBetter model found at epoch 7 with valid_precision_at_k value: 0.5957888493475683.\nBetter model found at epoch 8 with valid_precision_at_k value: 0.59636219849743.\nBetter model found at epoch 9 with valid_precision_at_k value: 0.5966587584025307.\nBetter model found at epoch 10 with valid_precision_at_k value: 0.5971134835903521.\nBetter model found at epoch 11 with valid_precision_at_k value: 0.5972321075523922.\n\n\n\nlearn.fit(12, lr=1e-6, wd=0.3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\ntrain_precision_at_k\nvalid_loss\nvalid_precision_at_k\ntime\n\n\n\n\n0\n0.004173\n0.580595\n0.005147\n0.597628\n1:27:59\n\n\n1\n0.003887\n0.581874\n0.005142\n0.598082\n1:26:58\n\n\n2\n0.003538\n0.584128\n0.005137\n0.598280\n1:25:40\n\n\n3\n0.004319\n0.580220\n0.005133\n0.598557\n1:22:04\n\n\n4\n0.004077\n0.580681\n0.005128\n0.598754\n1:29:05\n\n\n5\n0.004035\n0.581153\n0.005124\n0.599229\n1:23:48\n\n\n6\n0.003955\n0.581074\n0.005120\n0.599644\n1:27:02\n\n\n7\n0.003943\n0.581586\n0.005117\n0.599960\n1:25:09\n\n\n8\n0.003891\n0.582050\n0.005113\n0.600138\n1:25:04\n\n\n9\n0.004217\n0.582461\n0.005110\n0.600178\n1:32:48\n\n\n10\n0.004067\n0.582578\n0.005106\n0.600811\n1:34:01\n\n\n11\n0.003936\n0.583004\n0.005103\n0.601008\n1:27:10\n\n\n\n\n\nBetter model found at epoch 0 with valid_precision_at_k value: 0.5976275207591935.\nBetter model found at epoch 1 with valid_precision_at_k value: 0.5980822459470149.\nBetter model found at epoch 2 with valid_precision_at_k value: 0.5982799525504153.\nBetter model found at epoch 3 with valid_precision_at_k value: 0.5985567417951758.\nBetter model found at epoch 4 with valid_precision_at_k value: 0.5987544483985765.\nBetter model found at epoch 5 with valid_precision_at_k value: 0.5992289442467379.\nBetter model found at epoch 6 with valid_precision_at_k value: 0.5996441281138793.\nBetter model found at epoch 7 with valid_precision_at_k value: 0.5999604586793202.\nBetter model found at epoch 8 with valid_precision_at_k value: 0.6001383946223807.\nBetter model found at epoch 9 with valid_precision_at_k value: 0.6001779359430608.\nBetter model found at epoch 10 with valid_precision_at_k value: 0.6008105970739425.\nBetter model found at epoch 11 with valid_precision_at_k value: 0.601008303677343.\n\n\n\nlearn.fit(12, lr=1e-6, wd=0.3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\ntrain_precision_at_k\nvalid_loss\nvalid_precision_at_k\ntime\n\n\n\n\n0\n0.004131\n0.584655\n0.005100\n0.601364\n1:27:13\n\n\n1\n0.003851\n0.585673\n0.005098\n0.601522\n1:27:38\n\n\n2\n0.003534\n0.587853\n0.005095\n0.601839\n1:27:55\n\n\n3\n0.004281\n0.583691\n0.005092\n0.602096\n1:26:51\n\n\n4\n0.004044\n0.584044\n0.005090\n0.602135\n1:23:44\n\n\n5\n0.004004\n0.584326\n0.005088\n0.602550\n1:25:08\n\n\n6\n0.003927\n0.584212\n0.005085\n0.602669\n1:28:46\n\n\n7\n0.003916\n0.584371\n0.005083\n0.602748\n1:35:59\n\n\n8\n0.003863\n0.584690\n0.005081\n0.603005\n1:32:43\n\n\n9\n0.004190\n0.585073\n0.005079\n0.603203\n1:35:33\n\n\n10\n0.004041\n0.584967\n0.005077\n0.603401\n1:27:33\n\n\n11\n0.003914\n0.585363\n0.005075\n0.603816\n1:29:02\n\n\n\n\n\nBetter model found at epoch 0 with valid_precision_at_k value: 0.601364175563464.\nBetter model found at epoch 1 with valid_precision_at_k value: 0.6015223408461846.\nBetter model found at epoch 2 with valid_precision_at_k value: 0.6018386714116253.\nBetter model found at epoch 3 with valid_precision_at_k value: 0.6020956899960457.\nBetter model found at epoch 4 with valid_precision_at_k value: 0.6021352313167257.\nBetter model found at epoch 5 with valid_precision_at_k value: 0.6025504151838669.\nBetter model found at epoch 6 with valid_precision_at_k value: 0.6026690391459072.\nBetter model found at epoch 7 with valid_precision_at_k value: 0.6027481217872676.\nBetter model found at epoch 8 with valid_precision_at_k value: 0.6030051403716885.\nBetter model found at epoch 9 with valid_precision_at_k value: 0.603202846975089.\nBetter model found at epoch 10 with valid_precision_at_k value: 0.6034005535784894.\nBetter model found at epoch 11 with valid_precision_at_k value: 0.6038157374456307.\n\n\n\nlearn.fit(17, lr=1e-6, wd=0.3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\ntrain_precision_at_k\nvalid_loss\nvalid_precision_at_k\ntime\n\n\n\n\n0\n0.004109\n0.586924\n0.005073\n0.603934\n1:30:05\n\n\n1\n0.003831\n0.587917\n0.005072\n0.604053\n1:27:47\n\n\n2\n0.003542\n0.589895\n0.005070\n0.604013\n1:27:51\n\n\n3\n0.004260\n0.585822\n0.005069\n0.603895\n1:28:24\n\n\n4\n0.004025\n0.586037\n0.005067\n0.603855\n1:28:43\n\n\n5\n0.003987\n0.586026\n0.005066\n0.604172\n1:27:41\n\n\n6\n0.003912\n0.586035\n0.005064\n0.604290\n1:25:56\n\n\n7\n0.003901\n0.586076\n0.005063\n0.604409\n1:27:44\n\n\n8\n0.003848\n0.586360\n0.005061\n0.604389\n1:25:51\n\n\n9\n0.004176\n0.586516\n0.005060\n0.604350\n1:25:42\n\n\n10\n0.004026\n0.586483\n0.005059\n0.604369\n1:33:33\n\n\n11\n0.003901\n0.586768\n0.005058\n0.604508\n1:24:20\n\n\n12\n0.004077\n0.586777\n0.005057\n0.604745\n1:26:26\n\n\n13\n0.003919\n0.586842\n0.005056\n0.604923\n1:25:43\n\n\n14\n0.003948\n0.586588\n0.005055\n0.605081\n1:23:59\n\n\n15\n0.003915\n0.586954\n0.005054\n0.605042\n1:28:20\n\n\n16\n0.003949\n0.587083\n0.005053\n0.605042\n1:32:08\n\n\n\n\n\nBetter model found at epoch 0 with valid_precision_at_k value: 0.603934361407671.\nBetter model found at epoch 1 with valid_precision_at_k value: 0.6040529853697115.\nBetter model found at epoch 5 with valid_precision_at_k value: 0.6041716093317517.\nBetter model found at epoch 6 with valid_precision_at_k value: 0.6042902332937922.\nBetter model found at epoch 7 with valid_precision_at_k value: 0.6044088572558325.\nBetter model found at epoch 11 with valid_precision_at_k value: 0.6045077105575327.\nBetter model found at epoch 12 with valid_precision_at_k value: 0.6047449584816134.\nBetter model found at epoch 13 with valid_precision_at_k value: 0.6049228944246741.\nBetter model found at epoch 14 with valid_precision_at_k value: 0.6050810597073943.\n\n\n\nlearn.fit(12, lr=1e-6, wd=0.3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\ntrain_precision_at_k\nvalid_loss\nvalid_precision_at_k\ntime\n\n\n\n\n0\n0.004095\n0.588552\n0.005054\n0.604982\n1:27:02\n\n\n1\n0.003819\n0.589632\n0.005053\n0.605081\n1:23:54\n\n\n2\n0.003560\n0.591372\n0.005052\n0.605397\n1:23:37\n\n\n3\n0.004247\n0.587154\n0.005051\n0.605417\n1:24:47\n\n\n4\n0.004014\n0.587362\n0.005050\n0.605575\n1:25:40\n\n\n5\n0.003976\n0.587346\n0.005050\n0.605595\n1:25:16\n\n\n6\n0.003904\n0.587354\n0.005049\n0.605516\n1:30:16\n\n\n7\n0.003893\n0.587376\n0.005048\n0.605575\n1:23:22\n\n\n8\n0.003840\n0.587661\n0.005048\n0.605556\n1:27:31\n\n\n9\n0.004168\n0.587554\n0.005047\n0.605753\n1:27:43\n\n\n10\n0.004018\n0.587563\n0.005046\n0.605793\n1:27:50\n\n\n11\n0.003895\n0.587731\n0.005046\n0.605931\n1:22:51\n\n\n\n\n\nBetter model found at epoch 1 with valid_precision_at_k value: 0.6050810597073941.\nBetter model found at epoch 2 with valid_precision_at_k value: 0.6053973902728351.\nBetter model found at epoch 3 with valid_precision_at_k value: 0.6054171609331752.\nBetter model found at epoch 4 with valid_precision_at_k value: 0.6055753262158957.\nBetter model found at epoch 5 with valid_precision_at_k value: 0.6055950968762357.\nBetter model found at epoch 9 with valid_precision_at_k value: 0.6057532621589561.\nBetter model found at epoch 10 with valid_precision_at_k value: 0.6057928034796363.\nBetter model found at epoch 11 with valid_precision_at_k value: 0.6059311981020166.\n\n\nstart here:\n\nprint(learn.save_model.fname)\nlearn.save_model.reset_on_fit=False\nassert not learn.save_model.reset_on_fit\nlearn = learn.load(learn.save_model.fname)\nvalidate(learn)\n\nmimic3-9k_clas_full\nbest so far = None\n[0.00504577299579978, 0.6059311981020166]\nbest so far = None\n\n\n\n\n\n\n\n\n\n\nlearn.save_model.best = 0.605931198\n\nNow unfreeze and train more if you want:\n\nlearn.unfreeze()\n\n\nlearn.fit(12, lr=1e-6, wd=0.3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\ntrain_precision_at_k\nvalid_loss\nvalid_precision_at_k\ntime\n\n\n\n\n0\n0.004091\n0.589346\n0.005045\n0.605832\n1:26:58\n\n\n1\n0.003816\n0.590275\n0.005045\n0.605872\n1:39:14\n\n\n2\n0.003580\n0.591960\n0.005044\n0.605971\n1:24:25\n\n\n3\n0.004243\n0.587718\n0.005044\n0.606030\n1:29:50\n\n\n4\n0.004012\n0.587808\n0.005044\n0.606168\n1:26:04\n\n\n5\n0.003974\n0.587973\n0.005043\n0.606070\n1:25:14\n\n\n6\n0.003903\n0.587727\n0.005043\n0.605991\n1:25:52\n\n\n7\n0.003892\n0.587734\n0.005043\n0.606010\n1:30:01\n\n\n8\n0.003839\n0.588023\n0.005042\n0.606010\n1:27:09\n\n\n9\n0.004167\n0.588038\n0.005042\n0.606070\n1:27:05\n\n\n10\n0.004016\n0.587855\n0.005042\n0.606070\n1:33:34\n\n\n11\n0.003895\n0.588136\n0.005041\n0.606089\n1:35:38\n\n\n\n\n\nBetter model found at epoch 2 with valid_precision_at_k value: 0.6059707394226967.\nBetter model found at epoch 3 with valid_precision_at_k value: 0.6060300514037169.\nBetter model found at epoch 4 with valid_precision_at_k value: 0.6061684460260972.\n\n\n\nThe last step (yes, the madness will end soon) is to train with: - discriminative learning rates: define here - gradual unfreezing: define here\nas defined here in ULMFit\nIf we are using the LabelForcing callback, then the loss will spike after certain number of epochs. So we need to remove this callback before doing lr_find. This is because lr_find aborts if things go south with regard to loss.\n\nlbl_fcr = None\nwith suppress(ValueError): lbl_fcr = learn.cbs[learn.cbs.map(type).index(LabelForcing)]\nwith learn.removed_cbs(lbl_fcr):\n    lr_min, lr_steep, lr_valley, lr_slide = learn.lr_find(suggest_funcs=(minimum, steep, valley, slide), num_it=5000)\n    print(f\"{lr_min=}, {lr_steep=}, {lr_valley=}, {lr_slide=}\")\n\n\n\n\n\n\n\n\nlr_min=0.03156457841396332, lr_steep=6.867521165077051e-07, lr_valley=0.016935577616095543, lr_slide=9.255502700805664\n\n\n\n\n\n\n# learn.opt.hypers.map(lambda d: d['lr'])\n\n\n[0.14*i/4 for i in range(4, 0, -1)], [0.14*i for i in range(4, 0, -1)],\n\n([0.14, 0.10500000000000001, 0.07, 0.035],\n [0.56, 0.42000000000000004, 0.28, 0.14])\n\n\n\nlearn.cbs[-4].reset_on_fit = False\nprint(learn.cbs[-4].reset_on_fit)\n# for i in range(4, 0, -1):\n#     learn.fit_one_cycle(1, lr_max=0.14*i, moms=(0.8,0.7,0.8), wd=0.1)\n#     learn.fit_one_cycle(5, lr_max=0.14*i, moms=(0.8,0.7,0.8), wd=0.1)\nlearn.fit_sgdr(4, 1, lr_max=0.2, wd=0.1)\n\nFalse\nBetter model found at epoch 0 with valid_precision_at_k value: 0.26.\nBetter model found at epoch 1 with valid_precision_at_k value: 0.3433333333333333.\nBetter model found at epoch 2 with valid_precision_at_k value: 0.39.\nBetter model found at epoch 3 with valid_precision_at_k value: 0.41000000000000003.\nBetter model found at epoch 4 with valid_precision_at_k value: 0.43.\nBetter model found at epoch 5 with valid_precision_at_k value: 0.4366666666666667.\nBetter model found at epoch 6 with valid_precision_at_k value: 0.44000000000000006.\nBetter model found at epoch 8 with valid_precision_at_k value: 0.4533333333333333.\nBetter model found at epoch 10 with valid_precision_at_k value: 0.4666666666666667.\nBetter model found at epoch 11 with valid_precision_at_k value: 0.4733333333333333.\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\ntrain_precision_at_k\nvalid_loss\nvalid_precision_at_k\ntime\n\n\n\n\n0\n0.354789\n0.174444\n0.234256\n0.260000\n00:17\n\n\n1\n0.234948\n0.373611\n0.124908\n0.343333\n00:20\n\n\n2\n0.172152\n0.488333\n0.116975\n0.390000\n00:19\n\n\n3\n0.139922\n0.495000\n0.103215\n0.410000\n00:19\n\n\n4\n0.118237\n0.547778\n0.095149\n0.430000\n00:19\n\n\n5\n0.102651\n0.587500\n0.093051\n0.436667\n00:19\n\n\n6\n0.091334\n0.613611\n0.092191\n0.440000\n00:19\n\n\n7\n0.084536\n0.586389\n0.085612\n0.440000\n00:18\n\n\n8\n0.078545\n0.589722\n0.082743\n0.453333\n00:18\n\n\n9\n0.072898\n0.593889\n0.085372\n0.420000\n00:18\n\n\n10\n0.068474\n0.606944\n0.079234\n0.466667\n00:19\n\n\n11\n0.064556\n0.627222\n0.077119\n0.473333\n00:20\n\n\n12\n0.060896\n0.641389\n0.076804\n0.453333\n00:19\n\n\n13\n0.058002\n0.651389\n0.075883\n0.470000\n00:19\n\n\n14\n0.055731\n0.655000\n0.075790\n0.470000\n00:18\n\n\n\n\n\n\nvalidate(learn)\n\nbest so far = 0.4733333333333333\n[0.07711941003799438, 0.4733333333333333]\nbest so far = 0.4733333333333333\n\n\n\n\n\n\n\n\n\n\nlearn.opt.hypers.map(lambda d: d['lr'])\n# L(apply(d.get, ['wd', 'lr']))\n\n(#5) [0.06416322019620913,0.06416322019620913,0.06416322019620913,0.06416322019620913,0.06416322019620913]\n\n\nThis was the result after just three epochs. Now let’s unfreeze the last two layers and do discriminative training:\nNow we will pass -2 to freeze_to to freeze all except the last two parameter groups:\n\nlearn.freeze_to(-2)\n# learn.freeze_to(-4)\n\n\n# learn.fit_one_cycle(1, lr_max=1e-3*i, moms=(0.8,0.7,0.8), wd=0.2)\n# learn.fit_one_cycle(4, lr_max=1e-3*i, moms=(0.8,0.7,0.8), wd=0.2)\nlearn.fit_sgdr(3, 1, lr_max=0.02, wd=0.1)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nprecision_at_k\ntime\n\n\n\n\n0\n0.053839\n0.078058\n0.453333\n00:07\n\n\n1\n0.050421\n0.077731\n0.453333\n00:06\n\n\n2\n0.047472\n0.077222\n0.436667\n00:06\n\n\n3\n0.045848\n0.073436\n0.436667\n00:06\n\n\n4\n0.044292\n0.074997\n0.413333\n00:06\n\n\n5\n0.042457\n0.076048\n0.423333\n00:06\n\n\n6\n0.040752\n0.076921\n0.430000\n00:06\n\n\n\n\n\n\nlearn.opt.hypers.map(lambda d: d['lr'])\n# L(apply(d.get, ['wd', 'lr']))\n\n(#5) [0.06416322019620913,0.06416322019620913,0.06416322019620913,0.06416322019620913,0.06416322019620913]\n\n\n\nvalidate(learn)\n\nbest so far = 0.47333333333333333\n[0.07113126665353775, 0.47333333333333333]\nbest so far = 0.47333333333333333\n\n\n\n\n\n\n\n\n\nNow we will unfreeze a bit more, recompute learning rate and continue training:\n\nlearn.freeze_to(-3)\n# learn.freeze_to(-5)\n\n\nlearn.fit_one_cycle(1, lr_max=1e-3, moms=(0.8,0.7,0.8), wd=0.2)\nlearn.fit_one_cycle(4, lr_max=1e-3, moms=(0.8,0.7,0.8), wd=0.2)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nprecision_at_k\ntime\n\n\n\n\n0\n0.043529\n0.071279\n0.473333\n00:10\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nprecision_at_k\ntime\n\n\n\n\n0\n0.043608\n0.071249\n0.476667\n00:09\n\n\n1\n0.043263\n0.071772\n0.463333\n00:09\n\n\n2\n0.042877\n0.071997\n0.456667\n00:09\n\n\n3\n0.042536\n0.072053\n0.456667\n00:09\n\n\n\n\n\nBetter model found at epoch 0 with precision_at_k value: 0.4766666666666667.\n\n\n\nlearn.opt.hypers.map(lambda d: d['lr'])\n# L(apply(d.get, ['wd', 'lr']))\n\n(#5) [0.000989510849598057,0.000989510849598057,0.000989510849598057,0.000989510849598057,0.000989510849598057]\n\n\n\nvalidate(learn)\n\nbest so far = 0.4766666666666667\n[0.07124889642000198, 0.4766666666666667]\nbest so far = 0.4766666666666667\n\n\n\n\n\n\n\n\n\nFinally, we will unfreeze the whole model and perform training: (Caution: Check if you got enough memory left!)\n\ncudamem()\n\nGPU: Quadro RTX 8000\nYou are using 16.390625 GB\nTotal GPU memory = 44.99969482421875 GB\n\n\n\nlearn.unfreeze()\n\nAt this point, it starts overfitting too soon so either progressively increase bs or increase wd or decrease lr_max. Another way would be to drop out label embeddings, but for this one needs to carefully adjust the dropout prob. Same concept regarding dropping out the seqs nh.\n\nlr_min, lr_steep, lr_valley, lr_slide = learn.lr_find(suggest_funcs=(minimum,steep,valley,slide), num_it=100)\n\n\n\n\n\n\n\n\n\n\n\n\nlr_min, lr_steep, lr_valley, lr_slide\n\n(0.00043651582673192023,\n 0.00015848931798245758,\n 0.0002290867705596611,\n 0.0010000000474974513)\n\n\n\n# learn.fit_one_cycle(1, lr_max=1e-3, moms=(0.8,0.7,0.8), wd=0.2)\nfor i in range(3, 0, -1):\n    learn.fit_one_cycle(1, lr_max=1e-3*i, moms=(0.8,0.7,0.8), wd=0.1)\n    learn.fit_one_cycle(5, lr_max=1e-3*i, moms=(0.8,0.7,0.8), wd=0.1)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nprecision_at_k\ntime\n\n\n\n\n0\n0.042745\n0.072136\n0.446667\n00:12\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nprecision_at_k\ntime\n\n\n\n\n0\n0.042994\n0.071712\n0.453333\n00:11\n\n\n1\n0.042261\n0.075044\n0.450000\n00:11\n\n\n2\n0.041705\n0.074658\n0.456667\n00:11\n\n\n3\n0.040608\n0.075577\n0.446667\n00:11\n\n\n4\n0.039769\n0.075746\n0.446667\n00:13\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nprecision_at_k\ntime\n\n\n\n\n0\n0.042843\n0.071680\n0.460000\n00:12\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nprecision_at_k\ntime\n\n\n\n\n0\n0.043052\n0.071593\n0.470000\n00:12\n\n\n1\n0.042404\n0.072811\n0.460000\n00:12\n\n\n2\n0.041661\n0.073665\n0.463333\n00:12\n\n\n3\n0.040861\n0.074412\n0.456667\n00:12\n\n\n4\n0.040264\n0.074554\n0.456667\n00:11\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nprecision_at_k\ntime\n\n\n\n\n0\n0.043041\n0.071457\n0.466667\n00:11\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nprecision_at_k\ntime\n\n\n\n\n0\n0.043180\n0.071420\n0.466667\n00:13\n\n\n1\n0.042783\n0.071878\n0.456667\n00:11\n\n\n2\n0.042280\n0.072338\n0.453333\n00:12\n\n\n3\n0.041820\n0.072718\n0.460000\n00:11\n\n\n4\n0.041488\n0.072763\n0.460000\n00:11\n\n\n\n\n\n\nlearn.opt.hypers.map(lambda d: d['lr'])\n# L(apply(d.get, ['wd', 'lr']))\n\n(#5) [3.588356645241649e-05,3.588356645241649e-05,3.588356645241649e-05,3.588356645241649e-05,3.588356645241649e-05]\n\n\n\nvalidate(learn)\n\nbest so far = 0.29333333333333333\n[0.07909457385540009, 0.29333333333333333]\nbest so far = 0.29333333333333333"
  },
  {
    "objectID": "tutorial.train_mimic3.html#fine-tune-the-fwd-dev",
    "href": "tutorial.train_mimic3.html#fine-tune-the-fwd-dev",
    "title": "Training an XML Text Classifier",
    "section": "Fine-Tune the Fwd (Dev)",
    "text": "Fine-Tune the Fwd (Dev)\n\nlearn = learn.load(learn.save_model.fname)\nvalidate(learn)\n\n\nlearn.save_model.reset_on_fit = False\nlearn.save_model.fname\n\n'mimic3-9k_clas_tiny'\n\n\n\nlearn.fit(10, lr=3e-2)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\ntrain_precision_at_k\nvalid_loss\nvalid_precision_at_k\ntime\n\n\n\n\n0\n0.196740\n0.152500\n0.072112\n0.243333\n00:18\n\n\n1\n0.116046\n0.264167\n0.076217\n0.280000\n00:14\n\n\n2\n0.086854\n0.334722\n0.074655\n0.283333\n00:13\n\n\n3\n0.070159\n0.407222\n0.073141\n0.320000\n00:13\n\n\n4\n0.058983\n0.467500\n0.071807\n0.313333\n00:14\n\n\n5\n0.050570\n0.530556\n0.071735\n0.333333\n00:14\n\n\n6\n0.043707\n0.590833\n0.074884\n0.306667\n00:20\n\n\n7\n0.037904\n0.650833\n0.074323\n0.306667\n00:14\n\n\n8\n0.032732\n0.701667\n0.076984\n0.303333\n00:17\n\n\n9\n0.028173\n0.736111\n0.081250\n0.296667\n00:13\n\n\n\n\n\nBetter model found at epoch 0 with valid_precision_at_k value: 0.24333333333333335.\nBetter model found at epoch 1 with valid_precision_at_k value: 0.27999999999999997.\nBetter model found at epoch 2 with valid_precision_at_k value: 0.2833333333333333.\nBetter model found at epoch 3 with valid_precision_at_k value: 0.31999999999999995.\nBetter model found at epoch 5 with valid_precision_at_k value: 0.33333333333333337.\n\n\n\nlearn.freeze_to(-2)\n\n\nlearn.fit(10, lr=1e-2)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\ntrain_precision_at_k\nvalid_loss\nvalid_precision_at_k\ntime\n\n\n\n\n0\n0.032477\n0.546111\n0.077029\n0.316667\n00:19\n\n\n1\n0.026895\n0.631667\n0.082825\n0.336667\n00:19\n\n\n2\n0.022810\n0.695000\n0.088930\n0.306667\n00:19\n\n\n3\n0.019261\n0.748333\n0.094657\n0.290000\n00:20\n\n\n4\n0.016189\n0.776667\n0.102535\n0.306667\n00:19\n\n\n5\n0.013916\n0.785278\n0.109710\n0.260000\n00:19\n\n\n6\n0.011869\n0.796944\n0.111829\n0.293333\n00:19\n\n\n7\n0.010106\n0.799722\n0.120422\n0.293333\n00:21\n\n\n8\n0.008519\n0.803611\n0.120180\n0.290000\n00:19\n\n\n9\n0.007059\n0.810000\n0.124685\n0.300000\n00:20\n\n\n\n\n\nBetter model found at epoch 1 with valid_precision_at_k value: 0.33666666666666667.\n\n\n\nlearn.freeze_to(-3)\n\n\nlearn.fit(10, lr=1e-2)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\ntrain_precision_at_k\nvalid_loss\nvalid_precision_at_k\ntime\n\n\n\n\n0\n0.022163\n0.624444\n0.086960\n0.316667\n00:28\n\n\n1\n0.017831\n0.728889\n0.093251\n0.320000\n00:25\n\n\n2\n0.014486\n0.773889\n0.098625\n0.303333\n00:30\n\n\n3\n0.011709\n0.795833\n0.106103\n0.280000\n00:24\n\n\n4\n0.009461\n0.802778\n0.115305\n0.300000\n00:24\n\n\n5\n0.007549\n0.808056\n0.118747\n0.293333\n00:24\n\n\n6\n0.006116\n0.808611\n0.123826\n0.303333\n00:25\n\n\n7\n0.005031\n0.808333\n0.121675\n0.286667\n00:25\n\n\n8\n0.004195\n0.809444\n0.124114\n0.303333\n00:25\n\n\n9\n0.003598\n0.809167\n0.122602\n0.293333\n00:26\n\n\n\n\n\n\nlearn.unfreeze()\n\n\nlearn.fit(25, lr=1e-6, wd=0.3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\ntrain_precision_at_k\nvalid_loss\nvalid_precision_at_k\ntime\n\n\n\n\n0\n0.017937\n0.685833\n0.082829\n0.336667\n00:33\n\n\n1\n0.017736\n0.686667\n0.082837\n0.333333\n00:32\n\n\n2\n0.017744\n0.687500\n0.082840\n0.333333\n00:32\n\n\n3\n0.017788\n0.688889\n0.082847\n0.333333\n00:33\n\n\n4\n0.017752\n0.689444\n0.082856\n0.333333\n00:33\n\n\n5\n0.017747\n0.686111\n0.082861\n0.333333\n00:31\n\n\n6\n0.017696\n0.687778\n0.082863\n0.333333\n00:32\n\n\n7\n0.017713\n0.688056\n0.082867\n0.333333\n00:33\n\n\n8\n0.017615\n0.687778\n0.082875\n0.333333\n00:29\n\n\n9\n0.017625\n0.688889\n0.082881\n0.336667\n00:34\n\n\n10\n0.017603\n0.689444\n0.082886\n0.333333\n00:28\n\n\n11\n0.017586\n0.686667\n0.082890\n0.333333\n00:28\n\n\n12\n0.017585\n0.689444\n0.082897\n0.333333\n00:28\n\n\n13\n0.017607\n0.688611\n0.082901\n0.333333\n00:29\n\n\n14\n0.017584\n0.687222\n0.082910\n0.333333\n00:29\n\n\n15\n0.017577\n0.689722\n0.082915\n0.333333\n00:28\n\n\n16\n0.017550\n0.689722\n0.082920\n0.333333\n00:30\n\n\n17\n0.017587\n0.687778\n0.082926\n0.333333\n00:31\n\n\n18\n0.017601\n0.688611\n0.082934\n0.333333\n00:27\n\n\n19\n0.017594\n0.688333\n0.082938\n0.333333\n00:28\n\n\n20\n0.017609\n0.688611\n0.082942\n0.333333\n00:28\n\n\n21\n0.017589\n0.687500\n0.082949\n0.333333\n00:28\n\n\n22\n0.017558\n0.687778\n0.082952\n0.333333\n00:28\n\n\n23\n0.017565\n0.692500\n0.082960\n0.333333\n00:28\n\n\n24\n0.017519\n0.689722\n0.082966\n0.333333\n00:28"
  },
  {
    "objectID": "tutorial.train_mimic3.html#checking-how-to-avoid-overfitting",
    "href": "tutorial.train_mimic3.html#checking-how-to-avoid-overfitting",
    "title": "Training an XML Text Classifier",
    "section": "Checking how to avoid overfitting:",
    "text": "Checking how to avoid overfitting:\n\nlearn.save_model.reset_on_fit=False\n\n\nlearn.fit(10, lr=1e-2)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\ntrain_precision_at_k\nvalid_loss\nvalid_precision_at_k\ntime\n\n\n\n\n0\n0.391221\n0.074167\n0.197629\n0.196667\n00:13\n\n\n1\n0.218384\n0.225556\n0.080403\n0.253333\n00:13\n\n\n2\n0.147795\n0.256389\n0.068796\n0.270000\n00:12\n\n\n3\n0.111931\n0.277778\n0.067111\n0.276667\n00:12\n\n\n4\n0.090809\n0.308333\n0.066692\n0.276667\n00:12\n\n\n5\n0.077108\n0.332222\n0.066395\n0.286667\n00:13\n\n\n6\n0.067511\n0.354722\n0.066039\n0.280000\n00:14\n\n\n7\n0.060452\n0.377500\n0.065895\n0.296667\n00:12\n\n\n8\n0.054908\n0.410556\n0.065720\n0.310000\n00:12\n\n\n9\n0.050470\n0.431944\n0.065801\n0.316667\n00:12\n\n\n\n\n\nBetter model found at epoch 0 with valid_precision_at_k value: 0.19666666666666666.\nBetter model found at epoch 1 with valid_precision_at_k value: 0.2533333333333333.\nBetter model found at epoch 2 with valid_precision_at_k value: 0.27.\nBetter model found at epoch 3 with valid_precision_at_k value: 0.2766666666666667.\nBetter model found at epoch 5 with valid_precision_at_k value: 0.2866666666666667.\nBetter model found at epoch 7 with valid_precision_at_k value: 0.29666666666666675.\nBetter model found at epoch 8 with valid_precision_at_k value: 0.31.\nBetter model found at epoch 9 with valid_precision_at_k value: 0.31666666666666665.\n\n\n\nlearn.freeze_to(-2)\n\n\nlearn.fit(5, lr=1e-2)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\ntrain_precision_at_k\nvalid_loss\nvalid_precision_at_k\ntime\n\n\n\n\n0\n0.042890\n0.413056\n0.069452\n0.303333\n00:16\n\n\n1\n0.037558\n0.481111\n0.068302\n0.340000\n00:17\n\n\n2\n0.033383\n0.543333\n0.073505\n0.313333\n00:16\n\n\n3\n0.029333\n0.617500\n0.076961\n0.306667\n00:16\n\n\n4\n0.025810\n0.668889\n0.083302\n0.286667\n00:16\n\n\n\n\n\nBetter model found at epoch 1 with valid_precision_at_k value: 0.34.\n\n\n\nlearn.freeze_to(-3)\n\n\nlearn.fit(5, lr=1e-2)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\ntrain_precision_at_k\nvalid_loss\nvalid_precision_at_k\ntime\n\n\n\n\n0\n0.032361\n0.489444\n0.072848\n0.313333\n00:20\n\n\n1\n0.027504\n0.595278\n0.077162\n0.286667\n00:21\n\n\n2\n0.023695\n0.679722\n0.082962\n0.290000\n00:22\n\n\n3\n0.019833\n0.746389\n0.091231\n0.306667\n00:22\n\n\n4\n0.016458\n0.782500\n0.097963\n0.300000\n00:23\n\n\n\n\n\n\nlearn.opt.hypers\n\n(#5) [{'wd': 0.01, 'sqr_mom': 0.99, 'lr': 0.01, 'mom': 0.9, 'eps': 1e-05},{'wd': 0.01, 'sqr_mom': 0.99, 'lr': 0.01, 'mom': 0.9, 'eps': 1e-05},{'wd': 0.01, 'sqr_mom': 0.99, 'lr': 0.01, 'mom': 0.9, 'eps': 1e-05},{'wd': 0.01, 'sqr_mom': 0.99, 'lr': 0.01, 'mom': 0.9, 'eps': 1e-05},{'wd': 0.01, 'sqr_mom': 0.99, 'lr': 0.01, 'mom': 0.9, 'eps': 1e-05}]\n\n\n\nlearn = xmltext_classifier_learner(dls_clas, AWD_LSTM, max_len=72*40, \n                                   metrics=partial(precision_at_k, k=15), path=tmp, cbs=cbs, \n                                   drop_mult=0.1,\n                                   pretrained=False,\n                                   splitter=None,\n                                   running_decoder=True).to_fp16()\n\n\n# tell `Recorder` to track `train_metrics`\nassert learn.cbs[1].__class__ is Recorder\nsetattr(learn.cbs[1], 'train_metrics', True)\n\n\n@patch\ndef after_batch(self: ProgressCallback):\n        self.pbar.update(self.iter+1)\n        mets = ('_valid_mets', '_train_mets')[self.training]\n        self.pbar.comment = ' '.join([f'{met.name} = {met.value.item():.4f}' for met in getattr(self.recorder, mets)])\n\n\nlearn = learn.load(learn.save_model.fname)\nvalidate(learn)\n\nbest so far = 0.3433333333333334\n[0.06831542402505875, 0.3433333333333334]\nbest so far = 0.3433333333333334\n\n\n\nlearn.save_model.best=0.34333\n\n\nlearn.unfreeze()\n\n\n# %%debug\nlearn.fit(10, lr=1e-6, wd=0.3)\n\n\n# learn.opt.hypers, learn.wd, learn.lr"
  },
  {
    "objectID": "tutorial.train_mimic3.html#fine-tuning-the-bwd-classifier",
    "href": "tutorial.train_mimic3.html#fine-tuning-the-bwd-classifier",
    "title": "Training an XML Text Classifier",
    "section": "Fine-Tuning the Bwd Classifier",
    "text": "Fine-Tuning the Bwd Classifier\n\n# learn_r.opt.hypers#.map(lambda d: d['lr'])\n# L(apply(d.get, ['wd', 'lr']))\n\n\nlearn_r.fit(2, lr=3e-2)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\ntrain_precision_at_k\nvalid_loss\nvalid_precision_at_k\ntime\n\n\n\n\n0\n0.007564\n0.384488\n0.008315\n0.454982\n39:42\n\n\n1\n0.006742\n0.458227\n0.007797\n0.481930\n38:37\n\n\n\n\n\nBetter model found at epoch 0 with valid_precision_at_k value: 0.45498220640569376.\nBetter model found at epoch 1 with valid_precision_at_k value: 0.48192961644918897.\n\n\n\nlearn_r.fit(9, lr=3e-2)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\ntrain_precision_at_k\nvalid_loss\nvalid_precision_at_k\ntime\n\n\n\n\n0\n0.006838\n0.473014\n0.007535\n0.494899\n39:47\n\n\n1\n0.007002\n0.479700\n0.007376\n0.503084\n40:23\n\n\n2\n0.006739\n0.482954\n0.007317\n0.506702\n40:47\n\n\n3\n0.006478\n0.485122\n0.007257\n0.506445\n40:42\n\n\n4\n0.006433\n0.486549\n0.007218\n0.511052\n39:54\n\n\n5\n0.006414\n0.487762\n0.007178\n0.510320\n40:14\n\n\n6\n0.006309\n0.489128\n0.007146\n0.514353\n40:14\n\n\n7\n0.006853\n0.489367\n0.007107\n0.514334\n40:19\n\n\n8\n0.006412\n0.489717\n0.007141\n0.513741\n39:47\n\n\n\n\n\nBetter model found at epoch 0 with valid_precision_at_k value: 0.49489916963226593.\nBetter model found at epoch 1 with valid_precision_at_k value: 0.5030842230130488.\nBetter model found at epoch 2 with valid_precision_at_k value: 0.506702253855279.\nBetter model found at epoch 4 with valid_precision_at_k value: 0.511051799130091.\nBetter model found at epoch 6 with valid_precision_at_k value: 0.5143534994068806.\n\n\n\nlearn_r.freeze_to(-2)\n\n\nlearn_r.fit(14, lr=1e-2)\n\n\n\n\n\n\n    \n      \n      64.29% [9/14 7:53:40&lt;4:23:09]\n    \n    \n\n\n\n\nepoch\ntrain_loss\ntrain_precision_at_k\nvalid_loss\nvalid_precision_at_k\ntime\n\n\n\n\n0\n0.005077\n0.504411\n0.006206\n0.537722\n52:54\n\n\n1\n0.005185\n0.512415\n0.006007\n0.545275\n50:30\n\n\n2\n0.004890\n0.516533\n0.005913\n0.547687\n52:12\n\n\n3\n0.004909\n0.520331\n0.005819\n0.553341\n51:44\n\n\n4\n0.004789\n0.522809\n0.005807\n0.551839\n52:14\n\n\n5\n0.004796\n0.526800\n0.005746\n0.557750\n54:06\n\n\n6\n0.004978\n0.529563\n0.005712\n0.562772\n51:44\n\n\n7\n0.004752\n0.531859\n0.005678\n0.562060\n54:39\n\n\n8\n0.004784\n0.533404\n0.005707\n0.559035\n53:30\n\n\n\n\n\n    \n      \n      15.50% [478/3084 08:43&lt;47:31 avg_smooth_loss = 0.0045 precision_at_k = 0.5358]\n    \n    \n\n\nBetter model found at epoch 0 with valid_precision_at_k value: 0.5377224199288257.\nBetter model found at epoch 1 with valid_precision_at_k value: 0.5452748121787271.\nBetter model found at epoch 2 with valid_precision_at_k value: 0.5476868327402139.\nBetter model found at epoch 3 with valid_precision_at_k value: 0.5533412415974692.\nBetter model found at epoch 5 with valid_precision_at_k value: 0.5577500988533018.\nBetter model found at epoch 6 with valid_precision_at_k value: 0.5627718465796757.\n\n\n\nlearn_r.fit(5, lr=1e-2)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\ntrain_precision_at_k\nvalid_loss\nvalid_precision_at_k\ntime\n\n\n\n\n0\n0.004934\n0.530920\n0.005644\n0.562396\n53:13\n\n\n1\n0.004600\n0.533206\n0.005640\n0.562119\n53:24\n\n\n2\n0.004838\n0.534152\n0.005616\n0.564591\n53:26\n\n\n3\n0.004992\n0.535571\n0.005625\n0.563721\n56:08\n\n\n4\n0.004729\n0.537246\n0.005623\n0.564690\n52:13\n\n\n\n\n\nBetter model found at epoch 2 with valid_precision_at_k value: 0.5645907473309606.\nBetter model found at epoch 4 with valid_precision_at_k value: 0.5646896006326609.\n\n\n\nlearn_r.freeze_to(-3)\n\n\nlearn_r.fit(5, lr=1e-2)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\ntrain_precision_at_k\nvalid_loss\nvalid_precision_at_k\ntime\n\n\n\n\n0\n0.004512\n0.542960\n0.005425\n0.574812\n1:08:34\n\n\n1\n0.004399\n0.549391\n0.005388\n0.580803\n1:07:03\n\n\n2\n0.004393\n0.551346\n0.005378\n0.580269\n1:11:35\n\n\n3\n0.004305\n0.552264\n0.005399\n0.580249\n1:07:49\n\n\n4\n0.004638\n0.552713\n0.005320\n0.582780\n1:10:15\n\n\n\n\n\nBetter model found at epoch 0 with valid_precision_at_k value: 0.5748121787267698.\nBetter model found at epoch 1 with valid_precision_at_k value: 0.5808026888098061.\nBetter model found at epoch 4 with valid_precision_at_k value: 0.5827797548438116.\n\n\n\nlearn_r.unfreeze()\n\n\nlearn_r.fit(8, lr=1e-6, wd=0.3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\ntrain_precision_at_k\nvalid_loss\nvalid_precision_at_k\ntime\n\n\n\n\n0\n0.004360\n0.564825\n0.005301\n0.584322\n1:21:00\n\n\n1\n0.004090\n0.566434\n0.005286\n0.585765\n1:23:50\n\n\n2\n0.004282\n0.567989\n0.005273\n0.587050\n1:22:50\n\n\n3\n0.004444\n0.569103\n0.005261\n0.588078\n1:23:37\n\n\n4\n0.004214\n0.570427\n0.005251\n0.588790\n1:22:28\n\n\n5\n0.004118\n0.571420\n0.005242\n0.589522\n1:21:29\n\n\n6\n0.004051\n0.572369\n0.005234\n0.590134\n1:39:58\n\n\n7\n0.004023\n0.573819\n0.005226\n0.590589\n1:23:11\n\n\n\n\n\nBetter model found at epoch 0 with valid_precision_at_k value: 0.5843218663503366.\nBetter model found at epoch 1 with valid_precision_at_k value: 0.58576512455516.\nBetter model found at epoch 2 with valid_precision_at_k value: 0.5870502174772637.\nBetter model found at epoch 3 with valid_precision_at_k value: 0.5880782918149465.\nBetter model found at epoch 4 with valid_precision_at_k value: 0.5887900355871885.\nBetter model found at epoch 5 with valid_precision_at_k value: 0.5895215500197706.\nBetter model found at epoch 6 with valid_precision_at_k value: 0.5901344404903124.\nBetter model found at epoch 7 with valid_precision_at_k value: 0.5905891656781339.\n\n\n\nlearn_r.fit(2, lr=1e-6, wd=0.3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\ntrain_precision_at_k\nvalid_loss\nvalid_precision_at_k\ntime\n\n\n\n\n0\n0.003961\n0.575112\n0.005219\n0.591182\n1:21:12\n\n\n1\n0.003683\n0.577906\n0.005213\n0.591894\n1:20:49\n\n\n\n\n\nBetter model found at epoch 0 with valid_precision_at_k value: 0.5911822854883352.\nBetter model found at epoch 1 with valid_precision_at_k value: 0.591894029260577.\n\n\n\nlearn_r.fit(10, lr=1e-6, wd=0.3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\ntrain_precision_at_k\nvalid_loss\nvalid_precision_at_k\ntime\n\n\n\n\n0\n0.004261\n0.574649\n0.005207\n0.592329\n1:24:13\n\n\n1\n0.004006\n0.575258\n0.005201\n0.592724\n1:21:54\n\n\n2\n0.004200\n0.575567\n0.005196\n0.592803\n1:24:06\n\n\n3\n0.004367\n0.576012\n0.005191\n0.593041\n1:25:26\n\n\n4\n0.004151\n0.576616\n0.005186\n0.593575\n1:27:14\n\n\n5\n0.004059\n0.577233\n0.005182\n0.594089\n1:26:12\n\n\n6\n0.003996\n0.577824\n0.005178\n0.594365\n1:28:34\n\n\n7\n0.003973\n0.578928\n0.005174\n0.594504\n1:24:08\n\n\n8\n0.003915\n0.579819\n0.005170\n0.594919\n1:25:19\n\n\n9\n0.003663\n0.582147\n0.005166\n0.595275\n1:24:36\n\n\n\n\n\nBetter model found at epoch 0 with valid_precision_at_k value: 0.5923289837880584.\nBetter model found at epoch 1 with valid_precision_at_k value: 0.5927243969948597.\nBetter model found at epoch 2 with valid_precision_at_k value: 0.5928034796362198.\nBetter model found at epoch 3 with valid_precision_at_k value: 0.593040727560301.\nBetter model found at epoch 4 with valid_precision_at_k value: 0.5935745353894823.\nBetter model found at epoch 5 with valid_precision_at_k value: 0.5940885725583238.\nBetter model found at epoch 6 with valid_precision_at_k value: 0.5943653618030844.\nBetter model found at epoch 7 with valid_precision_at_k value: 0.5945037564254647.\nBetter model found at epoch 8 with valid_precision_at_k value: 0.5949189402926058.\nBetter model found at epoch 9 with valid_precision_at_k value: 0.5952748121787272.\n\n\n\nlearn_r.fit(12, lr=1e-6, wd=0.3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\ntrain_precision_at_k\nvalid_loss\nvalid_precision_at_k\ntime\n\n\n\n\n0\n0.004219\n0.578651\n0.005163\n0.595888\n1:29:03\n\n\n1\n0.003969\n0.578949\n0.005159\n0.596303\n1:33:03\n\n\n2\n0.004162\n0.579253\n0.005156\n0.596718\n1:28:22\n\n\n3\n0.004331\n0.579554\n0.005153\n0.596758\n1:25:06\n\n\n4\n0.004119\n0.580130\n0.005150\n0.597133\n1:27:07\n\n\n5\n0.004029\n0.580469\n0.005148\n0.597351\n1:25:24\n\n\n6\n0.003968\n0.580746\n0.005145\n0.597568\n1:28:07\n\n\n7\n0.003946\n0.581680\n0.005142\n0.597786\n1:24:29\n\n\n8\n0.003890\n0.582365\n0.005140\n0.597924\n1:25:56\n\n\n9\n0.003661\n0.584672\n0.005138\n0.598062\n1:24:18\n\n\n10\n0.004071\n0.581008\n0.005135\n0.598181\n1:30:41\n\n\n11\n0.003967\n0.581212\n0.005133\n0.598399\n1:26:47\n\n\n\n\n\nBetter model found at epoch 0 with valid_precision_at_k value: 0.5958877026492689.\nBetter model found at epoch 1 with valid_precision_at_k value: 0.5963028865164102.\nBetter model found at epoch 2 with valid_precision_at_k value: 0.5967180703835511.\nBetter model found at epoch 3 with valid_precision_at_k value: 0.5967576117042313.\nBetter model found at epoch 4 with valid_precision_at_k value: 0.5971332542506922.\nBetter model found at epoch 5 with valid_precision_at_k value: 0.597350731514433.\nBetter model found at epoch 6 with valid_precision_at_k value: 0.5975682087781735.\nBetter model found at epoch 7 with valid_precision_at_k value: 0.5977856860419142.\nBetter model found at epoch 8 with valid_precision_at_k value: 0.5979240806642946.\nBetter model found at epoch 9 with valid_precision_at_k value: 0.598062475286675.\nBetter model found at epoch 10 with valid_precision_at_k value: 0.5981810992487153.\nBetter model found at epoch 11 with valid_precision_at_k value: 0.5983985765124561.\n\n\n\nlearn_r.fit(12, lr=1e-6, wd=0.3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\ntrain_precision_at_k\nvalid_loss\nvalid_precision_at_k\ntime\n\n\n\n\n0\n0.004192\n0.581391\n0.005131\n0.598517\n1:29:22\n\n\n1\n0.003944\n0.581576\n0.005129\n0.598715\n1:25:02\n\n\n2\n0.004137\n0.581732\n0.005127\n0.598952\n1:25:53\n\n\n3\n0.004306\n0.581890\n0.005125\n0.599091\n1:26:09\n\n\n4\n0.004097\n0.582328\n0.005123\n0.599308\n1:25:22\n\n\n5\n0.004008\n0.582727\n0.005122\n0.599387\n1:25:37\n\n\n6\n0.003949\n0.582932\n0.005120\n0.599249\n1:27:32\n\n\n7\n0.003927\n0.583663\n0.005119\n0.599585\n1:30:06\n\n\n8\n0.003872\n0.584260\n0.005117\n0.599703\n1:26:04\n\n\n9\n0.003669\n0.586653\n0.005116\n0.599941\n1:28:20\n\n\n10\n0.004054\n0.582681\n0.005114\n0.599901\n1:25:49\n\n\n11\n0.003952\n0.583013\n0.005113\n0.600217\n1:30:22\n\n\n\n\n\nBetter model found at epoch 0 with valid_precision_at_k value: 0.5985172004744963.\nBetter model found at epoch 1 with valid_precision_at_k value: 0.5987149070778969.\nBetter model found at epoch 2 with valid_precision_at_k value: 0.5989521550019774.\nBetter model found at epoch 3 with valid_precision_at_k value: 0.5990905496243577.\nBetter model found at epoch 4 with valid_precision_at_k value: 0.5993080268880984.\nBetter model found at epoch 5 with valid_precision_at_k value: 0.5993871095294585.\nBetter model found at epoch 7 with valid_precision_at_k value: 0.5995848161328591.\nBetter model found at epoch 8 with valid_precision_at_k value: 0.5997034400948994.\nBetter model found at epoch 9 with valid_precision_at_k value: 0.5999406880189802.\nBetter model found at epoch 11 with valid_precision_at_k value: 0.600217477263741.\n\n\n\nlearn_r.fit(12, lr=1e-6, wd=0.4)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\ntrain_precision_at_k\nvalid_loss\nvalid_precision_at_k\ntime\n\n\n\n\n0\n0.004177\n0.583113\n0.005111\n0.600356\n1:25:28\n\n\n1\n0.003931\n0.583075\n0.005110\n0.600336\n1:29:16\n\n\n2\n0.004125\n0.583175\n0.005109\n0.600395\n1:25:16\n\n\n3\n0.004296\n0.583358\n0.005109\n0.600455\n1:29:21\n\n\n4\n0.004089\n0.583721\n0.005108\n0.600316\n1:31:31\n\n\n5\n0.004001\n0.583767\n0.005107\n0.600435\n1:21:54\n\n\n6\n0.003945\n0.583991\n0.005107\n0.600593\n1:25:02\n\n\n7\n0.003924\n0.584695\n0.005107\n0.600672\n1:26:36\n\n\n8\n0.003871\n0.585206\n0.005106\n0.601008\n1:24:04\n\n\n9\n0.003696\n0.587589\n0.005106\n0.600989\n1:30:52\n\n\n10\n0.004054\n0.583418\n0.005106\n0.601186\n1:30:08\n\n\n11\n0.003954\n0.583651\n0.005105\n0.601147\n1:26:53\n\n\n\n\n\nBetter model found at epoch 2 with valid_precision_at_k value: 0.6003954132068013.\nBetter model found at epoch 3 with valid_precision_at_k value: 0.6004547251878214.\nBetter model found at epoch 6 with valid_precision_at_k value: 0.6005931198102016.\nBetter model found at epoch 7 with valid_precision_at_k value: 0.6006722024515617.\nBetter model found at epoch 8 with valid_precision_at_k value: 0.6010083036773427.\nBetter model found at epoch 10 with valid_precision_at_k value: 0.6011862396204035.\n\n\n\nlearn_r.fit(10, lr=1e-6, wd=0.4)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\ntrain_precision_at_k\nvalid_loss\nvalid_precision_at_k\ntime\n\n\n\n\n0\n0.004128\n0.583594\n0.005105\n0.601186\n1:24:23\n\n\n1\n0.004299\n0.583698\n0.005105\n0.601305\n1:25:33\n\n\n2\n0.004092\n0.584118\n0.005105\n0.601423\n1:27:00\n\n\n3\n0.004005\n0.584058\n0.005105\n0.601463\n1:25:26\n\n\n4\n0.003949\n0.584217\n0.005105\n0.601522\n1:24:53\n\n\n5\n0.003928\n0.584968\n0.005105\n0.601404\n1:27:26\n\n\n6\n0.003875\n0.585415\n0.005105\n0.601661\n1:22:49\n\n\n7\n0.003719\n0.587758\n0.005105\n0.601779\n1:24:33\n\n\n8\n0.004059\n0.583728\n0.005105\n0.601700\n1:23:08\n\n\n9\n0.003960\n0.583779\n0.005105\n0.601799\n1:26:34\n\n\n\n\n\nBetter model found at epoch 0 with valid_precision_at_k value: 0.6011862396204033.\nBetter model found at epoch 1 with valid_precision_at_k value: 0.6013048635824436.\nBetter model found at epoch 2 with valid_precision_at_k value: 0.6014234875444842.\nBetter model found at epoch 3 with valid_precision_at_k value: 0.6014630288651643.\nBetter model found at epoch 4 with valid_precision_at_k value: 0.6015223408461846.\nBetter model found at epoch 6 with valid_precision_at_k value: 0.6016607354685648.\nBetter model found at epoch 7 with valid_precision_at_k value: 0.6017793594306049.\nBetter model found at epoch 9 with valid_precision_at_k value: 0.6017991300909451.\n\n\nStart here:\n\nprint(learn_r.save_model.fname)\nlearn_r.save_model.reset_on_fit = False\nassert learn_r.save_model.reset_on_fit is False\nlearn_r = learn_r.load(learn_r.save_model.fname)\nvalidate(learn_r)\n\nmimic3-9k_clas_full_r\nbest so far = None\n[0.005104772746562958, 0.6017991300909451]\nbest so far = None\n\n\n\n\n\n\n\n\n\n\nlearn_r.save_model.best = 0.60179913\n\nNow you can unfreeze and train more if you want:\n\nlearn_r.unfreeze()\n\nEnsemble:\n\nlearn = learn.load(learn.save_model.fname)\nlearn_r = learn_r.load(learn_r.save_model.fname)\npreds, targs = learn.get_preds()\npreds_r, targs = learn_r.get_preds()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprecision_at_k(preds.add(preds_r), targs, k=15)\n\n0.6167457493080268\n\n\n\n\nset_seed(1, reproducible=True)\n\n\nlr_min, lr_steep, lr_valley, lr_slide = learn_r.lr_find(suggest_funcs=(minimum, steep, valley, slide))\nlr_min, lr_steep, lr_valley, lr_slide\n\n\n\n\n\n\n\n\n(0.2754228591918945,\n 0.033113110810518265,\n 0.002511886414140463,\n 0.033113110810518265)\n\n\n\n\n\n\nlearn_r.fit_one_cycle(1, lr_max=0.14, moms=(0.8,0.7,0.8), wd=0.1)\nlearn_r.fit_one_cycle(5, lr_max=0.14, moms=(0.8,0.7,0.8), wd=0.1)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nprecision_at_k\ntime\n\n\n\n\n0\n0.396181\n0.218404\n0.223333\n00:05\n\n\n\n\n\nBetter model found at epoch 0 with precision_at_k value: 0.22333333333333333.\nBetter model found at epoch 0 with precision_at_k value: 0.2533333333333333.\nBetter model found at epoch 2 with precision_at_k value: 0.35333333333333333.\nBetter model found at epoch 3 with precision_at_k value: 0.37333333333333335.\nBetter model found at epoch 4 with precision_at_k value: 0.37666666666666665.\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nprecision_at_k\ntime\n\n\n\n\n0\n0.196439\n0.160080\n0.253333\n00:04\n\n\n1\n0.164353\n0.133897\n0.240000\n00:04\n\n\n2\n0.140529\n0.114523\n0.353333\n00:05\n\n\n3\n0.119238\n0.105214\n0.373333\n00:05\n\n\n4\n0.102833\n0.103365\n0.376667\n00:05\n\n\n\n\n\n\nvalidate(learn_r)\n\nbest so far = 0.3166666666666667\n[0.06839973479509354, 0.3166666666666667]\nbest so far = 0.3166666666666667\n\n\n\n\n\n\n\n\n\nThis was the result after just three epochs. Now let’s unfreeze the last two layers and do discriminative training:\nNow we will pass -2 to freeze_to to freeze all except the last two parameter groups:\n\nlearn_r.freeze_to(-2)\n\n\nlearn_r.fit_one_cycle(1, lr_max=1e-3, moms=(0.8,0.7,0.8), wd=0.1)\nlearn_r.fit_one_cycle(4, lr_max=1e-3, moms=(0.8,0.7,0.8), wd=0.1)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nprecision_at_k\ntime\n\n\n\n\n0\n0.066128\n0.101364\n0.363333\n00:06\n\n\n\n\n\nBetter model found at epoch 0 with precision_at_k value: 0.36333333333333334.\nBetter model found at epoch 0 with precision_at_k value: 0.37333333333333335.\nBetter model found at epoch 3 with precision_at_k value: 0.37666666666666665.\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nprecision_at_k\ntime\n\n\n\n\n0\n0.063144\n0.099782\n0.373333\n00:06\n\n\n1\n0.060468\n0.100892\n0.320000\n00:06\n\n\n2\n0.058185\n0.098354\n0.353333\n00:07\n\n\n3\n0.056312\n0.096989\n0.376667\n00:05\n\n\n\n\n\n\nvalidate(learn_r)\n\nbest so far = 0.29000000000000004\n[0.08007880300283432, 0.29000000000000004]\nbest so far = 0.29000000000000004\n\n\n\n\n\n\n\n\n\nNow we will unfreeze a bit more, recompute learning rate and continue training:\n\nlearn_r.freeze_to(-3)\n\n\nlearn_r.fit_one_cycle(5, lr_max=1e-3, moms=(0.8,0.7,0.8), wd=0.2)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nprecision_at_k\ntime\n\n\n\n\n0\n0.052942\n0.098026\n0.373333\n00:08\n\n\n1\n0.053420\n0.097696\n0.373333\n00:07\n\n\n2\n0.051679\n0.097380\n0.380000\n00:07\n\n\n3\n0.049411\n0.098192\n0.370000\n00:07\n\n\n4\n0.047627\n0.097431\n0.376667\n00:08\n\n\n\n\n\nBetter model found at epoch 0 with precision_at_k value: 0.37333333333333335.\nBetter model found at epoch 2 with precision_at_k value: 0.38.\n\n\n\nvalidate(learn_r)\n\nbest so far = 0.2966666666666667\n[0.08150946348905563, 0.2966666666666667]\nbest so far = 0.2966666666666667\n\n\n\n\n\n\n\n\n\nFinally, we will unfreeze the whole model and perform training: (Caution: Check if you got enough memory left!)\n\ncudamem()\n\nGPU: Quadro RTX 8000\nYou are using 9.798828125 GB\nTotal GPU memory = 44.99969482421875 GB\n\n\n\nlearn_r.unfreeze()\n\nAt this point, it starts overfitting too soon so either progressively increase bs or increase wd or decrease lr_max. Another way would be to drop out label embeddings, but for this one needs to carefully adjust the dropout prob. Same concept regarding dropping out the seqs nh.\n\nlr_min, lr_steep, lr_valley, lr_slide = learn_r.lr_find(suggest_funcs=(minimum,steep,valley,slide), num_it=100)\n\n\n\n\n\n\n\n\n\n\n\n\nlr_min, lr_steep, lr_valley, lr_slide\n\n(0.0003019951749593019,\n 6.309573450380412e-07,\n 0.0003981071640737355,\n 0.009120108559727669)\n\n\n\nlearn_r.fit_one_cycle(10, lr_max=1e-4, moms=(0.8,0.7,0.8), wd=0.1)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nprecision_at_k\ntime\n\n\n\n\n0\n0.064373\n0.087895\n0.296667\n00:11\n\n\n1\n0.064310\n0.087949\n0.296667\n00:10\n\n\n2\n0.063739\n0.089096\n0.293333\n00:10\n\n\n3\n0.062852\n0.090236\n0.293333\n00:10\n\n\n4\n0.062443\n0.091487\n0.293333\n00:10\n\n\n5\n0.062105\n0.091792\n0.290000\n00:10\n\n\n6\n0.061126\n0.091894\n0.286667\n00:10\n\n\n7\n0.060621\n0.092186\n0.283333\n00:10\n\n\n8\n0.060148\n0.092261\n0.283333\n00:10\n\n\n9\n0.059983\n0.092269\n0.283333\n00:10\n\n\n\n\n\nBetter model found at epoch 0 with precision_at_k value: 0.29666666666666675.\n\n\n\nvalidate(learn_r)\n\nbest so far = 0.2866666666666667\n[0.08382819592952728, 0.2866666666666667]\nbest so far = 0.2866666666666667\n\n\n\n\n\n\n\n\n\nEnsemble of fwd+bwd:\n\npreds, targs = learn.get_preds()\npreds_r, targs = learn_r.get_preds()\nprecision_at_k(preds+preds_r, targs, k=15)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0.3933333333333333"
  },
  {
    "objectID": "tutorial.train_mimic3.html#tta",
    "href": "tutorial.train_mimic3.html#tta",
    "title": "Training an XML Text Classifier",
    "section": "TTA",
    "text": "TTA\n\nlearn = xmltext_classifier_learner(dls_clas, AWD_LSTM, drop_mult=0.1, max_len=None, \n                                   metrics=partial(precision_at_k, k=15), path=tmp, cbs=cbs,\n                                   pretrained=False,\n                                   splitter=None,\n                                   running_decoder=True).to_fp16()\nlearn_r = xmltext_classifier_learner(dls_clas_r, AWD_LSTM, drop_mult=0.1, max_len=None, \n                                     metrics=partial(precision_at_k, k=15), path=tmp, cbs=cbs,\n                                     pretrained=False,\n                                     splitter=None,\n                                     running_decoder=True).to_fp16()\n\n\nlearn = learn.load(learn.save_model.fname)\n# learn = learn.load((learn.path/learn.model_dir)/'mimic3-9k_clas_tiny')\nlearn_r = learn_r.load(learn_r.save_model.fname)\n\n\nvalidate(learn)\n\nbest so far = -inf\n[0.005114026367664337, 0.5988730723606168]\nbest so far = -inf\n\n\n\n\n\n\n\n\n\n\nvalidate(learn_r)\n\nbest so far = -inf\n[0.0051385280676186085, 0.5965005931198106]\nbest so far = -inf\n\n\n\n\n\n\n\n\n\n\nL(learn, learn_r).attrgot('model').itemgot(1)\n\n(#2) [LabelAttentionClassifier(\n  (pay_attn): XMLAttention(\n    (lbs): Embedding(8922, 400)\n    (attn): fastai.layers.Lambda(func=&lt;xcube.layers._Pay_Attention object&gt;)\n  )\n  (boost_attn): ElemWiseLin(\n    (lin): Linear(in_features=400, out_features=8922, bias=True)\n  )\n),LabelAttentionClassifier(\n  (pay_attn): XMLAttention(\n    (lbs): Embedding(8922, 400)\n    (attn): fastai.layers.Lambda(func=&lt;xcube.layers._Pay_Attention object&gt;)\n  )\n  (boost_attn): ElemWiseLin(\n    (lin): Linear(in_features=400, out_features=8922, bias=True)\n  )\n)]\n\n\n\nfrom xcube.layers import *\nfrom xcube.layers import _linear_attention, _planted_attention\nfrom xcube.text.learner import _get_text_vocab, _get_label_vocab\n\n\n@patch \ndef load_brain(self:TextLearner,\n    file_wgts: str, # Filename of the saved attention wgts\n    file_bias: str, # Filename of the saved label bias\n    device:(int,str,torch.device)=None # Device used to load, defaults to `dls` device\n    ):\n    \"\"\"Load the pre-learnt label specific attention weights for each token from `file` located in the \n    model directory, optionally ensuring it's one `device`\n    \"\"\"\n    brain_path = join_path_file(file_wgts, self.path/self.model_dir, ext='.pkl')\n    bias_path = join_path_file(file_bias, self.path/self.model_dir, ext='.pkl')\n    brain_bootstrap = torch.load(brain_path, map_location=default_device() if device is None else device)\n    *brain_vocab, brain = mapt(brain_bootstrap.get, ['toks', 'lbs', 'mutual_info_jaccard'])\n    brain_vocab = L(brain_vocab).map(listify)\n    vocab = L(_get_text_vocab(self.dls), _get_label_vocab(self.dls)).map(listify)\n    brain_bias = torch.load(bias_path, map_location=default_device() if device is None else device)\n    brain_bias = brain_bias[:, :, 0].squeeze(-1)\n    print(\"Performing brainsplant...\")\n    self.brain, self.lbsbias, *_ = brainsplant(vocab, brain_vocab, brain, brain_bias)\n    print(\"Successfull!\")\n    # import pdb; pdb.set_trace()\n    plant_attn_layer = Lambda(Planted_Attention(self.brain))\n    setattr(self.model[1].pay_attn, 'plant_attn', plant_attn_layer)\n    assert self.model[1].pay_attn.plant_attn.func.f is _planted_attention\n    return self\n\n\n# learn = learn.load_brain('mimic3-9k_tok_lbl_info', 'p_L')\nlearn_r = learn_r.load_brain('mimic3-9k_tok_lbl_info', 'p_L')\n\nPerforming brainsplant...\nSuccessfull!\n\n\n\n\n\n\n\n\n\n\nL(learn, learn_r).attrgot('model').itemgot(1)\n\n(#2) [LabelAttentionClassifier(\n  (pay_attn): XMLAttention(\n    (lbs): Embedding(8922, 400)\n    (attn): fastai.layers.Lambda(func=&lt;xcube.layers._Pay_Attention object&gt;)\n  )\n  (boost_attn): ElemWiseLin(\n    (lin): Linear(in_features=400, out_features=8922, bias=True)\n  )\n),LabelAttentionClassifier(\n  (pay_attn): XMLAttention(\n    (lbs): Embedding(8922, 400)\n    (attn): fastai.layers.Lambda(func=&lt;xcube.layers._Pay_Attention object&gt;)\n    (plant_attn): fastai.layers.Lambda(func=&lt;xcube.layers._Pay_Attention object&gt;)\n  )\n  (boost_attn): ElemWiseLin(\n    (lin): Linear(in_features=400, out_features=8922, bias=True)\n  )\n)]\n\n\n\n@patch\ndef forward(self:XMLAttention, inp, sentc, mask):\n    # sent is the ouput of SentenceEncoder i.e., (bs, max_len tokens, nh)\n    test_eqs(inp.shape, sentc.shape[:-1], mask.shape)\n    top_tok_attn_wgts = F.softmax(self.attn(sentc), dim=1).masked_fill(mask[:,:,None], 0) # lbl specific wts for each token (bs, max_len, n_lbs)\n    attn_wgts = self.plant_attn(inp).masked_fill(mask[:,:,None], 0) #shape (bs, bptt, n_lbs)\n    top_lbs_attn_wgts = attn_wgts.inattention(k=1, sort_dim=-1, sp_dim=1) # applying `inattention` across the lbs dim\n    lbs_cf = top_lbs_attn_wgts.sum(dim=1) #shape (bs, n_lbs)\n    # pdb.set_trace()\n    return lincomb(sentc, wgts=top_tok_attn_wgts.transpose(1,2)), top_tok_attn_wgts, lbs_cf # for each lbl do a linear combi of all the tokens based on attn_wgts (bs, num_lbs, nh)\n\n\nfrom xcube.text.models.core import _pad_tensor\n\n\n@patch\ndef forward(self:LabelAttentionClassifier, sentc):\n    if isinstance(sentc, tuple): inp, sentc, mask = sentc # sentc is the stuff coming outta SentenceEncoder i.e., shape (bs, max_len, nh) in other words the concatenated output of the AWD_LSTM\n    test_eqs(inp.shape, sentc.shape[:-1], mask.shape)\n    sentc = sentc.masked_fill(mask[:, :, None], 0)\n    attn, wgts, lbs_cf = self.pay_attn(inp, sentc, mask) #shape attn: (bs, n_lbs, n_hidden), wgts: (bs, max_len, n_lbs), lbs_cf : (bs, n_lbs)\n    attn = self.boost_attn(attn) # shape (bs, n_lbs, n_hidden)\n    bs = self.hl.size(0)\n    self.hl = self.hl.to(sentc.device)\n    pred = self.hl + _pad_tensor(attn.sum(dim=2), bs) + self.label_bias  # shape (bs, n_lbs)\n\n    if self.y_range is not None: pred = sigmoid_range(pred, *self.y_range)\n    return pred, attn, wgts, lbs_cf\n\n\n@patch\ndef after_pred(self:RNNCallback): self.learn.pred,self.raw_out,self.out, _ = [o[-1] if is_listy(o) else o for o in self.pred]\n\n\nfrom xcube.layers import inattention\n\n\nclass FetchCallback(Callback):\n    order=RNNCallback.order-1\n    \"Save the preds, attention wgts and labels confidence outputs\"\n    \n    def __init__(self, predslog, rank): store_attr()\n    \n    def before_validate(self):\n        self.predslog =  open(self.predslog, 'wb')\n        self.csv_rank =  open(self.rank, 'w', newline='')\n        self.writer_rank = csv.writer(self.csv_rank)\n        self.writer_rank.writerow(L('subset', '#true (t)', '#true^subset (ts)', '% (ts/t)', '#top_preds', '#top_preds^subset', '#true^top_preds (ttp)', '#top_preds^subset^true', 'precision (ttp/15)', '#true^topk (ttk)', 'prec (ttk/15)'))\n    \n    @staticmethod\n    def _get_stat(y, cf, preds):\n        subset = cf&gt;0\n        truth = y.sum()\n        num_true_subset = torch.logical_and(y == 1, subset).sum()\n        assert preds.logical_and(tensor(1)).all() # check preds does not have any zeros\n        top_preds = preds.inattention(k=14)\n        num_top_preds = torch.as_tensor(top_preds.nonzero().flatten().numel())\n        num_top_preds_subset = torch.logical_and(top_preds, subset).sum() #\n        num_true_top_preds = torch.logical_and(y==1, top_preds).sum()\n        num_top_preds_subset_true = torch.logical_and(top_preds, subset).logical_and(y==1).sum()#\n        topk = preds.topk(k=15).indices\n        num_true_topk = y[topk].sum()\n        _d = dict(subsum = subset.sum(), num_truth = truth, num_true_subset = num_true_subset, pct_true_subset = num_true_subset.div(truth).mul(100), \n                    num_top_preds = num_top_preds, num_top_preds_subset = num_top_preds_subset, true_top_preds = num_true_top_preds, \n                    num_top_preds_subset_also_true =  num_top_preds_subset_true, \n                    prec_shdbe = torch.min(num_true_top_preds.div(15.0), torch.tensor(1)), \n                    num_true_top_k=num_true_topk, prec=num_true_topk.div(15))\n        # pdb.set_trace()\n        return L(_d.values()).map(Self.item())\n    \n    def after_pred(self): \n        self.actual_pred, self.attn, self.wgts, self.lbs_cf = [o[-1] if is_listy(o) else o for o in self.pred]\n        for y,cf, preds in zip(self.y, self.lbs_cf, self.actual_pred):\n            # self.writer_predslog.writerow(L(y, cf, preds).map(cpupy))\n            # self.csv_predslog.flush()\n            pickle.dump((y,cf,preds), self.predslog)\n            self.writer_rank.writerow(FetchCallback._get_stat(y, cf, preds))\n            self.csv_rank.flush()\n\n    def after_validate(self): \n        self.predslog.close(); \n        self.csv_rank.close()\n\n\nvalidate(learn)\n\nbest so far = None\n[0.005114026367664337, 0.5988730723606168]\nbest so far = None\n\n\n\n\n\n\n\n\n\n\nvalidate(learn_r)\n\nbest so far = -inf\n[0.0051385280676186085, 0.5965005931198106]\nbest so far = -inf\n\n\n\n\n\n\n\n\n\n\npredslog = join_path_file(fname+'_predslog', learn.path/learn.model_dir, ext='.pkl')\nrank = join_path_file(fname+'_rank', learn.path/learn.model_dir, ext='.csv')\nfc = FetchCallback(predslog, rank)\n\n\nprint(f\"pid = {os.getpid()}\")\nwith learn.added_cbs([fc]):\n    validate(learn)\n\npid = 1791\n[0.0052354661747813225, 0.5901542111506525]\n\n\n\n\n\n\n\n\n\n\nwith open(Path.cwd()/'tmp.pkl', 'wb') as pf:\n    for _ in range(7):\n        tl = [torch.randn(5) for _ in range(3)]\n        print(tl)\n        pickle.dump(tl, pf)\n\n\n# with open(Path.cwd()/'tmp.pkl', 'rb') as f:\nwith open(predslog, 'rb') as f:\n    while True:\n        try:\n            obj = pickle.load(f)\n            print(obj)\n        except EOFError: break\n\n\ndatetime.fromtimestamp(predslog.stat().st_mtime).time()\n\ndatetime.time(18, 21, 50, 118789)\n\n\n\nwith open(predslog, 'rb') as preds_file, open(rank, 'w', newline='') as rank_file:\n    writer = csv.writer(rank_file)\n    writer.writerow(L('subset', '#true (t)', '#true^subset (ts)', '% (ts/t)', '#top_preds', '#top_preds^subset', '#true^top_preds (ttp)', '#top_preds^subset^true', 'precision (ttp/15)', '#true^topk (ttk)', 'prec (ttk/15)'))\n    while True:\n        try:\n            y, cf, preds = pickle.load(preds_file)\n            writer.writerow(FetchCallback._get_stat(y, cf, preds))\n            rank_file.flush()\n        except EOFError: break\n\n\ndatetime.fromtimestamp(rank.stat().st_mtime).time()\n\ndatetime.time(18, 35, 9, 58730)\n\n\n\ndf_rank = pd.read_csv(rank)\nprint(f\"{len(df_rank)=}\")\ndf_rank.head()\n\nlen(df_rank)=3372\n\n\n\n\n\n\n\n\n\nsubset\n#true (t)\n#true^subset (ts)\n% (ts/t)\n#top_preds\n#top_preds^subset\n#true^top_preds (ttp)\n#top_preds^subset^true\nprecision (ttp/15)\n#true^topk (ttk)\nprec (ttk/15)\n\n\n\n\n0\n936\n39.0\n33\n84.615387\n15\n15\n9\n9\n0.600000\n9.0\n0.600000\n\n\n1\n827\n38.0\n30\n78.947372\n15\n15\n13\n13\n0.866667\n13.0\n0.866667\n\n\n2\n700\n40.0\n32\n80.000000\n15\n15\n13\n13\n0.866667\n13.0\n0.866667\n\n\n3\n797\n27.0\n17\n62.962959\n15\n14\n8\n7\n0.533333\n8.0\n0.533333\n\n\n4\n642\n30.0\n27\n90.000000\n15\n14\n9\n8\n0.600000\n9.0\n0.600000\n\n\n\n\n\n\n\nWe are missing out on:\n\ndf_rank[df_rank['#true^top_preds (ttp)'] != df_rank['#true^topk (ttk)']]\n\n\n\n\n\n\n\n\nsubset\n#true (t)\n#true^subset (ts)\n% (ts/t)\n#top_preds\n#top_preds^subset\n#true^top_preds (ttp)\n#top_preds^subset^true\nprecision (ttp/15)\n#true^topk (ttk)\nprec (ttk/15)\n\n\n\n\n\n\n\n\n\n\ndf_rank[df_rank['#true^top_preds (ttp)'] - df_rank['#top_preds^subset^true'] &gt;= 1]\n\n\n\n\n\n\n\n\nsubset\n#true (t)\n#true^subset (ts)\n% (ts/t)\n#top_preds\n#top_preds^subset\n#true^top_preds (ttp)\n#top_preds^subset^true\nprecision (ttp/15)\n#true^topk (ttk)\nprec (ttk/15)\n\n\n\n\n3\n797\n27.0\n17\n62.962959\n15\n14\n8\n7\n0.533333\n8.0\n0.533333\n\n\n4\n642\n30.0\n27\n90.000000\n15\n14\n9\n8\n0.600000\n9.0\n0.600000\n\n\n11\n782\n15.0\n12\n80.000000\n15\n13\n10\n9\n0.666667\n10.0\n0.666667\n\n\n18\n656\n22.0\n15\n68.181816\n15\n14\n9\n8\n0.600000\n9.0\n0.600000\n\n\n21\n653\n38.0\n31\n81.578949\n15\n14\n11\n10\n0.733333\n11.0\n0.733333\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3363\n203\n14.0\n10\n71.428574\n15\n12\n8\n7\n0.533333\n8.0\n0.533333\n\n\n3364\n122\n6.0\n4\n66.666672\n15\n11\n5\n4\n0.333333\n5.0\n0.333333\n\n\n3366\n168\n18.0\n13\n72.222221\n15\n10\n8\n6\n0.533333\n8.0\n0.533333\n\n\n3367\n167\n18.0\n9\n50.000000\n15\n10\n12\n8\n0.800000\n12.0\n0.800000\n\n\n3371\n111\n4.0\n2\n50.000000\n15\n6\n1\n0\n0.066667\n1.0\n0.066667\n\n\n\n\n1582 rows × 11 columns\n\n\n\nTODO: - Implement sortish=shuffle+split+sort+cat\n\ndf_rank['precision (ttp/15)'].mean(), df_rank['prec (ttk/15)'].mean()\n\n(0.590154242462093, 0.590154242462093)\n\n\n\ndf = pd.DataFrame({\n    'Name': ['Alice', 'Bob', 'Charlie', 'David'],\n    'Age': [25, 30, 35, 40],\n    'Salary': [50000, 60000, 70000, 80000]\n})\ndf.style.applymap(lambda _: f\"color:{'blue'}\", subset=['Salary', 'Name'])\n\n\n\n\n\n\n \nName\nAge\nSalary\n\n\n\n\n0\nAlice\n25\n50000\n\n\n1\nBob\n30\n60000\n\n\n2\nCharlie\n35\n70000\n\n\n3\nDavid\n40\n80000\n\n\n\n\n\n\n# df.loc[df['precision (tpc/t)'] &gt; 1.0, 'precision (tpc/t)'] = 1.0\n# df.head()\n# df.loc[:, 'precision (tpc/t)'].mean(), df['prec'].mean()\n\n\ntbar = tqdm(learn.dls.valid, leave=False) \nfor xb,yb in tbar:\n    tbar.set_postfix({'xb':xb.shape, 'yb': yb.shape})\n    pred, *_ = learn.model(xb)\n    prec = precision_at_k(pred, yb)\n    print(prec)\n\n\n\n\n0.6833333333333333\n0.6791666666666667\n0.65\n0.6625000000000001\n0.7083333333333334\n0.7458333333333333\n0.6375\n0.675\n0.7\n0.6000000000000001\n0.6625000000000001\n0.7041666666666666\n0.6666666666666666\n0.6875\n0.7416666666666667\n0.65\n0.6458333333333334\n0.675\n0.5958333333333333\n0.7166666666666666\n0.6791666666666667\n0.6666666666666667\n0.6\n0.6749999999999999\n0.6666666666666666\n0.6291666666666667\n0.7541666666666667\n0.6666666666666667\n0.6416666666666666\n0.6416666666666666\n0.7083333333333333\n0.6458333333333334\n0.6916666666666667\n0.6833333333333333\n0.6916666666666667\n0.6375\n0.6375000000000001\n0.6125\n0.6541666666666667\n0.7000000000000001\n0.7125000000000001\n0.6291666666666667\n0.6624999999999999\n0.6166666666666667\n0.6625000000000001\n0.75\n0.6916666666666667\n0.6958333333333333\n0.6291666666666667\n0.6666666666666667\n0.5625\n0.6041666666666666\n0.75\n0.6291666666666667\n0.6041666666666666\n0.65\n0.6458333333333333\n0.6791666666666667\n0.5833333333333333\n0.6333333333333333\n0.6416666666666666\n0.5833333333333333\n0.5791666666666666\n0.575\n0.6124999999999999\n0.5833333333333333\n0.6625\n0.5958333333333332\n0.6583333333333333\n0.625\n0.7125\n0.6375\n0.6416666666666667\n0.5249999999999999\n0.6666666666666667\n0.6083333333333333\n0.6958333333333333\n0.6458333333333334\n0.6375\n0.55\n0.6458333333333333\n0.6291666666666667\n0.5916666666666666\n0.6041666666666667\n0.7208333333333333\n0.6000000000000001\n0.65\n0.6166666666666667\n0.5916666666666666\n0.6208333333333333\n0.5458333333333334\n0.6166666666666667\n0.6000000000000001\n0.5791666666666666\n0.5791666666666666\n0.5708333333333333\n0.5875\n0.6166666666666667\n0.6666666666666666\n0.6583333333333334\n0.7124999999999999\n0.6625000000000001\n0.7124999999999999\n0.6833333333333333\n0.6291666666666667\n0.5541666666666667\n0.6666666666666667\n0.6416666666666666\n0.6208333333333333\n0.5791666666666666\n0.5708333333333333\n0.5958333333333334\n0.5666666666666667\n0.625\n0.6125\n0.6583333333333333\n0.6416666666666666\n0.6583333333333333\n0.6416666666666666\n0.6666666666666667\n0.6375\n0.49166666666666664\n0.575\n0.6458333333333333\n0.5333333333333333\n0.5833333333333333\n0.6083333333333334\n0.5874999999999999\n0.5958333333333334\n0.5833333333333334\n0.5625\n0.5375\n0.5916666666666667\n0.5291666666666667\n0.6166666666666667\n0.5916666666666667\n0.5875\n0.5458333333333334\n0.5041666666666667\n0.6083333333333334\n0.6083333333333334\n0.5874999999999999\n0.5125\n0.55\n0.625\n0.5708333333333333\n0.5166666666666666\n0.5541666666666667\n0.5291666666666666\n0.5958333333333333\n0.625\n0.5875\n0.5458333333333334\n0.4708333333333333\n0.48333333333333334\n0.6458333333333333\n0.5333333333333333\n0.425\n0.5125\n0.5041666666666667\n0.49583333333333335\n0.525\n0.5208333333333334\n0.4125\n0.5791666666666667\n0.6125\n0.49583333333333335\n0.5166666666666667\n0.5333333333333333\n0.5125\n0.5833333333333333\n0.65\n0.5208333333333334\n0.5041666666666667\n0.46249999999999997\n0.4375\n0.41250000000000003\n0.5375\n0.43333333333333335\n0.48750000000000004\n0.43333333333333335\n0.5333333333333333\n0.5249999999999999\n0.5666666666666667\n0.525\n0.45\n0.45\n0.4708333333333333\n0.5625\n0.4833333333333333\n0.4458333333333333\n0.4708333333333333\n0.48750000000000004\n0.4666666666666667\n0.4083333333333333\n0.45\n0.4083333333333333\n0.4916666666666667\n0.5166666666666666\n0.49583333333333335\n0.43333333333333335\n0.5083333333333333\n0.4708333333333333\n0.4666666666666667\n0.43333333333333335\n0.31666666666666665\n0.4666666666666667\n0.4125\n0.4041666666666667\n0.5\n0.4388888888888889"
  },
  {
    "objectID": "tutorial.train_mimic3.html#plotting-the-label-embeddings",
    "href": "tutorial.train_mimic3.html#plotting-the-label-embeddings",
    "title": "Training an XML Text Classifier",
    "section": "Plotting the Label Embeddings",
    "text": "Plotting the Label Embeddings\n\nTo know if the model learnt embeddings that are meaningful\n\n\nlbs_emb = learn.model[-1].pay_attn.lbs.weight\nX = to_np(lbs_emb)\n\nNow let’s do a PCA and t-SNE on the labels embedding. Before doing PCA though we need to standardize X:\n\nfrom sklearn.preprocessing import StandardScaler\n\n\nX_stand = StandardScaler().fit_transform(X)\n\n\nX_reduced, _vars = plot_reduction(X, tSNE=True) \n# X_random, _vars_rnd = plot_reduction(np.random.normal(size=(X.shape[0], 400)), tSNE=True) # to compare with a random embeddings\n\n\n\n\n\n# print('\\n'.join(L(source.glob('**/*desc*')).map(str)))\ncodes = load_pickle(source/'code_desc.pkl')\ndf_lbl = pd.DataFrame([(lbl, codes.get(lbl, \"not found\"), freq) for lbl, freq in lbl_freqs.items()], columns=['label', 'description', 'frequency'])\ndf_lbl = df_lbl.sort_values(by='frequency', ascending=False, ignore_index=True)\ndf_lbl.head()\n\n\n\n\n\n\n\n\nlabel\ndescription\nfrequency\n\n\n\n\n0\n401.9\nUnspecified essential hypertension\n84\n\n\n1\n38.93\nVenous catheterization, not elsewhere classified\n77\n\n\n2\n428.0\nCongestive heart failure, unspecified\n61\n\n\n3\n272.4\nOther and unspecified hyperlipidemia\n60\n\n\n4\n427.31\nAtrial fibrillation\n56\n\n\n\n\n\n\n\nInstead of doing a PCA on all the labels, let’s do it on the top most frequent labels:\n\ntop = 100\ntop_lbs = [k for k, v in lbl_freqs.most_common(top)]\ntfm_cat = dls_clas.tfms[1][1]\ntop_lbs_emb = lbs_emb[tfm_cat(top_lbs)]\ntopX = to_np(top_lbs_emb)\n\n\nfrom sklearn.manifold import TSNE\n\n\ntopX_tsne = TSNE(n_components=2, perplexity=40).fit_transform(topX)\n\n\nfig = plt.figure(figsize=(12,12))\nax = fig.add_subplot(1, 1, 1)\nplt.scatter(topX_tsne[:, 0], topX_tsne[:, 1], marker='x', s=5)\nfor lbl, x, y in zip(top_lbs, topX_tsne[:, 0], topX_tsne[:, 1]):\n    plt.annotate(lbl,\n                xy=(x,y),\n                xytext=(5,-5),\n                textcoords='offset points',\n                size=7)\nplt.show()\n\n\n\n\nTake a look at some of the closely clustered labels to see if they have the same meaning. This will tell us if the model learned meaningful label embeddings:\n\ndf_lbl[df_lbl.label.isin(['36.13', '995.91', '88.72', '37.22'])]\n\n\n\n\n\n\n\n\nlabel\ndescription\nfrequency\n\n\n\n\n48\n88.72\nDiagnostic ultrasound of heart\n14\n\n\n56\n37.22\nLeft heart cardiac catheterization\n13\n\n\n64\n995.91\nSepsis\n11\n\n\n70\n36.13\n(Aorto)coronary bypass of three coronary arteries\n10\n\n\n\n\n\n\n\nLooks like it did!"
  },
  {
    "objectID": "l2r.data.info_gain.html",
    "href": "l2r.data.info_gain.html",
    "title": "Information Gain",
    "section": "",
    "text": "! [ -e /content ] && pip install -Uqq xcube # upgrade xcube on colab\n\nThis module contains the all classes and functions needed to compute mutual information gain for the tokens and labels. This mutual information is then used to bootstrap a L2R model from xml text data. Please follow the Boot L2R to understand how this module is used.\n\nsource\n\nBatchLbsChunkify\n\n BatchLbsChunkify (chnk_st, chnk_end)\n\nA transform that always take tuples as items\n\nsource\n\n\nMutualInfoGain\n\n MutualInfoGain (df, bs=8, chnk_sz=200, device=None, lbs_desc=None)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n@property\n@patch\ndef lbs_frqs(self:MutualInfoGain):\n    f = ColReader('labels', label_delim=';')\n    self._frqs = Counter()\n    for o in self.df.itertuples(): self._frqs.update(f(o))\n    return self._frqs\n\n\nsource\n\n\nMutualInfoGain.show\n\n MutualInfoGain.show (*args, save_as=None, **kwargs)"
  },
  {
    "objectID": "metrics.html",
    "href": "metrics.html",
    "title": "Metrics",
    "section": "",
    "text": "source\n\n\n\n precision_at_k (yhat_raw, y, k=15)\n\nInputs: yhat_raw: activation matrix of ndarray and shape (n_samples, n_labels) y: binary ground truth matrix of type ndarray and shape (n_samples, n_labels) k: for @k metric\n\nsource\n\n\n\n\n precision_at_r (yhat_raw, y)\n\nInputs: yhat_raw: activation matrix of ndarray and shape (n_samples, n_labels) y: binary ground truth matrix of type ndarray and shape (n_samples, n_labels)\n\nsource\n\n\n\n\n recall_at_k (pred_probs, true_labels, k=15)"
  },
  {
    "objectID": "metrics.html#extreme-multilabel-classification",
    "href": "metrics.html#extreme-multilabel-classification",
    "title": "Metrics",
    "section": "",
    "text": "source\n\n\n\n precision_at_k (yhat_raw, y, k=15)\n\nInputs: yhat_raw: activation matrix of ndarray and shape (n_samples, n_labels) y: binary ground truth matrix of type ndarray and shape (n_samples, n_labels) k: for @k metric\n\nsource\n\n\n\n\n precision_at_r (yhat_raw, y)\n\nInputs: yhat_raw: activation matrix of ndarray and shape (n_samples, n_labels) y: binary ground truth matrix of type ndarray and shape (n_samples, n_labels)\n\nsource\n\n\n\n\n recall_at_k (pred_probs, true_labels, k=15)"
  },
  {
    "objectID": "metrics.html#learning-to-rank",
    "href": "metrics.html#learning-to-rank",
    "title": "Metrics",
    "section": "Learning to Rank",
    "text": "Learning to Rank\nWe want to compute a metric which measures how many orderings did the model get right:\n\nsource\n\nbatch_lbs_accuracy\n\n batch_lbs_accuracy (preds, xb, len=1000, resamps=10, threshold=0.5)\n\n\nsource\n\n\naccuracy\n\n accuracy (xb, model)\n\nNOTE: The following ndcg only used on a batch: \n\nsource\n\n\nndcg\n\n ndcg (preds, xb, k=None)\n\nNOTE: The following ndcg_at_k only used on the entite dataset: \n\nsource\n\n\nndcg_at_k\n\n ndcg_at_k (dset, model, k=20)"
  },
  {
    "objectID": "collab.html",
    "href": "collab.html",
    "title": "Collaborative filtering",
    "section": "",
    "text": "This module adds to the tools from fastai collab to use transfer learning by loading embeddings needed for collaborative filtering from a pretrained model. Additionally, it also adds the capability of saving the vocabulary the collab model was trained on. The most important function in this module is collab_learner."
  },
  {
    "objectID": "collab.html#loading-usersitems-embeddings-from-a-pretrained-model",
    "href": "collab.html#loading-usersitems-embeddings-from-a-pretrained-model",
    "title": "Collaborative filtering",
    "section": "Loading users/items embeddings from a pretrained model",
    "text": "Loading users/items embeddings from a pretrained model\nIn a collab model, to load a pretrained vocabulary, we need to adapt the embeddings of the vocabulary used for the pre-training to the vocabulary of our current collab corpus.\n\nsource\n\nmatch_embeds\n\n match_embeds (old_wgts:dict, old_vocab:list, new_vocab:dict)\n\nConvert the users and items (possibly saved as 0.module.encoder.weight and 1.attn.lbs_weight.weight respectively) embedding in old_wgts to go from old_vocab to new_vocab\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nold_wgts\ndict\nEmbedding weights of the pretrained model\n\n\nold_vocab\nlist\nVocabulary (tokens and labels) of the corpus used for pretraining\n\n\nnew_vocab\ndict\nCurrent collab corpus vocabulary (users and items)\n\n\nReturns\ndict"
  },
  {
    "objectID": "collab.html#create-a-learner",
    "href": "collab.html#create-a-learner",
    "title": "Collaborative filtering",
    "section": "Create a Learner",
    "text": "Create a Learner\n\nsource\n\nload_pretrained_keys\n\n load_pretrained_keys (model, wgts:dict)\n\nLoad relevant pretrained wgts in `model\n\n\n\n\nType\nDetails\n\n\n\n\nmodel\n\nModel architecture\n\n\nwgts\ndict\nModel weights\n\n\nReturns\ntuple\n\n\n\n\n\nsource\n\n\nCollabLearner\n\n CollabLearner (dls:DataLoaders, model:callable,\n                loss_func:callable|None=None,\n                opt_func:Optimizer|OptimWrapper=&lt;function Adam&gt;,\n                lr:float|slice=0.001, splitter:callable=&lt;function\n                trainable_params&gt;, cbs:Callback|MutableSequence|None=None,\n                metrics:callable|MutableSequence|None=None,\n                path:str|Path|None=None, model_dir:str|Path='models',\n                wd:float|int|None=None, wd_bn_bias:bool=False,\n                train_bn:bool=True, moms:tuple=(0.95, 0.85, 0.95),\n                default_cbs:bool=True)\n\nBasic class for a Learner in Collab.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndls\nDataLoaders\n\nDataLoaders containing fastai or PyTorch DataLoaders\n\n\nmodel\ncallable\n\nPyTorch model for training or inference\n\n\nloss_func\ncallable | None\nNone\nLoss function. Defaults to dls loss\n\n\nopt_func\nOptimizer | OptimWrapper\nAdam\nOptimization function for training\n\n\nlr\nfloat | slice\n0.001\nDefault learning rate\n\n\nsplitter\ncallable\ntrainable_params\nSplit model into parameter groups. Defaults to one parameter group\n\n\ncbs\nCallback | MutableSequence | None\nNone\nCallbacks to add to Learner\n\n\nmetrics\ncallable | MutableSequence | None\nNone\nMetrics to calculate on validation set\n\n\npath\nstr | Path | None\nNone\nParent directory to save, load, and export models. Defaults to dls path\n\n\nmodel_dir\nstr | Path\nmodels\nSubdirectory to save and load models\n\n\nwd\nfloat | int | None\nNone\nDefault weight decay\n\n\nwd_bn_bias\nbool\nFalse\nApply weight decay to normalization and bias parameters\n\n\ntrain_bn\nbool\nTrue\nTrain frozen normalization layers\n\n\nmoms\ntuple\n(0.95, 0.85, 0.95)\nDefault momentum for schedulers\n\n\ndefault_cbs\nbool\nTrue\nInclude default Callbacks\n\n\n\nIt works exactly as a normal learner, the only difference is that it also saves the items vocabulary used by self.model\nThe following function lets us quickly create a Learner for collaborative filtering from the data.\n\nsource\n\n\ncollab_learner\n\n collab_learner (dls, n_factors=50, use_nn=False, emb_szs=None,\n                 layers=None, config=None, y_range=None, loss_func=None,\n                 pretrained=False, opt_func:Union[fastai.optimizer.Optimiz\n                 er,fastai.optimizer.OptimWrapper]=&lt;function Adam&gt;,\n                 lr:Union[float,slice]=0.001, splitter:&lt;built-\n                 infunctioncallable&gt;=&lt;function trainable_params&gt;, cbs:Unio\n                 n[fastai.callback.core.Callback,collections.abc.MutableSe\n                 quence,NoneType]=None, metrics:Union[&lt;built-infunctioncal\n                 lable&gt;,collections.abc.MutableSequence,NoneType]=None,\n                 path:Union[str,pathlib.Path,NoneType]=None,\n                 model_dir:Union[str,pathlib.Path]='models',\n                 wd:Union[float,int,NoneType]=None, wd_bn_bias:bool=False,\n                 train_bn:bool=True, moms:tuple=(0.95, 0.85, 0.95),\n                 default_cbs:bool=True)\n\nCreate a Learner for collaborative filtering on dls.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndls\nDataLoaders\n\nDataLoaders containing fastai or PyTorch DataLoaders\n\n\nn_factors\nint\n50\n\n\n\nuse_nn\nbool\nFalse\n\n\n\nemb_szs\nNoneType\nNone\n\n\n\nlayers\nNoneType\nNone\n\n\n\nconfig\nNoneType\nNone\n\n\n\ny_range\nNoneType\nNone\n\n\n\nloss_func\ncallable | None\nNone\nLoss function. Defaults to dls loss\n\n\npretrained\nbool\nFalse\n\n\n\nopt_func\nOptimizer | OptimWrapper\nAdam\nOptimization function for training\n\n\nlr\nfloat | slice\n0.001\nDefault learning rate\n\n\nsplitter\ncallable\ntrainable_params\nSplit model into parameter groups. Defaults to one parameter group\n\n\ncbs\nCallback | MutableSequence | None\nNone\nCallbacks to add to Learner\n\n\nmetrics\ncallable | MutableSequence | None\nNone\nMetrics to calculate on validation set\n\n\npath\nstr | Path | None\nNone\nParent directory to save, load, and export models. Defaults to dls path\n\n\nmodel_dir\nstr | Path\nmodels\nSubdirectory to save and load models\n\n\nwd\nfloat | int | None\nNone\nDefault weight decay\n\n\nwd_bn_bias\nbool\nFalse\nApply weight decay to normalization and bias parameters\n\n\ntrain_bn\nbool\nTrue\nTrain frozen normalization layers\n\n\nmoms\ntuple\n(0.95, 0.85, 0.95)\nDefault momentum for schedulers\n\n\ndefault_cbs\nbool\nTrue\nInclude default Callbacks\n\n\n\nIf use_nn=False, the model used is an EmbeddingDotBias with n_factors and y_range. Otherwise, it’s a EmbeddingNN for which you can pass emb_szs (will be inferred from the dls with get_emb_sz if you don’t provide any), layers (defaults to [n_factors]) y_range, and a config that you can create with tabular_config to customize your model.\nloss_func will default to MSELossFlat and all the other arguments are passed to Learner.\n\npath = untar_data(URLs.ML_SAMPLE)\nratings = pd.read_csv(path/'ratings.csv')\nratings.head()\n\n\n\n\n\n\n\n\nuserId\nmovieId\nrating\ntimestamp\n\n\n\n\n0\n73\n1097\n4.0\n1255504951\n\n\n1\n561\n924\n3.5\n1172695223\n\n\n2\n157\n260\n3.5\n1291598691\n\n\n3\n358\n1210\n5.0\n957481884\n\n\n4\n130\n316\n2.0\n1138999234\n\n\n\n\n\n\n\n\ndls = CollabDataLoaders.from_df(ratings, bs=64)\ndls.show_batch()\n\n\n\n\n\nuserId\nmovieId\nrating\n\n\n\n\n0\n199\n3578\n4.0\n\n\n1\n564\n165\n4.0\n\n\n2\n664\n1198\n4.5\n\n\n3\n608\n1682\n4.0\n\n\n4\n654\n1\n5.0\n\n\n5\n213\n457\n4.0\n\n\n6\n56\n58559\n5.0\n\n\n7\n292\n597\n4.5\n\n\n8\n102\n1036\n4.0\n\n\n9\n262\n3578\n2.5\n\n\n\n\n\n\nwith tempfile.TemporaryDirectory() as d:\n    learn = collab_learner(dls, y_range=(0,5), path=d)\n    learn.fit(1)\n    \n    # Test save created a file\n    learn.save('tmp')\n    assert (Path(d)/'models/tmp.pth').exists()\n    assert (Path(d)/'models/tmp_vocab.pkl').exists()\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n2.480442\n2.294809\n00:00"
  },
  {
    "objectID": "l2r.models.core.html",
    "href": "l2r.models.core.html",
    "title": "L2R Models",
    "section": "",
    "text": "source\n\n\n\n L2R_DotProductBias (num_lbs, num_toks, num_factors, y_range=None)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool"
  },
  {
    "objectID": "l2r.models.core.html#linear",
    "href": "l2r.models.core.html#linear",
    "title": "L2R Models",
    "section": "",
    "text": "source\n\n\n\n L2R_DotProductBias (num_lbs, num_toks, num_factors, y_range=None)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool"
  },
  {
    "objectID": "l2r.models.core.html#neural-network",
    "href": "l2r.models.core.html#neural-network",
    "title": "L2R Models",
    "section": "Neural Network",
    "text": "Neural Network\n\nsource\n\nL2R_NN\n\n L2R_NN (num_lbs, num_toks, num_factors, layers, ps=None, use_bn=True,\n         bn_final=False, lin_first=True, embed_p=0.0, y_range=None)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool"
  },
  {
    "objectID": "tutorial.boot_l2r.html",
    "href": "tutorial.boot_l2r.html",
    "title": "Boot L2R",
    "section": "",
    "text": "! [ -e /content ] && pip install -Uqq xcube # upgrade xcube on colab\n\n\nfrom fastai.data.core import *\nfrom xcube.l2r.all import *\n\nMake sure we have that “beast”:\n\nic(torch.cuda.get_device_name(default_device()));\ntest_eq(torch.cuda.get_device_name(0), torch.cuda.get_device_name(default_device()))\ntest_eq(default_device(), torch.device(0))\nprint(f\"GPU memory = {torch.cuda.get_device_properties(default_device()).total_memory/1024**3}GB\")\n\nic| torch.cuda.get_device_name(default_device()): 'Quadro RTX 8000'\n\n\nGPU memory = 44.99969482421875GB\n\n\nSetting some environment variables:\n\n# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n\nSetting defaults for pandas and matplotlib:\n\n# Set the default figure size\nplt.rcParams[\"figure.figsize\"] = (8, 4)\n# Set pandas column width\npd.set_option('display.max_colwidth', None)\n\nAltering some default jupyter settings:\n\nfrom IPython.core.interactiveshell import InteractiveShell\n# InteractiveShell.ast_node_interactivity = \"last\" # \"all\"\n\nIn this tutorial we will find a needle in the haystack with mutual infomation gain:\n\nMutual-Information Computation\n\nsource = untar_xxx(XURLs.MIMIC3_L2R)\n\n(#11) [Path('/home/deb/.xcube/data/mimic3_l2r/info.pkl'),Path('/home/deb/.xcube/data/mimic3_l2r/code_descriptions.csv'),Path('/home/deb/.xcube/data/mimic3_l2r/mimic3-9k_tok_lbl_info.pkl'),Path('/home/deb/.xcube/data/mimic3_l2r/code_desc.pkl'),Path('/home/deb/.xcube/data/mimic3_l2r/p_TL.pkl'),Path('/home/deb/.xcube/data/mimic3_l2r/trn_val_split.pkl'),Path('/home/deb/.xcube/data/mimic3_l2r/mimic3-9k_tok.ft'),Path('/home/deb/.xcube/data/mimic3_l2r/mimic3-9k_lbl.ft'),Path('/home/deb/.xcube/data/mimic3_l2r/mimic3-9k.csv'),Path('/home/deb/.xcube/data/mimic3_l2r/scored_tokens.pth')...]\n\n\n\ndata = source/'mimic3-9k.csv'\ndf = pd.read_csv(data,\n                 header=0,\n                 names=['subject_id', 'hadm_id', 'text', 'labels', 'length', 'is_valid'],\n                 dtype={'subject_id': str, 'hadm_id': str, 'text': str, 'labels': str, 'length': np.int64, 'is_valid': bool})\ndf[['text', 'labels']] = df[['text', 'labels']].astype(str)\nlen(df)\n\n52726\n\n\n\ndf.head(3)\n\n\n\n\n\n\n\n\nsubject_id\nhadm_id\ntext\nlabels\nlength\nis_valid\n\n\n\n\n0\n86006\n111912\nadmission date discharge date date of birth sex f service surgery allergies patient recorded as having no known allergies to drugs attending first name3 lf chief complaint 60f on coumadin was found slightly drowsy tonight then fell down stairs paramedic found her unconscious and she was intubated w o any medication head ct shows multiple iph transferred to hospital1 for further eval major surgical or invasive procedure none past medical history her medical history is significant for hypertension osteoarthritis involving bilateral knee joints with a dependence on cane for ambulation chronic...\n801.35;348.4;805.06;807.01;998.30;707.24;E880.9;427.31;414.01;401.9;V58.61;V43.64;707.00;E878.1;96.71\n230\nFalse\n\n\n1\n85950\n189769\nadmission date discharge date service neurosurgery allergies sulfa sulfonamides attending first name3 lf chief complaint cc cc contact info major surgical or invasive procedure none history of present illness hpi 88m who lives with family had fall yesterday today had decline in mental status ems called pt was unresponsive on arrival went to osh head ct showed large r sdh pt was intubated at osh and transferred to hospital1 for further care past medical history cad s p mi in s p cabg in ventricular aneurysm at that time cath in with occluded rca unable to intervene chf reported ef 1st degre...\n852.25;E888.9;403.90;585.9;250.00;414.00;V45.81;96.71\n304\nFalse\n\n\n2\n88025\n180431\nadmission date discharge date date of birth sex f service surgery allergies no known allergies adverse drug reactions attending first name3 lf chief complaint s p fall major surgical or invasive procedure none history of present illness 45f etoh s p fall from window at feet found ambulating and slurring speech on scene intubated en route for declining mental status in the er the patient was found to be bradycardic to the s with bp of systolic she was given atropine dilantin and was started on saline past medical history unknown social history unknown family history unknown physical exam ex...\n518.81;348.4;348.82;801.25;427.89;E882;V49.86;305.00;96.71;38.93\n359\nFalse\n\n\n\n\n\n\n\nThe file 'code_desc.pkl' contains a short description for the labels.\n\n# with open(source/'code_desc.pkl', 'rb') as f: lbs_desc = pickle.load(f)\nlbs_desc = load_pickle(source/'code_desc.pkl')\nassert isinstance(lbs_desc, dict)\ntest_eq(mapt(lbs_desc.get, ['00.93', '427.31', '00.09']), ('Transplant from cadaver',\n                                                          'Atrial fibrillation',\n                                                          'Other therapeutic ultrasound'))\n\nNote that performing some computations in this notebook on the full dataset is going to take a lot of time. But don’t worry untar_xxx has already downloaded everything you need. But you can still run the following cells if you want to generate everything from scratch. Preferably, run the following cells on a sampled dataset for quick iterations.\nRun the cell below only if you want to sample from the full dataset to create a tiny dataset for the purpose of quick iterations.\nTechnical Point: If we want to sample to perform quick iterations, we need to make sure the number of data points in the sample is a multiple of bs. So that we do not have to do a drop_last=True while creating the Dataloaders. This is because we are about to do some probability computations, and dropping data points is not a good idea as probabilities would not sum to 1.\n\nbs = 8\ncut = len(df) - len(df)%bs\ndf = df[:cut]\nlen(df)\n\n52720\n\n\n\n_arr = np.arange(0, len(df), bs)\n# mask = (_arr &gt; 4000) & (_arr &lt; 5000)\nmask = (_arr &gt; 500) & (_arr &lt; 1000)\n_n = np.random.choice(_arr[mask], 1)\ndf = df.sample(n=_n, random_state=89, ignore_index=True)\nlen(df)\n\n744\n\n\n\ndf.head(3)\n\n\n\n\n\n\n\n\nsubject_id\nhadm_id\ntext\nlabels\nlength\nis_valid\n\n\n\n\n0\n2258\n139169\nadmission date discharge date date of birth sex m service cardiothoracic surgery history of present illness the patient is a year old male with a past medical history significant for poorly controlled diabetes mellitus and hypertension as well as known coronary disease and a previous non q myocardial infarction and right coronary artery stenting in he was admitted to an outside hospital on the day prior to admission with unstable angina and found to have borderline positive troponin hypertension and st depressions in the lateral lead he was given aspirin nitrates beta blockers morphine and...\n414.01;998.31;411.1;599.0;412;V45.82;250.00;401.9;530.81;36.13;37.22;36.15;36.19;39.61;39.64;88.56;88.53;33.23;96.56;33.24;78.41\n1271\nFalse\n\n\n1\n41217\n161582\nadmission date discharge date date of birth sex m service medicine allergies no known allergies adverse drug reactions attending first name3 lf chief complaint new diagnosis of scc of base of tongue major surgical or invasive procedure egd w biopsy history of present illness yo man with h o cad heavy smoking and new diagnosis of scc of base of tongue with lymph node involvement pt was referred to dr last name stitle ent in for a rt neck mass at that time a cm rt cervical lymph node was palpated and fiberoptic laryngoscopy showed a cm rt base of tongue mass a ct and biopsy were recommended ...\n141.0;507.0;196.0;293.0;519.09;786.30;286.9;427.89;790.29;276.52;414.01;338.3;280.0;272.0;412;V69.4;V15.82;V45.82;V66.7;E879.8;E932.0;31.42;25.01;42.23;43.11;96.6;38.93;99.25;38.93\n2743\nFalse\n\n\n2\n30204\n172114\nadmission date discharge date date of birth sex f service medicine allergies etomidate norpace quinidine demerol penicillins lipitor attending doctor first name chief complaint cardiac tamponade s p pulmonary vein isolation major surgical or invasive procedure attempted pulmonary vein isolation pericardiocentesis history of present illness year old woman with a long history of paroxysmal atrial fibrillation refractory to mulitple pharmacologic interventions and multiple cardioversions who presents to the ccu with cardiac tamponade s p pulmonary vein isolation procedure past medical history...\n427.31;998.2;423.3;423.9;573.0;276.6;E878.8;37.34;37.27;37.0;37.21\n1764\nFalse\n\n\n\n\n\n\n\nMutual Information\n\nThe mutual information of two jointly discrete random variables X and Y is calculated as a double sum:\n\\[I(T;L) = \\sum_{l \\in \\mathcal{L}} \\sum_{t in \\mathcal{T}} P_{(T,L)}(t,l) \\log \\Bigg(\\frac{P_{(T,L)}(t,l)}{P_T(t) P_L(l)} \\Bigg)\\]\nwhere \\(P_{(T,L)}\\) is the joint probability mass function of \\(T\\) and \\(L\\), and \\(P_T\\) and \\(P_L\\) are the marginal probability mass fucntions of \\(T\\) and \\(L\\) respectively. To compute \\(I\\), the only quantity we need to compute is the joint pmf \\(P_{(T,L)}\\), as the marginal pmfs can be computed from the joint pmf.\nWith regard to implementation, \\(P_{(T,L)}\\) can be thought of as a 2x2 tensor as shown below:\n\np_TL = pd.DataFrame(0, columns=['t', 'not t'], index=['lbl', 'not lbl'])\np_TL\n\n\n\n\n\n\n\n\nt\nnot t\n\n\n\n\nlbl\n0\n0\n\n\nnot lbl\n0\n0\n\n\n\n\n\n\n\n…and we need to compute this \\(P_{(T,L)}\\) for every token-label pair. In other words, we need to fill in the joint dataframe shown below. Note that each cell in joint dataframe can be thought of to be further subdivided into a 2x2 grid containing the corresponding p_TL.\n\nbs, chnk_sz = 8, 200\ninfo = MutualInfoGain(df, bs=bs, chnk_sz=chnk_sz, lbs_desc=source/'code_desc.pkl') # provide lbs_desc if you have it\n\n\ndsets = info.onehotify()\n\n\n\n\n\n\n\n\nCPU times: user 597 ms, sys: 244 ms, total: 842 ms\nWall time: 4.21 s\n\n\n\ntoks, lbs = dsets.vocab\nL(toks), L(lbs), len(toks)*len(lbs)\n\n((#10632) ['xxunk','xxpad','xxbos','xxeos','xxfld','xxrep','xxwrep','xxup','xxmaj','the'...],\n (#2150) ['008.45','008.8','009.0','009.1','031.0','031.2','038.0','038.10','038.11','038.19'...],\n 22858800)\n\n\n\njoint = pd.DataFrame(0, columns=range(len(lbs)), index=range(len(toks)))\njoint.index.name = 'toks (T)'\njoint.columns.name = 'lbs (L)'\njoint\n\n\n\n\n\n\n\nlbs (L)\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n...\n2140\n2141\n2142\n2143\n2144\n2145\n2146\n2147\n2148\n2149\n\n\ntoks (T)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n10627\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n10628\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n10629\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n10630\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n10631\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n10632 rows × 2150 columns\n\n\n\nWe can perform tensorized computation if we think of p_TL as a 4 dim tensor of size (len(toks), len(lbs), 2, 2). Next, to be able to estimate p_TL we just need to iterate over the dataset and for each data point and each token-label pair record the p_TL information in the last two dimension of the tensor p_TL. And, at the end divide by size of the dataset.\nSome more implementation details (Skip this if not iterested):\n\nWe are going to one-hot encode the dataset (both text and labels field in the df). This is done by onehot_dsets\nFor efficieny, in reality we are not going to iterate over the dataset one by one, instead we are going to use a dataloader and perform p_TL computation on a mini-batch.\nUnless you are doing this in 2035 you probably do not have enogh GPU-RAM to fit the entire p_TL tensor of dimension (len(toks), len(lbs), 2, 2). So we are going to split the lbs dimension into chunks. (Why the lbs dimension and not the toks? Because in XML datsets toks are approximately 60000, but the number of lbs could be really large of the order of millions.) With reagrd to implementation this would mean that instead of one dataloader we would roll with multiple dataloaders. And each dataloader would load the dataset in a way that mini-batches would contain the full one-hot encoding of the text field but only a certain chunk of the one-hot encoded labels field in df. Another way to think about this is that each datapoint, specifically the labels are splitted across multiple dataloaders. This way once we are done iterating over one such dataloader we would have filled a ceratin chunk of the joint dataframe shown above. And we would fill the entire joint only once we are done iterating over all the dataloaders.\n\n\nx, y = dsets[0]\ntest_eq(tensor(dsets.tfms[1][2].decode(y)), torch.where(y==1)[0])\ntest_eq(tensor(dsets.tfms[0][-1].decode(x)), torch.where(x==1)[0])\n\n\n' '.join(L(toks)[torch.where(x==1)[0]])\n\n'xxunk xxbos the and to of was with a on in for mg no patient is he blood at name or discharge as day his one left last history were had right by this admission date that pain hospital an from p pt normal first has have which but medications up d chest o hours also well given status time care dr after stable course follow started please stitle disease known x continued days two service prior per showed artery m it q medical without namepattern1 glucose past cardiac post heart present unit physical aortic pulmonary i weeks transferred year md allergies edema due t pressure did surgery surgical number condition fluid b found procedure lower prn remained admitted soft hypertension further non coronary rate all placed diagnosis should bilaterally increased three sodium birth abdomen over bilateral aspirin illness social than old sex secondary however primary examination following some positive significant disposition floor take lung room moderate insulin bleeding namepattern4 extremities f upper back use count lasix regular therapy sinus intubated rhythm discharged underwent facility lungs continue job alert off felt sounds hematocrit heparin transfer anterior made distress contrast wound nausea four pulses down diabetes alcohol very extended postoperative followed white both removed diet creatinine hospital3 drip morning note do greater drainage male previous name11 stent intensive oriented worsening oral s1 sent currently catheterization site carotid bypass pattern1 through control weaned office albuterol extubated extremity incision warm controlled tolerated lesion increase amiodarone dictated plavix issues st levofloxacin strength physician patch difficulty infarction night next medquist36 repair increasing minimal dyspnea descending lesions laboratory morphine afebrile though hospital6 venous beta operative lateral outside later lopressor peripheral inferior operating incisions persistent masses denied go myocardial perfused cardiology murmurs bun diuresis went help evening colace lipitor began mellitus smoking nontender wheezing meds occasional ap drain cardiologist cardiothoracic commands wave knee intermittent sternal nondistended intra ambulating bruits rubs pump troponin system nitroglycerin despite lovenox tubes dependent instructed bronchoscopy meq ibuprofen main aggressive puffs came platelet referred palpable dilaudid obese decision heavy pacing stabilized secretions balloon jugular exertion minute neo follows seven leads wires many propofol frequency waves put electrocardiogram attempt paroxysmal begun grafting stenting little ten distention half treat diabetic depressions reflux coarse angina index orthopnea hepatosplenomegaly diameter diffusely urinalysis ready zantac afternoon poorly transdermal gastroesophageal borderline synephrine hemodynamic occurred quite lead refer mobility dominant moved encouraged codeine wire standpoint circumflex includes activities extremely coughing nocturnal unstable allergic stability uneventful toilet observation pole enteric blockers 1l drips incentive coated nicotine sternum weaning dye pericarditis inhalers inversions dobutamine kcl nexium packs burning participate serosanguinous complain lyme exercises spent noninsulin restenosis amaryl cks uncooperative spirometry urination isordil disabled rhonchorous nitrates build concurrently marginally'\n\n\n\nlbs.map_ids(torch.where(y==1)[0])\n\n(#21) ['250.00','33.23','33.24','36.13','36.15','36.19','37.22','39.61','39.64','401.9'...]\n\n\n\ndls = info.lbs_chunked()\n\n\nassert isinstance(dls[0], TfmdDL)\ntest_eq(len(dls),  np.ceil(len(lbs)/200))\ntest_eq(len(dls[0]), np.ceil(len(dsets)/bs)) # drop_last is False\n# test to prove that the labels for each data point is split across multiple dataloaders\nlbs_0 = torch.cat([yb[0] for dl in dls for _,yb in itertools.islice(dl, 1)])\ny = y.to(default_device())\ntest_eq(lbs_0, y)\n\nNow let’s compute the joint_pmf table we had seen earlier.\n\np_TL = info.joint_pmf()\n\n\n\n\n\n\n    \n      \n      100.00% [11/11 00:13&lt;00:00]\n    \n    \n\n\nCPU times: user 8.73 s, sys: 2.58 s, total: 11.3 s\nWall time: 13.6 s\n\n\n\ntest_eq(p_TL.shape, (info.toksize, info.lblsize, 2, 2))\n\nTechnicality: p_TL is not really the joint pmf (yes, I lied before!) but contains all the information needed to compute the joint pmf p_TxL and mutual info gain I_TL. This computation is going to be comnputed by compute:\n\np_T, p_L, p_TxL, H_T, H_L, I_TL = info.compute()\n\nCPU times: user 751 ms, sys: 253 ms, total: 1 s\nWall time: 2.41 s\n\n\nAll this while if you have been working with the sampled dataset you can continue to do so for the rest of this notebook. But if you want a real feel of how things look, at this point you can load the pregenerated p_TL and (p_T, p_L, p_TxL, H_T, H_L, I_TL) for the full dataset which untar_xxx downloaded:\n\n# print('\\n'.join(L(source.glob(\"**/*.pkl\")).map(str)))\n# or better yet\n!tree -sh -P \"*.pkl\" {source}\n\n/home/deb/.xcube/data/mimic3_l2r\n├── [1.2M]  code_desc.pkl\n├── [9.5G]  info.pkl\n├── [3.8G]  mimic3-9k_tok_lbl_info.pkl\n├── [7.6G]  p_TL.pkl\n└── [7.6G]  trn_val_split.pkl\n\n0 directories, 5 files\n\n\n\n# %%time \np_TL = torch.load(source/'p_TL.pkl', map_location=torch.device('cpu'))\np_T, p_L, p_TxL, H_T, H_L, I_TL = torch.load(source/'info.pkl', map_location=torch.device('cpu'))\n\nMake sure there aren’t any of those pesky nans or negs:\n\ndef test_nanegs(*args):\n    for o in args:\n        has_nans = o.isnan().all() # check for nans\n        has_negs = not torch.where(o&gt;=0, True, False).all()\n        if has_nans: raise Exception(f\"{namestr(o, globals())[0]} has nans\")\n        if has_negs: raise Exception(f\"{namestr(o, globals())[0]} has negs\")\n\n\ntest_fail(test_nanegs, args=(p_T, p_L, p_TxL, H_T, H_L, I_TL), contains='I_TL has negs')\n\nTheoretically, Mutual-Info as defined here is suposed to be non-negative (can be proved by tossing in Jensen). But, practically, it turns out I_TL has some negs because we distorted the p_TL and p_TxL with eps in the I_TL computation.\n\ntorch.topk(I_TL.flatten(), 10, largest=False)\n\ntorch.return_types.topk(\nvalues=TensorMultiCategory([-1.9016e-07, -1.8314e-07, -1.8314e-07, -1.7385e-07,\n                     -1.7277e-07, -1.7277e-07, -1.6798e-07, -1.6798e-07,\n                     -1.6798e-07, -1.6767e-07]),\nindices=TensorMultiCategory([22423614,  2735913,  2731838,  1911099,  6393113,  6389159,\n                      6695355,  6695018,  6693073, 32253137]))\n\n\n\nhowmany = torch.where(I_TL &lt; 0, True, False).sum().item()\nnegs = torch.where(I_TL &lt; 0, I_TL, I_TL.new_zeros(I_TL.shape))\nnegs.sum()/howmany\n\nTensorMultiCategory(-3.9054e-08)\n\n\nThose negs on an avg are pretty close to zero. So we need not worry. Let’s roll!\n\ntest_eq(p_TL.shape, (info.toksize, info.lblsize, 2, 2))\ntest_eq(p_T.shape, (info.toksize, 2, 1))\ntest_eq(p_L.shape, (info.lblsize, 1, 2))\ntest_eq(p_TxL.shape, (info.toksize, info.lblsize, 2, 2))\ntest_eq(H_T.shape, [info.toksize])\ntest_eq(H_L.shape, [info.lblsize])\ntest_eq(I_TL.shape, (info.toksize, info.lblsize))\n\n\neps = I_TL.new_empty(1).fill_(1e-15)\ninfo_lbl_entropy = I_TL/(H_L + eps)\ninfo_jaccard = I_TL/(H_T.unsqueeze(-1) + H_L.unsqueeze(0) - I_TL + eps)\nassert not info_lbl_entropy.isnan().all(); assert not info_jaccard.isnan().all()\nl2r_bootstrap = {'toks': toks, 'lbs': lbs, 'mut_info_lbl_entropy': info_lbl_entropy, 'mutual_info_jaccard': info_jaccard}\n\nl2r_bootstrap for the full dataset was downloaded by untar_xxx in boot_path. You can load it up in the following cell. l2r_bootstrap will be used to bootstrap our learning-to-rank model.\n\n\nSave those Mutual Information Gain values\n\n# l2r_bootstrap = torch.load(boot_path)\n\n\n# l2r_bootstrap['info_jaccard'] = l2r_bootstrap.pop('mutual_info_jaccard')\n\n\n# globals().update(l2r_bootstrap)\n\n\n# info_jaccard.shape\n\nLet’s take a look at the Mutual Information Gain (I_TL) for each of the labels:\n\nwith tempfile.TemporaryDirectory() as tmpdir:\n    args = (p_TL, p_T, p_L, info_jaccard, H_T, H_L)\n    kwargs = {'k':10, 'save_as': Path(tmpdir)/'mut_info_jaccard.ft'}\n    df_info = info.show(*args, **kwargs)\n    assert (Path(tmpdir)/'mut_info_jaccard.ft').exists()\n\n\ndf_info.head()\n\n\n\n\n\n\n\n\nlabel\nfreq\nprob\nentropy\ndescription\ntop-k (token, prob, entropy, joint, info)\n\n\n\n\n0\n008.45\n17\n0.022849\n0.108931\nIntestinal infection due to clostridium difficile\n[['difficile' '0.05107527' '0.20166922' '0.014784946' '0.11373689']\\n ['cdiff' '0.010752688' '0.05943229' '0.005376344' '0.088108845']\\n ['loosely' '0.002688172' '0.018595558' '0.002688172' '0.08804317']\\n ['reformatted' '0.00672043' '0.04031744' '0.004032258' '0.08063226']\\n ['colitis' '0.05913979' '0.22459403' '0.01344086' '0.07943697']\\n ['flagyl' '0.17069893' '0.45699275' '0.021505376' '0.064542644']\\n ['enteritis' '0.004032258' '0.026255678' '0.002688172' '0.061064955']\\n ['ogt' '0.004032258' '0.026255678' '0.002688172' '0.061064955']\\n ['retardation' '0.004032258' '0.026255678' '0.00...\n\n\n1\n008.8\n2\n0.002688\n0.018596\nIntestinal infection due to other organism, not elsewhere classified\n[['vasotec' '0.002688172' '0.018595558' '0.001344086' '0.2120071']\\n ['gastroenteritis' '0.010752688' '0.05943235' '0.002688172' '0.19164896']\\n ['tachyarrhythmia' '0.004032258' '0.026255678' '0.001344086'\\n '0.14864387']\\n ['bumex' '0.004032258' '0.026255678' '0.001344086' '0.14864387']\\n ['rta' '0.004032258' '0.026255678' '0.001344086' '0.14864387']\\n ['neoral' '0.004032258' '0.026255678' '0.001344086' '0.14864387']\\n ['probenecid' '0.004032258' '0.026255678' '0.001344086' '0.14864387']\\n ['electronics' '0.004032258' '0.026255678' '0.001344086' '0.14864387']\\n ['cauterized' '0.004032258...\n\n\n2\n009.0\n1\n0.001344\n0.010230\nInfectious colitis, enteritis, and gastroenteritis\n[['presacral' '0.001344086' '0.010230334' '0.001344086' '0.9999958']\\n ['vibrio' '0.001344086' '0.010230334' '0.001344086' '0.9999958']\\n ['yersinia' '0.002688172' '0.018595558' '0.001344086' '0.40896738']\\n ['ova' '0.002688172' '0.018595558' '0.001344086' '0.40896738']\\n ['parasites' '0.002688172' '0.018595558' '0.001344086' '0.40896738']\\n ['resucitation' '0.002688172' '0.018595558' '0.001344086' '0.40896738']\\n ['exlap' '0.004032258' '0.026255678' '0.001344086' '0.26589572']\\n ['tenting' '0.004032258' '0.026255678' '0.001344086' '0.26589572']\\n ['adhesiolysis' '0.004032258' '0.026255678...\n\n\n3\n009.1\n2\n0.002688\n0.018596\nColitis, enteritis, and gastroenteritis of presumed infectious origin\n[['44yf' '0.001344086' '0.010230334' '0.001344086' '0.40896738']\\n ['ischioanal' '0.001344086' '0.010230334' '0.001344086' '0.40896738']\\n ['perianal' '0.001344086' '0.010230334' '0.001344086' '0.40896738']\\n ['paraplegia' '0.002688172' '0.018595558' '0.001344086' '0.2120071']\\n ['hunger' '0.002688172' '0.018595558' '0.001344086' '0.2120071']\\n ['intrathecal' '0.002688172' '0.018595558' '0.001344086' '0.2120071']\\n ['paraplegic' '0.002688172' '0.018595558' '0.001344086' '0.2120071']\\n ['vicarious' '0.002688172' '0.018595558' '0.001344086' '0.2120071']\\n ['spasticity' '0.002688172' '0.01859...\n\n\n4\n031.0\n1\n0.001344\n0.010230\nPulmonary diseases due to other mycobacteria\n[['gist' '0.002688172' '0.018595558' '0.001344086' '0.40896738']\\n ['disrupted' '0.002688172' '0.018595558' '0.001344086' '0.40896738']\\n ['77f' '0.002688172' '0.018595558' '0.001344086' '0.40896738']\\n ['eroding' '0.002688172' '0.018595558' '0.001344086' '0.40896738']\\n ['vaginitis' '0.002688172' '0.018595558' '0.001344086' '0.40896738']\\n ['circumscribed' '0.002688172' '0.018595558' '0.001344086' '0.40896738']\\n ['discern' '0.002688172' '0.018595558' '0.001344086' '0.40896738']\\n ['inseparable' '0.002688172' '0.018595558' '0.001344086' '0.40896738']\\n ['pearls' '0.002688172' '0.018595558...\n\n\n\n\n\n\n\n\n\nLet’s look at those Mutual-Information Gain values:\n\nmask = (df_info.freq&gt;50) & (df_info.freq&lt;150)\n# with pd.option_context('display.max_colwidth', 100):\n# pd.reset_option('all')\ndf_info = df_info[mask].reset_index(drop=True)\nlen(df_info)\n\n29\n\n\nThe dataframe below shows the top 10 tokens (based on the mutual-info-gain values) for labels are rare (freq between 50 and 150). Feel free to ChatGPT the label descriptions and the tokens to find out if we’re able to find the needle in a haystack.\n\npd.set_option('display.max_colwidth', None)\ndf_info.head()\n\n\n\n\n\n\n\n\nlabel\nfreq\nprob\nentropy\ndescription\ntop-k (token, prob, entropy, joint, info)\n\n\n\n\n0\n038.9\n52\n0.069892\n0.253361\nUnspecified septicemia\n[['pressors' '0.13037634' '0.38710147' '0.041666668' '0.079481095']\\n ['expired' '0.10215054' '0.32978266' '0.034946237' '0.073719725']\\n ['septic' '0.06586021' '0.24279651' '0.024193548' '0.05889168']\\n ['spectrum' '0.061827958' '0.23196787' '0.021505376' '0.04939346']\\n ['levophed' '0.06989247' '0.25336072' '0.022849463' '0.04748282']\\n ['sepsis' '0.18548387' '0.47960785' '0.041666668' '0.04552333']\\n ['tpn' '0.0483871' '0.1937385' '0.017473118' '0.04391181']\\n ['lactate' '0.26478493' '0.5780026' '0.049731184' '0.041423406']\\n ['rescusitated' '0.004032258' '0.026255678' '0.004032258' '0.04032902']\\n ['broad' '0.07123656' '0.25682586' '0.021505376' '0.040083304']]\n\n\n1\n244.9\n71\n0.095430\n0.314924\nUnspecified hypothyroidism\n[['hypothyroidism' '0.10887097' '0.34414828' '0.083333336' '0.4098433']\\n ['levothyroxine' '0.10752688' '0.34131324' '0.07392473' '0.28809547']\\n ['synthroid' '0.04973118' '0.19772297' '0.033602152' '0.11988581']\\n ['levoxyl' '0.018817205' '0.093399525' '0.016129032' '0.08422138']\\n ['hypothyroid' '0.014784946' '0.07698107' '0.01344086' '0.07722074']\\n ['mcg' '0.36021507' '0.6535418' '0.076612905' '0.047165737']\\n ['88mcg' '0.005376344' '0.03345727' '0.005376344' '0.038052656']\\n ['cystitis' '0.004032258' '0.026255678' '0.004032258' '0.028801844']\\n ['kyphotic' '0.004032258' '0.026255678' '0.004032258' '0.028801844']\\n ['cvat' '0.004032258' '0.026255678' '0.004032258' '0.028801844']]\n\n\n2\n250.00\n115\n0.154570\n0.430555\ntype II diabetes mellitus [non-insulin dependent type] [NIDDM type] [adult-onset type] or unspecified type, not stated as uncontrolled, without mention of complication\n[['diabetes' '0.2876344' '0.60002' '0.1155914' '0.09059631']\\n ['metformin' '0.0672043' '0.24634655' '0.045698926' '0.08375815']\\n ['glyburide' '0.037634406' '0.1603519' '0.028225806' '0.063347906']\\n ['dm' '0.14784946' '0.4189606' '0.06317204' '0.048482798']\\n ['mellitus' '0.14919354' '0.42130768' '0.061827958' '0.044503253']\\n ['noninsulin' '0.010752688' '0.05943235' '0.010752688' '0.043445654']\\n ['tricor' '0.00672043' '0.04031744' '0.00672043' '0.027659079']\\n ['avandia' '0.012096774' '0.06542839' '0.0094086025' '0.024442347']\\n ['insulin' '0.2795699' '0.59254664' '0.08064516' '0.024319947']\\n ['glipizide' '0.021505376' '0.103841364' '0.01344086' '0.024206813']]\n\n\n3\n272.0\n91\n0.122312\n0.371506\nPure hypercholesterolemia\n[['hypercholesterolemia' '0.13978495' '0.40457296' '0.07795699'\\n '0.14934917']\\n ['lipitor' '0.17204301' '0.45911095' '0.049731184' '0.027298862']\\n ['aspirin' '0.54569894' '0.68896455' '0.10080645' '0.022963593']\\n ['crestor' '0.016129032' '0.08256498' '0.0094086025' '0.022421718']\\n ['carotids' '0.02688172' '0.12372976' '0.012096774' '0.018986586']\\n ['nonreactive' '0.08736559' '0.29640004' '0.0' '0.018241761']\\n ['gallop' '0.010752688' '0.05943235' '0.00672043' '0.018127378']\\n ['crossclamp' '0.010752688' '0.05943235' '0.00672043' '0.018127378']\\n ['palate' '0.086021505' '0.29323512' '0.0' '0.018028865']\\n ['mrs' '0.037634406' '0.1603519' '0.014784946' '0.01788708']]\n\n\n4\n272.4\n131\n0.176075\n0.465390\nOther and unspecified hyperlipidemia\n[['hyperlipidemia' '0.17876343' '0.46951324' '0.11155914' '0.14867449']\\n ['dyslipidemia' '0.0672043' '0.24634655' '0.04032258' '0.048867557']\\n ['medquist36' '0.28225806' '0.59507334' '0.004032258' '0.048413806']\\n ['brief' '0.75' '0.56233513' '0.1733871' '0.045796935']\\n ['invasive' '0.72983867' '0.5834192' '0.17204301' '0.045730278']\\n ['major' '0.7338709' '0.5793706' '0.17204301' '0.044849273']\\n ['job' '0.29435483' '0.606005' '0.00672043' '0.04331313']\\n ['attending' '0.74596775' '0.56672186' '0.17204301' '0.042243805']\\n ['dictated' '0.29435483' '0.606005' '0.008064516' '0.039907183']\\n ['exam' '0.77956986' '0.5274518' '0.1733871' '0.03951623']]\n\n\n\n\n\n\n\n\n# pd.reset_option('all')\n\n\n# df_info.to_excel('jaccard.xls', index=False)\n\n\n\nScratchpad\n\nfrom fastai.data.transforms import *\n\n\nsource = untar_xxx(XURLs.MIMIC3_L2R)\nboot_path = source/'mimic3-9k_tok_lbl_info.pkl'\nassert boot_path.exists()\n\n\nl2r_bootstrap = torch.load(boot_path)\ntoks, lbs, info = mapt(l2r_bootstrap.get, ['toks', 'lbs', 'mutual_info_jaccard'])\nprint(toks[-2:]) # last two places has 'xxfake'\n# toks = CategoryMap(toks, sort=False)\nlbs_des = load_pickle(source/'code_desc.pkl')\nassert isinstance(lbs_des, dict)\ntest_eq(info.shape, (len(toks), len(lbs))) # last two places has 'xxfake'\n\n['xxfake', 'xxfake']\n\n\n\nL(toks[89:99]), lbs[:10]\n\n((#10) ['course','follow','after','disease','stitle','needed','known','capsule','refills','started'],\n (#10) ['003.0','003.1','003.8','003.9','004.1','004.8','004.9','005.1','005.81','005.9'])\n\n\n\nicd_codes = ['008.45', '009.0', '244.9', '250.00']\n\n\nd= dict(zip(icd_codes, mapt(lbs_des.get, icd_codes)))\ndf_des = pd.DataFrame(d, index=range(1))\n\nlbs_idxs = lbs.map_objs(icd_codes)\ntop_infos = info[:, lbs_idxs].topk(dim=0, k=10, largest=True).values.cpu()\ntop_idxs = info[:, lbs_idxs].topk(dim=0, k=10, largest=True).indices.cpu().long()\ntoks = array(toks).astype(str)\n# toks.map_ids(tok_idxs[:, 0])\ndf_toks = pd.DataFrame(toks[top_idxs], columns=icd_codes)\ndf_infos = pd.DataFrame(top_infos, columns=icd_codes)\n\n\ndisplay(df_des, df_toks, df_infos)\n\n\n\n\n\n\n\n\n008.45\n009.0\n244.9\n250.00\n\n\n\n\n0\nIntestinal infection due to clostridium difficile\nInfectious colitis, enteritis, and gastroenteritis\nUnspecified hypothyroidism\ntype II diabetes mellitus [non-insulin dependent type] [NIDDM type] [adult-onset type] or unspecified type, not stated as uncontrolled, without mention of complication\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n008.45\n009.0\n244.9\n250.00\n\n\n\n\n0\ndifficile\nileoloop\nhypothyroidism\ndiabetes\n\n\n1\ncolitis\nentercort\nlevothyroxine\nmetformin\n\n\n2\ndiff\ncoumidin\nsynthroid\nmellitus\n\n\n3\nclostridium\n33u\nhypothyroid\ndm\n\n\n4\nmetronidazole\n8x\nmcg\ninsulin\n\n\n5\nflagyl\nproctocolitis\nlevoxyl\nglyburide\n\n\n6\ntoxin\nchux\ntsh\nglipizide\n\n\n7\ncdiff\nmetronidzole\nt4\ndm2\n\n\n8\nmegacolon\nbayview\n88mcg\nsliding\n\n\n9\nfeces\n117bpm\n50mcg\nscale\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n008.45\n009.0\n244.9\n250.00\n\n\n\n\n0\n0.121274\n0.027056\n0.353851\n0.089817\n\n\n1\n0.097799\n0.027056\n0.319011\n0.085321\n\n\n2\n0.093017\n0.027056\n0.093766\n0.063133\n\n\n3\n0.086582\n0.027056\n0.076361\n0.052869\n\n\n4\n0.064245\n0.023884\n0.063794\n0.043760\n\n\n5\n0.061297\n0.023884\n0.052179\n0.040929\n\n\n6\n0.057576\n0.023737\n0.020855\n0.040014\n\n\n7\n0.042272\n0.023737\n0.014761\n0.033789\n\n\n8\n0.032625\n0.023737\n0.013494\n0.030756\n\n\n9\n0.026234\n0.023737\n0.012336\n0.023395\n\n\n\n\n\n\n\n\nvocab = load_pickle(Path.cwd()/'tmp/models/mimic3-9k_clas_full_vocab.pkl')\n\n\ntoks = L(toks, use_list=True)\n\nThe tokens which are there in the xml vocab but we do not have any ‘info’ on:\n\nnot_found = L(set(vocab[0]).difference(set(toks)))\nnot_found\n\n(#20) ['foi','luteinizing','dobhoof','theses','q2day','promiscuity','dissension','sharpio','mhc','remiained'...]\n\n\n\ntest_fail(lambda : toks.index('unrmarkable'), contains='is not in list')\n\nThe tokens which we have info about but were not present in the xml vocab\n\nset(toks).difference(vocab[0])\n\nset()\n\n\nThankfully, we have info for all the labels in the xml vocab:\n\ntest_shuffled(vocab[1], lbs)\n\nNow we need to create a mapping between the indices of the xml vocab and the information gain vocab:\n\ndef xml2info(xml_vocab, info_vocab):\n    \"Creates a mapping between the indices of the xml vocab and the information-gain vocab\"\n    xml2info = {i: info_vocab.index(o) if o in info_vocab else np.inf  for i,o in enumerate(xml_vocab)}\n    xml2info_notfnd = [o for o in xml2info if xml2info[o] is np.inf]\n    return xml2info, xml2info_notfnd\n\n\ntoks_xml2info, toks_notfnd = xml2info(vocab[0], toks)\n\n\ntoks_found = set(toks_xml2info).difference(set(toks_notfnd))\ntest_shuffled(array(vocab[0])[toks_notfnd], not_found)\nsome_xml_idxs = np.random.choice(array(L(toks_found)), size=10)\nsome_xml_toks = array(vocab[0])[some_xml_idxs]\ncorres_info_idxs = L(map(toks_xml2info.get, some_xml_idxs))\ncorres_info_toks = array(toks)[corres_info_idxs]\nassert all_equal(some_xml_toks, corres_info_toks)\n\n\nlbs_xml2info, lbs_notfnd = xml2info(L(vocab[1]), L(lbs))\n\n\nlbs_found = set(lbs_xml2info).difference(set(lbs_notfnd))\nsome_xml_idxs = np.random.choice(array(L(lbs_found)), size=10)\nsome_xml_lbs = array(vocab[1])[some_xml_idxs]\ncorres_info_idxs = L(map(lbs_xml2info.get, some_xml_idxs))\ncorres_info_lbs = array(lbs)[corres_info_idxs]\nassert all_equal(some_xml_lbs, corres_info_lbs)"
  },
  {
    "objectID": "l2r.data.stat.html",
    "href": "l2r.data.stat.html",
    "title": "Stat",
    "section": "",
    "text": "! [ -e /content ] && pip install -Uqq xcube # upgrade xcube on colab\n\nThis module contains all the classes and functions to perform statistical analysis on the mutual information gain computed in module Mutual Information. To understand how the functionalities here get used follow along the tutorial training L2R"
  },
  {
    "objectID": "utils.html",
    "href": "utils.html",
    "title": "utils",
    "section": "",
    "text": "source\n\nnamestr\n\n namestr (obj, namespace=None)\n\nReturns the name of the object obj passed\nHere’s an example of how namestr works:\n\na = 'some_var'\ntest_eq(namestr(a, globals()), ['a'])\n\n\nsource\n\n\nlist_files\n\n list_files (startpath)\n\n[simulates the linux tree cmd] (https://stackoverflow.com/questions/9727673/list-directory-tree-structure-in-python)\n\nsource\n\n\nmake_paths\n\n make_paths (path, prefix=None)\n\nwith path as basedir, makes data and models dir and returns a dictionary of relevant pathlib objects\n\nwith tempfile.TemporaryDirectory() as tempdirname:\n    print(f\"created temporary dir: {tempdirname}\")\n    _paths = make_paths(Path(tempdirname), \"mimic3-9k\")\n    for v in _paths.values(): v.touch()\n    list_files(tempdirname)\n\ncreated temporary dir: /tmp/tmpi1evi3rs\ntmpi1evi3rs/\n    data/\n        mimic3-9k_tok.ft\n        mimic3-9k_lbl.ft\n        mimic3-9k.csv\n        mimic3-9k_tok_lbl.ft\n    models/\n        mimic3-9k_dls_clas.pkl\n        mimic3-9k_dls_lm.pkl\n        mimic3-9k_lm_r.pth\n        mimic3-9k_lm_finetuned_r.pth\n        mimic3-9k_tok_lbl_info.pkl\n        mimic3-9k_dls_lm_vocab_r.pkl\n        mimic3-9k_dls_collab.pkl\n        mimic3-9k_clas.pth\n        mimic3-9k_dset_clas.pkl\n        mimic3-9k_dls_lm_vocab.pkl\n        mimic3-9k_dls_learn_rank.pkl\n        mimic3-9k_dls_lm_r.pkl\n        mimic3-9k_dset_clas_r.pkl\n        mimic3-9k_lm.pth\n        mimic3-9k_clas_r.pth\n        mimic3-9k_lm_finetuned.pth\n        mimic3-9k_dls_clas_r.pkl\n        collab/\n            mimic3-9k_collab.pth\n\n\n\nsource\n\n\nplot_hist\n\n plot_hist (data, x_label=None, y_label=None, title='Histogram')\n\n\nsource\n\n\nplot_reduction\n\n plot_reduction (X, tSNE=True, n_comps=None, perplexity=30, figsize=(6,\n                 4))\n\nPCA on X and plots the first two principal components, returns the decomposition and the explained variances for each directions, if tSNE then does a tSNE after PCA.\n\nsource\n\n\ntest_eqs\n\n test_eqs (*args)\n\n\ntest_eqs(1, 1, 9//9)\n\n\nsource\n\n\nvalidate\n\n validate (learner, cb=&lt;class\n           'fastai.callback.tracker.SaveModelCallback'&gt;)\n\nvalidates a learner within a context manager after temporarily removing cb if it exists\n\nsource\n\n\ncudamem\n\n cudamem (device=device(type='cpu'))"
  },
  {
    "objectID": "text.models.core.html",
    "href": "text.models.core.html",
    "title": "Core XML Text Modules",
    "section": "",
    "text": "The models provided here are variations of the ones provided by fastai with modifications tailored for XML."
  },
  {
    "objectID": "text.models.core.html#basic-models",
    "href": "text.models.core.html#basic-models",
    "title": "Core XML Text Modules",
    "section": "Basic Models",
    "text": "Basic Models\n\nsource\n\nSequentialRNN\n\n SequentialRNN (*args)\n\nA sequential pytorch module that passes the reset call to its children."
  },
  {
    "objectID": "text.models.core.html#classification-models",
    "href": "text.models.core.html#classification-models",
    "title": "Core XML Text Modules",
    "section": "Classification Models",
    "text": "Classification Models\nThe SentenceEncoder below is the fastai’s source code. Copied here for understanding its components and chaning it to AttentiveSentenceEncoder:\n\nsource\n\nSentenceEncoder\n\n SentenceEncoder (bptt, module, pad_idx=1, max_len=None)\n\nCreate an encoder over module that can process a full sentence.\n\n\n\n\n\n\nWarning\n\n\n\nThis module expects the inputs padded with most of the padding first, with the sequence beginning at a round multiple of bptt (and the rest of the padding at the end). Use pad_input_chunk to get your data in a suitable format.\n\n\n\nsource\n\n\nAttentiveSentenceEncoder\n\n AttentiveSentenceEncoder (bptt, module, decoder, pad_idx=1, max_len=None,\n                           running_decoder=True)\n\nCreate an encoder over module that can process a full sentence.\n\nsource\n\n\nmasked_concat_pool\n\n masked_concat_pool (output, mask, bptt)\n\nPool MultiBatchEncoder outputs into one vector [last_hidden, max_pool, avg_pool]\n\nsource\n\n\nXPoolingLinearClassifier\n\n XPoolingLinearClassifier (dims, ps, bptt, y_range=None)\n\nSame as nn.Module, but no need for subclasses to call super().__init__\nNote that XPoolingLinearClassifier is exactly same as fastai’s PoolingLinearClassifier except that we do not do the feature compression from 1200 to 50 linear features.\nNote: Also try XPoolingLinearClassifier w/o dropouts and batch normalization (Verify this, but as far as what I found it does not work well as compared to /w batch normalization)\n\nsource\n\n\nLabelAttentionClassifier\n\n LabelAttentionClassifier (n_hidden, n_lbs, y_range=None)\n\nSame as nn.Module, but no need for subclasses to call super().__init__\nTODOS: Deb - ~Find out what happens with respect to RNN Regularizer callback after LabelAttentionClassifier returns a tuple of 3. (Check the learner cbs and follow the RNNcallback)~ - ~Check if we are losing anything by ignoring the mask in LabelAttentionClassifier. That is should we be ignoring the masked tokens while computing atten wgts.~\n- Change the label bias initial distribution from uniform to the one we leanerd seperately. - ~Implement Treacher Forcing~\n\n# %%debug\nattn_clas = LabelAttentionClassifier(400, 1271)\ntest_eq(getattrs(attn_clas, 'n_hidden', 'n_lbs'), (400, 1271))\ninps, outs, mask = torch.zeros(16, 72*20).random_(10), torch.randn(16, 72*20, 400), torch.randint(2, size=(16, 72*20))\nx, *_ = attn_clas((inps, outs, mask))\ntest_eq(x.shape, (16, 1271))\n\n\nsource\n\n\nget_xmltext_classifier\n\n get_xmltext_classifier (arch, vocab_sz, n_class, seq_len=72, config=None,\n                         drop_mult=1.0, pad_idx=1, max_len=1440,\n                         y_range=None)\n\nCreate a text classifier from arch and its config, maybe pretrained\n\nsource\n\n\nget_xmltext_classifier2\n\n get_xmltext_classifier2 (arch, vocab_sz, n_class, seq_len=72,\n                          config=None, drop_mult=1.0, pad_idx=1,\n                          max_len=1440, y_range=None,\n                          running_decoder=True)\n\nCreate a text classifier from arch and its config, maybe pretrained"
  },
  {
    "objectID": "l2r.gradients.html",
    "href": "l2r.gradients.html",
    "title": "L2R Gradients",
    "section": "",
    "text": "The following notations are borrowed from From RankNet to LambdaRank to LambdaMART: An Overview\nLet \\(I\\) denote the pair of indices \\(\\{i, j\\}\\), for which we desire token_i to be ranked differently from token_j (for a given label group). Since we must include each pair just once, so it is convenient to consider pairs of indices \\(\\{i, j\\}\\) for which token_i is more relevant than token_j.\n\\[\\lambda_{ij} = \\sigma \\left\\{ \\frac{1}{2}(1 - S_{ij}) - \\frac{1}{1+e^{ \\sigma(p_i - p_j)}} \\right\\}  \\textsf{  Eq: 3},\\] where \\(\\sigma\\) is a hyper-parameter which controls the shape of the sigmoid and \\(p_i, p_j\\) are predictions made by the model for token_i and token_j respectively, and\n\\[S_{ij} = \\begin{cases}\n                1, & \\text{if token_i is more relevant} \\\\\n                0, & \\text{if token_i is as relevant as token_j} \\\\\n                -1, & \\text{if token_j is ore relevant}\n            \\end{cases}\\]\nThe weight update rule in gradient descent is given by: \\[\\delta w_k = \\eta \\sum_{\\{i,j\\} \\in I} (\\lambda_{ij} \\frac{\\partial p_i}{\\partial w_k} - \\lambda_{ij} \\frac{\\partial p_j}{\\partial w_k}) = -\\eta \\sum_i \\lambda_i \\frac{\\partial p_i}{\\partial w_k},\\] where\n\\[\\lambda_i = \\sum_{j: \\{i,j\\} \\in I} \\lambda_{ij} - \\sum_{j: \\{j,i\\} \\in I} \\lambda_{ji} \\textsf{  Eq: 4}.\\]\nImplementing the above equations:\n(Handcrfted Gradients)\nWe can think of the tensor returned by _summation as essentially the summation notation in eq:4 above. It has three dimension. The length of the zeroth dim is the number of tokens. And each token contains a 2d tensor. For each token the zeroth and the first dim of 2d tensor has the following interpretation.\nFor each token in a sequence (i.e. the i’s) it contains the information about the other tokens (i.e. the j’s) that\n1. The first column value tells us the row num we got to index in the pairs array. 2. The last column value tells us whether i is more relevant or less relevant than j. In other words, it determines the sign while computing \\(\\lambda_i\\) in eq: 4.\n\nsource\n\nrank_loss2\n\n rank_loss2 (preds, xb, sigma=0.5, lambrank=False, gain_fn=None, k=6)\n\n\nsource\n\n\nrank_loss3\n\n rank_loss3 (preds, xb, sigma=0.5, lambrank=False, gain_fn=None, k=6)\n\nIf we were to use a loss fuunction instead of hand creafted gradients:\n\\[C = \\sum_{\\{i,j\\} \\in I} \\frac{1}{2}(1 - S_{ij})\\sigma(p_i-p_j) + \\log(1 + e^{-\\sigma(p_i - p_j)})\\]\n\nsource\n\n\nloss_fn2\n\n loss_fn2 (preds, xb, sigma=0.5)\n\nComputes average pairwise cross-entropy loss\n\nsource\n\n\nloss_fn\n\n loss_fn (preds, xb, sigma=0.5)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "xcube",
    "section": "",
    "text": "This is under development.\nThis file will become your README and also the index of your documentation."
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "xcube",
    "section": "Install",
    "text": "Install\nInstall using:\npip install xcube\nor\nconda install xcube"
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "xcube",
    "section": "How to use",
    "text": "How to use\nFill me in please! Don’t forget code examples:"
  },
  {
    "objectID": "index.html#acknowledgement",
    "href": "index.html#acknowledgement",
    "title": "xcube",
    "section": "Acknowledgement",
    "text": "Acknowledgement\nThis repository is my attempt to create Extreme Multi-Label Text Classifiers using Language Model Fine-Tuning as proposed by Jeremy Howard and Sebastian Ruder in ULMFit. I am also heavily influenced by the fast.ai’s course Practical Deep Learning for Coders and the excellent library fastai. I have adopted the style of coding from fastai using the jupyter based dev environment nbdev. Since this is one of my fast attempt to create a full fledged python library, I have at times replicated implementations from fastai with some modifications. A big thanks to Jeremy and his team from fast.ai for everything they have been doing to make AI accessible to everyone."
  },
  {
    "objectID": "tutorial.train_l2r.html",
    "href": "tutorial.train_l2r.html",
    "title": "L2R Training",
    "section": "",
    "text": "! [ -e /content ] && pip install -Uqq xcube # upgrade xcube on colab\nfrom xcube.l2r.all import *\nMake sure we have that “beast”:\nic(torch.cuda.get_device_name(default_device()));\ntest_eq(torch.cuda.get_device_name(0), torch.cuda.get_device_name(default_device()))\ntest_eq(default_device(), torch.device(0))\nprint(f\"GPU memory = {torch.cuda.get_device_properties(default_device()).total_memory/1024**3}GB\")\n\nic| torch.cuda.get_device_name(default_device()): 'Quadro RTX 8000'\n\n\nGPU memory = 44.99969482421875GB\nSetting some environment variables:\n# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\nSetting defaults for pandas and matplotlib:\n# Set the default figure size\nplt.rcParams[\"figure.figsize\"] = (6, 4)\nIn this tutorial we will train a l2r model. We will bootstrap the model using the data we prepared in tutorial booting L2R"
  },
  {
    "objectID": "tutorial.train_l2r.html#getting-ready",
    "href": "tutorial.train_l2r.html#getting-ready",
    "title": "L2R Training",
    "section": "Getting ready",
    "text": "Getting ready\nPrepping l2r data for xcube’s L2RDataLoader\n\nsource = untar_xxx(XURLs.MIMIC3_L2R)\nsource.ls()\n\n(#11) [Path('/home/deb/.xcube/data/mimic3_l2r/info.pkl'),Path('/home/deb/.xcube/data/mimic3_l2r/code_descriptions.csv'),Path('/home/deb/.xcube/data/mimic3_l2r/mimic3-9k_tok_lbl_info.pkl'),Path('/home/deb/.xcube/data/mimic3_l2r/code_desc.pkl'),Path('/home/deb/.xcube/data/mimic3_l2r/p_TL.pkl'),Path('/home/deb/.xcube/data/mimic3_l2r/trn_val_split.pkl'),Path('/home/deb/.xcube/data/mimic3_l2r/mimic3-9k_tok.ft'),Path('/home/deb/.xcube/data/mimic3_l2r/mimic3-9k_lbl.ft'),Path('/home/deb/.xcube/data/mimic3_l2r/mimic3-9k.csv'),Path('/home/deb/.xcube/data/mimic3_l2r/scored_tokens.pth')...]\n\n\nNote: If you don’t have enough GPU/CPU memory just run the last cell of this section to load the pregenerated ones.\nHere we can just load the file which contains the relevant information about the tokens, labels and their mutual-information-gain:\n\n# Cheking if you have enough memory to set device\ncuda_memory = torch.cuda.get_device_properties(torch.cuda.current_device()).total_memory/1024**3\nif cuda_memory &lt; 10.: print(f\"Not Enough GPU Memory (just {cuda_memory} GB), we'll use {default_device(use=False)}\")\nl2r_bootstrap = torch.load(source/'mimic3-9k_tok_lbl_info.pkl', map_location=default_device())\n\n\ntest_eq(l2r_bootstrap.keys(), ['toks', 'lbs', 'mut_info_lbl_entropy', 'mutual_info_jaccard'])\ntoks = l2r_bootstrap.get('toks', None)\nlbs = l2r_bootstrap.get('lbs', None)\ninfo = l2r_bootstrap.get('mutual_info_jaccard', None)\nfor o in (toks, lbs, info): assert o is not None\ntest_eq(info.shape, (len(toks), len(lbs)))\n\ninfo contains the mutual-information-gain values for the tokens and labels. In what follows we’ll toss in some pandas to take a good hard look at the data before we proceed towards making xcube’s L2RDataLoader:\nNote: Storing the tokens and the labels in a dataframe as object will take up a lot of RAM space when we prepare that DataLoader. So we are going to store the corresponding token and label indices instead in a dataframe called df_l2r. We are also going to store the tokens and the labels with their corresponding indices in seperate dataframes (this will help in quick merging for analysis).\nHere we will rank the tokens for each label based on the decreasing values of the mutual-info and stack them up with mutual-info.\n\nranked = info.argsort(descending=True, dim=0).argsort(dim=0)\ninfo_ranked =torch.stack((info, ranked), dim=2).flatten(start_dim=1)\n\n\ncols = pd.MultiIndex.from_product([range(len(lbs)), ['mutual_info', 'rank']], names=['label', 'key2'])\ndf_l2r = pd.DataFrame(info_ranked, index=range(len(toks)), columns=cols)\ndf_l2r.index.name='token'\n\n\ndf_l2r.head(3)\n\n\n\n\n\n\n\nlabel\n0\n1\n2\n3\n4\n...\n8917\n8918\n8919\n8920\n8921\n\n\nkey2\nmutual_info\nrank\nmutual_info\nrank\nmutual_info\nrank\nmutual_info\nrank\nmutual_info\nrank\n...\nmutual_info\nrank\nmutual_info\nrank\nmutual_info\nrank\nmutual_info\nrank\nmutual_info\nrank\n\n\ntoken\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0\n0.000022\n866.0\n0.000011\n1022.0\n0.000022\n1156.0\n0.000011\n823.0\n0.000033\n984.0\n...\n0.000011\n850.0\n0.000033\n944.0\n0.000011\n960.0\n0.000011\n771.0\n6.888287e-07\n31821.0\n\n\n1\n0.000000\n56854.0\n0.000000\n41418.0\n0.000000\n56853.0\n0.000000\n41410.0\n0.000000\n22836.0\n...\n0.000000\n41421.0\n0.000000\n22861.0\n0.000000\n41423.0\n0.000000\n41412.0\n0.000000e+00\n32385.0\n\n\n2\n0.000000\n56855.0\n0.000000\n41419.0\n0.000000\n56854.0\n0.000000\n41411.0\n0.000000\n22837.0\n...\n0.000000\n41422.0\n0.000000\n22862.0\n0.000000\n41424.0\n0.000000\n41413.0\n0.000000e+00\n32386.0\n\n\n\n\n3 rows × 17844 columns\n\n\n\n\ndf_l2r = df_l2r.stack(level=0).reset_index().rename_axis(None, axis=1)\n# the above pandas trick can be simulated using numpy as follows\n# n = df_l2r.to_numpy()\n# n_toks, n_lbs = len(df_l2r.index), len(df_l2r.columns.levels[0])\n# n = n.reshape(-1, 2)\n# tok_lbs_idxs = np.mgrid[slice(0,n_toks), slice(0,n_lbs)].reshape(2,-1).T\n# n = np.concatenate((tok_lbs_idxs,n), axis=-1)\n# df_l2r = pd.DataFrame(n, columns=['token', 'label', 'mutual_info', 'rank'])\ndf_l2r[['token', 'label']] = df_l2r[['token', 'label']].astype(np.int32) \ntest_eq(len(df_l2r), len(toks) * len(lbs))\n\n\ndf_l2r.head(3)\n\n\n\n\n\n\n\n\ntoken\nlabel\nmutual_info\nrank\n\n\n\n\n0\n0\n0\n0.000022\n866.0\n\n\n1\n0\n1\n0.000011\n1022.0\n\n\n2\n0\n2\n0.000022\n1156.0\n\n\n\n\n\n\n\n\ndf_l2r.memory_usage()/1024**3\n\nIndex          1.192093e-07\ntoken          1.906211e+00\nlabel          1.906211e+00\nmutual_info    1.906211e+00\nrank           1.906211e+00\ndtype: float64\n\n\n\ndf_toks = pd.DataFrame([(i, w) for i,w in enumerate(toks)], columns=['token', 'tok_val'])\ndf_lbs = pd.DataFrame([(i,w) for i, w in enumerate(lbs)], columns=['lbl', 'lbl_val'])\n\n\ndf_toks.head(3)\n\n\n\n\n\n\n\n\ntoken\ntok_val\n\n\n\n\n0\n0\nxxunk\n\n\n1\n1\nxxpad\n\n\n2\n2\nxxbos\n\n\n\n\n\n\n\n\ndf_lbs.head(3)\n\n\n\n\n\n\n\n\nlbl\nlbl_val\n\n\n\n\n0\n0\n003.0\n\n\n1\n1\n003.1\n\n\n2\n2\n003.8\n\n\n\n\n\n\n\nYou can save df_l2r, df_toks and df_lbs if you are working on your own dataset. In this case though untar_xxx has already downloaded those for you.\n\nL(source.glob(\"**/*.ft\"))\n\n(#3) [Path('/home/deb/.xcube/data/mimic3_l2r/mimic3-9k_tok.ft'),Path('/home/deb/.xcube/data/mimic3_l2r/mimic3-9k_lbl.ft'),Path('/home/deb/.xcube/data/mimic3_l2r/mimic3-9k_tok_lbl.ft')]"
  },
  {
    "objectID": "tutorial.train_l2r.html#statistical-analysis",
    "href": "tutorial.train_l2r.html#statistical-analysis",
    "title": "L2R Training",
    "section": "Statistical Analysis",
    "text": "Statistical Analysis\n\ndf_l2r = pd.read_feather(source/'mimic3-9k_tok_lbl.ft')\ntest_eq(df_l2r.dtypes.mutual_info, np.float32)\n\n\ndf_l2r.head(3)\n\n\n\n\n\n\n\n\ntoken\nlabel\nmutual_info\nrank\n\n\n\n\n0\n0\n0\n0.000022\n866.0\n\n\n1\n0\n1\n0.000011\n1022.0\n\n\n2\n0\n2\n0.000022\n1156.0\n\n\n\n\n\n\n\nIf you loaded the pregenerated df_l2r then you will see the column “bcx_mutual_info”. It is a box-cox transformation of the “mutual-info”. In this section we’ll justify that transformation. So let’s perform some statistical analysis of that mutual_info column before we build the L2RDataLoader in the next section.\n\n# import gc; gc.collect()\n# df_l2r.info()\n# ic(df_l2r.memory_usage().sum()/1024**3)\n# ic(sys.getsizeof(df_l2r)/1024**3);\n# df_collab.token.nunique()\n\n\nmut_infos = df_l2r['mutual_info'].to_numpy()\n\n\nmut_infos.min(), mut_infos.max(), mut_infos.mean()\n\n(-6.852321e-05, 0.99999636, 7.175153e-05)\n\n\n\nskew(mut_infos)\n\nCPU times: user 2.22 s, sys: 1.14 s, total: 3.36 s\nWall time: 3.36 s\n\n\n142.75660007849734\n\n\nThe mutual-info values are incredibly skewed. So we need to apply some transformation. Sometimes mut_infos might contain negs, we need to convert those to eps.\n\n# np.where(mut_infos&lt;0, 1, 0).sum() # or, better yet\nwhere_negs = mut_infos &lt; 0\nic(np.sum(where_negs))\neps = np.float32(1e-20)\nmut_infos[where_negs] = eps\ntest_eq(np.sum(mut_infos&lt;0), 0)\nic(np.min(mut_infos), np.max(mut_infos), np.mean(mut_infos));\n\nic| np.sum(where_negs): 111226814\nic| np.min(mut_infos): 0.0\n    np.max(mut_infos): 0.99999636\n    np.mean(mut_infos): 7.697003e-05\n\n\n\nhist, bins, _ = plt.hist(mut_infos, bins=50)\n# plt.yscale('log')\n\n\n\n\nApplying log transform:\n\nlog_mut_infos = np.log(mut_infos + eps)\n\n\nnp.isnan(log_mut_infos).sum(), np.isneginf(log_mut_infos).sum(), np.isinf(log_mut_infos).sum()\n\n(0, 0, 0)\n\n\n\n\n\nCPU times: user 2.35 s, sys: 959 ms, total: 3.31 s\nWall time: 3.3 s\n\n\n-1.3383214188674972\n\n\nA little better skewness than before!\n\nhist, bins, _ = plt.hist(log_mut_infos, bins=50,)\n\n\n\n\nApplying sqrt transform:\n\nsqrt_mut_infos = np.sqrt(mut_infos)\n\n\nnp.isnan(sqrt_mut_infos).sum(), np.isinf(sqrt_mut_infos).sum(), np.isneginf(sqrt_mut_infos).sum()\n\n(0, 0, 0)\n\n\n\n\n\nCPU times: user 2.38 s, sys: 1.25 s, total: 3.63 s\nWall time: 3.63 s\n\n\n16.40865608826817\n\n\nWorse than log transform!\n\nhist, bins, _ = plt.hist(sqrt_mut_infos, bins=50)\n\n\n\n\nApply box-cox transfrom:\n\nbcx_mut_infos, *_ = boxcox(mut_infos+eps)\n\n/home/deb/miniconda3/envs/deep/lib/python3.10/site-packages/scipy/stats/_morestats.py:933: RuntimeWarning: overflow encountered in power\n  variance = np.var(data**lmb / lmb, axis=0)\n/home/deb/miniconda3/envs/deep/lib/python3.10/site-packages/numpy/core/_methods.py:233: RuntimeWarning: invalid value encountered in subtract\n  x = asanyarray(arr - arrmean)\n\n\n\nnp.isnan(bcx_mut_infos).sum(), np.isinf(bcx_mut_infos).sum(), np.isneginf(bcx_mut_infos).sum()\n\n(0, 0, 0)\n\n\n\n\n\nCPU times: user 2.45 s, sys: 1.04 s, total: 3.49 s\nWall time: 3.49 s\n\n\n-0.885981418331696\n\n\nThis is the best skew so we’ll go with boxcox.\n\ndf_l2r['bcx_mutual_info'] = bcx_mut_infos\n\n\nhist, bins, _ = plt.hist(bcx_mut_infos, bins=50)\n\n\n\n\n\nnp.min(bcx_mut_infos), np.max(bcx_mut_infos), np.mean(bcx_mut_infos), np.median(bcx_mut_infos)\n\n(-9.734209, -3.6358892e-06, -7.381837, -6.9605794)\n\n\n\n# from IPython.display import clear_output\n\n# clear_output(wait=True)\n\n# from tqdm import tqdm\n# from time import sleep\n# import psutil\n\n# with tqdm(total=100, desc='cpu%', position=1) as cpubar, tqdm(total=100, desc='ram%', position=0) as rambar:\n#     while True:\n#         rambar.n=psutil.virtual_memory().percent\n#         cpubar.n=psutil.cpu_percent()\n#         rambar.refresh()\n#         cpubar.refresh()\n#         sleep(0.5)\n#         clear_output(wait=True)\n\nBox plots using matplotlib\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\nax1 = fig.add_subplot(1, 2, 1)\nax2 = fig.add_subplot(1, 2, 2)\n\nax1.boxplot(mut_infos, vert=0, notch=True, patch_artist=True)\nax1.set_xscale('log')\nax1.set_xlabel('Mutual Information')\n\nax2.boxplot(bcx_mut_infos, vert=0, notch=True, patch_artist=True)\n# ax2.set_xscale('symlog')\nax2.set_xlabel('Box-Cox Mutual Information')\n\nplt.show()\n\n\n\n\nCPU times: user 1min 35s, sys: 9.16 s, total: 1min 44s\nWall time: 1min 44s\n\n\nHistograms and kde using matplotlib:\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\n# hist, bins, pathches = ax1.hist(df_l2r['mutual_info'])\nhist, bins, pathches = ax1.hist(mut_infos)\nax1.set_xlabel('Mutual Information')\nax1.set_ylabel('frequency')\nax1.grid(axis='y', color='black')\nax1.set_yscale('log')\n\n# ax2.hist(df_l2r['bcx_mutual_info'])\nax2.hist(bcx_mut_infos)\nax2.set_xlabel('Boxcox Mutual Information')\nax2.set_ylabel('frequency')\nax2.grid(axis='y', color='black')\nax2.set_yscale('log')\n\nfig.suptitle('Histograms of with and without BoxCox of Mutual Information')\nplt.show()\n\n\n\n\n\nbdrs = [bins[i:i+2] for i in range(0, len(bins)-1)]\npd.DataFrame({'mut_infos bdrs': bdrs, 'counts': hist})\n\n\n\n\n\n\n\n\nmut_infos bdrs\ncounts\n\n\n\n\n0\n[0.0, 0.09999963641166687]\n511675599.0\n\n\n1\n[0.09999963641166687, 0.19999927282333374]\n14848.0\n\n\n2\n[0.19999927282333374, 0.2999989092350006]\n3323.0\n\n\n3\n[0.2999989092350006, 0.3999985456466675]\n191.0\n\n\n4\n[0.3999985456466675, 0.49999818205833435]\n454.0\n\n\n5\n[0.49999818205833435, 0.5999978184700012]\n30.0\n\n\n6\n[0.5999978184700012, 0.6999974250793457]\n5.0\n\n\n7\n[0.6999974250793457, 0.799997091293335]\n0.0\n\n\n8\n[0.799997091293335, 0.8999967575073242]\n0.0\n\n\n9\n[0.8999967575073242, 0.9999963641166687]\n94.0\n\n\n\n\n\n\n\n\n# from scipy.stats import gaussian_kde\n# density = gaussian_kde(df_l2r['mutual_info'])\n# xs = np.linspace(0, 1, 200)\n# density.covariance_factor = lambda : .25\n# density._compute_covariance()\n# plt.plot(xs, density(xs))\n# plt.show()\n\nWe can now build the Dataloaders object from this dataframe df_collab, by defaultit takes the first column as the user (in our case the token) and the second column as the item (in our case the label), and the third column as the ratings (in our case the frequency):"
  },
  {
    "objectID": "tutorial.train_l2r.html#build-l2rdataloader",
    "href": "tutorial.train_l2r.html#build-l2rdataloader",
    "title": "L2R Training",
    "section": "Build L2RDataloader",
    "text": "Build L2RDataloader\nIn this section we’ll build L2RDataLoader for Learning to Rank (L2R)\n\ndf_l2r = pd.read_feather(source/'mimic3-9k_tok_lbl.ft')\n\n\ndf_l2r = df_l2r.drop(['mutual_info', 'bcx_mutual_info'], axis=1)\ndf_l2r.token.nunique(), df_l2r.label.nunique()\ndf_l2r.head(3)\n\n\n\n\n\n\n\n\ntoken\nlabel\nrank\n\n\n\n\n0\n0\n0\n866.0\n\n\n1\n0\n1\n1022.0\n\n\n2\n0\n2\n1156.0\n\n\n\n\n\n\n\ndf_tiny: If we need a smaller dataset for quick iterations\nNote: For technical reasons behind building a L2RDataloader the number of tokens should be \\(x (mod 64) \\equiv 8\\).\n\nnum_toks, num_lbs = 8 + 5*64, 104\n\n\n# might have to repeat this a few times until the cell asserst true\nnp.random.seed(101)\nrnd_toks = np.random.randint(0, len(df_l2r.token.unique()), size=(num_toks,) )\nnp.random.seed(101)\nrnd_lbs = np.random.randint(0, len(df_l2r.label.unique()), size=(num_lbs,) )\nmask = df_l2r.token.isin(rnd_toks) & df_l2r.label.isin(rnd_lbs)\ndf_tiny = df_l2r[mask].reset_index(drop=True)\ntest_eq(df_tiny.token.nunique(), num_toks) \ntest_eq(df_tiny.label.nunique(), num_lbs) \n# df_tiny.apply(lambda x: x.nunique())\n\n\ndf_tiny.head()\n\n\n\n\n\n\n\n\ntoken\nlabel\nrank\n\n\n\n\n0\n22\n49\n1877.0\n\n\n1\n22\n239\n21308.0\n\n\n2\n22\n394\n39854.0\n\n\n3\n22\n436\n8618.0\n\n\n4\n22\n561\n1646.0\n\n\n\n\n\n\n\nLet’s just delete the df_l2r to free up RAM:\n\n# df_l2r = pd.DataFrame()\n# lst = [df_l2r]\n# del lst\n# del df_l2r\n# import gc; gc.collect()\n\nOnly for df_tiny:\nDue to random sampling the rankings are not uniform i.e., not from 0 to num_toks. A litte preprocessing to make sure that we have uniform rankings for all labels.\n\ngrouped = df_tiny.groupby('label', group_keys=False)\n\n\ndef sort_rerank(df, column='rank'):\n    df = df.sort_values(by=column)\n    df['rank'] = range(len(df))\n    return df\n\n\ndf_tiny = grouped.apply(sort_rerank)\ndict_grouped = dict(list(df_tiny.groupby('label')))\n# checking a random label has ranks 0 thru `num_toks`\na_lbl = random.choice(list(dict_grouped.keys()))\ntest_eq(range(num_toks), dict_grouped[a_lbl]['rank'].values)\n\n\ndict_grouped[a_lbl].head()\n\n\n\n\n\n\n\n\ntoken\nlabel\nrank\n\n\n\n\n5660\n9679\n3455\n0\n\n\n8364\n13976\n3455\n1\n\n\n4620\n6801\n3455\n2\n\n\n772\n1788\n3455\n3\n\n\n2748\n4458\n3455\n4\n\n\n\n\n\n\n\nUsing Pandas groupby to add quantized relevance scores to each token-label pair based on the corresponding ranks:\n\ngrouped = df_tiny.groupby('label')\n\n\n# dict_grouped = dict(list(grouped))\n# _tmp = dict_grouped[16].copy()\n# _tmp.head()\n\n\ndef cut(df, qnts, column='rank'):\n    num = df.to_numpy()\n    bins = np.quantile(num[:, -1], qnts)\n    num[:, -1] = len(bins) - np.digitize(num[:, -1], bins)\n    # bins = np.quantile(df['rank'], qnts)\n    # df[column] = len(bins) - np.digitize(df['rank'], bins)\n    # df[column] = pd.qcut(df[column], qnts, labels=labels)\n    return num\n\n\nqnts = np.concatenate([array([0]), np.geomspace(1e-2, 1, 10)])\nscored = grouped.apply(cut, qnts)\n\n11.9 ms ± 279 µs per loop (mean ± std. dev. of 15 runs, 50 loops each)\n\n\nPandas groupby was just to ellucidate how we do the scoring. It ain’t all that good when dealing with big datasets. So in reality we are going to use tensorized implemnetation. Follow along:\n\npdl = PreLoadTrans(df_tiny, device=torch.device('cpu'))\n\nIf interested please read sourcecode of [PreLoadTrans.quantized_score](https://debjyotiSRoy.github.io/xcube/l2r.data.load.html#preloadtrans.quantized_score):\n\n# %%timeit -n 50 -r 15\n\nscored_toks = pdl.quantized_score()\n\nCPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\nWall time: 3.34 µs\n\n\n/home/deb/xcube/xcube/l2r/data/load.py:56: UserWarning: torch.searchsorted(): input value tensor is non-contiguous, this will lower the performance due to extra data copy when converting non-contiguous tensor to contiguous, please use contiguous input value tensor if possible. This message will only appear once per program. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525541990/work/aten/src/ATen/native/BucketizationUtils.h:33.)\n  relv_scores = bins.shape[0] - torch.searchsorted(bins.T, data[:, :, -1], right=False) # shape (8922, 57352)\n\n\n\ntest_eqs(scored_toks.shape, \n         (df_tiny.label.nunique(), df_tiny.token.nunique(), 4), \n         (pdl.num_lbs, pdl.num_toks, 4))\n\nSave if you want to! BTW untar_xxx has got the one for the full dataset:\n\nL(source.glob(\"**/*scored*.pth\"))\n\n(#1) [Path('/home/deb/.xcube/data/mimic3_l2r/scored_tokens.pth')]\n\n\nCreate training and validation split:\nRemember: In scored_toks dim 0: labels, dim 1: 4 tuple (token, label, rank, score). Below is an example:\n\ntok, lbl, rank, score = L(scored_toks[97, 32], use_list=True).map(Tensor.item)\nic(tok, lbl, rank, score);\n\nic| tok: 41514.0, lbl: 8124.0, rank: 234.0, score: 4.0\n\n\n\ndf_tiny[(df_tiny.token == tok)  & (df_tiny.label == lbl)]\n\n\n\n\n\n\n\n\ntoken\nlabel\nrank\n\n\n\n\n23393\n41514\n8124\n234\n\n\n\n\n\n\n\nRemember: For each label the tokens are ranked 0 through num_toks\n\nranks = scored_toks[:, :, 2].unique(dim=1).sort(-1).values\nranks_shouldbe = torch.arange(scored_toks.shape[1], dtype=torch.float).expand(scored_toks.shape[0], -1)\ntest_eq(ranks, ranks_shouldbe)\n\nRemember: For each label quantized_score scores the tokens on a log scale based on their ranks. The score scale is 1-101: 101 being the highest score (assigned to most relevant token), and 1 is the lowest score (assigned to least relevant tokens).\n\nscores = scored_toks[:, :, -1].unique(dim=1).sort(-1).values\nscores[0]\n\ntensor([  1.,   1.,   1.,   1.,   2.,   2.,   2.,   2.,   2.,   2.,   3.,   3.,\n          3.,   3.,   4.,   4.,   4.,   4.,   4.,   4.,   5.,   5.,   5.,   6.,\n          6.,   6.,   6.,   6.,   6.,   7.,   7.,   7.,   7.,   7.,   8.,   8.,\n          8.,   8.,   8.,   8.,   8.,   8.,   9.,   9.,   9.,   9.,   9.,   9.,\n         10.,  10.,  10.,  10.,  11.,  11.,  11.,  11.,  11.,  11.,  12.,  12.,\n         12.,  12.,  12.,  13.,  13.,  13.,  13.,  13.,  13.,  14.,  14.,  14.,\n         14.,  15.,  15.,  15.,  16.,  16.,  16.,  17.,  17.,  17.,  17.,  17.,\n         18.,  18.,  18.,  19.,  19.,  19.,  19.,  20.,  20.,  20.,  20.,  21.,\n         21.,  21.,  21.,  22.,  22.,  22.,  22.,  23.,  23.,  23.,  23.,  24.,\n         24.,  24.,  25.,  25.,  25.,  26.,  26.,  27.,  27.,  27.,  28.,  28.,\n         29.,  29.,  30.,  30.,  31.,  31.,  32.,  32.,  33.,  34.,  34.,  35.,\n         36.,  37.,  38.,  39.,  40.,  42.,  43.,  45.,  48.,  51.,  55.,  63.,\n        101.])\n\n\n\nscored_toks, binned_toks, probs, is_valid, bin_size, bin_bds = pdl.train_val_split()\n\nCPU times: user 90.4 ms, sys: 3.56 ms, total: 94 ms\nWall time: 13.8 ms\n\n\n\nval_sl = pdl.pad_split()\ntest_eq(is_valid.sum(dim=-1).unique().item(), val_sl)\nprint(f\"{val_sl=}\")\n\nval_sl=16\nCPU times: user 353 ms, sys: 0 ns, total: 353 ms\nWall time: 48.4 ms\n\n\nTaking a look at the train/valid split for some labels (just to make sure we ticked all boxes!):\n\ndf1 = pd.DataFrame(scored_toks[89], columns=['token', 'label', 'rank', 'score']).sort_values(by='score', ascending=False)\ndf1.head()\n\n\n\n\n\n\n\n\ntoken\nlabel\nrank\nscore\n\n\n\n\n226\n28488.0\n6820.0\n0.0\n101.0\n\n\n225\n2274.0\n6820.0\n1.0\n63.0\n\n\n224\n50503.0\n6820.0\n2.0\n55.0\n\n\n223\n56935.0\n6820.0\n3.0\n51.0\n\n\n222\n20945.0\n6820.0\n4.0\n48.0\n\n\n\n\n\n\n\n\nname = partial(namestr, namespace=globals())\nrow_vals = apply(torch.Tensor.size, (scored_toks, binned_toks, probs, is_valid, bin_size, bin_bds))\npd.DataFrame(index = list(itertools.chain.from_iterable(apply(name, [scored_toks, binned_toks, probs, is_valid, bin_size, bin_bds]))), columns=['shape'], data={'shape': row_vals})\n\n\n\n\n\n\n\n\nshape\n\n\n\n\nscored_toks\n(104, 328, 4)\n\n\nbinned_toks\n(104, 328)\n\n\nprobs\n(104, 328)\n\n\nis_valid\n(104, 328)\n\n\nbin_size\n(11,)\n\n\nbin_bds\n(8, 2)\n\n\n\n\n\n\n\nLowest numbered bin contains irrelevant tokens for a label, and the highest numbered bin contains most relevant tokens:\n\ndf2 = pd.DataFrame({'bin #': range(len(bin_size)), \n                    #'bin_bds': list(bin_bds.numpy()), \n                    'bin_size': bin_size})\ndf2.head()\n\n\n\n\n\n\n\n\nbin #\nbin_size\n\n\n\n\n0\n0\n30\n\n\n1\n1\n180\n\n\n2\n2\n75\n\n\n3\n3\n27\n\n\n4\n4\n10\n\n\n\n\n\n\n\n\ndf_toks = pd.read_feather(source/'mimic3-9k_tok.ft')\ndf_lbs = pd.read_feather(source/'mimic3-9k_lbl.ft')\n\na_lbl = np.random.choice(pdl.num_lbs)\ndf_lbs.iloc[[a_lbl]]\n# df_lbs.loc[[a_lbl]]\n\n\n\n\n\n\n\n\nlbl\nlbl_val\n\n\n\n\n52\n52\n018.95\n\n\n\n\n\n\n\n\ndf3 = pd.DataFrame({'token': scored_toks[a_lbl, :, 0] ,'score': scored_toks[a_lbl, :, -1], 'probs': probs[a_lbl], \n                    'binned_toks': binned_toks[a_lbl], \n                    #'bds': list(bin_bds[binned_toks[a_lbl]].numpy()), \n                    'size': bin_size[binned_toks[a_lbl]], \n                    'is_valid': is_valid[a_lbl]})\ndf3 = df_toks.merge(df3, on='token')\ndf3.sort_values(by='score', ascending=False).head(20)\n\n\n\n\n\n\n\n\ntoken\ntok_val\nscore\nprobs\nbinned_toks\nsize\nis_valid\n\n\n\n\n30\n4924\ndistance\n101.0\n0.333333\n10\n1\n0.0\n\n\n25\n4056\ndefined\n63.0\n0.333333\n6\n1\n1.0\n\n\n10\n2100\nwnl\n55.0\n0.166667\n5\n4\n0.0\n\n\n2\n436\nchange\n51.0\n0.166667\n5\n4\n0.0\n\n\n3\n591\nmotion\n48.0\n0.166667\n5\n4\n0.0\n\n\n5\n1327\nresidual\n45.0\n0.166667\n5\n4\n0.0\n\n\n0\n22\npatient\n43.0\n0.066667\n4\n10\n0.0\n\n\n7\n1788\nthursday\n42.0\n0.066667\n4\n10\n0.0\n\n\n6\n1699\nclopidogrel\n40.0\n0.066667\n4\n10\n0.0\n\n\n9\n2021\nsaturations\n39.0\n0.066667\n4\n10\n0.0\n\n\n8\n1944\n4l\n38.0\n0.066667\n4\n10\n0.0\n\n\n11\n2265\nhemodynamics\n37.0\n0.066667\n4\n10\n0.0\n\n\n13\n2467\naccording\n36.0\n0.066667\n4\n10\n0.0\n\n\n12\n2274\nmedial\n35.0\n0.066667\n4\n10\n0.0\n\n\n14\n2998\nsynagis\n34.0\n0.066667\n4\n10\n1.0\n\n\n15\n3159\nfilm\n34.0\n0.066667\n4\n10\n0.0\n\n\n20\n3748\ndispo\n33.0\n0.024691\n3\n27\n0.0\n\n\n16\n3401\ntwenty\n32.0\n0.024691\n3\n27\n0.0\n\n\n18\n3575\nattached\n32.0\n0.024691\n3\n27\n0.0\n\n\n27\n4598\nstopping\n31.0\n0.024691\n3\n27\n0.0\n\n\n\n\n\n\n\n\ntest_eqs(is_valid[a_lbl].sum(), df3['is_valid'].sum(), pdl.val_sl)\n\n\ndf3[df3['is_valid'] == 1].sort_values(by='score', ascending=False)#.groupby('binned_toks').size()\n\n\n\n\n\n\n\n\ntoken\ntok_val\nscore\nprobs\nbinned_toks\nsize\nis_valid\n\n\n\n\n25\n4056\ndefined\n63.0\n0.333333\n6\n1\n1.0\n\n\n14\n2998\nsynagis\n34.0\n0.066667\n4\n10\n1.0\n\n\n29\n4750\nccy\n29.0\n0.024691\n3\n27\n1.0\n\n\n17\n3469\nbrace\n28.0\n0.024691\n3\n27\n1.0\n\n\n28\n4608\npericarditis\n25.0\n0.024691\n3\n27\n1.0\n\n\n170\n32817\ncadmium\n19.0\n0.008889\n2\n75\n1.0\n\n\n195\n36874\ndiscrepant\n18.0\n0.008889\n2\n75\n1.0\n\n\n47\n8157\nfundi\n17.0\n0.008889\n2\n75\n1.0\n\n\n272\n48514\nvorinostat\n13.0\n0.008889\n2\n75\n1.0\n\n\n306\n54261\nentroclysis\n13.0\n0.008889\n2\n75\n1.0\n\n\n114\n21311\nicbg\n8.0\n0.003704\n1\n180\n1.0\n\n\n183\n35391\nbender\n5.0\n0.003704\n1\n180\n1.0\n\n\n216\n39783\nsusbsequently\n4.0\n0.003704\n1\n180\n1.0\n\n\n261\n46217\ncuurently\n3.0\n0.003704\n1\n180\n1.0\n\n\n302\n53619\ntransfred\n1.0\n0.022222\n0\n30\n1.0\n\n\n322\n56325\npsuedoanuerysm\n1.0\n0.022222\n0\n30\n1.0\n\n\n\n\n\n\n\n\ntop_lens = pdl.count_topbins()\ntest_eq(top_lens.shape, [pdl.num_lbs])\nprint(f\"For {torch.where(top_lens &gt;= 1)[0].numel()} labels out of total {pdl.num_lbs}, in the validation set we have at least one top 2 bin\")\n\nFor 56 labels out of total 104, in the validation set we have at least one top 2 bin\n\n\nPrepare the train/val dataset:\n\ntrn_dset, val_dset = pdl.datasets()\n\n\ntest_eq(val_dset.shape, (scored_toks.shape[0], val_sl, scored_toks.shape[2]))\ntest_eq(trn_dset.shape, scored_toks.shape) \nic(trn_dset.shape, val_dset.shape);\n\nic| trn_dset.shape: torch.Size([104, 328, 4])\n    val_dset.shape: torch.Size([104, 16, 4])\n\n\nAgain, untar_xxx has got the trn/val split for the full dataset:\n\nL(source.glob(\"**/*split*.pkl\"))\n\n(#1) [Path('/home/deb/.xcube/data/mimic3_l2r/trn_val_split.pkl')]\n\n\nIf you want to load the splits for the full dataset:\n\ntrn_dset, val_dset = torch.load(source/'trn_val_split.pkl')\nic(trn_dset.shape, val_dset.shape);\n\nic| trn_dset.shape: torch.Size([8922, 57352, 4])\n    val_dset.shape: torch.Size([8922, 32, 4])\n\n\nNow we are ready to create the train/valid DataLoaders:\nImplementation note: We have written the training dataloader which we call L2RDataLoader. It ofcourse inherits from fastai’s incredibly hackable DataLoader class. In a little more technical terms, another way to say this is that L2RDataLoader provides different implementation of the callbacks before_iter and create_batches. However for the validation dataloader we directly use fastai’s DataLoader. Lastly, we store the training and validation dataloder objects using fastai’s DataLoaders class.\n\nbs_full = 32\nbs_tiny = 8\nsl = 64\nlbs_chunks_full = 4\nlbs_chunks_tiny = 32\ntrn_dl = L2RDataLoader(dataset=trn_dset, sl=sl, bs=bs_tiny, lbs_chunks=lbs_chunks_tiny, shuffle=False, after_batch=partial(to_device, device=default_device(use=True)), num_workers=0)\n\nDon’t forget to check the length\n\nlen(trn_dl)\n\n24\n\n\n\nic(trn_dl.num_workers, trn_dl.fake_l.num_workers);\n\nic| trn_dl.num_workers: 1, trn_dl.fake_l.num_workers: 0\n\n\n\nxb = trn_dl.one_batch()\nic(xb.shape, xb.device);\n\nic| xb.shape: torch.Size([8, 4, 64, 4])\n    xb.device: device(type='cuda', index=0)\n\n\nA fake rundown of the training loop to make sure the training dataloader got created alright:\n\nfor xb in progress_bar(trn_dl):\n    time.sleep(0.01)\n\n\n\n\n\n\n    \n      \n      100.00% [24/24 00:00&lt;00:00]\n    \n    \n\n\nCPU times: user 1.4 s, sys: 12.8 ms, total: 1.41 s\nWall time: 298 ms\n\n\n\nfrom fastai.data.load import DataLoader\nfrom fastai.data.core import DataLoaders\n\n\nval_dset = val_dset.unsqueeze(0)\nval_dl = DataLoader(val_dset, bs=1, shuffle=False, after_batch=partial(to_device, device=default_device()), num_workers=0)\nic(val_dl.num_workers, val_dl.fake_l.num_workers);\n\nic| val_dl.num_workers: 1, val_dl.fake_l.num_workers: 0\n\n\n\nxb = val_dl.one_batch()\nic(xb.shape, xb.device);\n\nic| xb.shape: torch.Size([1, 104, 16, 4])\n    xb.device: device(type='cuda', index=0)\n\n\nA fake rundown of the validation set to make sure the validation dataloader got created alright:\n\nfor xb in progress_bar(val_dl):\n    time.sleep(0.01)\n\n\n\n\n\n\n    \n      \n      100.00% [1/1 00:00&lt;00:00]\n    \n    \n\n\nCPU times: user 6.07 ms, sys: 0 ns, total: 6.07 ms\nWall time: 15.8 ms\n\n\nBunching together the training and validation dataloaders:\n\ndls = DataLoaders(trn_dl, val_dl)"
  },
  {
    "objectID": "tutorial.train_l2r.html#training",
    "href": "tutorial.train_l2r.html#training",
    "title": "L2R Training",
    "section": "Training",
    "text": "Training\n…yes, finally!\n\nKeeping records:\n\nm = ['lin', 'nn']\nalgos = ['ranknet', 'lambdarank']\nidx = pd.Index(['tiny', 'full'], name='dataset')\ncols = pd.MultiIndex.from_product([m, algos], names = ['model', 'algo'])\n\ndf = pd.DataFrame(columns=cols, index=idx)\ndf[:] = 'TBD'\n\ndf.loc['tiny']['nn']['ranknet'] = \"{'grad_func': functools.partial(&lt;function rank_loss3 at 0x7f6f87e800d0&gt;, gain_fn='exp', k=6), 'opt_func': functools.partial(&lt;function RMSProp at 0x7f6f87e31870&gt;, mom=0.9, wd=0.2), 'opt': None, 'lr': 0.001, 'loss_func': &lt;function loss_fn2 at 0x7f6f7cdb1990&gt;, 'num_factors': 200, 'n_act': 100, 'num_lbs': 105, 'num_toks': 329, 'seed': 877979, 'epochs': 15, 'best': 75.66}\"\ndf.loc['tiny']['lin']['ranknet'] = \"{'grad_func': functools.partial(&lt;function rank_loss3 at 0x7f6f87e800d0&gt;, gain_fn='exp', k=6), 'opt_func': functools.partial(&lt;function SGD at 0x7f6f87e31750&gt;, mom=0.9), 'opt': None, 'lr': 0.0001, 'loss_func': &lt;function loss_fn2 at 0x7f6f73f748b0&gt;, 'num_factors': 200, 'num_lbs': 105, 'num_toks': 329, 'seed': 877979, 'epochs': 15, 'best': 75}\"\ndf.loc['tiny']['nn']['lambdarank'] = \"{'grad_func': functools.partial(&lt;function rank_loss3 at 0x7fdc0ed6ad40&gt;, gain_fn='exp', k=15, lambrank=True), 'opt_func': functools.partial(&lt;function RMSProp at 0x7fdc0ed28280&gt;, mom=0.9, wd=0.0), 'opt': &lt;fastai.optimizer.Optimizer object at 0x7fdc0ede79a0&gt;, 'lr': 0.001, 'loss_func': &lt;function loss_fn2 at 0x7fdc0ed6ae60&gt;, 'lr_max': 0.01445439737290144, 'num_factors': 200, 'n_act': 100, 'num_lbs': 105, 'num_toks': 329, 'seed': 1, 'epochs': 15, 'best': 'ndcg_at_6 = 39.05', 'model': 'L2R_NN(\\\\n  (token_factors): Embedding(329, 200, padding_idx=328)\\\\n  (label_factors): Embedding(105, 200, padding_idx=104)\\\\n  (layers): Sequential(\\\\n    (0): Linear(in_features=200, out_features=100, bias=True)\\\\n    (1): ReLU()\\\\n    (2): Linear(in_features=100, out_features=1, bias=True)\\\\n    (3): Dropout(p=0.2, inplace=False)\\\\n  )\\\\n)\\\\n self.n_act = 100, self.dp = 0.2'}\"\ndf.loc['tiny']['lin']['lambdarank'] = \"{'grad_func': functools.partial(&lt;function rank_loss3 at 0x7f4c7dd42c20&gt;, gain_fn='exp', k=15, lambrank=True), 'opt_func': functools.partial(&lt;function RMSProp at 0x7f4c7dd04160&gt;, mom=0.9, wd=0.0), 'opt': &lt;fastai.optimizer.Optimizer object at 0x7f4c5c1da410&gt;, 'lr': 0.007, 'loss_func': &lt;function loss_fn2 at 0x7f4c74b9add0&gt;, 'num_factors': 200, 'n_act': None, 'num_lbs': 105, 'num_toks': 329, 'seed': 1, 'epochs': 15, 'best': 52.21}\"\n\ndf.loc['full']['nn']['ranknet'] = 'TBD'\ndf.loc['full']['lin']['ranknet'] = {'lr': 1e-5, 'opt': 'partial(SGD, mom=0.9, wd=0.0)', 'best': 63.73, 'epochs': 3, 'seed': 1, 'gain': 'cubic', 'factors': 100}\ndf.loc['full']['nn']['lambdarank'] = 'TBD'\ndf.loc['full']['lin']['lambdarank'] = {'lr': [7e-4, 7e-4, 7e-4], 'opt': 'partial(RMSProp, mom=0.9, wd=0.0)', 'best': 12.85, 'epochs': [4, 2, 4, 4], 'seed': 1, 'gain': 'exp', 'factors': 200}\n\n\ndf\n\n\n\n\n\n\n\nmodel\nlin\nnn\n\n\nalgo\nranknet\nlambdarank\nranknet\nlambdarank\n\n\ndataset\n\n\n\n\n\n\n\n\ntiny\n{'grad_func': functools.partial(&lt;function rank_loss3 at 0x7f6f87e800d0&gt;, gain_fn='exp', k=6), 'opt_func': functools.partial(&lt;function SGD at 0x7f6f87e31750&gt;, mom=0.9), 'opt': None, 'lr': 0.0001, 'loss_func': &lt;function loss_fn2 at 0x7f6f73f748b0&gt;, 'num_factors': 200, 'num_lbs': 105, 'num_toks': 329, 'seed': 877979, 'epochs': 15, 'best': 75}\n{'grad_func': functools.partial(&lt;function rank_loss3 at 0x7f4c7dd42c20&gt;, gain_fn='exp', k=15, lambrank=True), 'opt_func': functools.partial(&lt;function RMSProp at 0x7f4c7dd04160&gt;, mom=0.9, wd=0.0), 'opt': &lt;fastai.optimizer.Optimizer object at 0x7f4c5c1da410&gt;, 'lr': 0.007, 'loss_func': &lt;function loss_fn2 at 0x7f4c74b9add0&gt;, 'num_factors': 200, 'n_act': None, 'num_lbs': 105, 'num_toks': 329, 'seed': 1, 'epochs': 15, 'best': 52.21}\n{'grad_func': functools.partial(&lt;function rank_loss3 at 0x7f6f87e800d0&gt;, gain_fn='exp', k=6), 'opt_func': functools.partial(&lt;function RMSProp at 0x7f6f87e31870&gt;, mom=0.9, wd=0.2), 'opt': None, 'lr': 0.001, 'loss_func': &lt;function loss_fn2 at 0x7f6f7cdb1990&gt;, 'num_factors': 200, 'n_act': 100, 'num_lbs': 105, 'num_toks': 329, 'seed': 877979, 'epochs': 15, 'best': 75.66}\n{'grad_func': functools.partial(&lt;function rank_loss3 at 0x7fdc0ed6ad40&gt;, gain_fn='exp', k=15, lambrank=True), 'opt_func': functools.partial(&lt;function RMSProp at 0x7fdc0ed28280&gt;, mom=0.9, wd=0.0), 'opt': &lt;fastai.optimizer.Optimizer object at 0x7fdc0ede79a0&gt;, 'lr': 0.001, 'loss_func': &lt;function loss_fn2 at 0x7fdc0ed6ae60&gt;, 'lr_max': 0.01445439737290144, 'num_factors': 200, 'n_act': 100, 'num_lbs': 105, 'num_toks': 329, 'seed': 1, 'epochs': 15, 'best': 'ndcg_at_6 = 39.05', 'model': 'L2R_NN(\\n (token_factors): Embedding(329, 200, padding_idx=328)\\n (label_factors): Embedding(105, 200, paddin...\n\n\nfull\n{'lr': 1e-05, 'opt': 'partial(SGD, mom=0.9, wd=0.0)', 'best': 63.73, 'epochs': 3, 'seed': 1, 'gain': 'cubic', 'factors': 100}\n{'lr': [0.0007, 0.0007, 0.0007], 'opt': 'partial(RMSProp, mom=0.9, wd=0.0)', 'best': 12.85, 'epochs': [4, 2, 4, 4], 'seed': 1, 'gain': 'exp', 'factors': 200}\nTBD\nTBD\n\n\n\n\n\n\n\n\n\nGet the DataLoaders:\n\ntmp = Path.cwd()/'tmp'\ntmp.mkdir(exist_ok=True)\nlist_files(str(tmp))\n\ntmp/\n    mimic3-9k_dls_clas_tiny.pkl\n    nn_lambdarank_tiny.pth\n    mimic3-9k_dls_clas_tiny_r.pkl\n    dls_full.pkl\n    dls_tiny.pkl\n    lin_lambdarank_full.pth\n    lin_lambdarank_tiny.pth\n    .ipynb_checkpoints/\n    models/\n        mimic3-9k_lm_finetuned_r.pth\n        mimic3-9k_clas_full.pth\n        mimic3-9k_clas_tiny_r.pth\n        mimic3-9k_lm_finetuned.pth\n        mimic3-9k_clas_tiny_vocab.pkl\n        mimic3-9k_clas_tiny_r_vocab.pkl\n        mimic3-9k_clas_tiny.pth\n        mimic3-9k_clas_full_vocab.pkl\n\n\n\nset_seed(1, True)\n\nSetting the fname capturing which model (neural net vs linear) we want to run, which algorithm (ranknet vs lambdarank) and on which dataset (tiny vs full). This fname is then used to automaticall grab the appropriate dataloder, make the model and set relevant learner parameters.\n\nfname = 'lin_lambdarank_tiny'\nmonitor = 'ndcg_at_6' if 'lambda' in fname else 'acc'\ns = fname.split('_')\nprint(f'We will run a {s[0]} model using the {s[1]} algorithm on the {s[2]} dataset. And our metric of interest(moi) is {monitor}.')\n\nWe will run a lin model using the lambdarank algorithm on the tiny dataset. And our metric of interest(moi) is ndcg_at_6.\n\n\n\n\n\nCPU times: user 11.7 ms, sys: 8.62 s, total: 8.63 s\nWall time: 15.3 s\n\n\n\n\nMake the Model:\nBased on the dataset:\n\nDatasizes = namedtuple(\"Datasizes\", ('num_lbs', 'num_toks', 'num_factors'))\nsizes = Datasizes(*dls.dataset.shape[:-1], 200) # or pdl.num_lbs, pdl.num_toks, 200\nsizes\n\nDatasizes(num_lbs=104, num_toks=328, num_factors=200)\n\n\n\nmodel = (L2R_NN(*sizes, layers=[100], embed_p=0.2, ps=[0.1], bn_final=False, y_range=None) if 'nn' in fname else L2R_DotProductBias(*sizes,y_range=None)).to(default_device())\n\nCreate the Learner and train:\n\nfrom fastai.optimizer import *\n\n\ndef grab_learner_params(fname):\n    \"Get relevant `learner` params depending on the `fname`\"\n    \n    nn, lambrank, tiny =  [sp == n for sp, n in zip(fname.split('_'), ['nn', 'lambdarank', 'tiny'])]\n    # create a dictionary that maps binary conditions to tuple (nn, lambdarank, tiny)\n    conditions = {\n        (True, True, True):  dict(lr = 1e-3, lambrank = lambrank, opt_func = partial(RMSProp, mom=0.9, wd=0.0)),   # nn_lambdarank_tiny\n        (True, True, False): dict(lr = 1e-2, lambrank = lambrank, opt_func = partial(Adam, mom=0.9, wd=0.1)),   # nn_lambdarank_full\n        (True, False, True):  dict(lr = 1e-2, lambrank = lambrank, opt_func = partial(RMSProp, mom=0.9, wd=0.2)),  # nn_ranknet_tiny\n        (True, False, False): dict(lr = None, lambrank = lambrank, opt_func = None),  # nn_ranknet_full\n        (False, True, True): dict(lr = 7e-3, lambrank = lambrank, opt_func = partial(RMSProp, mom=0.9, wd=0.0)),   # lin_lambdarank_tiny\n        (False, True, False): dict(lr = 7e-3, lambrank = lambrank, opt_func = partial(RMSProp, mom=0.9, wd=0.0)),  # lin_lambdarank_full\n        (False, False, True): dict(lr = 1e-4, lambrank = lambrank, opt_func = None),  # lin_ranknet_tiny\n        (False, False, False): dict(lr = None, lambrank = lambrank, opt_func = None), # lin_ranknet_full\n    }\n    learner_params = conditions.get((nn, lambrank, tiny), (True, True, True))\n    default_cbs = [TrainEval(), TrackResults(train_metrics=False, beta=0.98), ProgressBarCallback(), Monitor(), SaveCallBack(fname, monitor=monitor)]\n    grad_fn = partial(rank_loss3, gain_fn='exp', k=15)\n    learner_params = {**learner_params, **{'cbs':default_cbs, 'grad_fn':grad_fn}}\n    return learner_params\n\n\nlearner_params = grab_learner_params(fname)\nlearner_params\n\n{'lr': 0.007,\n 'lambrank': True,\n 'opt_func': functools.partial(&lt;function RMSProp&gt;, mom=0.9, wd=0.0),\n 'cbs': [TrainEval, TrackResults, ProgressBarCallback, Monitor, SaveCallBack],\n 'grad_fn': functools.partial(&lt;function rank_loss3&gt;, gain_fn='exp', k=15)}\n\n\n\nlearner = get_learner(model, dls, **learner_params)\n\n\nlearner.path = tmp\n\nLet’s record some useful hyperparameters in a record dict which we can store in the dataframe in the record keeping section:\n\nlearner_attrs = ['grad_func', 'opt_func', 'opt', 'lr', 'loss_func', 'lr_max']\nmodel_attrs = ['num_factors', 'n_act', 'num_lbs', 'num_toks']\nrecord = dict(zip(learner_attrs + model_attrs, getattrs(learner, *learner_attrs) + getattrs(learner.model, *model_attrs)))\nrecord['seed'] = torch.initial_seed()\nrecord['epochs'] = 15\nrecord['best'] = f'{monitor} = 37.97'\nrecord['model'] = str(learner.model)\nstr(record)\n\n\"{'grad_func': functools.partial(&lt;function rank_loss3&gt;, gain_fn='exp', k=15, lambrank=True), 'opt_func': functools.partial(&lt;function RMSProp&gt;, mom=0.9, wd=0.0), 'opt': None, 'lr': 0.007, 'loss_func': &lt;function loss_fn2&gt;, 'lr_max': None, 'num_factors': 200, 'n_act': None, 'num_lbs': 105, 'num_toks': 329, 'seed': 1, 'epochs': 15, 'best': 'ndcg_at_6 = 37.97', 'model': 'L2R_DotProductBias(\\\\n  (token_factors): Embedding(329, 200, padding_idx=328)\\\\n  (token_bias): Embedding(329, 1, padding_idx=328)\\\\n  (label_factors): Embedding(105, 200, padding_idx=104)\\\\n  (label_bias): Embedding(105, 1, padding_idx=104)\\\\n)'}\"\n\n\nFinding learning rate:\n\nfrom fastai.callback.schedule import valley, slide, steep\n\n\nlearner.xrl_find(num_it=300, suggest_funcs=(valley, slide, steep))\n\nSmoothing ndcg_at_6\n0 True 1.2428 0.732 0.6955 0.6977\n0 False NA NA NA NA\n1 True 1.2239 0.7202 0.6793 0.6992\n1 False NA NA NA NA\n\n\n\n\n\n\n\n\n\nSuggestedLRs(valley=0.010964781977236271, slide=0.05248074606060982, steep=0.07356422394514084)\n\n\n\n\n\n\n# learner.fit_one_cycle(1, lr_max=0.014454)\n# learner.fit_one_cycle(15, lr_max=0.014454)\n# learner.fit_one_cycle(1, lr_max=0.0611)\n# learner.fit_one_cycle(3, lr_max=0.0611)\n# learner.fit_one_cycle(1, lr_max=0.01239)\n# learner.fit_one_cycle(3, lr_max=0.01239)\nlearner.fit_one_cycle(1, lr_max=0.010964)\nlearner.fit_one_cycle(3, lr_max=0.010964)\n\n\n\n\n\n\n\n\n0 True 1.3046 NA NA NA\n0 False 0.9848 0.774 0.7596 0.7612\n0 True 1.2438 NA NA NA\n0 False 0.9702 0.7635 0.7474 0.7637\n1 True 1.2317 NA NA NA\n1 False 0.9498 0.7723 0.7571 0.7631\n2 True 1.2137 NA NA NA\n2 False 0.9418 0.7711 0.756 0.764\n\n\n\n\n\n\n\n\n\n\nlearner.track_results.plot_sched()\n\n\n\n\n\n# len(learner.cbs[1].grads_full['token_factors.weight'])\n# learner.cbs\n# learner.track_results\n# learner.opt.hypers[-1]\n\n\nlearner = learner.load(fname, device=default_device())\n\n\nlearner.validate()\n\n\n\n\n\n\n\n\n0 False 1.0591 0.7812 0.7686 0.76\n\n\n\nlearner.cbs[-1].best = 0.7686\n\n\n# emb_szs = get_emb_sz(dls.train_ds, {})\n\n\n\nPlots\nPlotting losses and metrics:\n\nfig, axes = plt.subplots(2, 2, figsize=(15,8))\nloss = L(loss_logger).map(torch.Tensor.item)\nval_loss = L(metric_logger).itemgot(0)\nval_acc = L(metric_logger).itemgot(-1)\nval_ndcg = L(metric_logger).itemgot(2)\n\n# axes[0,0].scatter(range(len(loss)), loss)\naxes[0,0].plot(range(len(loss)), loss)\naxes[0,0].set_xlabel('batches*epochs')\naxes[0,0].set_ylabel('train loss')\n\naxes[0,1].plot(val_loss)\naxes[0,1].set_xlabel('epochs')\naxes[0,1].set_ylabel('val loss')\n\naxes[1, 0].plot(val_acc)\naxes[1,0].set_xlabel('epochs')\naxes[1,0].set_ylabel('val accuracy')\n\naxes[1,1].plot(val_ndcg)\naxes[1,1].set_xlabel('epochs')\naxes[1,1].set_ylabel('val ndcg@6 (candidate 16)')\n\nplt.show()\n\nPlotting Statistics of the Model Parameters\n\nfig, axes = plt.subplots(2,2, figsize=(15,8), sharex=True)\nfor (k,v), ax in zip(grad_logger.items(), axes.flatten()):\n    mean_grads = L(v).map(compose(torch.Tensor.square, torch.Tensor.mean, torch.Tensor.sqrt, torch.Tensor.item))\n    # sparsity = L(v).map(sparsity)\n    ax.plot(mean_grads, color='r', label='mean')\n    ax.set_ylabel(k)\n    # ax_a = ax.twinx()\n    # ax_a.plot(sparsity, color='b', label='sparsity')\n    ax.legend(loc='best')\n    # ax_a.legend(loc='best')\nfig.suptitle('RMS of the Gradients of Model Parameters')\nplt.show()\n\n\ndef sparsity(t): \n    return 1 - (torch.count_nonzero(t)/t.numel()).item()\n\n\nfig, axes = plt.subplots(2,2, figsize=(15,8), sharex=True)\nfor (k,v), ax in zip(grad_logger.items(), axes.flatten()):\n    sp = L(v).map(sparsity)\n    ax.scatter(range(len(sp)), sp, color='r', label='sparsity')\n    ax.set_ylabel(k)\n    # ax_a = ax.twinx()\n    # ax_a.plot(sparsity, color='b', label='sparsity')\n    ax.legend(loc='best')\n    # ax_a.legend(loc='best')\nfig.suptitle('Sparsity of the Model Parameters')\nplt.show()\n\n\n\nAnalysis to find out what the L2R model is upto:\n\ndataset = to_device(learner.dls.train.dataset)\n\n\n_ndcg_at_k = ndcg_at_k(dataset, learner.model, k=15)\n\nCPU times: user 5.73 s, sys: 0 ns, total: 5.73 s\nWall time: 5.86 s\n\n\n\nic(_ndcg_at_k.shape, _ndcg_at_k.min(), _ndcg_at_k.mean(), _ndcg_at_k.max(), _ndcg_at_k.median(), _ndcg_at_k.std());\n\nic| _ndcg_at_k.shape: torch.Size([1, 8922])\n    _ndcg_at_k.min(): tensor(4.0372e-19, device='cuda:0')\n    _ndcg_at_k.mean(): tensor(0.3494, device='cuda:0')\n    _ndcg_at_k.max(): tensor(0.9934, device='cuda:0')\n    _ndcg_at_k.median(): tensor(0.3283, device='cuda:0')\n    _ndcg_at_k.std(): tensor(0.2672, device='cuda:0')\n\n\n\nqnts = torch.linspace(0, 1, 100)\nplt.plot(qnts, _ndcg_at_k.cpu().quantile(qnts, dim=-1).view(-1));\nplt.xlabel('quantiles')\nplt.ylabel('ndcg@k')\nplt.title('Quantile Plot for ndcg@k of all the labels')\nplt.show()\n\n\n\n\n\nacc = accuracy(dataset, learner.model)\n\nCPU times: user 276 ms, sys: 0 ns, total: 276 ms\nWall time: 286 ms\n\n\n\nic(acc.shape, acc.min(), acc.mean(), acc.max(), acc.median(), acc.std());\n\nic| acc.shape: torch.Size([1, 104])\n    acc.min(): tensor(0.6281, device='cuda:0')\n    acc.mean(): tensor(0.7566, device='cuda:0')\n    acc.max(): tensor(0.7964, device='cuda:0')\n    acc.median(): tensor(0.7626, device='cuda:0')\n    acc.std(): tensor(0.0296, device='cuda:0')\n\n\nLet’s pick some random labels and see the rankings produced by the model:\n\ndf_res, df_ndcg= learner.show_results(k=15)\n\n\ndf_ndcg[df_ndcg.ndcg_at_k &gt;= 0.5]\n\n\n\n\n\n\n\n\nlabels\nndcg_at_k\n\n\n\n\n5\n5644\n0.505686\n\n\n6\n5102\n0.608648\n\n\n8\n1804\n0.594066\n\n\n11\n2355\n0.581916\n\n\n22\n8161\n0.500200\n\n\n28\n2877\n0.557069\n\n\n30\n129\n0.706364\n\n\n33\n1245\n0.733052\n\n\n35\n16\n0.618285\n\n\n38\n1208\n0.518697\n\n\n53\n6305\n0.767246\n\n\n58\n1068\n0.748479\n\n\n89\n1104\n0.534632\n\n\n91\n5173\n0.572957\n\n\n92\n934\n0.650478\n\n\n\n\n\n\n\n\ndf_ndcg.head(10)\n\n\n\n\n\n\n\n\nlabels\nndcg_at_k\n\n\n\n\n0\n8526\n4.967139e-16\n\n\n1\n7962\n4.525589e-15\n\n\n2\n3987\n5.901048e-01\n\n\n3\n6165\n2.050589e-13\n\n\n4\n6168\n3.538292e-10\n\n\n5\n2640\n3.429065e-13\n\n\n6\n862\n1.326269e-14\n\n\n7\n2750\n2.974324e-14\n\n\n8\n7083\n1.927360e-12\n\n\n9\n7382\n3.322057e-13\n\n\n\n\n\n\n\n\ndf_res\n\n\n\n\n\n\n\nlabel\n4565\n6569\n...\n1047\n1349\n\n\nkey2\ntok\nlbl\nrank\nscore\npreds\nmodel_rank\ntok\nlbl\nrank\nscore\n...\nrank\nscore\npreds\nmodel_rank\ntok\nlbl\nrank\nscore\npreds\nmodel_rank\n\n\ntoks\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0\n51577.0\n4565.0\n53188.0\n1.0\n-41.782745\n54696.0\n51577.0\n6569.0\n51079.0\n2.0\n...\n50834.0\n2.0\n-45.150726\n55395.0\n51577.0\n1349.0\n53188.0\n1.0\n-32.015938\n34146.0\n\n\n1\n52360.0\n4565.0\n36926.0\n5.0\n-36.372681\n46839.0\n52360.0\n6569.0\n35079.0\n6.0\n...\n21593.0\n11.0\n-37.832306\n46060.0\n52360.0\n1349.0\n36928.0\n5.0\n-38.207287\n48127.0\n\n\n2\n37101.0\n4565.0\n23703.0\n10.0\n-42.323380\n55161.0\n37101.0\n6569.0\n40328.0\n4.0\n...\n32080.0\n7.0\n-31.006001\n31124.0\n37101.0\n1349.0\n23697.0\n10.0\n-38.562450\n48761.0\n\n\n3\n37705.0\n4565.0\n24090.0\n10.0\n-24.040859\n17313.0\n37705.0\n6569.0\n40714.0\n4.0\n...\n32466.0\n7.0\n-29.984770\n28882.0\n37705.0\n1349.0\n24084.0\n10.0\n-35.929695\n43524.0\n\n\n4\n14257.0\n4565.0\n4641.0\n28.0\n-27.471664\n24536.0\n14257.0\n6569.0\n13676.0\n16.0\n...\n12842.0\n17.0\n-24.753557\n18343.0\n14257.0\n1349.0\n4612.0\n28.0\n-25.678783\n19302.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n57347\n4537.0\n4565.0\n16797.0\n14.0\n-17.453377\n7259.0\n4537.0\n6569.0\n3001.0\n32.0\n...\n3601.0\n30.0\n-15.355591\n2176.0\n4537.0\n1349.0\n16785.0\n14.0\n-18.952923\n8060.0\n\n\n57348\n1622.0\n4565.0\n1927.0\n37.0\n-14.181708\n4227.0\n1622.0\n6569.0\n33860.0\n6.0\n...\n950.0\n45.0\n-9.783297\n112.0\n1622.0\n1349.0\n1878.0\n37.0\n-18.222450\n7188.0\n\n\n57349\n16373.0\n4565.0\n5797.0\n25.0\n-19.083836\n9199.0\n16373.0\n6569.0\n15122.0\n15.0\n...\n16773.0\n14.0\n-17.529833\n5061.0\n16373.0\n1349.0\n5781.0\n25.0\n-29.995731\n29059.0\n\n\n57350\n43863.0\n4565.0\n31410.0\n7.0\n-39.971527\n52866.0\n43863.0\n6569.0\n25107.0\n9.0\n...\n40880.0\n4.0\n-26.384148\n21487.0\n43863.0\n1349.0\n31405.0\n7.0\n-34.167110\n39351.0\n\n\n57351\n11619.0\n4565.0\n5573.0\n26.0\n-26.214912\n21706.0\n11619.0\n6569.0\n14697.0\n15.0\n...\n16236.0\n14.0\n-22.856853\n14855.0\n11619.0\n1349.0\n5553.0\n26.0\n-25.605606\n19152.0\n\n\n\n\n57352 rows × 600 columns\n\n\n\n\ndf_lbl = df_res.loc[:, 934]\ndf_lbl\n\n\n\n\n\n\n\nkey2\ntok\nlbl\nrank\nscore\npreds\nmodel_rank\n\n\ntoks\n\n\n\n\n\n\n\n\n\n\n0\n51577.0\n934.0\n53188.0\n1.0\n-19.667917\n5888.0\n\n\n1\n52360.0\n934.0\n34324.0\n6.0\n-31.594564\n32015.0\n\n\n2\n37101.0\n934.0\n31239.0\n7.0\n-39.322235\n50443.0\n\n\n3\n37705.0\n934.0\n31626.0\n7.0\n-41.059654\n52976.0\n\n\n4\n14257.0\n934.0\n12569.0\n17.0\n-24.408052\n15270.0\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n57347\n4537.0\n934.0\n3247.0\n31.0\n-17.787853\n3021.0\n\n\n57348\n1622.0\n934.0\n9534.0\n20.0\n-17.016525\n2166.0\n\n\n57349\n16373.0\n934.0\n14253.0\n15.0\n-25.219761\n16950.0\n\n\n57350\n43863.0\n934.0\n43040.0\n4.0\n-41.898140\n53898.0\n\n\n57351\n11619.0\n934.0\n13718.0\n16.0\n-31.796059\n32533.0\n\n\n\n\n57352 rows × 6 columns\n\n\n\n\ndf2 = df_lbl.sort_values(by='rank').head(15)\ndf2\n\n\n\n\n\n\n\nkey2\ntok\nlbl\nrank\nscore\npreds\nmodel_rank\n\n\ntoks\n\n\n\n\n\n\n\n\n\n\n38712\n9429.0\n934.0\n0.0\n101.0\n2.399450\n1.0\n\n\n30106\n31007.0\n934.0\n1.0\n100.0\n-5.644907\n17.0\n\n\n50302\n5150.0\n934.0\n2.0\n100.0\n-0.324139\n3.0\n\n\n20514\n42988.0\n934.0\n3.0\n100.0\n-5.003191\n13.0\n\n\n16688\n48423.0\n934.0\n4.0\n100.0\n5.765238\n0.0\n\n\n39796\n21173.0\n934.0\n5.0\n100.0\n-10.666720\n136.0\n\n\n26300\n24115.0\n934.0\n6.0\n99.0\n-7.009228\n26.0\n\n\n29622\n24101.0\n934.0\n7.0\n97.0\n-11.829512\n220.0\n\n\n29563\n52417.0\n934.0\n8.0\n96.0\n-6.944853\n25.0\n\n\n20455\n52706.0\n934.0\n9.0\n95.0\n-4.231724\n11.0\n\n\n40045\n56483.0\n934.0\n10.0\n94.0\n-8.524904\n49.0\n\n\n39812\n12054.0\n934.0\n11.0\n92.0\n-8.043360\n38.0\n\n\n1172\n40229.0\n934.0\n12.0\n92.0\n-13.823061\n541.0\n\n\n13627\n41785.0\n934.0\n13.0\n91.0\n-14.363610\n696.0\n\n\n56275\n47498.0\n934.0\n14.0\n90.0\n-10.826721\n151.0\n\n\n\n\n\n\n\n\n# idcg_at_k = pow(2, df2['score']) * (1 / np.log2(df2['rank']+2)  )\n# idcg_at_k\n\n# df3 = df_lbl.sort_values(by='model_rank').head(10)\n# df3\n\n# dcg_at_k = pow(2, df3['score'])  *(1/np.log2(df3['model_rank']+2))\n# dcg_at_k\n\n# dcg_at_k.sum()/idcg_at_k.sum()"
  },
  {
    "objectID": "tutorial.html",
    "href": "tutorial.html",
    "title": "Tutorials",
    "section": "",
    "text": "! [ -e /content ] && pip install -Uqq xcube # upgrade xcube on colab\n\n\ntraining XML text classifier\nbooting L2R\ntraining L2R"
  },
  {
    "objectID": "data.transforms.html",
    "href": "data.transforms.html",
    "title": "Data Transformation",
    "section": "",
    "text": "source\n\nListToTensor\n\n ListToTensor (enc=None, dec=None, split_idx=None, order=None)\n\nA transform with a __repr__ that shows its attrs\n\nt = ListToTensor()\nx = [1, 2]\ntest_eq(t(x), tensor([1, 2]))\ntest_eq(t.decode(t(x)), [tensor(1),tensor(2)])"
  },
  {
    "objectID": "text.learner.html",
    "href": "text.learner.html",
    "title": "Learner for the XML Text application:",
    "section": "",
    "text": "The most important function of this module is xmltext_classifier_learner. This will help you define a Learner using a pretrained Language Model for the encoder and a pretrained Learning-to-Rank-Model for the decoder. (Tutorial: Coming Soon!). This module is inspired from fastai’s TextLearner based on the paper ULMFit."
  },
  {
    "objectID": "text.learner.html#loading-label-embeddings-from-a-pretrained-colab-model",
    "href": "text.learner.html#loading-label-embeddings-from-a-pretrained-colab-model",
    "title": "Learner for the XML Text application:",
    "section": "Loading label embeddings from a pretrained colab model",
    "text": "Loading label embeddings from a pretrained colab model\n\nsource\n\nmatch_collab\n\n match_collab (old_wgts:dict, collab_vocab:dict, lbs_vocab:list)\n\nConvert the label embedding in old_wgts to go from old_vocab in colab to lbs_vocab\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nold_wgts\ndict\nEmbedding weights of the colab model\n\n\ncollab_vocab\ndict\nVocabulary of token and label used for colab pre-training\n\n\nlbs_vocab\nlist\nCurrent labels vocabulary\n\n\nReturns\ndict\n\n\n\n\n\nwgts = {'u_weight.weight': torch.randn(3,5), \n        'i_weight.weight': torch.randn(4,5),\n        'u_bias.weight'  : torch.randn(3,1),\n        'i_bias.weight'  : torch.randn(4,1)}\ncollab_vocab = {'token': ['#na#', 'sun', 'moon', 'earth', 'mars'],\n                'label': ['#na#', 'a', 'c', 'b']}\nlbs_vocab = ['a', 'b', 'c']\nnew_wgts, missing = match_collab(wgts.copy(), collab_vocab, lbs_vocab)\ntest_eq(missing, 0)\ntest_close(wgts['u_weight.weight'], new_wgts['u_weight.weight'])\ntest_close(wgts['u_bias.weight'], new_wgts['u_bias.weight'])\nwith ExceptionExpected(ex=AssertionError, regex=\"close\"):\n    test_close(wgts['i_weight.weight'][1:], new_wgts['i_weight.weight'])\n    test_close(wgts['i_bias.weight'][1:], new_wgts['i_bias.weight'])\nold_w, new_w = wgts['i_weight.weight'], new_wgts['i_weight.weight']\nold_b, new_b = wgts['i_bias.weight'], new_wgts['i_bias.weight']\nfor (old_k,old_v), (new_k, new_v) in zip(wgts.items(), new_wgts.items()): \n    if old_k.startswith('u'): test_eq(old_v.size(), new_v.size())\n    else: test_ne(old_v.size(), new_v.size());\n    # print(f\"old: {old_k} = {old_v.size()}, new: {new_k} = {new_v.size()}\")\ntest_eq(new_w[0], old_w[1]); test_eq(new_b[0], old_b[1])\ntest_eq(new_w[1], old_w[3]); test_eq(new_b[1], old_b[3])\ntest_eq(new_w[2], old_w[2]); test_eq(new_b[2], old_b[2])\ntest_shuffled(list(old_b[1:].squeeze().numpy()), list(new_b.squeeze().numpy()))\ntest_eq(torch.sort(old_b[1:], dim=0)[0], torch.sort(new_b, dim=0)[0])\ntest_eq(torch.sort(old_w[1:], dim=0)[0], torch.sort(new_w, dim=0)[0])"
  },
  {
    "objectID": "text.learner.html#loading-pretrained-information-gain-as-attention",
    "href": "text.learner.html#loading-pretrained-information-gain-as-attention",
    "title": "Learner for the XML Text application:",
    "section": "Loading Pretrained Information Gain as Attention",
    "text": "Loading Pretrained Information Gain as Attention\n\nfrom xcube.l2r.all import *\n\n\nsource_mimic = untar_xxx(XURLs.MIMIC3)\nxml_vocab = load_pickle(source_mimic/'mimic3-9k_clas_full_vocab.pkl')\nxml_vocab = L(xml_vocab).map(listify)\n\n\nsource_l2r = untar_xxx(XURLs.MIMIC3_L2R)\nboot_path = join_path_file('mimic3-9k_tok_lbl_info', source_l2r, ext='.pkl')\nbias_path = join_path_file('p_L', source_l2r, ext='.pkl')\nl2r_bootstrap = torch.load(boot_path, map_location=default_device())\nbrain_bias = torch.load(bias_path, map_location=default_device())\n\n\n*brain_vocab, brain = mapt(l2r_bootstrap.get, ['toks', 'lbs', 'mutual_info_jaccard'])\nbrain_vocab = L(brain_vocab).map(listify)\ntoks, lbs = brain_vocab\nprint(f\"last two places in brain vocab has {toks[-2:]}\")\n# toks = CategoryMap(toks, sort=False)\nbrain_bias = brain_bias[:, :, 0].squeeze(-1)\nlbs_des = load_pickle(source_mimic/'code_desc.pkl')\nassert isinstance(lbs_des, dict)\ntest_eq(brain.shape, (len(toks), len(lbs))) # last two places has 'xxfake'\ntest_eq(brain_bias.shape, [len(lbs)])\n\nlast two places in brain vocab has ['xxfake', 'xxfake']\n\n\nThe tokens which are there in the xml vocab but not in the brain:\n\nnot_found_in_brain = L(set(xml_vocab[0]).difference(set(brain_vocab[0])))\nnot_found_in_brain\n\n(#20) ['cella','q2day','remiained','luteinizing','promiscuity','sharpio','calcijex','dissension','mhc','theses'...]\n\n\n\ntest_fail(lambda : toks.index('cella'), contains='is not in list')\n\nThe tokens which are in the brain but were not present in the xml vocab:\n\nset(brain_vocab[0]).difference(xml_vocab[0])\n\nset()\n\n\nThankfully, we have info for all the labels in the xml vocab:\n\nassert set(brain_vocab[1]).symmetric_difference(brain_vocab[1]) == set()\n# test_shuffled(xml_vocab[1], mimic_vocab[1])\n\n\ntoks_xml2brain, toks_notfnd = _xml2brain(xml_vocab[0], brain_vocab[0])\n\ntoks_found = set(toks_xml2brain).difference(set(toks_notfnd))\ntest_shuffled(array(xml_vocab[0])[toks_notfnd], not_found_in_brain)\nsome_xml_idxs = np.random.choice(array(L(toks_found)), size=10)\nsome_xml_toks = array(xml_vocab[0])[some_xml_idxs]\ncorres_brain_idxs = L(map(toks_xml2brain.get, some_xml_idxs))\ncorres_brain_toks = array(toks)[corres_brain_idxs]\nassert all_equal(some_xml_toks, corres_brain_toks)\n\n\n\n\n\n\n    \n      \n      100.00% [57376/57376 00:19&lt;00:00]\n    \n    \n\n\n\nlbs_xml2brain, lbs_notfnd = _xml2brain(xml_vocab[1], brain_vocab[1])\n\nlbs_found = set(lbs_xml2brain).difference(set(lbs_notfnd))\nsome_xml_idxs = np.random.choice(array(L(lbs_found)), size=10)\nsome_xml_lbs = array(xml_vocab[1])[some_xml_idxs]\ncorres_brain_idxs = L(map(lbs_xml2brain.get, some_xml_idxs))\ncorres_brain_lbs = array(lbs)[corres_brain_idxs]\nassert all_equal(some_xml_lbs, corres_brain_lbs)\n\n\n\n\n\n\n    \n      \n      100.00% [8922/8922 00:00&lt;00:00]\n    \n    \n\n\n\nsource\n\nbrainsplant\n\n brainsplant (xml_vocab, brain_vocab, brain, brain_bias, device=None)\n\n\nxml_brain, xml_lbsbias, toks_map, lbs_map, toks_xml2brain, lbs_xml2brain = brainsplant(xml_vocab, brain_vocab, brain, brain_bias)\ntest_eq(xml_brain.shape, xml_vocab.map(len))\ntest_eq(xml_brain[toks_notfnd], xml_brain.new_zeros(len(toks_notfnd), len(xml_vocab[1])))\nassert all_equal(array(xml_vocab[0])[toks_map.itemgot(0)], array(brain_vocab[0])[toks_map.itemgot(1)])\nassert all_equal(array(xml_vocab[1])[lbs_map.itemgot(0)], array(brain_vocab[1])[lbs_map.itemgot(1)])\n\n\n\n\n\n\n\n\n\n# tests to ensure `brainsplant` was successful \nlbl = '642.41'\nlbl = '38.93'\nlbl = '51.10'\nlbl = '996.87'\nlbl_idx_from_brn = brain_vocab[1].index(lbl)\ntok_vals_from_brn, top_toks_from_brn= L(brain[:, lbl_idx_from_brn].topk(k=20)).map(Self.cpu())\nlbl_idx_from_xml = xml_vocab[1].index(lbl)\ntok_vals_from_xml, top_toks_from_xml = L(xml_brain[:, lbl_idx_from_xml].topk(k=20)).map(Self.cpu())\ntest_eq(lbs_xml2brain[lbl_idx_from_xml], lbl_idx_from_brn)\ntest_eq(tok_vals_from_brn, tok_vals_from_xml)\ntest_eq(array(brain_vocab[0])[top_toks_from_brn], array(xml_vocab[0])[top_toks_from_xml])\ntest_eq(brain_bias[lbl_idx_from_brn], xml_lbsbias[lbl_idx_from_xml])\nprint(f\"For the lbl {lbl} ({lbs_des.get(lbl)}), the top tokens that needs attention are:\")\nprint('\\n'.join(L(array(xml_vocab[0])[top_toks_from_xml], use_list=True).zipwith(L(tok_vals_from_xml.numpy(), use_list=True)).map(str).map(lambda o: \"+ \"+o)))\n\nFor the lbl 996.87 (Complications of transplanted intestine), the top tokens that needs attention are:\n+ ('consultued', 0.25548762)\n+ ('cip', 0.25548762)\n+ ('parlor', 0.24661502)\n+ ('transplantations', 0.18601614)\n+ ('scaffoid', 0.18601614)\n+ ('epineprine', 0.18601614)\n+ ('culinary', 0.17232327)\n+ ('coordinates', 0.1469037)\n+ ('aminotransferases', 0.12153866)\n+ ('hydronephroureter', 0.12153866)\n+ ('27yom', 0.12153866)\n+ ('27y', 0.103684604)\n+ ('hardward', 0.090407245)\n+ ('leukoreduction', 0.08014185)\n+ ('venting', 0.07831942)\n+ ('secrete', 0.07196123)\n+ ('orthogonal', 0.07196123)\n+ ('naac', 0.06891022)\n+ ('mgso4', 0.0662555)\n+ ('septecemia', 0.065286644)\n\n\n\ntok = 'fibrillation'\ntok = 'colpo'\ntok = 'amiodarone'\ntok = 'flagyl'\ntok = 'nasalilid'\ntok = 'hemetemesis'\ntok = 'restitched'\ntok_idx_from_brn = brain_vocab[0].index(tok)\nlbs_vals_from_brn, top_lbs_from_brn = L(brain[tok_idx_from_brn].topk(k=20)).map(Self.cpu())\ntok_idx_from_xml = xml_vocab[0].index(tok)\ntest_eq(tok_idx_from_brn, toks_xml2brain[tok_idx_from_xml])\nlbs_vals_from_xml, top_lbs_from_xml = L(xml_brain[tok_idx_from_xml].topk(k=20)).map(Self.cpu())\ntest_eq(lbs_vals_from_brn, lbs_vals_from_xml)\ntry: \n    test_eq(array(brain_vocab[1])[top_lbs_from_brn], array(xml_vocab[1])[top_lbs_from_xml])\nexcept AssertionError as e: \n    print(type(e).__name__, \"due to instability in sorting (nothing to worry!)\");\n    test_shuffled(array(brain_vocab[1])[top_lbs_from_brn], array(xml_vocab[1])[top_lbs_from_xml])\nprint('')\nprint(f\"For the token {tok}, the top labels that needs attention are:\")\nprint('\\n'.join(L(mapt(lbs_des.get, array(xml_vocab[1])[top_lbs_from_xml])).zipwith(L(lbs_vals_from_xml.numpy(), use_list=True)).map(str).map(lambda o: \"+ \"+o)))\n\n\nFor the token restitched, the top labels that needs attention are:\n+ ('Other operations on supporting structures of uterus', 0.29102018)\n+ ('Other proctopexy', 0.29102018)\n+ ('Other operations on cul-de-sac', 0.18601614)\n+ (None, 0.07494824)\n+ ('Intervertebral disc disorder with myelopathy, thoracic region', 0.055331517)\n+ ('Excision of scapula, clavicle, and thorax [ribs and sternum] for graft', 0.04382947)\n+ ('Other repair of omentum', 0.028067086)\n+ ('Chronic lymphocytic thyroiditis', 0.01986737)\n+ (None, 0.019236181)\n+ ('Reclosure of postoperative disruption of abdominal wall', 0.016585195)\n+ ('Other disorders of calcium metabolism', 0.009393147)\n+ ('Pain in joint involving pelvic region and thigh', 0.008421187)\n+ ('Exteriorization of small intestine', 0.00817792)\n+ ('Fusion or refusion of 9 or more vertebrae', 0.00762466)\n+ ('Kyphosis (acquired) (postural)', 0.0074228523)\n+ ('Unspecified procedure as the cause of abnormal reaction of patient, or of later complication, without mention of misadventure at time of procedure', 0.0063889036)\n+ ('Application or administration of adhesion barrier substance', 0.00610513)\n+ ('Acute osteomyelitis involving other specified sites', 0.0054434645)\n+ ('Body Mass Index less than 19, adult', 0.004719585)\n+ ('Dorsal and dorsolumbar fusion, anterior technique', 0.0046444684)\n\n\n\nsome_toks = random.sample(toks_map.itemgot(0), 10)\ncounts = [c*6 for c in random.sample(range(10), 10)]\nsome_toks = random.sample(some_toks, 20, counts=counts)\n# Counter(some_toks)\ncors_toks_brn = L(mapt(toks_xml2brain.get, some_toks))\ntest_eq(array(brain_vocab[0])[cors_toks_brn], array(xml_vocab[0])[some_toks])\nprint(\"some tokens (with repetitions):\\n\",'\\n'.join(['-'+xml_vocab[0][t]for t in some_toks]))\n\nsome tokens (with repetitions):\n -disorientated\n-disorientated\n-dmh\n-ibp\n-literacy\n-abruptly\n-faxed\n-delsym\n-literacy\n-delsym\n-literacy\n-ibp\n-literacy\n-delsym\n-abruptly\n-caox3\n-caox3\n-caox3\n-caox3\n-literacy\n\n\n\nattn = xml_brain[some_toks]\ntest_eq(attn.shape, (len(some_toks), xml_brain.shape[1]))\n# semantics of attn\n# for each token we can compute the attention each label deserves by pulling out all the columns for a label\nfor t, a in zip(some_toks,attn):\n    test_eq(xml_brain[t], a)\n# for each label we can compute the attention those tokens deserve by pulling out all rows for a label\nfor lbl in range(xml_brain.shape[1]):\n    test_eq(xml_brain[:, lbl][some_toks], attn[:, lbl])\n\n\npd.DataFrame([(xml_vocab[0][t], l:=xml_vocab[1][lbl_idx], val.item(), lbs_des.get(l, 'NF')) for t,lbl_idx,val in zip(some_toks,attn.max(dim=1).indices.cpu(), attn.max(dim=1).values.cpu())],\n            columns=['token', 'most_relevant_lbl', 'lbl_attn', 'description']).sort_values(by='lbl_attn', ascending=False)\n\n\n\n\n\n\n\n\ntoken\nmost_relevant_lbl\nlbl_attn\ndescription\n\n\n\n\n7\ndelsym\n344.2\n0.096369\nDiplegia of upper limbs\n\n\n13\ndelsym\n344.2\n0.096369\nDiplegia of upper limbs\n\n\n9\ndelsym\n344.2\n0.096369\nDiplegia of upper limbs\n\n\n2\ndmh\n983.1\n0.041843\nToxic effect of acids\n\n\n0\ndisorientated\n171.0\n0.036627\nMalignant neoplasm of connective and other soft tissue of head, face, and neck\n\n\n1\ndisorientated\n171.0\n0.036627\nMalignant neoplasm of connective and other soft tissue of head, face, and neck\n\n\n18\ncaox3\n375.01\n0.028963\nAcute dacryoadenitis\n\n\n17\ncaox3\n375.01\n0.028963\nAcute dacryoadenitis\n\n\n16\ncaox3\n375.01\n0.028963\nAcute dacryoadenitis\n\n\n15\ncaox3\n375.01\n0.028963\nAcute dacryoadenitis\n\n\n12\nliteracy\n449\n0.018083\nSeptic arterial embolism\n\n\n10\nliteracy\n449\n0.018083\nSeptic arterial embolism\n\n\n8\nliteracy\n449\n0.018083\nSeptic arterial embolism\n\n\n4\nliteracy\n449\n0.018083\nSeptic arterial embolism\n\n\n19\nliteracy\n449\n0.018083\nSeptic arterial embolism\n\n\n6\nfaxed\n15.9\n0.012289\nOther operations on extraocular muscles and tendons\n\n\n11\nibp\n39.90\n0.006865\nInsertion of non-drug-eluting peripheral vessel stent(s)\n\n\n3\nibp\n39.90\n0.006865\nInsertion of non-drug-eluting peripheral vessel stent(s)\n\n\n14\nabruptly\n315.8\n0.006252\nOther specified delays in development\n\n\n5\nabruptly\n315.8\n0.006252\nOther specified delays in development\n\n\n\n\n\n\n\n\nfrom xcube.layers import inattention\n\n\n# define label inattention cutoff\nk = 5\n\n\ntop_lbs_attn = attn.clone().unsqueeze(0).permute(0,2,1).inattention(k=k).permute(0,2,1).squeeze(0).contiguous() # applying `inattention` across the lbs dim\ntest_eq(top_lbs_attn.shape, (len(some_toks), xml_brain.shape[1]))\ntest_ne(attn, top_lbs_attn)\ntest_eq(top_lbs_attn.argmax(dim=1), attn.argmax(dim=1))\nlbs_cf = top_lbs_attn.sum(dim=0)\ntest_eq(lbs_cf.shape, [top_lbs_attn.shape[1]])\nidxs = lbs_cf.nonzero().flatten().cpu()\nprint(f\"After looking at the tokens {[xml_vocab[0][t]for t in some_toks]}, I am confident about the following labels:\")\npd.DataFrame([(l:=xml_vocab[1][idx], val.item(), lbs_des.get(l, 'NF')) for idx,val in zip(idxs,lbs_cf[idxs])],\n            columns=['lbl', 'lbl_cf', 'description']).sort_values(by='lbl_cf', ascending=False)\n\nAfter looking at the tokens ['disorientated', 'disorientated', 'dmh', 'ibp', 'literacy', 'abruptly', 'faxed', 'delsym', 'literacy', 'delsym', 'literacy', 'ibp', 'literacy', 'delsym', 'abruptly', 'caox3', 'caox3', 'caox3', 'caox3', 'literacy'], I am confident about the following labels:\n\n\n\n\n\n\n\n\n\nlbl\nlbl_cf\ndescription\n\n\n\n\n10\n344.2\n0.289106\nDiplegia of upper limbs\n\n\n22\n367.1\n0.289106\nMyopia\n\n\n36\n706.1\n0.158741\nOther acne\n\n\n35\n691.8\n0.145640\nOther atopic dermatitis and related conditions\n\n\n21\n442.89\n0.134519\nAneurysm of other specified site\n\n\n50\n374.89\n0.115853\nOther disorders of eyelid\n\n\n49\n375.01\n0.115853\nAcute dacryoadenitis\n\n\n20\n449\n0.090417\nSeptic arterial embolism\n\n\n11\n438.13\n0.087779\nNF\n\n\n23\n423.1\n0.080689\nAdhesive pericarditis\n\n\n13\n304.71\n0.080689\nCombinations of opioid type drug with any other drug dependence, continuous use\n\n\n18\n315.9\n0.079333\nUnspecified delay in development\n\n\n24\n171.0\n0.073254\nMalignant neoplasm of connective and other soft tissue of head, face, and neck\n\n\n29\n259.9\n0.072001\nUnspecified endocrine disorder\n\n\n28\n701.2\n0.072001\nAcquired acanthosis nigricans\n\n\n44\n370.00\n0.066347\nCorneal ulcer, unspecified\n\n\n34\n814.00\n0.057924\nClosed fracture of carpal bone, unspecified\n\n\n7\nE968.8\n0.050906\nAssault by other specified means\n\n\n30\n722.72\n0.049080\nIntervertebral disc disorder with myelopathy, thoracic region\n\n\n17\n444.21\n0.047748\nArterial embolism and thrombosis of upper extremity\n\n\n42\n618.1\n0.045409\nUterine prolapse without mention of vaginal wall prolapse\n\n\n45\n531.30\n0.041843\nAcute gastric ulcer without mention of hemorrhage or perforation, without mention of obstruction\n\n\n41\n54.73\n0.041843\nOther repair of peritoneum\n\n\n37\n983.1\n0.041843\nToxic effect of acids\n\n\n52\n824.7\n0.041843\nTrimalleolar fracture, open\n\n\n6\n921.0\n0.039063\nBlack eye, NOS\n\n\n39\n959.9\n0.036281\nOther and unspecified injury to unspecified site\n\n\n16\n290.3\n0.032885\nSenile dementia with delirium\n\n\n43\n784.92\n0.032388\nNF\n\n\n15\n444.89\n0.028912\nEmbolism and thrombosis of other artery\n\n\n32\n39.90\n0.013729\nInsertion of non-drug-eluting peripheral vessel stent(s)\n\n\n19\n315.8\n0.012504\nOther specified delays in development\n\n\n2\n15.9\n0.012289\nOther operations on extraocular muscles and tendons\n\n\n14\n38.12\n0.011341\nEndarterectomy of other vessels of head and neck\n\n\n9\n68.16\n0.010278\nClosed biopsy of uterus\n\n\n46\n674.14\n0.009737\nDisruption of cesarean wound, postpartum\n\n\n47\n654.44\n0.009737\nOther abnormalities in shape or position of gravid uterus and of neighboring structures, postpartum\n\n\n48\n669.44\n0.009737\nOther complications of obstetrical surgery and procedures, postpartum condition or complication\n\n\n51\n205.90\n0.009737\nUnspecified myeloid leukemia without mention of remission\n\n\n27\n86.19\n0.009737\nOther diagnostic procedures on skin and subcutaneous tissue\n\n\n53\n021.8\n0.009737\nOther specified tularemia\n\n\n40\n362.07\n0.009737\nDiabetic macular edema\n\n\n38\n989.3\n0.009737\nToxic effect of organophosphate and carbamate\n\n\n26\n17.7\n0.009737\nNF\n\n\n25\n706.9\n0.009737\nUnspecified disease of sebaceous glands\n\n\n31\n39.50\n0.009523\nAngioplasty or atherectomy of other non-coronary vessel(s)\n\n\n5\n421.0\n0.008755\nAcute and subacute bacterial endocarditis\n\n\n0\n348.5\n0.007448\nCerebral edema\n\n\n1\n13.1\n0.006795\nIntracapsular extraction of lens\n\n\n4\n93.0\n0.005886\nDiagnostic physical therapy\n\n\n3\n21.2\n0.005648\nDiagnostic procedures on nose\n\n\n33\n36.05\n0.005054\nNF\n\n\n8\n440.1\n0.004669\nAtherosclerosis of renal artery\n\n\n12\n37.94\n0.004538\nImplantation or replacement of automatic cardioverter/defibrillator, total system [AICD]\n\n\n\n\n\n\n\n\nl2r_wgts = torch.load(join_path_file('lin_lambdarank_full', source_l2r, ext='.pth'), map_location=default_device())\nif 'model' in l2r_wgts: l2r_wgts = l2r_wgts['model']\n\nNeed to match the wgts in xml and brain:\n\ndef brainsplant_diffntble(xml_vocab, brain_vocab, l2r_wgts, device=None):\n    toks_lbs = 'toks lbs'.split()\n    mb = master_bar(range(2))\n    for i in mb:\n        globals().update(dict(zip((toks_lbs[i]+'_xml2brain', toks_lbs[i]+'_notfnd'), (_xml2brain(xml_vocab[i], brain_vocab[i], parent_bar=mb)))))\n        mb.write = f\"Finished Loop {i}\" \n    toks_map = L((xml_idx, brn_idx) for xml_idx, brn_idx in toks_xml2brain.items() if brn_idx is not np.inf) \n    lbs_map = L((xml_idx, brn_idx) for xml_idx, brn_idx in lbs_xml2brain.items() if brn_idx is not np.inf) \n    tf_xml = torch.zeros(len(xml_vocab[0]), 200).to(default_device() if device is None else device) \n    tb_xml = torch.zeros(len(xml_vocab[0]), 1).to(default_device() if device is None else device) \n    lf_xml = torch.zeros(len(xml_vocab[1]), 200).to(default_device() if device is None else device) \n    lb_xml = torch.zeros(len(xml_vocab[1]), 1).to(default_device() if device is None else device) \n    tf_l2r, tb_l2r, lf_l2r, lb_l2r = list(l2r_wgts.values())\n    tf_xml[toks_map.itemgot(0)] = tf_l2r[toks_map.itemgot(1)].clone()\n    tb_xml[toks_map.itemgot(0)] = tb_l2r[toks_map.itemgot(1)].clone()\n    lf_xml[lbs_map.itemgot(0)] = lf_l2r[lbs_map.itemgot(1)].clone()\n    lb_xml[lbs_map.itemgot(0)] = lb_l2r[lbs_map.itemgot(1)].clone()\n    # import pdb; pdb.set_trace()\n    xml_wgts = {k: xml_val for k, xml_val in zip(l2r_wgts.keys(), (tf_xml, tb_xml, lf_xml, lb_xml))}\n    mod_dict = nn.ModuleDict({k.split('.')[0]: nn.Embedding(*v.size()) for k,v in xml_wgts.items()}).to(default_device() if device is None else device) \n    mod_dict.load_state_dict(xml_wgts)\n    return mod_dict, toks_map, lbs_map\n\n\nmod_dict, toks_map, lbs_map = brainsplant_diffntble(xml_vocab, brain_vocab, l2r_wgts)\nassert isinstance(mod_dict, nn.Module)\nassert nn.Module in mod_dict.__class__.__mro__ \n\ntest_eq(mod_dict['token_factors'].weight.data[toks_map.itemgot(0)], l2r_wgts['token_factors.weight'][toks_map.itemgot(1)])\ntest_eq(mod_dict['token_bias'].weight.data[toks_map.itemgot(0)], l2r_wgts['token_bias.weight'][toks_map.itemgot(1)])\ntest_eq(mod_dict['label_factors'].weight.data[lbs_map.itemgot(0)], l2r_wgts['label_factors.weight'][lbs_map.itemgot(1)])\ntest_eq(mod_dict['label_bias'].weight.data[lbs_map.itemgot(0)], l2r_wgts['label_bias.weight'][lbs_map.itemgot(1)])\n\n\n\n\n\n\n\n\n\nmod_dict\n\nModuleDict(\n  (token_factors): Embedding(57376, 200)\n  (token_bias): Embedding(57376, 1)\n  (label_factors): Embedding(8922, 200)\n  (label_bias): Embedding(8922, 1)\n)\n\n\n\nsome_lbs = ['996.87', '51.10', '38.93']\n\nfor lbl in some_lbs:\n    print(f\"{lbl}: {lbs_des.get(lbl, 'NF')}\")\n\n996.87: Complications of transplanted intestine\n51.10: Endoscopic retrograde cholangiopancreatography [ERCP]\n38.93: Venous catheterization, not elsewhere classified\n\n\n\nlbs_idx = tensor(mapt(xml_vocab[1].index, some_lbs)).to(default_device())\n\n\ntoks_idx = torch.randint(0, len(xml_vocab[0]), (72,)).to(default_device())\nprint(\"-\"+'\\n-'.join(array(xml_vocab[0])[toks_idx.cpu()].tolist()))\n\n-influx\n-latissimus\n-equinovarus\n-deteriorates\n-aap\n-mvh\n-135\n-incipient\n-rhubarb\n-nizhoni\n-trancutaneous\n-indicaton\n-subset\n-largyngeal\n-lemonade\n-debulk\n-aerations\n-l34\n-perserverates\n-trendelenberg\n-kettr\n-meningitic\n-bored\n-hashimoto\n-mountains\n-wit\n-asts\n-ellicits\n-pax\n-adb\n-alcholism\n-violinist\n-301b\n-subpopulation\n-intraorally\n-98o2\n-agreesive\n-monilla\n-jig\n-paroxysmalatrial\n-10pts\n-knees\n-conventionally\n-soonest\n-recap\n-rediscuss\n-spontanous\n-pulmary\n-repletement\n-450x12\n-symetrically\n-fdi\n-pshx\n-svco2\n-topimax\n-2100cc\n-conceal\n-nauasea\n-decontamination\n-administrator\n-fraction\n-tachyarrythmia\n-oversee\n-dabigutran\n-reiterated\n-aftetr\n-bues\n-symettric\n-powerful\n-depocyte\n-hyperextension\n-hepsc\n\n\n\napprx_brain = mod_dict['token_factors'](toks_idx) @ mod_dict['label_factors'](lbs_idx).T + mod_dict['token_bias'](toks_idx) + mod_dict['label_bias'](lbs_idx).T\napprx_brain.shape\n\ntorch.Size([72, 3])\n\n\nThese are the tokens as ranked by the pretrained L2R model (which is essentially an approximation of the actual brain):\n\npd.DataFrame(array(xml_vocab[0])[toks_idx[apprx_brain.argsort(dim=0, descending=True)].cpu()], columns=L(zip(some_lbs, mapt(lbs_des.get, some_lbs))).map(': '.join))\n\n\n\n\n\n\n\n\n996.87: Complications of transplanted intestine\n51.10: Endoscopic retrograde cholangiopancreatography [ERCP]\n38.93: Venous catheterization, not elsewhere classified\n\n\n\n\n0\nfraction\nwit\nfraction\n\n\n1\nknees\nfraction\nsubpopulation\n\n\n2\nsubpopulation\nadministrator\nknees\n\n\n3\nwit\nsubset\nsubset\n\n\n4\nadministrator\nknees\npshx\n\n\n...\n...\n...\n...\n\n\n67\nparoxysmalatrial\nmvh\npowerful\n\n\n68\nindicaton\nellicits\nindicaton\n\n\n69\nrhubarb\nindicaton\nperserverates\n\n\n70\ndepocyte\naftetr\nrhubarb\n\n\n71\naftetr\nconceal\nmonilla\n\n\n\n\n72 rows × 3 columns\n\n\n\nJust to compare: This is how an actual brain would rank those tokens:\n\n# array(xml_vocab[0])[xml_brain[:, lbl_idx].topk(k=20, dim=0).indices.cpu()]\npd.DataFrame(array(xml_vocab[0])[toks_idx[xml_brain[:, lbs_idx][toks_idx].argsort(descending=True, dim=0)].cpu()], columns=L(zip(some_lbs, mapt(lbs_des.get, some_lbs))).map(': '.join))\n\n\n\n\n\n\n\n\n996.87: Complications of transplanted intestine\n51.10: Endoscopic retrograde cholangiopancreatography [ERCP]\n38.93: Venous catheterization, not elsewhere classified\n\n\n\n\n0\nfraction\nwit\nknees\n\n\n1\nknees\nadministrator\nsvco2\n\n\n2\nhyperextension\npshx\nmeningitic\n\n\n3\nmeningitic\nhashimoto\nfraction\n\n\n4\n301b\nreiterated\nsubset\n\n\n...\n...\n...\n...\n\n\n67\nlatissimus\ntopimax\npshx\n\n\n68\nmonilla\nconceal\nequinovarus\n\n\n69\ndabigutran\naftetr\ndebulk\n\n\n70\ntrendelenberg\nsymettric\noversee\n\n\n71\ndeteriorates\ndepocyte\nl34\n\n\n\n\n72 rows × 3 columns"
  },
  {
    "objectID": "text.learner.html#base-learner-for-nlp",
    "href": "text.learner.html#base-learner-for-nlp",
    "title": "Learner for the XML Text application:",
    "section": "Base Learner for NLP",
    "text": "Base Learner for NLP\n\nsource\n\nload_collab_keys\n\n load_collab_keys (model, wgts:dict)\n\nLoad only collab wgts (i_weight and i_bias) in model, keeping the rest as is\n\n\n\n\nType\nDetails\n\n\n\n\nmodel\n\nModel architecture\n\n\nwgts\ndict\nModel weights\n\n\nReturns\ntuple\n\n\n\n\n\nconfig = awd_lstm_clas_config.copy()\nconfig.update({'n_hid': 10, 'emb_sz': 5})\n# tst = get_text_classifier(AWD_LSTM, 100, 3, config=config)\ntst = get_xmltext_classifier(AWD_LSTM, 100, 3, config=config)\nold_sd = tst.state_dict().copy()\nr = re.compile(\".*attn.*\")\ntest_eq([key for key in old_sd if 'attn' in key], list(filter(r.match, old_sd)))\nprint(\"\\n\".join(list(filter(r.match, old_sd))))\n\n1.pay_attn.lbs.weight\n1.boost_attn.lin.weight\n1.boost_attn.lin.bias\n\n\n\nimport copy\n\n\nold_sd = copy.deepcopy(tst.state_dict())\nload_collab_keys(tst, new_wgts)\n# &lt;TODO: Deb&gt; fix the following tests later\n# test_ne(old_sd['1.attn.lbs_weight.weight'], tst.state_dict()['1.attn.lbs_weight.weight'])\n# test_eq(tst.state_dict()['1.pay_attn.lbs_weight.weight'], new_wgts['i_weight.weight'])\n# test_ne(old_sd['1.attn.lbs_weight_dp.emb.weight'], tst.state_dict()['1.attn.lbs_weight_dp.emb.weight'])\n# test_eq(tst.state_dict()['1.attn.lbs_weight_dp.emb.weight'], new_wgts['i_weight.weight'])\n\n&lt;All keys matched successfully&gt;\n\n\n\nsource\n\n\nTextLearner\n\n TextLearner (dls:fastai.data.core.DataLoaders, model, alpha:float=2.0,\n              beta:float=1.0, moms:tuple=(0.8, 0.7, 0.8),\n              loss_func:callable|None=None,\n              opt_func:Optimizer|OptimWrapper=&lt;function Adam&gt;,\n              lr:float|slice=0.001, splitter:callable=&lt;function\n              trainable_params&gt;, cbs:Callback|MutableSequence|None=None,\n              metrics:callable|MutableSequence|None=None,\n              path:str|Path|None=None, model_dir:str|Path='models',\n              wd:float|int|None=None, wd_bn_bias:bool=False,\n              train_bn:bool=True, default_cbs:bool=True)\n\nBasic class for a Learner in NLP.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndls\nDataLoaders\n\nText DataLoaders\n\n\nmodel\n\n\nA standard PyTorch model\n\n\nalpha\nfloat\n2.0\nParam for RNNRegularizer\n\n\nbeta\nfloat\n1.0\nParam for RNNRegularizer\n\n\nmoms\ntuple\n(0.8, 0.7, 0.8)\nMomentum for Cosine Annealing Scheduler\n\n\nloss_func\ncallable | None\nNone\nLoss function. Defaults to dls loss\n\n\nopt_func\nOptimizer | OptimWrapper\nAdam\nOptimization function for training\n\n\nlr\nfloat | slice\n0.001\nDefault learning rate\n\n\nsplitter\ncallable\ntrainable_params\nSplit model into parameter groups. Defaults to one parameter group\n\n\ncbs\nCallback | MutableSequence | None\nNone\nCallbacks to add to Learner\n\n\nmetrics\ncallable | MutableSequence | None\nNone\nMetrics to calculate on validation set\n\n\npath\nstr | Path | None\nNone\nParent directory to save, load, and export models. Defaults to dls path\n\n\nmodel_dir\nstr | Path\nmodels\nSubdirectory to save and load models\n\n\nwd\nfloat | int | None\nNone\nDefault weight decay\n\n\nwd_bn_bias\nbool\nFalse\nApply weight decay to normalization and bias parameters\n\n\ntrain_bn\nbool\nTrue\nTrain frozen normalization layers\n\n\ndefault_cbs\nbool\nTrue\nInclude default Callbacks\n\n\n\n\nsource\n\n\nLMLearner.save_decoder\n\n LMLearner.save_decoder (file:str)\n\nSave the decoder to file in the model directory\n\n\n\n\nType\nDetails\n\n\n\n\nfile\nstr\nFilename for Decoder\n\n\n\nAdds a ModelResetter and an RNNRegularizer with alpha and beta to the callbacks, the rest is the same as Learner init.\nThis Learner adds functionality to the base class:"
  },
  {
    "objectID": "text.learner.html#learner-convenience-functions",
    "href": "text.learner.html#learner-convenience-functions",
    "title": "Learner for the XML Text application:",
    "section": "Learner convenience functions",
    "text": "Learner convenience functions\n\nsource\n\nxmltext_classifier_learner\n\n xmltext_classifier_learner (dls, arch, seq_len=72, config=None,\n                             backwards=False, pretrained=True,\n                             collab=False, drop_mult=0.5, n_out=None,\n                             lin_ftrs=None, ps=None, max_len=1440,\n                             y_range=None, splitter=None,\n                             running_decoder=True,\n                             loss_func:Optional[&lt;built-\n                             infunctioncallable&gt;]=None, opt_func:Union[fas\n                             tai.optimizer.Optimizer,fastai.optimizer.Opti\n                             mWrapper]=&lt;function Adam&gt;,\n                             lr:Union[float,slice]=0.001, cbs:Union[fastai\n                             .callback.core.Callback,collections.abc.Mutab\n                             leSequence,NoneType]=None,\n                             metrics:Union[&lt;built-infunctioncallable&gt;,coll\n                             ections.abc.MutableSequence,NoneType]=None,\n                             path:Union[str,pathlib.Path,NoneType]=None,\n                             model_dir:Union[str,pathlib.Path]='models',\n                             wd:Union[float,int,NoneType]=None,\n                             wd_bn_bias:bool=False, train_bn:bool=True,\n                             moms:tuple=(0.95, 0.85, 0.95),\n                             default_cbs:bool=True)\n\nCreate a Learner with a text classifier from dls and arch.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndls\nDataLoaders\n\nDataLoaders containing fastai or PyTorch DataLoaders\n\n\narch\n\n\n\n\n\nseq_len\nint\n72\n\n\n\nconfig\nNoneType\nNone\n\n\n\nbackwards\nbool\nFalse\n\n\n\npretrained\nbool\nTrue\n\n\n\ncollab\nbool\nFalse\n\n\n\ndrop_mult\nfloat\n0.5\n\n\n\nn_out\nNoneType\nNone\n\n\n\nlin_ftrs\nNoneType\nNone\n\n\n\nps\nNoneType\nNone\n\n\n\nmax_len\nint\n1440\n\n\n\ny_range\nNoneType\nNone\n\n\n\nsplitter\ncallable\ntrainable_params\nSplit model into parameter groups. Defaults to one parameter group\n\n\nrunning_decoder\nbool\nTrue\n\n\n\nloss_func\ncallable | None\nNone\nLoss function. Defaults to dls loss\n\n\nopt_func\nOptimizer | OptimWrapper\nAdam\nOptimization function for training\n\n\nlr\nfloat | slice\n0.001\nDefault learning rate\n\n\ncbs\nCallback | MutableSequence | None\nNone\nCallbacks to add to Learner\n\n\nmetrics\ncallable | MutableSequence | None\nNone\nMetrics to calculate on validation set\n\n\npath\nstr | Path | None\nNone\nParent directory to save, load, and export models. Defaults to dls path\n\n\nmodel_dir\nstr | Path\nmodels\nSubdirectory to save and load models\n\n\nwd\nfloat | int | None\nNone\nDefault weight decay\n\n\nwd_bn_bias\nbool\nFalse\nApply weight decay to normalization and bias parameters\n\n\ntrain_bn\nbool\nTrue\nTrain frozen normalization layers\n\n\nmoms\ntuple\n(0.95, 0.85, 0.95)\nDefault momentum for schedulers\n\n\ndefault_cbs\nbool\nTrue\nInclude default Callbacks"
  },
  {
    "objectID": "l2r.learner.html",
    "href": "l2r.learner.html",
    "title": "L2R Learner",
    "section": "",
    "text": "This module contains a specialized version of fastai’s full fledged Learner. Every functionality here can also be achieved with fastai’s Learner. The purpose of re-creating a learner was purely educational.\n\nsource\n\nL2RLearner\n\n L2RLearner (model, dls, grad_func, loss_func, lr, cbs, opt_func=&lt;function\n             SGD&gt;, path=None, moms:tuple=(0.95, 0.08, 0.95))\n\nInitialize self. See help(type(self)) for accurate signature.\nSerializing\n\nsource\n\n\nL2RLearner.save\n\n L2RLearner.save (file, with_opt=True, pickle_protocol=2)\n\nSave model and optimizer state (if ‘with_opt’) to self.path/file\n\nsource\n\n\nL2RLearner.load\n\n L2RLearner.load (file, device=None, with_opt=True, strict=True)\n\nLoad model and optimizer state (if with_opt) from self.path/file using device\n\nsource\n\n\nL2RLearner.show_results\n\n L2RLearner.show_results (device=None, k=None)\n\nProduces the ranking for 100 random labels\n\n\nLearner convenience functions\n\nsource\n\n\nget_learner\n\n get_learner (model, dls, grad_fn=&lt;function rank_loss3&gt;, loss_fn=&lt;function\n              loss_fn2&gt;, lr=1e-05, cbs=None,\n              opt_func=functools.partial(&lt;function SGD at 0x7f99c2c4a1f0&gt;,\n              mom=0.9), lambrank=False, **kwargs)"
  }
]