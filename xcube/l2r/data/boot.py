# AUTOGENERATED! DO NOT EDIT! File to edit: ../../../nbs/13_l2r.data.boot.ipynb.

# %% auto 0
__all__ = ['onehot_dsets', 'BatchLbsChunkify', 'lbs_chunked']

# %% ../../../nbs/13_l2r.data.boot.ipynb 2
# from fastai.torch_imports import *
# from fastai.data.load import DataLoader
# from xcube.imports import *

# %% ../../../nbs/13_l2r.data.boot.ipynb 3
from fastcore.basics import *
from fastai.torch_core import *
from fastai.data.core import *
from fastai.data.transforms import *
from fastai.text.core import *
from fastai.text.data import *
from ...imports import *

# %% ../../../nbs/13_l2r.data.boot.ipynb 7
def onehot_dsets(df):
    x_tfms = [Tokenizer.from_df('text', n_workers=num_cpus()), attrgetter("text"), Numericalize(), OneHotEncode()]
    y_tfms = [ColReader('labels', label_delim=';'), MultiCategorize(), OneHotEncode()]
    tfms = [x_tfms, y_tfms]
    dsets = Datasets(df, tfms=[x_tfms, y_tfms])
    return dsets

# %% ../../../nbs/13_l2r.data.boot.ipynb 8
class BatchLbsChunkify(ItemTransform):
    order = 100
    def __init__(self, chnk_st, chnk_end): store_attr('chnk_st,chnk_end')
    def encodes(self, x): 
        return (x[0], x[1][:, self.chnk_st:self.chnk_end])

# %% ../../../nbs/13_l2r.data.boot.ipynb 9
def lbs_chunked(dsets, bs=8, chnk_sz=200, device=None):
    lbs = dsets.vocab[1]
    dls = []
    for chnk_st in range(0, len(lbs), chnk_sz):
        dls.append(TfmdDL(dsets, bs=bs, 
                          after_batch=[BatchLbsChunkify(chnk_st, min(chnk_st+chnk_sz, len(lbs)))], 
                          device=default_device() if device is None else device))
    return dls
