# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/01_layers.ipynb.

# %% auto 0
__all__ = ['ElemWiseLin', 'LinBnFlatDrop', 'LinBnDrop', 'Embedding', 'Linear_Attention', 'Ranked_Attention', 'XMLAttention_old',
           'XMLAttention']

# %% ../nbs/01_layers.ipynb 2
from fastai.imports import *
from fastai.torch_imports import *
from fastai.torch_core import *
from fastai.layers import *
from fastai.text.models.awdlstm import EmbeddingDropout, RNNDropout

from .utils import *

# %% ../nbs/01_layers.ipynb 13
def _create_bias(size, with_zeros=False):
    if with_zeros: return nn.Parameter(torch.zeros(*size))
    return nn.Parameter(torch.zeros(*size).uniform_(-0.1, 0.1))

# %% ../nbs/01_layers.ipynb 14
class ElemWiseLin(Module):
    initrange=0.1
    def __init__(self, dim0, dim1, add_bias=False, **kwargs):
        store_attr()
        self.lin = nn.Linear(dim1, dim0, **kwargs)
        # init_default(self.lin, func=partial(torch.nn.init.uniform_, a=-self.initrange, b=self.initrange))
        init_default(self.lin)
        if self.add_bias: self.bias = _create_bias((1, ))
        
    def forward(self, x):
        res = torch.addcmul(self.bias if self.add_bias else x.new_zeros(1), x, self.lin.weight)# * self.lin.weight
        return res #+ self.bias if self.add_bias else res

# %% ../nbs/01_layers.ipynb 17
class LinBnFlatDrop(nn.Sequential):
    "Module grouping `BatchNorm1dFlat`, `Dropout` and `Linear` layers"
    def __init__(self, n_in, n_out, bn=True, p=0., act=None, lin_first=False):
        layers = [BatchNorm1dFlat(n_out if lin_first else n_in)] if bn else []
        if p != 0: layers.append(nn.Dropout(p))
        lin = [nn.Linear(n_in, n_out, bias=not bn)]
        if act is not None: lin.append(act)
        layers = lin+layers if lin_first else layers+lin
        super().__init__(*layers)

# %% ../nbs/01_layers.ipynb 18
class LinBnDrop(nn.Sequential):
    "Module grouping `BatchNorm1d`, `Dropout` and `Linear` layers"
    def __init__(self, n_in, n_out=None, bn=True, ln=True, p=0., act=None, lin_first=False, ndim=1):
        if not ln and lin_first: raise Exception(AssertionError)
        layers = [BatchNorm(n_out if ln and lin_first else n_in, ndim=ndim)] if bn else []
        if p != 0: layers.append(nn.Dropout(p))
        lin = [nn.Linear(n_in, n_out, bias=not bn)] if ln else []
        if ln and act is not None: lin.append(act)
        layers = lin+layers if lin_first else layers+lin
        super().__init__(*layers)

# %% ../nbs/01_layers.ipynb 24
class Embedding(nn.Embedding):
    "Embedding layer with truncated normal initialization"
    def __init__(self, ni, nf, std=0.01, **kwargs):
        super().__init__(ni, nf, **kwargs)
        trunc_normal_(self.weight.data, std=std)

# %% ../nbs/01_layers.ipynb 26
def _linear_attention(sentc:Tensor, # Sentence typically `(bs, bptt, nh)`
                   based_on: Module|Embedding # xcube's `Embedding(n_lbs, nh)` layer holding the label embeddings
                  ):
    return sentc @ based_on.weight.transpose(0,1)

# %% ../nbs/01_layers.ipynb 27
class _Pay_Attention:
    def __init__(self, f, based_on): store_attr('f,based_on')
    def __call__(self, sentc): return self.f(sentc, self.based_on)

# %% ../nbs/01_layers.ipynb 28
def Linear_Attention(based_on: Module): return _Pay_Attention(_linear_attention, based_on)

# %% ../nbs/01_layers.ipynb 29
def Ranked_Attention(based_on: Module):
    # coming soon!
    pass

# %% ../nbs/01_layers.ipynb 31
class XMLAttention_old(Module):
    "Compute label specific attention weights for each token in a sequence"
    def __init__(self, n_lbs, emb_sz, embed_p=0.0):
         store_attr('n_lbs,emb_sz,embed_p')
         self.lbs_weight = Embedding(n_lbs, emb_sz)
         # self.lbs_weight_dp = EmbeddingDropout(self.lbs_weight, embed_p)
         # self.lbs_weight.weight.data.normal_(0, 0.01)   
         # self.input_dp = RNNDropout(0.02)

    def forward(self, sent):
        # sent is the ouput of SentenceEncoder i.e., (bs, max_len tokens, nh)
        # lbs_emb = self.lbs_weight(torch.arange(self.n_lbs, device=x.device)) # pulling out the lbs embeddings
        lbs_emb = self.lbs_weight.weight
        # x_dp = self.input_dp(x)
        attn_wgts = F.softmax(sent @ lbs_emb.transpose(0,1), dim=1) # lbl specific wts for each token (bs, max_len, n_lbs)
        return attn_wgts.transpose(1,2) @ sent # for each lbl do a linear combi of all the tokens based on attn_wgts (bs, num_lbs, nh)
    

# %% ../nbs/01_layers.ipynb 32
class XMLAttention(Module):
    "Compute label specific attention weights for each token in a sequence"
    def __init__(self, n_lbs, emb_sz, embed_p=0.0):
        store_attr('n_lbs,emb_sz,embed_p')
        self.lbs = Embedding(n_lbs, emb_sz)
        # self.lbs_weight_dp = EmbeddingDropout(self.lbs_weight, embed_p)
        # self.lbs_weight.weight.data.normal_(0, 0.01)   
        # self.input_dp = RNNDropout(0.02)
        self.LinAttn = Lambda(Linear_Attention(self.lbs))

    def forward(self, sentc):
        # sent is the ouput of SentenceEncoder i.e., (bs, max_len tokens, nh)
        # lbs_emb = self.lbs_weight(torch.arange(self.n_lbs, device=x.device)) # pulling out the lbs embeddings
        # lbs_emb = self.lbs_weight.weight
        # x_dp = self.input_dp(x)
        attn_wgts = F.softmax(self.LinAttn(sentc), dim=1) # lbl specific wts for each token (bs, max_len, n_lbs)
        # return attn_wgts.transpose(1,2) @ sentc # for each lbl do a linear combi of all the tokens based on attn_wgts (bs, num_lbs, nh)
        return torch.bmm(attn_wgts.transpose(1,2), sentc) # for each lbl do a linear combi of all the tokens based on attn_wgts (bs, num_lbs, nh)
    
